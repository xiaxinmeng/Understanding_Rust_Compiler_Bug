{"sha": "127a11a344eb59b5aea1464e98257c262dcba967", "node_id": "MDY6Q29tbWl0NzI0NzEyOjEyN2ExMWEzNDRlYjU5YjVhZWExNDY0ZTk4MjU3YzI2MmRjYmE5Njc=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-04-02T06:08:35Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-04-02T06:08:35Z"}, "message": "Auto merge of #70362 - TimDiekmann:alloc-overhaul, r=Amanieu\n\nOverhaul of the `AllocRef` trait to match allocator-wg's latest consens; Take 2\n\nGitHub won't let me reopen #69889 so I make a new PR.\n\nIn addition to #69889 this fixes the unsoundness of `RawVec::into_box` when using allocators supporting overallocating. Also it uses `MemoryBlock` in `AllocRef` to unify `_in_place` methods by passing `&mut MemoryBlock`. Additionally, `RawVec` now checks for `size_of::<T>()` again and ignore every ZST. The internal capacity of `RawVec` isn't used by ZSTs anymore, as `into_box` now requires a length to be specified.\n\nr? @Amanieu\n\nfixes rust-lang/wg-allocators#38\nfixes rust-lang/wg-allocators#41\nfixes rust-lang/wg-allocators#44\nfixes rust-lang/wg-allocators#51", "tree": {"sha": "2bc294b4383cc4446add6e4a96f57161eea9f78c", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/2bc294b4383cc4446add6e4a96f57161eea9f78c"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/127a11a344eb59b5aea1464e98257c262dcba967", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/127a11a344eb59b5aea1464e98257c262dcba967", "html_url": "https://github.com/rust-lang/rust/commit/127a11a344eb59b5aea1464e98257c262dcba967", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/127a11a344eb59b5aea1464e98257c262dcba967/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b793f403bdfbcc0ff3e15ed8177a81d79ba4a29b", "url": "https://api.github.com/repos/rust-lang/rust/commits/b793f403bdfbcc0ff3e15ed8177a81d79ba4a29b", "html_url": "https://github.com/rust-lang/rust/commit/b793f403bdfbcc0ff3e15ed8177a81d79ba4a29b"}, {"sha": "89ed59d8841a2b6057f61a3469c10bb2e6242160", "url": "https://api.github.com/repos/rust-lang/rust/commits/89ed59d8841a2b6057f61a3469c10bb2e6242160", "html_url": "https://github.com/rust-lang/rust/commit/89ed59d8841a2b6057f61a3469c10bb2e6242160"}], "stats": {"total": 3064, "additions": 1445, "deletions": 1619}, "files": [{"sha": "66575e3ef55170ae0e2ab5497a11287ea3f527d6", "filename": "src/liballoc/alloc.rs", "status": "modified", "additions": 76, "deletions": 36, "changes": 112, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Falloc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Falloc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Falloc.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -2,7 +2,7 @@\n \n #![stable(feature = \"alloc_module\", since = \"1.28.0\")]\n \n-use core::intrinsics::{min_align_of_val, size_of_val};\n+use core::intrinsics::{self, min_align_of_val, size_of_val};\n use core::ptr::{NonNull, Unique};\n use core::usize;\n \n@@ -165,11 +165,19 @@ pub unsafe fn alloc_zeroed(layout: Layout) -> *mut u8 {\n #[unstable(feature = \"allocator_api\", issue = \"32838\")]\n unsafe impl AllocRef for Global {\n     #[inline]\n-    fn alloc(&mut self, layout: Layout) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        if layout.size() == 0 {\n-            Ok((layout.dangling(), 0))\n-        } else {\n-            unsafe { NonNull::new(alloc(layout)).ok_or(AllocErr).map(|p| (p, layout.size())) }\n+    fn alloc(&mut self, layout: Layout, init: AllocInit) -> Result<MemoryBlock, AllocErr> {\n+        unsafe {\n+            let size = layout.size();\n+            if size == 0 {\n+                Ok(MemoryBlock { ptr: layout.dangling(), size: 0 })\n+            } else {\n+                let raw_ptr = match init {\n+                    AllocInit::Uninitialized => alloc(layout),\n+                    AllocInit::Zeroed => alloc_zeroed(layout),\n+                };\n+                let ptr = NonNull::new(raw_ptr).ok_or(AllocErr)?;\n+                Ok(MemoryBlock { ptr, size })\n+            }\n         }\n     }\n \n@@ -181,32 +189,71 @@ unsafe impl AllocRef for Global {\n     }\n \n     #[inline]\n-    unsafe fn realloc(\n+    unsafe fn grow(\n         &mut self,\n         ptr: NonNull<u8>,\n         layout: Layout,\n         new_size: usize,\n-    ) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        match (layout.size(), new_size) {\n-            (0, 0) => Ok((layout.dangling(), 0)),\n-            (0, _) => self.alloc(Layout::from_size_align_unchecked(new_size, layout.align())),\n-            (_, 0) => {\n-                self.dealloc(ptr, layout);\n-                Ok((layout.dangling(), 0))\n+        placement: ReallocPlacement,\n+        init: AllocInit,\n+    ) -> Result<MemoryBlock, AllocErr> {\n+        let size = layout.size();\n+        debug_assert!(\n+            new_size >= size,\n+            \"`new_size` must be greater than or equal to `memory.size()`\"\n+        );\n+\n+        if size == new_size {\n+            return Ok(MemoryBlock { ptr, size });\n+        }\n+\n+        match placement {\n+            ReallocPlacement::InPlace => Err(AllocErr),\n+            ReallocPlacement::MayMove if layout.size() == 0 => {\n+                let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n+                self.alloc(new_layout, init)\n+            }\n+            ReallocPlacement::MayMove => {\n+                // `realloc` probably checks for `new_size > size` or something similar.\n+                intrinsics::assume(new_size > size);\n+                let ptr = realloc(ptr.as_ptr(), layout, new_size);\n+                let memory =\n+                    MemoryBlock { ptr: NonNull::new(ptr).ok_or(AllocErr)?, size: new_size };\n+                init.init_offset(memory, size);\n+                Ok(memory)\n             }\n-            (_, _) => NonNull::new(realloc(ptr.as_ptr(), layout, new_size))\n-                .ok_or(AllocErr)\n-                .map(|p| (p, new_size)),\n         }\n     }\n \n     #[inline]\n-    fn alloc_zeroed(&mut self, layout: Layout) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        if layout.size() == 0 {\n-            Ok((layout.dangling(), 0))\n-        } else {\n-            unsafe {\n-                NonNull::new(alloc_zeroed(layout)).ok_or(AllocErr).map(|p| (p, layout.size()))\n+    unsafe fn shrink(\n+        &mut self,\n+        ptr: NonNull<u8>,\n+        layout: Layout,\n+        new_size: usize,\n+        placement: ReallocPlacement,\n+    ) -> Result<MemoryBlock, AllocErr> {\n+        let size = layout.size();\n+        debug_assert!(\n+            new_size <= size,\n+            \"`new_size` must be smaller than or equal to `memory.size()`\"\n+        );\n+\n+        if size == new_size {\n+            return Ok(MemoryBlock { ptr, size });\n+        }\n+\n+        match placement {\n+            ReallocPlacement::InPlace => Err(AllocErr),\n+            ReallocPlacement::MayMove if new_size == 0 => {\n+                self.dealloc(ptr, layout);\n+                Ok(MemoryBlock { ptr: layout.dangling(), size: 0 })\n+            }\n+            ReallocPlacement::MayMove => {\n+                // `realloc` probably checks for `new_size < size` or something similar.\n+                intrinsics::assume(new_size < size);\n+                let ptr = realloc(ptr.as_ptr(), layout, new_size);\n+                Ok(MemoryBlock { ptr: NonNull::new(ptr).ok_or(AllocErr)?, size: new_size })\n             }\n         }\n     }\n@@ -218,14 +265,10 @@ unsafe impl AllocRef for Global {\n #[lang = \"exchange_malloc\"]\n #[inline]\n unsafe fn exchange_malloc(size: usize, align: usize) -> *mut u8 {\n-    if size == 0 {\n-        align as *mut u8\n-    } else {\n-        let layout = Layout::from_size_align_unchecked(size, align);\n-        match Global.alloc(layout) {\n-            Ok((ptr, _)) => ptr.as_ptr(),\n-            Err(_) => handle_alloc_error(layout),\n-        }\n+    let layout = Layout::from_size_align_unchecked(size, align);\n+    match Global.alloc(layout, AllocInit::Uninitialized) {\n+        Ok(memory) => memory.ptr.as_ptr(),\n+        Err(_) => handle_alloc_error(layout),\n     }\n }\n \n@@ -239,11 +282,8 @@ unsafe fn exchange_malloc(size: usize, align: usize) -> *mut u8 {\n pub(crate) unsafe fn box_free<T: ?Sized>(ptr: Unique<T>) {\n     let size = size_of_val(ptr.as_ref());\n     let align = min_align_of_val(ptr.as_ref());\n-    // We do not allocate for Box<T> when T is ZST, so deallocation is also not necessary.\n-    if size != 0 {\n-        let layout = Layout::from_size_align_unchecked(size, align);\n-        Global.dealloc(ptr.cast().into(), layout);\n-    }\n+    let layout = Layout::from_size_align_unchecked(size, align);\n+    Global.dealloc(ptr.cast().into(), layout)\n }\n \n /// Abort on memory allocation error or failure."}, {"sha": "1ad40eca93b69e8c56c9648b3c44a95c9ac7aa9a", "filename": "src/liballoc/alloc/tests.rs", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Falloc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Falloc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Falloc%2Ftests.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -8,16 +8,17 @@ use test::Bencher;\n fn allocate_zeroed() {\n     unsafe {\n         let layout = Layout::from_size_align(1024, 1).unwrap();\n-        let (ptr, _) =\n-            Global.alloc_zeroed(layout.clone()).unwrap_or_else(|_| handle_alloc_error(layout));\n+        let memory = Global\n+            .alloc(layout.clone(), AllocInit::Zeroed)\n+            .unwrap_or_else(|_| handle_alloc_error(layout));\n \n-        let mut i = ptr.cast::<u8>().as_ptr();\n+        let mut i = memory.ptr.cast::<u8>().as_ptr();\n         let end = i.add(layout.size());\n         while i < end {\n             assert_eq!(*i, 0);\n             i = i.offset(1);\n         }\n-        Global.dealloc(ptr, layout);\n+        Global.dealloc(memory.ptr, layout);\n     }\n }\n "}, {"sha": "5406956a5288657a8fd70598d05208d364bd1c78", "filename": "src/liballoc/boxed.rs", "status": "modified", "additions": 16, "deletions": 25, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fboxed.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fboxed.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Fboxed.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -143,10 +143,9 @@ use core::ops::{\n };\n use core::pin::Pin;\n use core::ptr::{self, NonNull, Unique};\n-use core::slice;\n use core::task::{Context, Poll};\n \n-use crate::alloc::{self, AllocRef, Global};\n+use crate::alloc::{self, AllocInit, AllocRef, Global};\n use crate::raw_vec::RawVec;\n use crate::str::from_boxed_utf8_unchecked;\n use crate::vec::Vec;\n@@ -196,14 +195,12 @@ impl<T> Box<T> {\n     #[unstable(feature = \"new_uninit\", issue = \"63291\")]\n     pub fn new_uninit() -> Box<mem::MaybeUninit<T>> {\n         let layout = alloc::Layout::new::<mem::MaybeUninit<T>>();\n-        unsafe {\n-            let ptr = if layout.size() == 0 {\n-                NonNull::dangling()\n-            } else {\n-                Global.alloc(layout).unwrap_or_else(|_| alloc::handle_alloc_error(layout)).0.cast()\n-            };\n-            Box::from_raw(ptr.as_ptr())\n-        }\n+        let ptr = Global\n+            .alloc(layout, AllocInit::Uninitialized)\n+            .unwrap_or_else(|_| alloc::handle_alloc_error(layout))\n+            .ptr\n+            .cast();\n+        unsafe { Box::from_raw(ptr.as_ptr()) }\n     }\n \n     /// Constructs a new `Box` with uninitialized contents, with the memory\n@@ -226,11 +223,13 @@ impl<T> Box<T> {\n     /// [zeroed]: ../../std/mem/union.MaybeUninit.html#method.zeroed\n     #[unstable(feature = \"new_uninit\", issue = \"63291\")]\n     pub fn new_zeroed() -> Box<mem::MaybeUninit<T>> {\n-        unsafe {\n-            let mut uninit = Self::new_uninit();\n-            ptr::write_bytes::<T>(uninit.as_mut_ptr(), 0, 1);\n-            uninit\n-        }\n+        let layout = alloc::Layout::new::<mem::MaybeUninit<T>>();\n+        let ptr = Global\n+            .alloc(layout, AllocInit::Zeroed)\n+            .unwrap_or_else(|_| alloc::handle_alloc_error(layout))\n+            .ptr\n+            .cast();\n+        unsafe { Box::from_raw(ptr.as_ptr()) }\n     }\n \n     /// Constructs a new `Pin<Box<T>>`. If `T` does not implement `Unpin`, then\n@@ -265,15 +264,7 @@ impl<T> Box<[T]> {\n     /// ```\n     #[unstable(feature = \"new_uninit\", issue = \"63291\")]\n     pub fn new_uninit_slice(len: usize) -> Box<[mem::MaybeUninit<T>]> {\n-        let layout = alloc::Layout::array::<mem::MaybeUninit<T>>(len).unwrap();\n-        unsafe {\n-            let ptr = if layout.size() == 0 {\n-                NonNull::dangling()\n-            } else {\n-                Global.alloc(layout).unwrap_or_else(|_| alloc::handle_alloc_error(layout)).0.cast()\n-            };\n-            Box::from_raw(slice::from_raw_parts_mut(ptr.as_ptr(), len))\n-        }\n+        unsafe { RawVec::with_capacity(len).into_box(len) }\n     }\n }\n \n@@ -778,7 +769,7 @@ impl<T: Copy> From<&[T]> for Box<[T]> {\n         let buf = RawVec::with_capacity(len);\n         unsafe {\n             ptr::copy_nonoverlapping(slice.as_ptr(), buf.ptr(), len);\n-            buf.into_box()\n+            buf.into_box(slice.len()).assume_init()\n         }\n     }\n }"}, {"sha": "11c1429957326345ec5bb25215184583a6183359", "filename": "src/liballoc/collections/btree/node.rs", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fcollections%2Fbtree%2Fnode.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fcollections%2Fbtree%2Fnode.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Fcollections%2Fbtree%2Fnode.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -1142,7 +1142,7 @@ impl<'a, K, V> Handle<NodeRef<marker::Mut<'a>, K, V, marker::Internal>, marker::\n \n             (*left_node.as_leaf_mut()).len += right_len as u16 + 1;\n \n-            if self.node.height > 1 {\n+            let layout = if self.node.height > 1 {\n                 ptr::copy_nonoverlapping(\n                     right_node.cast_unchecked().as_internal().edges.as_ptr(),\n                     left_node\n@@ -1159,10 +1159,11 @@ impl<'a, K, V> Handle<NodeRef<marker::Mut<'a>, K, V, marker::Internal>, marker::\n                         .correct_parent_link();\n                 }\n \n-                Global.dealloc(right_node.node.cast(), Layout::new::<InternalNode<K, V>>());\n+                Layout::new::<InternalNode<K, V>>()\n             } else {\n-                Global.dealloc(right_node.node.cast(), Layout::new::<LeafNode<K, V>>());\n-            }\n+                Layout::new::<LeafNode<K, V>>()\n+            };\n+            Global.dealloc(right_node.node.cast(), layout);\n \n             Handle::new_edge(self.node, self.idx)\n         }"}, {"sha": "121c1cde548cbb3746ddeab2061eea6bccd9ace5", "filename": "src/liballoc/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Flib.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -100,6 +100,7 @@\n #![feature(lang_items)]\n #![feature(libc)]\n #![cfg_attr(not(bootstrap), feature(negative_impls))]\n+#![feature(new_uninit)]\n #![feature(nll)]\n #![feature(optin_builtin_traits)]\n #![feature(pattern)]"}, {"sha": "2bf40490e78190167af519e93a61ae321469dc9e", "filename": "src/liballoc/raw_vec.rs", "status": "modified", "additions": 263, "deletions": 392, "changes": 655, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fraw_vec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fraw_vec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Fraw_vec.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -1,13 +1,19 @@\n #![unstable(feature = \"raw_vec_internals\", reason = \"implementation detail\", issue = \"none\")]\n #![doc(hidden)]\n \n+use core::alloc::MemoryBlock;\n use core::cmp;\n-use core::mem;\n+use core::mem::{self, MaybeUninit};\n use core::ops::Drop;\n-use core::ptr::{self, NonNull, Unique};\n+use core::ptr::{NonNull, Unique};\n use core::slice;\n \n-use crate::alloc::{handle_alloc_error, AllocErr, AllocRef, Global, Layout};\n+use crate::alloc::{\n+    handle_alloc_error, AllocErr,\n+    AllocInit::{self, *},\n+    AllocRef, Global, Layout,\n+    ReallocPlacement::{self, *},\n+};\n use crate::boxed::Box;\n use crate::collections::TryReserveError::{self, *};\n \n@@ -21,81 +27,26 @@ mod tests;\n ///\n /// * Produces `Unique::empty()` on zero-sized types.\n /// * Produces `Unique::empty()` on zero-length allocations.\n+/// * Avoids freeing `Unique::empty()`.\n /// * Catches all overflows in capacity computations (promotes them to \"capacity overflow\" panics).\n /// * Guards against 32-bit systems allocating more than isize::MAX bytes.\n /// * Guards against overflowing your length.\n-/// * Aborts on OOM or calls `handle_alloc_error` as applicable.\n-/// * Avoids freeing `Unique::empty()`.\n+/// * Calls `handle_alloc_error` for fallible allocations.\n /// * Contains a `ptr::Unique` and thus endows the user with all related benefits.\n+/// * Uses the excess returned from the allocator to use the largest available capacity.\n ///\n /// This type does not in anyway inspect the memory that it manages. When dropped it *will*\n /// free its memory, but it *won't* try to drop its contents. It is up to the user of `RawVec`\n /// to handle the actual things *stored* inside of a `RawVec`.\n ///\n-/// Note that a `RawVec` always forces its capacity to be `usize::MAX` for zero-sized types.\n-/// This enables you to use capacity-growing logic catch the overflows in your length\n-/// that might occur with zero-sized types.\n-///\n-/// The above means that you need to be careful when round-tripping this type with a\n-/// `Box<[T]>`, since `capacity()` won't yield the length. However, `with_capacity`,\n-/// `shrink_to_fit`, and `from_box` will actually set `RawVec`'s private capacity\n-/// field. This allows zero-sized types to not be special-cased by consumers of\n-/// this type.\n+/// Note that the excess of a zero-sized types is always infinite, so `capacity()` always returns\n+/// `usize::MAX`. This means that you need to be careful when round-tripping this type with a\n+/// `Box<[T]>`, since `capacity()` won't yield the length.\n #[allow(missing_debug_implementations)]\n pub struct RawVec<T, A: AllocRef = Global> {\n     ptr: Unique<T>,\n     cap: usize,\n-    a: A,\n-}\n-\n-impl<T, A: AllocRef> RawVec<T, A> {\n-    /// Like `new`, but parameterized over the choice of allocator for\n-    /// the returned `RawVec`.\n-    pub const fn new_in(a: A) -> Self {\n-        let cap = if mem::size_of::<T>() == 0 { core::usize::MAX } else { 0 };\n-\n-        // `Unique::empty()` doubles as \"unallocated\" and \"zero-sized allocation\".\n-        RawVec { ptr: Unique::empty(), cap, a }\n-    }\n-\n-    /// Like `with_capacity`, but parameterized over the choice of\n-    /// allocator for the returned `RawVec`.\n-    #[inline]\n-    pub fn with_capacity_in(capacity: usize, a: A) -> Self {\n-        RawVec::allocate_in(capacity, false, a)\n-    }\n-\n-    /// Like `with_capacity_zeroed`, but parameterized over the choice\n-    /// of allocator for the returned `RawVec`.\n-    #[inline]\n-    pub fn with_capacity_zeroed_in(capacity: usize, a: A) -> Self {\n-        RawVec::allocate_in(capacity, true, a)\n-    }\n-\n-    fn allocate_in(mut capacity: usize, zeroed: bool, mut a: A) -> Self {\n-        let elem_size = mem::size_of::<T>();\n-\n-        let alloc_size = capacity.checked_mul(elem_size).unwrap_or_else(|| capacity_overflow());\n-        alloc_guard(alloc_size).unwrap_or_else(|_| capacity_overflow());\n-\n-        // Handles ZSTs and `capacity == 0` alike.\n-        let ptr = if alloc_size == 0 {\n-            NonNull::<T>::dangling()\n-        } else {\n-            let align = mem::align_of::<T>();\n-            let layout = Layout::from_size_align(alloc_size, align).unwrap();\n-            let result = if zeroed { a.alloc_zeroed(layout) } else { a.alloc(layout) };\n-            match result {\n-                Ok((ptr, size)) => {\n-                    capacity = size / elem_size;\n-                    ptr.cast()\n-                }\n-                Err(_) => handle_alloc_error(layout),\n-            }\n-        };\n-\n-        RawVec { ptr: ptr.into(), cap: capacity, a }\n-    }\n+    alloc: A,\n }\n \n impl<T> RawVec<T, Global> {\n@@ -138,39 +89,26 @@ impl<T> RawVec<T, Global> {\n     /// Aborts on OOM.\n     #[inline]\n     pub fn with_capacity(capacity: usize) -> Self {\n-        RawVec::allocate_in(capacity, false, Global)\n+        Self::with_capacity_in(capacity, Global)\n     }\n \n     /// Like `with_capacity`, but guarantees the buffer is zeroed.\n     #[inline]\n     pub fn with_capacity_zeroed(capacity: usize) -> Self {\n-        RawVec::allocate_in(capacity, true, Global)\n+        Self::with_capacity_zeroed_in(capacity, Global)\n     }\n-}\n \n-impl<T, A: AllocRef> RawVec<T, A> {\n-    /// Reconstitutes a `RawVec` from a pointer, capacity, and allocator.\n-    ///\n-    /// # Undefined Behavior\n-    ///\n-    /// The `ptr` must be allocated (via the given allocator `a`), and with the given `capacity`.\n-    /// The `capacity` cannot exceed `isize::MAX` (only a concern on 32-bit systems).\n-    /// If the `ptr` and `capacity` come from a `RawVec` created via `a`, then this is guaranteed.\n-    pub unsafe fn from_raw_parts_in(ptr: *mut T, capacity: usize, a: A) -> Self {\n-        RawVec { ptr: Unique::new_unchecked(ptr), cap: capacity, a }\n-    }\n-}\n-\n-impl<T> RawVec<T, Global> {\n     /// Reconstitutes a `RawVec` from a pointer and capacity.\n     ///\n-    /// # Undefined Behavior\n+    /// # Safety\n     ///\n     /// The `ptr` must be allocated (on the system heap), and with the given `capacity`.\n-    /// The `capacity` cannot exceed `isize::MAX` (only a concern on 32-bit systems).\n+    /// The `capacity` cannot exceed `isize::MAX` for sized types. (only a concern on 32-bit\n+    /// systems). ZST vectors may have a capacity up to `usize::MAX`.\n     /// If the `ptr` and `capacity` come from a `RawVec`, then this is guaranteed.\n+    #[inline]\n     pub unsafe fn from_raw_parts(ptr: *mut T, capacity: usize) -> Self {\n-        RawVec { ptr: Unique::new_unchecked(ptr), cap: capacity, a: Global }\n+        Self::from_raw_parts_in(ptr, capacity, Global)\n     }\n \n     /// Converts a `Box<[T]>` into a `RawVec<T>`.\n@@ -184,6 +122,56 @@ impl<T> RawVec<T, Global> {\n }\n \n impl<T, A: AllocRef> RawVec<T, A> {\n+    /// Like `new`, but parameterized over the choice of allocator for\n+    /// the returned `RawVec`.\n+    pub const fn new_in(alloc: A) -> Self {\n+        // `cap: 0` means \"unallocated\". zero-sized types are ignored.\n+        Self { ptr: Unique::empty(), cap: 0, alloc }\n+    }\n+\n+    /// Like `with_capacity`, but parameterized over the choice of\n+    /// allocator for the returned `RawVec`.\n+    #[inline]\n+    pub fn with_capacity_in(capacity: usize, alloc: A) -> Self {\n+        Self::allocate_in(capacity, Uninitialized, alloc)\n+    }\n+\n+    /// Like `with_capacity_zeroed`, but parameterized over the choice\n+    /// of allocator for the returned `RawVec`.\n+    #[inline]\n+    pub fn with_capacity_zeroed_in(capacity: usize, alloc: A) -> Self {\n+        Self::allocate_in(capacity, Zeroed, alloc)\n+    }\n+\n+    fn allocate_in(capacity: usize, init: AllocInit, mut alloc: A) -> Self {\n+        if mem::size_of::<T>() == 0 {\n+            Self::new_in(alloc)\n+        } else {\n+            let layout = Layout::array::<T>(capacity).unwrap_or_else(|_| capacity_overflow());\n+            alloc_guard(layout.size()).unwrap_or_else(|_| capacity_overflow());\n+\n+            let memory = alloc.alloc(layout, init).unwrap_or_else(|_| handle_alloc_error(layout));\n+            Self {\n+                ptr: memory.ptr.cast().into(),\n+                cap: Self::capacity_from_bytes(memory.size),\n+                alloc,\n+            }\n+        }\n+    }\n+\n+    /// Reconstitutes a `RawVec` from a pointer, capacity, and allocator.\n+    ///\n+    /// # Safety\n+    ///\n+    /// The `ptr` must be allocated (via the given allocator `a`), and with the given `capacity`.\n+    /// The `capacity` cannot exceed `isize::MAX` for sized types. (only a concern on 32-bit\n+    /// systems). ZST vectors may have a capacity up to `usize::MAX`.\n+    /// If the `ptr` and `capacity` come from a `RawVec` created via `a`, then this is guaranteed.\n+    #[inline]\n+    pub unsafe fn from_raw_parts_in(ptr: *mut T, capacity: usize, a: A) -> Self {\n+        Self { ptr: Unique::new_unchecked(ptr), cap: capacity, alloc: a }\n+    }\n+\n     /// Gets a raw pointer to the start of the allocation. Note that this is\n     /// `Unique::empty()` if `capacity == 0` or `T` is zero-sized. In the former case, you must\n     /// be careful.\n@@ -196,29 +184,30 @@ impl<T, A: AllocRef> RawVec<T, A> {\n     /// This will always be `usize::MAX` if `T` is zero-sized.\n     #[inline(always)]\n     pub fn capacity(&self) -> usize {\n-        if mem::size_of::<T>() == 0 { !0 } else { self.cap }\n+        if mem::size_of::<T>() == 0 { usize::MAX } else { self.cap }\n     }\n \n     /// Returns a shared reference to the allocator backing this `RawVec`.\n     pub fn alloc(&self) -> &A {\n-        &self.a\n+        &self.alloc\n     }\n \n     /// Returns a mutable reference to the allocator backing this `RawVec`.\n     pub fn alloc_mut(&mut self) -> &mut A {\n-        &mut self.a\n+        &mut self.alloc\n     }\n \n-    fn current_layout(&self) -> Option<Layout> {\n-        if self.cap == 0 {\n+    fn current_memory(&self) -> Option<(NonNull<u8>, Layout)> {\n+        if mem::size_of::<T>() == 0 || self.cap == 0 {\n             None\n         } else {\n             // We have an allocated chunk of memory, so we can bypass runtime\n             // checks to get our current layout.\n             unsafe {\n                 let align = mem::align_of::<T>();\n                 let size = mem::size_of::<T>() * self.cap;\n-                Some(Layout::from_size_align_unchecked(size, align))\n+                let layout = Layout::from_size_align_unchecked(size, align);\n+                Some((self.ptr.cast().into(), layout))\n             }\n         }\n     }\n@@ -274,50 +263,10 @@ impl<T, A: AllocRef> RawVec<T, A> {\n     #[inline(never)]\n     #[cold]\n     pub fn double(&mut self) {\n-        unsafe {\n-            let elem_size = mem::size_of::<T>();\n-\n-            // Since we set the capacity to `usize::MAX` when `elem_size` is\n-            // 0, getting to here necessarily means the `RawVec` is overfull.\n-            assert!(elem_size != 0, \"capacity overflow\");\n-\n-            let (ptr, new_cap) = match self.current_layout() {\n-                Some(cur) => {\n-                    // Since we guarantee that we never allocate more than\n-                    // `isize::MAX` bytes, `elem_size * self.cap <= isize::MAX` as\n-                    // a precondition, so this can't overflow. Additionally the\n-                    // alignment will never be too large as to \"not be\n-                    // satisfiable\", so `Layout::from_size_align` will always\n-                    // return `Some`.\n-                    //\n-                    // TL;DR, we bypass runtime checks due to dynamic assertions\n-                    // in this module, allowing us to use\n-                    // `from_size_align_unchecked`.\n-                    let new_cap = 2 * self.cap;\n-                    let new_size = new_cap * elem_size;\n-                    alloc_guard(new_size).unwrap_or_else(|_| capacity_overflow());\n-                    let ptr_res = self.a.realloc(NonNull::from(self.ptr).cast(), cur, new_size);\n-                    match ptr_res {\n-                        Ok((ptr, new_size)) => (ptr, new_size / elem_size),\n-                        Err(_) => handle_alloc_error(Layout::from_size_align_unchecked(\n-                            new_size,\n-                            cur.align(),\n-                        )),\n-                    }\n-                }\n-                None => {\n-                    // Skip to 4 because tiny `Vec`'s are dumb; but not if that\n-                    // would cause overflow.\n-                    let new_cap = if elem_size > (!0) / 8 { 1 } else { 4 };\n-                    let layout = Layout::array::<T>(new_cap).unwrap();\n-                    match self.a.alloc(layout) {\n-                        Ok((ptr, new_size)) => (ptr, new_size / elem_size),\n-                        Err(_) => handle_alloc_error(layout),\n-                    }\n-                }\n-            };\n-            self.ptr = ptr.cast().into();\n-            self.cap = new_cap;\n+        match self.grow(Double, MayMove, Uninitialized) {\n+            Err(CapacityOverflow) => capacity_overflow(),\n+            Err(AllocError { layout, .. }) => handle_alloc_error(layout),\n+            Ok(()) => { /* yay */ }\n         }\n     }\n \n@@ -336,99 +285,7 @@ impl<T, A: AllocRef> RawVec<T, A> {\n     #[inline(never)]\n     #[cold]\n     pub fn double_in_place(&mut self) -> bool {\n-        unsafe {\n-            let elem_size = mem::size_of::<T>();\n-            let old_layout = match self.current_layout() {\n-                Some(layout) => layout,\n-                None => return false, // nothing to double\n-            };\n-\n-            // Since we set the capacity to `usize::MAX` when `elem_size` is\n-            // 0, getting to here necessarily means the `RawVec` is overfull.\n-            assert!(elem_size != 0, \"capacity overflow\");\n-\n-            // Since we guarantee that we never allocate more than `isize::MAX`\n-            // bytes, `elem_size * self.cap <= isize::MAX` as a precondition, so\n-            // this can't overflow.\n-            //\n-            // Similarly to with `double` above, we can go straight to\n-            // `Layout::from_size_align_unchecked` as we know this won't\n-            // overflow and the alignment is sufficiently small.\n-            let new_cap = 2 * self.cap;\n-            let new_size = new_cap * elem_size;\n-            alloc_guard(new_size).unwrap_or_else(|_| capacity_overflow());\n-            match self.a.grow_in_place(NonNull::from(self.ptr).cast(), old_layout, new_size) {\n-                Ok(_) => {\n-                    // We can't directly divide `size`.\n-                    self.cap = new_cap;\n-                    true\n-                }\n-                Err(_) => false,\n-            }\n-        }\n-    }\n-\n-    /// The same as `reserve_exact`, but returns on errors instead of panicking or aborting.\n-    pub fn try_reserve_exact(\n-        &mut self,\n-        used_capacity: usize,\n-        needed_extra_capacity: usize,\n-    ) -> Result<(), TryReserveError> {\n-        self.reserve_internal(used_capacity, needed_extra_capacity, Fallible, Exact)\n-    }\n-\n-    /// Ensures that the buffer contains at least enough space to hold\n-    /// `used_capacity + needed_extra_capacity` elements. If it doesn't already,\n-    /// will reallocate the minimum possible amount of memory necessary.\n-    /// Generally this will be exactly the amount of memory necessary,\n-    /// but in principle the allocator is free to give back more than\n-    /// we asked for.\n-    ///\n-    /// If `used_capacity` exceeds `self.capacity()`, this may fail to actually allocate\n-    /// the requested space. This is not really unsafe, but the unsafe\n-    /// code *you* write that relies on the behavior of this function may break.\n-    ///\n-    /// # Panics\n-    ///\n-    /// * Panics if the requested capacity exceeds `usize::MAX` bytes.\n-    /// * Panics on 32-bit platforms if the requested capacity exceeds\n-    ///   `isize::MAX` bytes.\n-    ///\n-    /// # Aborts\n-    ///\n-    /// Aborts on OOM.\n-    pub fn reserve_exact(&mut self, used_capacity: usize, needed_extra_capacity: usize) {\n-        match self.reserve_internal(used_capacity, needed_extra_capacity, Infallible, Exact) {\n-            Err(CapacityOverflow) => capacity_overflow(),\n-            Err(AllocError { .. }) => unreachable!(),\n-            Ok(()) => { /* yay */ }\n-        }\n-    }\n-\n-    /// Calculates the buffer's new size given that it'll hold `used_capacity +\n-    /// needed_extra_capacity` elements. This logic is used in amortized reserve methods.\n-    /// Returns `(new_capacity, new_alloc_size)`.\n-    fn amortized_new_size(\n-        &self,\n-        used_capacity: usize,\n-        needed_extra_capacity: usize,\n-    ) -> Result<usize, TryReserveError> {\n-        // Nothing we can really do about these checks, sadly.\n-        let required_cap =\n-            used_capacity.checked_add(needed_extra_capacity).ok_or(CapacityOverflow)?;\n-        // Cannot overflow, because `cap <= isize::MAX`, and type of `cap` is `usize`.\n-        let double_cap = self.cap * 2;\n-        // `double_cap` guarantees exponential growth.\n-        Ok(cmp::max(double_cap, required_cap))\n-    }\n-\n-    /// The same as `reserve`, but returns on errors instead of panicking or aborting.\n-    pub fn try_reserve(\n-        &mut self,\n-        used_capacity: usize,\n-        needed_extra_capacity: usize,\n-    ) -> Result<(), TryReserveError> {\n-        self.reserve_internal(used_capacity, needed_extra_capacity, Fallible, Amortized)\n+        self.grow(Double, InPlace, Uninitialized).is_ok()\n     }\n \n     /// Ensures that the buffer contains at least enough space to hold\n@@ -484,12 +341,26 @@ impl<T, A: AllocRef> RawVec<T, A> {\n     /// # }\n     /// ```\n     pub fn reserve(&mut self, used_capacity: usize, needed_extra_capacity: usize) {\n-        match self.reserve_internal(used_capacity, needed_extra_capacity, Infallible, Amortized) {\n+        match self.try_reserve(used_capacity, needed_extra_capacity) {\n             Err(CapacityOverflow) => capacity_overflow(),\n-            Err(AllocError { .. }) => unreachable!(),\n+            Err(AllocError { layout, .. }) => handle_alloc_error(layout),\n             Ok(()) => { /* yay */ }\n         }\n     }\n+\n+    /// The same as `reserve`, but returns on errors instead of panicking or aborting.\n+    pub fn try_reserve(\n+        &mut self,\n+        used_capacity: usize,\n+        needed_extra_capacity: usize,\n+    ) -> Result<(), TryReserveError> {\n+        if self.needs_to_grow(used_capacity, needed_extra_capacity) {\n+            self.grow(Amortized { used_capacity, needed_extra_capacity }, MayMove, Uninitialized)\n+        } else {\n+            Ok(())\n+        }\n+    }\n+\n     /// Attempts to ensure that the buffer contains at least enough space to hold\n     /// `used_capacity + needed_extra_capacity` elements. If it doesn't already have\n     /// enough capacity, will reallocate in place enough space plus comfortable slack\n@@ -508,45 +379,54 @@ impl<T, A: AllocRef> RawVec<T, A> {\n     /// * Panics on 32-bit platforms if the requested capacity exceeds\n     ///   `isize::MAX` bytes.\n     pub fn reserve_in_place(&mut self, used_capacity: usize, needed_extra_capacity: usize) -> bool {\n-        unsafe {\n-            // NOTE: we don't early branch on ZSTs here because we want this\n-            // to actually catch \"asking for more than usize::MAX\" in that case.\n-            // If we make it past the first branch then we are guaranteed to\n-            // panic.\n-\n-            // Don't actually need any more capacity. If the current `cap` is 0, we can't\n-            // reallocate in place.\n-            // Wrapping in case they give a bad `used_capacity`\n-            let old_layout = match self.current_layout() {\n-                Some(layout) => layout,\n-                None => return false,\n-            };\n-            if self.capacity().wrapping_sub(used_capacity) >= needed_extra_capacity {\n-                return false;\n-            }\n+        // This is more readable than putting this in one line:\n+        // `!self.needs_to_grow(...) || self.grow(...).is_ok()`\n+        if self.needs_to_grow(used_capacity, needed_extra_capacity) {\n+            self.grow(Amortized { used_capacity, needed_extra_capacity }, InPlace, Uninitialized)\n+                .is_ok()\n+        } else {\n+            true\n+        }\n+    }\n \n-            let new_cap = self\n-                .amortized_new_size(used_capacity, needed_extra_capacity)\n-                .unwrap_or_else(|_| capacity_overflow());\n-\n-            // Here, `cap < used_capacity + needed_extra_capacity <= new_cap`\n-            // (regardless of whether `self.cap - used_capacity` wrapped).\n-            // Therefore, we can safely call `grow_in_place`.\n-\n-            let new_layout = Layout::new::<T>().repeat(new_cap).unwrap().0;\n-            // FIXME: may crash and burn on over-reserve\n-            alloc_guard(new_layout.size()).unwrap_or_else(|_| capacity_overflow());\n-            match self.a.grow_in_place(\n-                NonNull::from(self.ptr).cast(),\n-                old_layout,\n-                new_layout.size(),\n-            ) {\n-                Ok(_) => {\n-                    self.cap = new_cap;\n-                    true\n-                }\n-                Err(_) => false,\n-            }\n+    /// Ensures that the buffer contains at least enough space to hold\n+    /// `used_capacity + needed_extra_capacity` elements. If it doesn't already,\n+    /// will reallocate the minimum possible amount of memory necessary.\n+    /// Generally this will be exactly the amount of memory necessary,\n+    /// but in principle the allocator is free to give back more than\n+    /// we asked for.\n+    ///\n+    /// If `used_capacity` exceeds `self.capacity()`, this may fail to actually allocate\n+    /// the requested space. This is not really unsafe, but the unsafe\n+    /// code *you* write that relies on the behavior of this function may break.\n+    ///\n+    /// # Panics\n+    ///\n+    /// * Panics if the requested capacity exceeds `usize::MAX` bytes.\n+    /// * Panics on 32-bit platforms if the requested capacity exceeds\n+    ///   `isize::MAX` bytes.\n+    ///\n+    /// # Aborts\n+    ///\n+    /// Aborts on OOM.\n+    pub fn reserve_exact(&mut self, used_capacity: usize, needed_extra_capacity: usize) {\n+        match self.try_reserve_exact(used_capacity, needed_extra_capacity) {\n+            Err(CapacityOverflow) => capacity_overflow(),\n+            Err(AllocError { layout, .. }) => handle_alloc_error(layout),\n+            Ok(()) => { /* yay */ }\n+        }\n+    }\n+\n+    /// The same as `reserve_exact`, but returns on errors instead of panicking or aborting.\n+    pub fn try_reserve_exact(\n+        &mut self,\n+        used_capacity: usize,\n+        needed_extra_capacity: usize,\n+    ) -> Result<(), TryReserveError> {\n+        if self.needs_to_grow(used_capacity, needed_extra_capacity) {\n+            self.grow(Exact { used_capacity, needed_extra_capacity }, MayMove, Uninitialized)\n+        } else {\n+            Ok(())\n         }\n     }\n \n@@ -561,166 +441,157 @@ impl<T, A: AllocRef> RawVec<T, A> {\n     ///\n     /// Aborts on OOM.\n     pub fn shrink_to_fit(&mut self, amount: usize) {\n-        let elem_size = mem::size_of::<T>();\n-\n-        // Set the `cap` because they might be about to promote to a `Box<[T]>`\n-        if elem_size == 0 {\n-            self.cap = amount;\n-            return;\n-        }\n-\n-        // This check is my waterloo; it's the only thing `Vec` wouldn't have to do.\n-        assert!(self.cap >= amount, \"Tried to shrink to a larger capacity\");\n-\n-        if amount == 0 {\n-            // We want to create a new zero-length vector within the\n-            // same allocator. We use `ptr::write` to avoid an\n-            // erroneous attempt to drop the contents, and we use\n-            // `ptr::read` to sidestep condition against destructuring\n-            // types that implement Drop.\n-\n-            unsafe {\n-                let a = ptr::read(&self.a as *const A);\n-                self.dealloc_buffer();\n-                ptr::write(self, RawVec::new_in(a));\n-            }\n-        } else if self.cap != amount {\n-            unsafe {\n-                // We know here that our `amount` is greater than zero. This\n-                // implies, via the assert above, that capacity is also greater\n-                // than zero, which means that we've got a current layout that\n-                // \"fits\"\n-                //\n-                // We also know that `self.cap` is greater than `amount`, and\n-                // consequently we don't need runtime checks for creating either\n-                // layout.\n-                let old_size = elem_size * self.cap;\n-                let new_size = elem_size * amount;\n-                let align = mem::align_of::<T>();\n-                let old_layout = Layout::from_size_align_unchecked(old_size, align);\n-                match self.a.realloc(NonNull::from(self.ptr).cast(), old_layout, new_size) {\n-                    Ok((ptr, _)) => self.ptr = ptr.cast().into(),\n-                    Err(_) => {\n-                        handle_alloc_error(Layout::from_size_align_unchecked(new_size, align))\n-                    }\n-                }\n-            }\n-            self.cap = amount;\n+        match self.shrink(amount, MayMove) {\n+            Err(CapacityOverflow) => capacity_overflow(),\n+            Err(AllocError { layout, .. }) => handle_alloc_error(layout),\n+            Ok(()) => { /* yay */ }\n         }\n     }\n }\n \n-enum Fallibility {\n-    Fallible,\n-    Infallible,\n+#[derive(Copy, Clone)]\n+enum Strategy {\n+    Double,\n+    Amortized { used_capacity: usize, needed_extra_capacity: usize },\n+    Exact { used_capacity: usize, needed_extra_capacity: usize },\n }\n+use Strategy::*;\n \n-use Fallibility::*;\n+impl<T, A: AllocRef> RawVec<T, A> {\n+    /// Returns if the buffer needs to grow to fulfill the needed extra capacity.\n+    /// Mainly used to make inlining reserve-calls possible without inlining `grow`.\n+    fn needs_to_grow(&self, used_capacity: usize, needed_extra_capacity: usize) -> bool {\n+        needed_extra_capacity > self.capacity().wrapping_sub(used_capacity)\n+    }\n \n-enum ReserveStrategy {\n-    Exact,\n-    Amortized,\n-}\n+    fn capacity_from_bytes(excess: usize) -> usize {\n+        debug_assert_ne!(mem::size_of::<T>(), 0);\n+        excess / mem::size_of::<T>()\n+    }\n \n-use ReserveStrategy::*;\n+    fn set_memory(&mut self, memory: MemoryBlock) {\n+        self.ptr = memory.ptr.cast().into();\n+        self.cap = Self::capacity_from_bytes(memory.size);\n+    }\n \n-impl<T, A: AllocRef> RawVec<T, A> {\n-    fn reserve_internal(\n+    /// Single method to handle all possibilities of growing the buffer.\n+    fn grow(\n         &mut self,\n-        used_capacity: usize,\n-        needed_extra_capacity: usize,\n-        fallibility: Fallibility,\n-        strategy: ReserveStrategy,\n+        strategy: Strategy,\n+        placement: ReallocPlacement,\n+        init: AllocInit,\n     ) -> Result<(), TryReserveError> {\n         let elem_size = mem::size_of::<T>();\n+        if elem_size == 0 {\n+            // Since we return a capacity of `usize::MAX` when `elem_size` is\n+            // 0, getting to here necessarily means the `RawVec` is overfull.\n+            return Err(CapacityOverflow);\n+        }\n+        let new_layout = match strategy {\n+            Double => unsafe {\n+                // Since we guarantee that we never allocate more than `isize::MAX` bytes,\n+                // `elem_size * self.cap <= isize::MAX` as a precondition, so this can't overflow.\n+                // Additionally the alignment will never be too large as to \"not be satisfiable\",\n+                // so `Layout::from_size_align` will always return `Some`.\n+                //\n+                // TL;DR, we bypass runtime checks due to dynamic assertions in this module,\n+                // allowing us to use `from_size_align_unchecked`.\n+                let cap = if self.cap == 0 {\n+                    // Skip to 4 because tiny `Vec`'s are dumb; but not if that would cause overflow.\n+                    if elem_size > usize::MAX / 8 { 1 } else { 4 }\n+                } else {\n+                    self.cap * 2\n+                };\n+                Layout::from_size_align_unchecked(cap * elem_size, mem::align_of::<T>())\n+            },\n+            Amortized { used_capacity, needed_extra_capacity } => {\n+                // Nothing we can really do about these checks, sadly.\n+                let required_cap =\n+                    used_capacity.checked_add(needed_extra_capacity).ok_or(CapacityOverflow)?;\n+                // Cannot overflow, because `cap <= isize::MAX`, and type of `cap` is `usize`.\n+                let double_cap = self.cap * 2;\n+                // `double_cap` guarantees exponential growth.\n+                let cap = cmp::max(double_cap, required_cap);\n+                Layout::array::<T>(cap).map_err(|_| CapacityOverflow)?\n+            }\n+            Exact { used_capacity, needed_extra_capacity } => {\n+                let cap =\n+                    used_capacity.checked_add(needed_extra_capacity).ok_or(CapacityOverflow)?;\n+                Layout::array::<T>(cap).map_err(|_| CapacityOverflow)?\n+            }\n+        };\n+        alloc_guard(new_layout.size())?;\n \n-        unsafe {\n-            // NOTE: we don't early branch on ZSTs here because we want this\n-            // to actually catch \"asking for more than usize::MAX\" in that case.\n-            // If we make it past the first branch then we are guaranteed to\n-            // panic.\n-\n-            // Don't actually need any more capacity.\n-            // Wrapping in case they gave a bad `used_capacity`.\n-            if self.capacity().wrapping_sub(used_capacity) >= needed_extra_capacity {\n-                return Ok(());\n+        let memory = if let Some((ptr, old_layout)) = self.current_memory() {\n+            debug_assert_eq!(old_layout.align(), new_layout.align());\n+            unsafe {\n+                self.alloc\n+                    .grow(ptr, old_layout, new_layout.size(), placement, init)\n+                    .map_err(|_| AllocError { layout: new_layout, non_exhaustive: () })?\n+            }\n+        } else {\n+            match placement {\n+                MayMove => self.alloc.alloc(new_layout, init),\n+                InPlace => Err(AllocErr),\n             }\n+            .map_err(|_| AllocError { layout: new_layout, non_exhaustive: () })?\n+        };\n+        self.set_memory(memory);\n+        Ok(())\n+    }\n \n-            // Nothing we can really do about these checks, sadly.\n-            let new_cap = match strategy {\n-                Exact => {\n-                    used_capacity.checked_add(needed_extra_capacity).ok_or(CapacityOverflow)?\n-                }\n-                Amortized => self.amortized_new_size(used_capacity, needed_extra_capacity)?,\n-            };\n-            let new_layout = Layout::array::<T>(new_cap).map_err(|_| CapacityOverflow)?;\n+    fn shrink(\n+        &mut self,\n+        amount: usize,\n+        placement: ReallocPlacement,\n+    ) -> Result<(), TryReserveError> {\n+        assert!(amount <= self.capacity(), \"Tried to shrink to a larger capacity\");\n \n-            alloc_guard(new_layout.size())?;\n+        let (ptr, layout) = if let Some(mem) = self.current_memory() { mem } else { return Ok(()) };\n+        let new_size = amount * mem::size_of::<T>();\n \n-            let res = match self.current_layout() {\n-                Some(layout) => {\n-                    debug_assert!(new_layout.align() == layout.align());\n-                    self.a.realloc(NonNull::from(self.ptr).cast(), layout, new_layout.size())\n-                }\n-                None => self.a.alloc(new_layout),\n-            };\n-\n-            let (ptr, new_cap) = match (res, fallibility) {\n-                (Err(AllocErr), Infallible) => handle_alloc_error(new_layout),\n-                (Err(AllocErr), Fallible) => {\n-                    return Err(TryReserveError::AllocError {\n-                        layout: new_layout,\n-                        non_exhaustive: (),\n-                    });\n+        let memory = unsafe {\n+            self.alloc.shrink(ptr, layout, new_size, placement).map_err(|_| {\n+                TryReserveError::AllocError {\n+                    layout: Layout::from_size_align_unchecked(new_size, layout.align()),\n+                    non_exhaustive: (),\n                 }\n-                (Ok((ptr, new_size)), _) => (ptr, new_size / elem_size),\n-            };\n-\n-            self.ptr = ptr.cast().into();\n-            self.cap = new_cap;\n-\n-            Ok(())\n-        }\n+            })?\n+        };\n+        self.set_memory(memory);\n+        Ok(())\n     }\n }\n \n impl<T> RawVec<T, Global> {\n-    /// Converts the entire buffer into `Box<[T]>`.\n+    /// Converts the entire buffer into `Box<[MaybeUninit<T>]>` with the specified `len`.\n     ///\n     /// Note that this will correctly reconstitute any `cap` changes\n     /// that may have been performed. (See description of type for details.)\n     ///\n-    /// # Undefined Behavior\n+    /// # Safety\n     ///\n-    /// All elements of `RawVec<T, Global>` must be initialized. Notice that\n-    /// the rules around uninitialized boxed values are not finalized yet,\n-    /// but until they are, it is advisable to avoid them.\n-    pub unsafe fn into_box(self) -> Box<[T]> {\n+    /// `shrink_to_fit(len)` must be called immediately prior to calling this function. This\n+    /// implies, that `len` must be smaller than or equal to `self.capacity()`.\n+    pub unsafe fn into_box(self, len: usize) -> Box<[MaybeUninit<T>]> {\n+        debug_assert!(\n+            len <= self.capacity(),\n+            \"`len` must be smaller than or equal to `self.capacity()`\"\n+        );\n+\n         // NOTE: not calling `capacity()` here; actually using the real `cap` field!\n-        let slice = slice::from_raw_parts_mut(self.ptr(), self.cap);\n-        let output: Box<[T]> = Box::from_raw(slice);\n+        let slice = slice::from_raw_parts_mut(self.ptr() as *mut MaybeUninit<T>, len);\n+        let output = Box::from_raw(slice);\n         mem::forget(self);\n         output\n     }\n }\n \n-impl<T, A: AllocRef> RawVec<T, A> {\n-    /// Frees the memory owned by the `RawVec` *without* trying to drop its contents.\n-    pub unsafe fn dealloc_buffer(&mut self) {\n-        let elem_size = mem::size_of::<T>();\n-        if elem_size != 0 {\n-            if let Some(layout) = self.current_layout() {\n-                self.a.dealloc(NonNull::from(self.ptr).cast(), layout);\n-            }\n-        }\n-    }\n-}\n-\n unsafe impl<#[may_dangle] T, A: AllocRef> Drop for RawVec<T, A> {\n     /// Frees the memory owned by the `RawVec` *without* trying to drop its contents.\n     fn drop(&mut self) {\n-        unsafe {\n-            self.dealloc_buffer();\n+        if let Some((ptr, layout)) = self.current_memory() {\n+            unsafe { self.alloc.dealloc(ptr, layout) }\n         }\n     }\n }"}, {"sha": "e7ab8a305d2797c83e5cf399dfc007bf581565b5", "filename": "src/liballoc/raw_vec/tests.rs", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fraw_vec%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fraw_vec%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Fraw_vec%2Ftests.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -12,6 +12,7 @@ fn allocator_param() {\n     //\n     // Instead, this just checks that the `RawVec` methods do at\n     // least go through the Allocator API when it reserves\n+\n     // storage.\n \n     // A dumb allocator that consumes a fixed amount of fuel\n@@ -20,12 +21,12 @@ fn allocator_param() {\n         fuel: usize,\n     }\n     unsafe impl AllocRef for BoundedAlloc {\n-        fn alloc(&mut self, layout: Layout) -> Result<(NonNull<u8>, usize), AllocErr> {\n+        fn alloc(&mut self, layout: Layout, init: AllocInit) -> Result<MemoryBlock, AllocErr> {\n             let size = layout.size();\n             if size > self.fuel {\n                 return Err(AllocErr);\n             }\n-            match Global.alloc(layout) {\n+            match Global.alloc(layout, init) {\n                 ok @ Ok(_) => {\n                     self.fuel -= size;\n                     ok\n@@ -40,9 +41,9 @@ fn allocator_param() {\n \n     let a = BoundedAlloc { fuel: 500 };\n     let mut v: RawVec<u8, _> = RawVec::with_capacity_in(50, a);\n-    assert_eq!(v.a.fuel, 450);\n+    assert_eq!(v.alloc.fuel, 450);\n     v.reserve(50, 150); // (causes a realloc, thus using 50 + 150 = 200 units of fuel)\n-    assert_eq!(v.a.fuel, 250);\n+    assert_eq!(v.alloc.fuel, 250);\n }\n \n #[test]"}, {"sha": "6a78a7398a692814155c9c9513f572cd98b73905", "filename": "src/liballoc/rc.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Frc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Frc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Frc.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -252,7 +252,7 @@ use core::ptr::{self, NonNull};\n use core::slice::{self, from_raw_parts_mut};\n use core::usize;\n \n-use crate::alloc::{box_free, handle_alloc_error, AllocRef, Global, Layout};\n+use crate::alloc::{box_free, handle_alloc_error, AllocInit, AllocRef, Global, Layout};\n use crate::string::String;\n use crate::vec::Vec;\n \n@@ -936,10 +936,12 @@ impl<T: ?Sized> Rc<T> {\n         let layout = Layout::new::<RcBox<()>>().extend(value_layout).unwrap().0.pad_to_align();\n \n         // Allocate for the layout.\n-        let (mem, _) = Global.alloc(layout).unwrap_or_else(|_| handle_alloc_error(layout));\n+        let mem = Global\n+            .alloc(layout, AllocInit::Uninitialized)\n+            .unwrap_or_else(|_| handle_alloc_error(layout));\n \n         // Initialize the RcBox\n-        let inner = mem_to_rcbox(mem.as_ptr());\n+        let inner = mem_to_rcbox(mem.ptr.as_ptr());\n         debug_assert_eq!(Layout::for_value(&*inner), layout);\n \n         ptr::write(&mut (*inner).strong, Cell::new(1));"}, {"sha": "111a7651b5e77c235f4cda5adfd4867a96d8a1f8", "filename": "src/liballoc/sync.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Fsync.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -25,7 +25,7 @@ use core::sync::atomic;\n use core::sync::atomic::Ordering::{Acquire, Relaxed, Release, SeqCst};\n use core::{isize, usize};\n \n-use crate::alloc::{box_free, handle_alloc_error, AllocRef, Global, Layout};\n+use crate::alloc::{box_free, handle_alloc_error, AllocInit, AllocRef, Global, Layout};\n use crate::boxed::Box;\n use crate::rc::is_dangling;\n use crate::string::String;\n@@ -814,10 +814,12 @@ impl<T: ?Sized> Arc<T> {\n         // reference (see #54908).\n         let layout = Layout::new::<ArcInner<()>>().extend(value_layout).unwrap().0.pad_to_align();\n \n-        let (mem, _) = Global.alloc(layout).unwrap_or_else(|_| handle_alloc_error(layout));\n+        let mem = Global\n+            .alloc(layout, AllocInit::Uninitialized)\n+            .unwrap_or_else(|_| handle_alloc_error(layout));\n \n         // Initialize the ArcInner\n-        let inner = mem_to_arcinner(mem.as_ptr());\n+        let inner = mem_to_arcinner(mem.ptr.as_ptr());\n         debug_assert_eq!(Layout::for_value(&*inner), layout);\n \n         ptr::write(&mut (*inner).strong, atomic::AtomicUsize::new(1));"}, {"sha": "62f062b83d75d77c5e34dfc62b2b347501716236", "filename": "src/liballoc/tests/heap.rs", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Ftests%2Fheap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Ftests%2Fheap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Ftests%2Fheap.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -1,4 +1,4 @@\n-use std::alloc::{AllocRef, Global, Layout, System};\n+use std::alloc::{AllocInit, AllocRef, Global, Layout, System};\n \n /// Issue #45955 and #62251.\n #[test]\n@@ -20,7 +20,13 @@ fn check_overalign_requests<T: AllocRef>(mut allocator: T) {\n             unsafe {\n                 let pointers: Vec<_> = (0..iterations)\n                     .map(|_| {\n-                        allocator.alloc(Layout::from_size_align(size, align).unwrap()).unwrap().0\n+                        allocator\n+                            .alloc(\n+                                Layout::from_size_align(size, align).unwrap(),\n+                                AllocInit::Uninitialized,\n+                            )\n+                            .unwrap()\n+                            .ptr\n                     })\n                     .collect();\n                 for &ptr in &pointers {"}, {"sha": "96a6399d0518b8f9bc7251a12c677723b0c81cf2", "filename": "src/liballoc/vec.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fvec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Fliballoc%2Fvec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Fvec.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -679,8 +679,9 @@ impl<T> Vec<T> {\n         unsafe {\n             self.shrink_to_fit();\n             let buf = ptr::read(&self.buf);\n+            let len = self.len();\n             mem::forget(self);\n-            buf.into_box()\n+            buf.into_box(len).assume_init()\n         }\n     }\n "}, {"sha": "be20a1cde3694fe0fd89285a91cacfe601c19340", "filename": "src/libcore/alloc.rs", "status": "removed", "additions": 0, "deletions": 1043, "changes": 1043, "blob_url": "https://github.com/rust-lang/rust/blob/b793f403bdfbcc0ff3e15ed8177a81d79ba4a29b/src%2Flibcore%2Falloc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b793f403bdfbcc0ff3e15ed8177a81d79ba4a29b/src%2Flibcore%2Falloc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Falloc.rs?ref=b793f403bdfbcc0ff3e15ed8177a81d79ba4a29b", "patch": "@@ -1,1043 +0,0 @@\n-//! Memory allocation APIs\n-\n-// ignore-tidy-undocumented-unsafe\n-\n-#![stable(feature = \"alloc_module\", since = \"1.28.0\")]\n-\n-use crate::cmp;\n-use crate::fmt;\n-use crate::mem;\n-use crate::num::NonZeroUsize;\n-use crate::ptr::{self, NonNull};\n-use crate::usize;\n-\n-const fn size_align<T>() -> (usize, usize) {\n-    (mem::size_of::<T>(), mem::align_of::<T>())\n-}\n-\n-/// Layout of a block of memory.\n-///\n-/// An instance of `Layout` describes a particular layout of memory.\n-/// You build a `Layout` up as an input to give to an allocator.\n-///\n-/// All layouts have an associated non-negative size and a\n-/// power-of-two alignment.\n-///\n-/// (Note however that layouts are *not* required to have positive\n-/// size, even though many allocators require that all memory\n-/// requests have positive size. A caller to the `AllocRef::alloc`\n-/// method must either ensure that conditions like this are met, or\n-/// use specific allocators with looser requirements.)\n-#[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-#[lang = \"alloc_layout\"]\n-pub struct Layout {\n-    // size of the requested block of memory, measured in bytes.\n-    size_: usize,\n-\n-    // alignment of the requested block of memory, measured in bytes.\n-    // we ensure that this is always a power-of-two, because API's\n-    // like `posix_memalign` require it and it is a reasonable\n-    // constraint to impose on Layout constructors.\n-    //\n-    // (However, we do not analogously require `align >= sizeof(void*)`,\n-    //  even though that is *also* a requirement of `posix_memalign`.)\n-    align_: NonZeroUsize,\n-}\n-\n-impl Layout {\n-    /// Constructs a `Layout` from a given `size` and `align`,\n-    /// or returns `LayoutErr` if any of the following conditions\n-    /// are not met:\n-    ///\n-    /// * `align` must not be zero,\n-    ///\n-    /// * `align` must be a power of two,\n-    ///\n-    /// * `size`, when rounded up to the nearest multiple of `align`,\n-    ///    must not overflow (i.e., the rounded value must be less than\n-    ///    `usize::MAX`).\n-    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-    #[rustc_const_unstable(feature = \"const_alloc_layout\", issue = \"67521\")]\n-    #[inline]\n-    pub const fn from_size_align(size: usize, align: usize) -> Result<Self, LayoutErr> {\n-        if !align.is_power_of_two() {\n-            return Err(LayoutErr { private: () });\n-        }\n-\n-        // (power-of-two implies align != 0.)\n-\n-        // Rounded up size is:\n-        //   size_rounded_up = (size + align - 1) & !(align - 1);\n-        //\n-        // We know from above that align != 0. If adding (align - 1)\n-        // does not overflow, then rounding up will be fine.\n-        //\n-        // Conversely, &-masking with !(align - 1) will subtract off\n-        // only low-order-bits. Thus if overflow occurs with the sum,\n-        // the &-mask cannot subtract enough to undo that overflow.\n-        //\n-        // Above implies that checking for summation overflow is both\n-        // necessary and sufficient.\n-        if size > usize::MAX - (align - 1) {\n-            return Err(LayoutErr { private: () });\n-        }\n-\n-        unsafe { Ok(Layout::from_size_align_unchecked(size, align)) }\n-    }\n-\n-    /// Creates a layout, bypassing all checks.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe as it does not verify the preconditions from\n-    /// [`Layout::from_size_align`](#method.from_size_align).\n-    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-    #[rustc_const_stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-    #[inline]\n-    pub const unsafe fn from_size_align_unchecked(size: usize, align: usize) -> Self {\n-        Layout { size_: size, align_: NonZeroUsize::new_unchecked(align) }\n-    }\n-\n-    /// The minimum size in bytes for a memory block of this layout.\n-    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-    #[rustc_const_unstable(feature = \"const_alloc_layout\", issue = \"67521\")]\n-    #[inline]\n-    pub const fn size(&self) -> usize {\n-        self.size_\n-    }\n-\n-    /// The minimum byte alignment for a memory block of this layout.\n-    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-    #[rustc_const_unstable(feature = \"const_alloc_layout\", issue = \"67521\")]\n-    #[inline]\n-    pub const fn align(&self) -> usize {\n-        self.align_.get()\n-    }\n-\n-    /// Constructs a `Layout` suitable for holding a value of type `T`.\n-    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-    #[rustc_const_stable(feature = \"alloc_layout_const_new\", since = \"1.42.0\")]\n-    #[inline]\n-    pub const fn new<T>() -> Self {\n-        let (size, align) = size_align::<T>();\n-        // Note that the align is guaranteed by rustc to be a power of two and\n-        // the size+align combo is guaranteed to fit in our address space. As a\n-        // result use the unchecked constructor here to avoid inserting code\n-        // that panics if it isn't optimized well enough.\n-        unsafe { Layout::from_size_align_unchecked(size, align) }\n-    }\n-\n-    /// Produces layout describing a record that could be used to\n-    /// allocate backing structure for `T` (which could be a trait\n-    /// or other unsized type like a slice).\n-    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-    #[inline]\n-    pub fn for_value<T: ?Sized>(t: &T) -> Self {\n-        let (size, align) = (mem::size_of_val(t), mem::align_of_val(t));\n-        // See rationale in `new` for why this is using an unsafe variant below\n-        debug_assert!(Layout::from_size_align(size, align).is_ok());\n-        unsafe { Layout::from_size_align_unchecked(size, align) }\n-    }\n-\n-    /// Produces layout describing a record that could be used to\n-    /// allocate backing structure for `T` (which could be a trait\n-    /// or other unsized type like a slice).\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is only safe to call if the following conditions hold:\n-    ///\n-    /// - If `T` is `Sized`, this function is always safe to call.\n-    /// - If the unsized tail of `T` is:\n-    ///     - a [slice], then the length of the slice tail must be an intialized\n-    ///       integer, and the size of the *entire value*\n-    ///       (dynamic tail length + statically sized prefix) must fit in `isize`.\n-    ///     - a [trait object], then the vtable part of the pointer must point\n-    ///       to a valid vtable acquired by an unsizing coersion, and the size\n-    ///       of the *entire value* (dynamic tail length + statically sized prefix)\n-    ///       must fit in `isize`.\n-    ///     - an (unstable) [extern type], then this function is always safe to\n-    ///       call, but may panic or otherwise return the wrong value, as the\n-    ///       extern type's layout is not known. This is the same behavior as\n-    ///       [`Layout::for_value`] on a reference to an extern type tail.\n-    ///     - otherwise, it is conservatively not allowed to call this function.\n-    ///\n-    /// [slice]: ../../std/primitive.slice.html\n-    /// [trait object]: ../../book/ch17-02-trait-objects.html\n-    /// [extern type]: ../../unstable-book/language-features/extern-types.html\n-    #[inline]\n-    #[cfg(not(bootstrap))]\n-    #[unstable(feature = \"layout_for_ptr\", issue = \"69835\")]\n-    pub unsafe fn for_value_raw<T: ?Sized>(t: *const T) -> Self {\n-        let (size, align) = (mem::size_of_val_raw(t), mem::align_of_val_raw(t));\n-        // See rationale in `new` for why this is using an unsafe variant below\n-        debug_assert!(Layout::from_size_align(size, align).is_ok());\n-        Layout::from_size_align_unchecked(size, align)\n-    }\n-\n-    /// Creates a `NonNull` that is dangling, but well-aligned for this Layout.\n-    ///\n-    /// Note that the pointer value may potentially represent a valid pointer,\n-    /// which means this must not be used as a \"not yet initialized\"\n-    /// sentinel value. Types that lazily allocate must track initialization by\n-    /// some other means.\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    pub const fn dangling(&self) -> NonNull<u8> {\n-        // align is non-zero and a power of two\n-        unsafe { NonNull::new_unchecked(self.align() as *mut u8) }\n-    }\n-\n-    /// Creates a layout describing the record that can hold a value\n-    /// of the same layout as `self`, but that also is aligned to\n-    /// alignment `align` (measured in bytes).\n-    ///\n-    /// If `self` already meets the prescribed alignment, then returns\n-    /// `self`.\n-    ///\n-    /// Note that this method does not add any padding to the overall\n-    /// size, regardless of whether the returned layout has a different\n-    /// alignment. In other words, if `K` has size 16, `K.align_to(32)`\n-    /// will *still* have size 16.\n-    ///\n-    /// Returns an error if the combination of `self.size()` and the given\n-    /// `align` violates the conditions listed in\n-    /// [`Layout::from_size_align`](#method.from_size_align).\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    #[inline]\n-    pub fn align_to(&self, align: usize) -> Result<Self, LayoutErr> {\n-        Layout::from_size_align(self.size(), cmp::max(self.align(), align))\n-    }\n-\n-    /// Returns the amount of padding we must insert after `self`\n-    /// to ensure that the following address will satisfy `align`\n-    /// (measured in bytes).\n-    ///\n-    /// e.g., if `self.size()` is 9, then `self.padding_needed_for(4)`\n-    /// returns 3, because that is the minimum number of bytes of\n-    /// padding required to get a 4-aligned address (assuming that the\n-    /// corresponding memory block starts at a 4-aligned address).\n-    ///\n-    /// The return value of this function has no meaning if `align` is\n-    /// not a power-of-two.\n-    ///\n-    /// Note that the utility of the returned value requires `align`\n-    /// to be less than or equal to the alignment of the starting\n-    /// address for the whole allocated block of memory. One way to\n-    /// satisfy this constraint is to ensure `align <= self.align()`.\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    #[rustc_const_unstable(feature = \"const_alloc_layout\", issue = \"67521\")]\n-    #[inline]\n-    pub const fn padding_needed_for(&self, align: usize) -> usize {\n-        let len = self.size();\n-\n-        // Rounded up value is:\n-        //   len_rounded_up = (len + align - 1) & !(align - 1);\n-        // and then we return the padding difference: `len_rounded_up - len`.\n-        //\n-        // We use modular arithmetic throughout:\n-        //\n-        // 1. align is guaranteed to be > 0, so align - 1 is always\n-        //    valid.\n-        //\n-        // 2. `len + align - 1` can overflow by at most `align - 1`,\n-        //    so the &-mask with `!(align - 1)` will ensure that in the\n-        //    case of overflow, `len_rounded_up` will itself be 0.\n-        //    Thus the returned padding, when added to `len`, yields 0,\n-        //    which trivially satisfies the alignment `align`.\n-        //\n-        // (Of course, attempts to allocate blocks of memory whose\n-        // size and padding overflow in the above manner should cause\n-        // the allocator to yield an error anyway.)\n-\n-        let len_rounded_up = len.wrapping_add(align).wrapping_sub(1) & !align.wrapping_sub(1);\n-        len_rounded_up.wrapping_sub(len)\n-    }\n-\n-    /// Creates a layout by rounding the size of this layout up to a multiple\n-    /// of the layout's alignment.\n-    ///\n-    /// This is equivalent to adding the result of `padding_needed_for`\n-    /// to the layout's current size.\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    #[inline]\n-    pub fn pad_to_align(&self) -> Layout {\n-        let pad = self.padding_needed_for(self.align());\n-        // This cannot overflow. Quoting from the invariant of Layout:\n-        // > `size`, when rounded up to the nearest multiple of `align`,\n-        // > must not overflow (i.e., the rounded value must be less than\n-        // > `usize::MAX`)\n-        let new_size = self.size() + pad;\n-\n-        Layout::from_size_align(new_size, self.align()).unwrap()\n-    }\n-\n-    /// Creates a layout describing the record for `n` instances of\n-    /// `self`, with a suitable amount of padding between each to\n-    /// ensure that each instance is given its requested size and\n-    /// alignment. On success, returns `(k, offs)` where `k` is the\n-    /// layout of the array and `offs` is the distance between the start\n-    /// of each element in the array.\n-    ///\n-    /// On arithmetic overflow, returns `LayoutErr`.\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    #[inline]\n-    pub fn repeat(&self, n: usize) -> Result<(Self, usize), LayoutErr> {\n-        // This cannot overflow. Quoting from the invariant of Layout:\n-        // > `size`, when rounded up to the nearest multiple of `align`,\n-        // > must not overflow (i.e., the rounded value must be less than\n-        // > `usize::MAX`)\n-        let padded_size = self.size() + self.padding_needed_for(self.align());\n-        let alloc_size = padded_size.checked_mul(n).ok_or(LayoutErr { private: () })?;\n-\n-        unsafe {\n-            // self.align is already known to be valid and alloc_size has been\n-            // padded already.\n-            Ok((Layout::from_size_align_unchecked(alloc_size, self.align()), padded_size))\n-        }\n-    }\n-\n-    /// Creates a layout describing the record for `self` followed by\n-    /// `next`, including any necessary padding to ensure that `next`\n-    /// will be properly aligned. Note that the resulting layout will\n-    /// satisfy the alignment properties of both `self` and `next`.\n-    ///\n-    /// The resulting layout will be the same as that of a C struct containing\n-    /// two fields with the layouts of `self` and `next`, in that order.\n-    ///\n-    /// Returns `Some((k, offset))`, where `k` is layout of the concatenated\n-    /// record and `offset` is the relative location, in bytes, of the\n-    /// start of the `next` embedded within the concatenated record\n-    /// (assuming that the record itself starts at offset 0).\n-    ///\n-    /// On arithmetic overflow, returns `LayoutErr`.\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    #[inline]\n-    pub fn extend(&self, next: Self) -> Result<(Self, usize), LayoutErr> {\n-        let new_align = cmp::max(self.align(), next.align());\n-        let pad = self.padding_needed_for(next.align());\n-\n-        let offset = self.size().checked_add(pad).ok_or(LayoutErr { private: () })?;\n-        let new_size = offset.checked_add(next.size()).ok_or(LayoutErr { private: () })?;\n-\n-        let layout = Layout::from_size_align(new_size, new_align)?;\n-        Ok((layout, offset))\n-    }\n-\n-    /// Creates a layout describing the record for `n` instances of\n-    /// `self`, with no padding between each instance.\n-    ///\n-    /// Note that, unlike `repeat`, `repeat_packed` does not guarantee\n-    /// that the repeated instances of `self` will be properly\n-    /// aligned, even if a given instance of `self` is properly\n-    /// aligned. In other words, if the layout returned by\n-    /// `repeat_packed` is used to allocate an array, it is not\n-    /// guaranteed that all elements in the array will be properly\n-    /// aligned.\n-    ///\n-    /// On arithmetic overflow, returns `LayoutErr`.\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    #[inline]\n-    pub fn repeat_packed(&self, n: usize) -> Result<Self, LayoutErr> {\n-        let size = self.size().checked_mul(n).ok_or(LayoutErr { private: () })?;\n-        Layout::from_size_align(size, self.align())\n-    }\n-\n-    /// Creates a layout describing the record for `self` followed by\n-    /// `next` with no additional padding between the two. Since no\n-    /// padding is inserted, the alignment of `next` is irrelevant,\n-    /// and is not incorporated *at all* into the resulting layout.\n-    ///\n-    /// On arithmetic overflow, returns `LayoutErr`.\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    #[inline]\n-    pub fn extend_packed(&self, next: Self) -> Result<Self, LayoutErr> {\n-        let new_size = self.size().checked_add(next.size()).ok_or(LayoutErr { private: () })?;\n-        Layout::from_size_align(new_size, self.align())\n-    }\n-\n-    /// Creates a layout describing the record for a `[T; n]`.\n-    ///\n-    /// On arithmetic overflow, returns `LayoutErr`.\n-    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n-    #[inline]\n-    pub fn array<T>(n: usize) -> Result<Self, LayoutErr> {\n-        Layout::new::<T>().repeat(n).map(|(k, offs)| {\n-            debug_assert!(offs == mem::size_of::<T>());\n-            k\n-        })\n-    }\n-}\n-\n-/// The parameters given to `Layout::from_size_align`\n-/// or some other `Layout` constructor\n-/// do not satisfy its documented constraints.\n-#[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-#[derive(Clone, PartialEq, Eq, Debug)]\n-pub struct LayoutErr {\n-    private: (),\n-}\n-\n-// (we need this for downstream impl of trait Error)\n-#[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n-impl fmt::Display for LayoutErr {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        f.write_str(\"invalid parameters to Layout::from_size_align\")\n-    }\n-}\n-\n-/// The `AllocErr` error indicates an allocation failure\n-/// that may be due to resource exhaustion or to\n-/// something wrong when combining the given input arguments with this\n-/// allocator.\n-#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n-#[derive(Clone, PartialEq, Eq, Debug)]\n-pub struct AllocErr;\n-\n-// (we need this for downstream impl of trait Error)\n-#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n-impl fmt::Display for AllocErr {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        f.write_str(\"memory allocation failed\")\n-    }\n-}\n-\n-/// The `CannotReallocInPlace` error is used when [`grow_in_place`] or\n-/// [`shrink_in_place`] were unable to reuse the given memory block for\n-/// a requested layout.\n-///\n-/// [`grow_in_place`]: ./trait.AllocRef.html#method.grow_in_place\n-/// [`shrink_in_place`]: ./trait.AllocRef.html#method.shrink_in_place\n-#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n-#[derive(Clone, PartialEq, Eq, Debug)]\n-pub struct CannotReallocInPlace;\n-\n-#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n-impl CannotReallocInPlace {\n-    pub fn description(&self) -> &str {\n-        \"cannot reallocate allocator's memory in place\"\n-    }\n-}\n-\n-// (we need this for downstream impl of trait Error)\n-#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n-impl fmt::Display for CannotReallocInPlace {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        write!(f, \"{}\", self.description())\n-    }\n-}\n-\n-/// A memory allocator that can be registered as the standard library\u2019s default\n-/// through the `#[global_allocator]` attribute.\n-///\n-/// Some of the methods require that a memory block be *currently\n-/// allocated* via an allocator. This means that:\n-///\n-/// * the starting address for that memory block was previously\n-///   returned by a previous call to an allocation method\n-///   such as `alloc`, and\n-///\n-/// * the memory block has not been subsequently deallocated, where\n-///   blocks are deallocated either by being passed to a deallocation\n-///   method such as `dealloc` or by being\n-///   passed to a reallocation method that returns a non-null pointer.\n-///\n-///\n-/// # Example\n-///\n-/// ```no_run\n-/// use std::alloc::{GlobalAlloc, Layout, alloc};\n-/// use std::ptr::null_mut;\n-///\n-/// struct MyAllocator;\n-///\n-/// unsafe impl GlobalAlloc for MyAllocator {\n-///     unsafe fn alloc(&self, _layout: Layout) -> *mut u8 { null_mut() }\n-///     unsafe fn dealloc(&self, _ptr: *mut u8, _layout: Layout) {}\n-/// }\n-///\n-/// #[global_allocator]\n-/// static A: MyAllocator = MyAllocator;\n-///\n-/// fn main() {\n-///     unsafe {\n-///         assert!(alloc(Layout::new::<u32>()).is_null())\n-///     }\n-/// }\n-/// ```\n-///\n-/// # Safety\n-///\n-/// The `GlobalAlloc` trait is an `unsafe` trait for a number of reasons, and\n-/// implementors must ensure that they adhere to these contracts:\n-///\n-/// * It's undefined behavior if global allocators unwind. This restriction may\n-///   be lifted in the future, but currently a panic from any of these\n-///   functions may lead to memory unsafety.\n-///\n-/// * `Layout` queries and calculations in general must be correct. Callers of\n-///   this trait are allowed to rely on the contracts defined on each method,\n-///   and implementors must ensure such contracts remain true.\n-#[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n-pub unsafe trait GlobalAlloc {\n-    /// Allocate memory as described by the given `layout`.\n-    ///\n-    /// Returns a pointer to newly-allocated memory,\n-    /// or null to indicate allocation failure.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe because undefined behavior can result\n-    /// if the caller does not ensure that `layout` has non-zero size.\n-    ///\n-    /// (Extension subtraits might provide more specific bounds on\n-    /// behavior, e.g., guarantee a sentinel address or a null pointer\n-    /// in response to a zero-size allocation request.)\n-    ///\n-    /// The allocated block of memory may or may not be initialized.\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returning a null pointer indicates that either memory is exhausted\n-    /// or `layout` does not meet this allocator's size or alignment constraints.\n-    ///\n-    /// Implementations are encouraged to return null on memory\n-    /// exhaustion rather than aborting, but this is not\n-    /// a strict requirement. (Specifically: it is *legal* to\n-    /// implement this trait atop an underlying native allocation\n-    /// library that aborts on memory exhaustion.)\n-    ///\n-    /// Clients wishing to abort computation in response to an\n-    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n-    /// rather than directly invoking `panic!` or similar.\n-    ///\n-    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n-    #[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n-    unsafe fn alloc(&self, layout: Layout) -> *mut u8;\n-\n-    /// Deallocate the block of memory at the given `ptr` pointer with the given `layout`.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe because undefined behavior can result\n-    /// if the caller does not ensure all of the following:\n-    ///\n-    /// * `ptr` must denote a block of memory currently allocated via\n-    ///   this allocator,\n-    ///\n-    /// * `layout` must be the same layout that was used\n-    ///   to allocate that block of memory,\n-    #[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n-    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout);\n-\n-    /// Behaves like `alloc`, but also ensures that the contents\n-    /// are set to zero before being returned.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe for the same reasons that `alloc` is.\n-    /// However the allocated block of memory is guaranteed to be initialized.\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returning a null pointer indicates that either memory is exhausted\n-    /// or `layout` does not meet allocator's size or alignment constraints,\n-    /// just as in `alloc`.\n-    ///\n-    /// Clients wishing to abort computation in response to an\n-    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n-    /// rather than directly invoking `panic!` or similar.\n-    ///\n-    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n-    #[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n-    unsafe fn alloc_zeroed(&self, layout: Layout) -> *mut u8 {\n-        let size = layout.size();\n-        let ptr = self.alloc(layout);\n-        if !ptr.is_null() {\n-            ptr::write_bytes(ptr, 0, size);\n-        }\n-        ptr\n-    }\n-\n-    /// Shrink or grow a block of memory to the given `new_size`.\n-    /// The block is described by the given `ptr` pointer and `layout`.\n-    ///\n-    /// If this returns a non-null pointer, then ownership of the memory block\n-    /// referenced by `ptr` has been transferred to this allocator.\n-    /// The memory may or may not have been deallocated,\n-    /// and should be considered unusable (unless of course it was\n-    /// transferred back to the caller again via the return value of\n-    /// this method). The new memory block is allocated with `layout`, but\n-    /// with the `size` updated to `new_size`.\n-    ///\n-    /// If this method returns null, then ownership of the memory\n-    /// block has not been transferred to this allocator, and the\n-    /// contents of the memory block are unaltered.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe because undefined behavior can result\n-    /// if the caller does not ensure all of the following:\n-    ///\n-    /// * `ptr` must be currently allocated via this allocator,\n-    ///\n-    /// * `layout` must be the same layout that was used\n-    ///   to allocate that block of memory,\n-    ///\n-    /// * `new_size` must be greater than zero.\n-    ///\n-    /// * `new_size`, when rounded up to the nearest multiple of `layout.align()`,\n-    ///   must not overflow (i.e., the rounded value must be less than `usize::MAX`).\n-    ///\n-    /// (Extension subtraits might provide more specific bounds on\n-    /// behavior, e.g., guarantee a sentinel address or a null pointer\n-    /// in response to a zero-size allocation request.)\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returns null if the new layout does not meet the size\n-    /// and alignment constraints of the allocator, or if reallocation\n-    /// otherwise fails.\n-    ///\n-    /// Implementations are encouraged to return null on memory\n-    /// exhaustion rather than panicking or aborting, but this is not\n-    /// a strict requirement. (Specifically: it is *legal* to\n-    /// implement this trait atop an underlying native allocation\n-    /// library that aborts on memory exhaustion.)\n-    ///\n-    /// Clients wishing to abort computation in response to a\n-    /// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n-    /// rather than directly invoking `panic!` or similar.\n-    ///\n-    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n-    #[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n-    unsafe fn realloc(&self, ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {\n-        let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n-        let new_ptr = self.alloc(new_layout);\n-        if !new_ptr.is_null() {\n-            ptr::copy_nonoverlapping(ptr, new_ptr, cmp::min(layout.size(), new_size));\n-            self.dealloc(ptr, layout);\n-        }\n-        new_ptr\n-    }\n-}\n-\n-/// An implementation of `AllocRef` can allocate, reallocate, and\n-/// deallocate arbitrary blocks of data described via `Layout`.\n-///\n-/// `AllocRef` is designed to be implemented on ZSTs, references, or\n-/// smart pointers because having an allocator like `MyAlloc([u8; N])`\n-/// cannot be moved, without updating the pointers to the allocated\n-/// memory.\n-///\n-/// Some of the methods require that a memory block be *currently\n-/// allocated* via an allocator. This means that:\n-///\n-/// * the starting address for that memory block was previously\n-///   returned by a previous call to an allocation method (`alloc`,\n-///   `alloc_zeroed`) or reallocation method (`realloc`), and\n-///\n-/// * the memory block has not been subsequently deallocated, where\n-///   blocks are deallocated either by being passed to a deallocation\n-///   method (`dealloc`) or by being passed to a reallocation method\n-///  (see above) that returns `Ok`.\n-///\n-/// Unlike [`GlobalAlloc`], zero-sized allocations are allowed in\n-/// `AllocRef`. If an underlying allocator does not support this (like\n-/// jemalloc) or return a null pointer (such as `libc::malloc`), this case\n-/// must be caught. In this case [`Layout::dangling()`] can be used to\n-/// create a dangling, but aligned `NonNull<u8>`.\n-///\n-/// Some of the methods require that a layout *fit* a memory block.\n-/// What it means for a layout to \"fit\" a memory block means (or\n-/// equivalently, for a memory block to \"fit\" a layout) is that the\n-/// following two conditions must hold:\n-///\n-/// 1. The block's starting address must be aligned to `layout.align()`.\n-///\n-/// 2. The block's size must fall in the range `[use_min, use_max]`, where:\n-///\n-///    * `use_min` is `layout.size()`, and\n-///\n-///    * `use_max` is the capacity that was returned.\n-///\n-/// Note that:\n-///\n-///  * the size of the layout most recently used to allocate the block\n-///    is guaranteed to be in the range `[use_min, use_max]`, and\n-///\n-///  * a lower-bound on `use_max` can be safely approximated by a call to\n-///    `usable_size`.\n-///\n-///  * if a layout `k` fits a memory block (denoted by `ptr`)\n-///    currently allocated via an allocator `a`, then it is legal to\n-///    use that layout to deallocate it, i.e., `a.dealloc(ptr, k);`.\n-///\n-///  * if an allocator does not support overallocating, it is fine to\n-///    simply return `layout.size()` as the allocated size.\n-///\n-/// [`GlobalAlloc`]: self::GlobalAlloc\n-/// [`Layout::dangling()`]: self::Layout::dangling\n-///\n-/// # Safety\n-///\n-/// The `AllocRef` trait is an `unsafe` trait for a number of reasons, and\n-/// implementors must ensure that they adhere to these contracts:\n-///\n-/// * Pointers returned from allocation functions must point to valid memory and\n-///   retain their validity until at least one instance of `AllocRef` is dropped\n-///   itself.\n-///\n-/// * Cloning or moving the allocator must not invalidate pointers returned\n-///   from this allocator. Cloning must return a reference to the same allocator.\n-///\n-/// * `Layout` queries and calculations in general must be correct. Callers of\n-///   this trait are allowed to rely on the contracts defined on each method,\n-///   and implementors must ensure such contracts remain true.\n-///\n-/// Note that this list may get tweaked over time as clarifications are made in\n-/// the future.\n-#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n-pub unsafe trait AllocRef {\n-    /// On success, returns a pointer meeting the size and alignment\n-    /// guarantees of `layout` and the actual size of the allocated block,\n-    /// which must be greater than or equal to `layout.size()`.\n-    ///\n-    /// If this method returns an `Ok(addr)`, then the `addr` returned\n-    /// will be non-null address pointing to a block of storage\n-    /// suitable for holding an instance of `layout`.\n-    ///\n-    /// The returned block of storage may or may not have its contents\n-    /// initialized. (Extension subtraits might restrict this\n-    /// behavior, e.g., to ensure initialization to particular sets of\n-    /// bit patterns.)\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returning `Err` indicates that either memory is exhausted or\n-    /// `layout` does not meet allocator's size or alignment\n-    /// constraints.\n-    ///\n-    /// Implementations are encouraged to return `Err` on memory\n-    /// exhaustion rather than panicking or aborting, but this is not\n-    /// a strict requirement. (Specifically: it is *legal* to\n-    /// implement this trait atop an underlying native allocation\n-    /// library that aborts on memory exhaustion.)\n-    ///\n-    /// Clients wishing to abort computation in response to an\n-    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n-    /// rather than directly invoking `panic!` or similar.\n-    ///\n-    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n-    fn alloc(&mut self, layout: Layout) -> Result<(NonNull<u8>, usize), AllocErr>;\n-\n-    /// Deallocate the memory referenced by `ptr`.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe because undefined behavior can result\n-    /// if the caller does not ensure all of the following:\n-    ///\n-    /// * `ptr` must denote a block of memory currently allocated via\n-    ///   this allocator,\n-    ///\n-    /// * `layout` must *fit* that block of memory,\n-    ///\n-    /// * In addition to fitting the block of memory `layout`, the\n-    ///   alignment of the `layout` must match the alignment used\n-    ///   to allocate that block of memory.\n-    unsafe fn dealloc(&mut self, ptr: NonNull<u8>, layout: Layout);\n-\n-    /// Behaves like `alloc`, but also ensures that the contents\n-    /// are set to zero before being returned.\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returning `Err` indicates that either memory is exhausted or\n-    /// `layout` does not meet allocator's size or alignment\n-    /// constraints, just as in `alloc`.\n-    ///\n-    /// Clients wishing to abort computation in response to an\n-    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n-    /// rather than directly invoking `panic!` or similar.\n-    ///\n-    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n-    fn alloc_zeroed(&mut self, layout: Layout) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        let size = layout.size();\n-        let result = self.alloc(layout);\n-        if let Ok((p, _)) = result {\n-            unsafe { ptr::write_bytes(p.as_ptr(), 0, size) }\n-        }\n-        result\n-    }\n-\n-    // == METHODS FOR MEMORY REUSE ==\n-    // realloc, realloc_zeroed, grow_in_place, grow_in_place_zeroed, shrink_in_place\n-\n-    /// Returns a pointer suitable for holding data described by\n-    /// a new layout with `layout`\u2019s alignment and a size given\n-    /// by `new_size` and the actual size of the allocated block.\n-    /// The latter is greater than or equal to `layout.size()`.\n-    /// To accomplish this, the allocator may extend or shrink\n-    /// the allocation referenced by `ptr` to fit the new layout.\n-    ///\n-    /// If this returns `Ok`, then ownership of the memory block\n-    /// referenced by `ptr` has been transferred to this\n-    /// allocator. The memory may or may not have been freed, and\n-    /// should be considered unusable (unless of course it was\n-    /// transferred back to the caller again via the return value of\n-    /// this method).\n-    ///\n-    /// If this method returns `Err`, then ownership of the memory\n-    /// block has not been transferred to this allocator, and the\n-    /// contents of the memory block are unaltered.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe because undefined behavior can result\n-    /// if the caller does not ensure all of the following:\n-    ///\n-    /// * `ptr` must be currently allocated via this allocator,\n-    ///\n-    /// * `layout` must *fit* the `ptr` (see above). (The `new_size`\n-    ///   argument need not fit it.)\n-    ///\n-    /// * `new_size`, when rounded up to the nearest multiple of `layout.align()`,\n-    ///   must not overflow (i.e., the rounded value must be less than `usize::MAX`).\n-    ///\n-    /// (Extension subtraits might provide more specific bounds on\n-    /// behavior, e.g., guarantee a sentinel address or a null pointer\n-    /// in response to a zero-size allocation request.)\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returns `Err` only if the new layout\n-    /// does not meet the allocator's size\n-    /// and alignment constraints of the allocator, or if reallocation\n-    /// otherwise fails.\n-    ///\n-    /// Implementations are encouraged to return `Err` on memory\n-    /// exhaustion rather than panicking or aborting, but this is not\n-    /// a strict requirement. (Specifically: it is *legal* to\n-    /// implement this trait atop an underlying native allocation\n-    /// library that aborts on memory exhaustion.)\n-    ///\n-    /// Clients wishing to abort computation in response to a\n-    /// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n-    /// rather than directly invoking `panic!` or similar.\n-    ///\n-    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n-    unsafe fn realloc(\n-        &mut self,\n-        ptr: NonNull<u8>,\n-        layout: Layout,\n-        new_size: usize,\n-    ) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        let old_size = layout.size();\n-\n-        if new_size > old_size {\n-            if let Ok(size) = self.grow_in_place(ptr, layout, new_size) {\n-                return Ok((ptr, size));\n-            }\n-        } else if new_size < old_size {\n-            if let Ok(size) = self.shrink_in_place(ptr, layout, new_size) {\n-                return Ok((ptr, size));\n-            }\n-        } else {\n-            return Ok((ptr, new_size));\n-        }\n-\n-        // otherwise, fall back on alloc + copy + dealloc.\n-        let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n-        let result = self.alloc(new_layout);\n-        if let Ok((new_ptr, _)) = result {\n-            ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_ptr(), cmp::min(old_size, new_size));\n-            self.dealloc(ptr, layout);\n-        }\n-        result\n-    }\n-\n-    /// Behaves like `realloc`, but also ensures that the new contents\n-    /// are set to zero before being returned.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe for the same reasons that `realloc` is.\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returns `Err` only if the new layout\n-    /// does not meet the allocator's size\n-    /// and alignment constraints of the allocator, or if reallocation\n-    /// otherwise fails.\n-    ///\n-    /// Implementations are encouraged to return `Err` on memory\n-    /// exhaustion rather than panicking or aborting, but this is not\n-    /// a strict requirement. (Specifically: it is *legal* to\n-    /// implement this trait atop an underlying native allocation\n-    /// library that aborts on memory exhaustion.)\n-    ///\n-    /// Clients wishing to abort computation in response to a\n-    /// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n-    /// rather than directly invoking `panic!` or similar.\n-    ///\n-    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n-    unsafe fn realloc_zeroed(\n-        &mut self,\n-        ptr: NonNull<u8>,\n-        layout: Layout,\n-        new_size: usize,\n-    ) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        let old_size = layout.size();\n-\n-        if new_size > old_size {\n-            if let Ok(size) = self.grow_in_place_zeroed(ptr, layout, new_size) {\n-                return Ok((ptr, size));\n-            }\n-        } else if new_size < old_size {\n-            if let Ok(size) = self.shrink_in_place(ptr, layout, new_size) {\n-                return Ok((ptr, size));\n-            }\n-        } else {\n-            return Ok((ptr, new_size));\n-        }\n-\n-        // otherwise, fall back on alloc + copy + dealloc.\n-        let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n-        let result = self.alloc_zeroed(new_layout);\n-        if let Ok((new_ptr, _)) = result {\n-            ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_ptr(), cmp::min(old_size, new_size));\n-            self.dealloc(ptr, layout);\n-        }\n-        result\n-    }\n-\n-    /// Attempts to extend the allocation referenced by `ptr` to fit `new_size`.\n-    ///\n-    /// If this returns `Ok`, then the allocator has asserted that the\n-    /// memory block referenced by `ptr` now fits `new_size`, and thus can\n-    /// be used to carry data of a layout of that size and same alignment as\n-    /// `layout`. The returned value is the new size of the allocated block.\n-    /// (The allocator is allowed to expend effort to accomplish this, such\n-    /// as extending the memory block to include successor blocks, or virtual\n-    /// memory tricks.)\n-    ///\n-    /// Regardless of what this method returns, ownership of the\n-    /// memory block referenced by `ptr` has not been transferred, and\n-    /// the contents of the memory block are unaltered.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe because undefined behavior can result\n-    /// if the caller does not ensure all of the following:\n-    ///\n-    /// * `ptr` must be currently allocated via this allocator,\n-    ///\n-    /// * `layout` must *fit* the `ptr` (see above); note the\n-    ///   `new_size` argument need not fit it,\n-    ///\n-    /// * `new_size` must not be less than `layout.size()`,\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returns `Err(CannotReallocInPlace)` when the allocator is\n-    /// unable to assert that the memory block referenced by `ptr`\n-    /// could fit `layout`.\n-    ///\n-    /// Note that one cannot pass `CannotReallocInPlace` to the `handle_alloc_error`\n-    /// function; clients are expected either to be able to recover from\n-    /// `grow_in_place` failures without aborting, or to fall back on\n-    /// another reallocation method before resorting to an abort.\n-    #[inline]\n-    unsafe fn grow_in_place(\n-        &mut self,\n-        ptr: NonNull<u8>,\n-        layout: Layout,\n-        new_size: usize,\n-    ) -> Result<usize, CannotReallocInPlace> {\n-        let _ = ptr;\n-        let _ = layout;\n-        let _ = new_size;\n-        Err(CannotReallocInPlace)\n-    }\n-\n-    /// Behaves like `grow_in_place`, but also ensures that the new\n-    /// contents are set to zero before being returned.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe for the same reasons that `grow_in_place` is.\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returns `Err(CannotReallocInPlace)` when the allocator is\n-    /// unable to assert that the memory block referenced by `ptr`\n-    /// could fit `layout`.\n-    ///\n-    /// Note that one cannot pass `CannotReallocInPlace` to the `handle_alloc_error`\n-    /// function; clients are expected either to be able to recover from\n-    /// `grow_in_place` failures without aborting, or to fall back on\n-    /// another reallocation method before resorting to an abort.\n-    unsafe fn grow_in_place_zeroed(\n-        &mut self,\n-        ptr: NonNull<u8>,\n-        layout: Layout,\n-        new_size: usize,\n-    ) -> Result<usize, CannotReallocInPlace> {\n-        let size = self.grow_in_place(ptr, layout, new_size)?;\n-        ptr.as_ptr().add(layout.size()).write_bytes(0, new_size - layout.size());\n-        Ok(size)\n-    }\n-\n-    /// Attempts to shrink the allocation referenced by `ptr` to fit `new_size`.\n-    ///\n-    /// If this returns `Ok`, then the allocator has asserted that the\n-    /// memory block referenced by `ptr` now fits `new_size`, and\n-    /// thus can only be used to carry data of that smaller\n-    /// layout. The returned value is the new size the allocated block.\n-    /// (The allocator is allowed to take advantage of this,\n-    /// carving off portions of the block for reuse elsewhere.) The\n-    /// truncated contents of the block within the smaller layout are\n-    /// unaltered, and ownership of block has not been transferred.\n-    ///\n-    /// If this returns `Err`, then the memory block is considered to\n-    /// still represent the original (larger) `layout`. None of the\n-    /// block has been carved off for reuse elsewhere, ownership of\n-    /// the memory block has not been transferred, and the contents of\n-    /// the memory block are unaltered.\n-    ///\n-    /// # Safety\n-    ///\n-    /// This function is unsafe because undefined behavior can result\n-    /// if the caller does not ensure all of the following:\n-    ///\n-    /// * `ptr` must be currently allocated via this allocator,\n-    ///\n-    /// * `layout` must *fit* the `ptr` (see above); note the\n-    ///   `new_size` argument need not fit it,\n-    ///\n-    /// * `new_size` must not be greater than `layout.size()`,\n-    ///\n-    /// # Errors\n-    ///\n-    /// Returns `Err(CannotReallocInPlace)` when the allocator is\n-    /// unable to assert that the memory block referenced by `ptr`\n-    /// could fit `layout`.\n-    ///\n-    /// Note that one cannot pass `CannotReallocInPlace` to the `handle_alloc_error`\n-    /// function; clients are expected either to be able to recover from\n-    /// `shrink_in_place` failures without aborting, or to fall back\n-    /// on another reallocation method before resorting to an abort.\n-    #[inline]\n-    unsafe fn shrink_in_place(\n-        &mut self,\n-        ptr: NonNull<u8>,\n-        layout: Layout,\n-        new_size: usize,\n-    ) -> Result<usize, CannotReallocInPlace> {\n-        let _ = ptr;\n-        let _ = layout;\n-        let _ = new_size;\n-        Err(CannotReallocInPlace)\n-    }\n-}"}, {"sha": "147fe696ac02fb2433f1071705f15c6882d0c37f", "filename": "src/libcore/alloc/global.rs", "status": "added", "additions": 198, "deletions": 0, "changes": 198, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibcore%2Falloc%2Fglobal.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibcore%2Falloc%2Fglobal.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Falloc%2Fglobal.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -0,0 +1,198 @@\n+use crate::alloc::Layout;\n+use crate::cmp;\n+use crate::ptr;\n+\n+/// A memory allocator that can be registered as the standard library\u2019s default\n+/// through the `#[global_allocator]` attribute.\n+///\n+/// Some of the methods require that a memory block be *currently\n+/// allocated* via an allocator. This means that:\n+///\n+/// * the starting address for that memory block was previously\n+///   returned by a previous call to an allocation method\n+///   such as `alloc`, and\n+///\n+/// * the memory block has not been subsequently deallocated, where\n+///   blocks are deallocated either by being passed to a deallocation\n+///   method such as `dealloc` or by being\n+///   passed to a reallocation method that returns a non-null pointer.\n+///\n+///\n+/// # Example\n+///\n+/// ```no_run\n+/// use std::alloc::{GlobalAlloc, Layout, alloc};\n+/// use std::ptr::null_mut;\n+///\n+/// struct MyAllocator;\n+///\n+/// unsafe impl GlobalAlloc for MyAllocator {\n+///     unsafe fn alloc(&self, _layout: Layout) -> *mut u8 { null_mut() }\n+///     unsafe fn dealloc(&self, _ptr: *mut u8, _layout: Layout) {}\n+/// }\n+///\n+/// #[global_allocator]\n+/// static A: MyAllocator = MyAllocator;\n+///\n+/// fn main() {\n+///     unsafe {\n+///         assert!(alloc(Layout::new::<u32>()).is_null())\n+///     }\n+/// }\n+/// ```\n+///\n+/// # Safety\n+///\n+/// The `GlobalAlloc` trait is an `unsafe` trait for a number of reasons, and\n+/// implementors must ensure that they adhere to these contracts:\n+///\n+/// * It's undefined behavior if global allocators unwind. This restriction may\n+///   be lifted in the future, but currently a panic from any of these\n+///   functions may lead to memory unsafety.\n+///\n+/// * `Layout` queries and calculations in general must be correct. Callers of\n+///   this trait are allowed to rely on the contracts defined on each method,\n+///   and implementors must ensure such contracts remain true.\n+#[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n+pub unsafe trait GlobalAlloc {\n+    /// Allocate memory as described by the given `layout`.\n+    ///\n+    /// Returns a pointer to newly-allocated memory,\n+    /// or null to indicate allocation failure.\n+    ///\n+    /// # Safety\n+    ///\n+    /// This function is unsafe because undefined behavior can result\n+    /// if the caller does not ensure that `layout` has non-zero size.\n+    ///\n+    /// (Extension subtraits might provide more specific bounds on\n+    /// behavior, e.g., guarantee a sentinel address or a null pointer\n+    /// in response to a zero-size allocation request.)\n+    ///\n+    /// The allocated block of memory may or may not be initialized.\n+    ///\n+    /// # Errors\n+    ///\n+    /// Returning a null pointer indicates that either memory is exhausted\n+    /// or `layout` does not meet this allocator's size or alignment constraints.\n+    ///\n+    /// Implementations are encouraged to return null on memory\n+    /// exhaustion rather than aborting, but this is not\n+    /// a strict requirement. (Specifically: it is *legal* to\n+    /// implement this trait atop an underlying native allocation\n+    /// library that aborts on memory exhaustion.)\n+    ///\n+    /// Clients wishing to abort computation in response to an\n+    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n+    /// rather than directly invoking `panic!` or similar.\n+    ///\n+    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n+    #[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n+    unsafe fn alloc(&self, layout: Layout) -> *mut u8;\n+\n+    /// Deallocate the block of memory at the given `ptr` pointer with the given `layout`.\n+    ///\n+    /// # Safety\n+    ///\n+    /// This function is unsafe because undefined behavior can result\n+    /// if the caller does not ensure all of the following:\n+    ///\n+    /// * `ptr` must denote a block of memory currently allocated via\n+    ///   this allocator,\n+    ///\n+    /// * `layout` must be the same layout that was used\n+    ///   to allocate that block of memory,\n+    #[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n+    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout);\n+\n+    /// Behaves like `alloc`, but also ensures that the contents\n+    /// are set to zero before being returned.\n+    ///\n+    /// # Safety\n+    ///\n+    /// This function is unsafe for the same reasons that `alloc` is.\n+    /// However the allocated block of memory is guaranteed to be initialized.\n+    ///\n+    /// # Errors\n+    ///\n+    /// Returning a null pointer indicates that either memory is exhausted\n+    /// or `layout` does not meet allocator's size or alignment constraints,\n+    /// just as in `alloc`.\n+    ///\n+    /// Clients wishing to abort computation in response to an\n+    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n+    /// rather than directly invoking `panic!` or similar.\n+    ///\n+    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n+    #[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n+    unsafe fn alloc_zeroed(&self, layout: Layout) -> *mut u8 {\n+        let size = layout.size();\n+        let ptr = self.alloc(layout);\n+        if !ptr.is_null() {\n+            ptr::write_bytes(ptr, 0, size);\n+        }\n+        ptr\n+    }\n+\n+    /// Shrink or grow a block of memory to the given `new_size`.\n+    /// The block is described by the given `ptr` pointer and `layout`.\n+    ///\n+    /// If this returns a non-null pointer, then ownership of the memory block\n+    /// referenced by `ptr` has been transferred to this allocator.\n+    /// The memory may or may not have been deallocated,\n+    /// and should be considered unusable (unless of course it was\n+    /// transferred back to the caller again via the return value of\n+    /// this method). The new memory block is allocated with `layout`, but\n+    /// with the `size` updated to `new_size`.\n+    ///\n+    /// If this method returns null, then ownership of the memory\n+    /// block has not been transferred to this allocator, and the\n+    /// contents of the memory block are unaltered.\n+    ///\n+    /// # Safety\n+    ///\n+    /// This function is unsafe because undefined behavior can result\n+    /// if the caller does not ensure all of the following:\n+    ///\n+    /// * `ptr` must be currently allocated via this allocator,\n+    ///\n+    /// * `layout` must be the same layout that was used\n+    ///   to allocate that block of memory,\n+    ///\n+    /// * `new_size` must be greater than zero.\n+    ///\n+    /// * `new_size`, when rounded up to the nearest multiple of `layout.align()`,\n+    ///   must not overflow (i.e., the rounded value must be less than `usize::MAX`).\n+    ///\n+    /// (Extension subtraits might provide more specific bounds on\n+    /// behavior, e.g., guarantee a sentinel address or a null pointer\n+    /// in response to a zero-size allocation request.)\n+    ///\n+    /// # Errors\n+    ///\n+    /// Returns null if the new layout does not meet the size\n+    /// and alignment constraints of the allocator, or if reallocation\n+    /// otherwise fails.\n+    ///\n+    /// Implementations are encouraged to return null on memory\n+    /// exhaustion rather than panicking or aborting, but this is not\n+    /// a strict requirement. (Specifically: it is *legal* to\n+    /// implement this trait atop an underlying native allocation\n+    /// library that aborts on memory exhaustion.)\n+    ///\n+    /// Clients wishing to abort computation in response to a\n+    /// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n+    /// rather than directly invoking `panic!` or similar.\n+    ///\n+    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n+    #[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n+    unsafe fn realloc(&self, ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {\n+        let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n+        let new_ptr = self.alloc(new_layout);\n+        if !new_ptr.is_null() {\n+            ptr::copy_nonoverlapping(ptr, new_ptr, cmp::min(layout.size(), new_size));\n+            self.dealloc(ptr, layout);\n+        }\n+        new_ptr\n+    }\n+}"}, {"sha": "fa644cfe99ed63572cdb8d0fbe034e368300aca3", "filename": "src/libcore/alloc/layout.rs", "status": "added", "additions": 346, "deletions": 0, "changes": 346, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibcore%2Falloc%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibcore%2Falloc%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Falloc%2Flayout.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -0,0 +1,346 @@\n+// ignore-tidy-undocumented-unsafe\n+\n+use crate::cmp;\n+use crate::fmt;\n+use crate::mem;\n+use crate::num::NonZeroUsize;\n+use crate::ptr::NonNull;\n+\n+const fn size_align<T>() -> (usize, usize) {\n+    (mem::size_of::<T>(), mem::align_of::<T>())\n+}\n+\n+/// Layout of a block of memory.\n+///\n+/// An instance of `Layout` describes a particular layout of memory.\n+/// You build a `Layout` up as an input to give to an allocator.\n+///\n+/// All layouts have an associated size and a power-of-two alignment.\n+///\n+/// (Note that layouts are *not* required to have non-zero size,\n+/// even though `GlobalAlloc` requires that all memory requests\n+/// be non-zero in size. A caller must either ensure that conditions\n+/// like this are met, use specific allocators with looser\n+/// requirements, or use the more lenient `AllocRef` interface.)\n+#[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+#[lang = \"alloc_layout\"]\n+pub struct Layout {\n+    // size of the requested block of memory, measured in bytes.\n+    size_: usize,\n+\n+    // alignment of the requested block of memory, measured in bytes.\n+    // we ensure that this is always a power-of-two, because API's\n+    // like `posix_memalign` require it and it is a reasonable\n+    // constraint to impose on Layout constructors.\n+    //\n+    // (However, we do not analogously require `align >= sizeof(void*)`,\n+    //  even though that is *also* a requirement of `posix_memalign`.)\n+    align_: NonZeroUsize,\n+}\n+\n+impl Layout {\n+    /// Constructs a `Layout` from a given `size` and `align`,\n+    /// or returns `LayoutErr` if any of the following conditions\n+    /// are not met:\n+    ///\n+    /// * `align` must not be zero,\n+    ///\n+    /// * `align` must be a power of two,\n+    ///\n+    /// * `size`, when rounded up to the nearest multiple of `align`,\n+    ///    must not overflow (i.e., the rounded value must be less than\n+    ///    or equal to `usize::MAX`).\n+    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+    #[rustc_const_unstable(feature = \"const_alloc_layout\", issue = \"67521\")]\n+    #[inline]\n+    pub const fn from_size_align(size: usize, align: usize) -> Result<Self, LayoutErr> {\n+        if !align.is_power_of_two() {\n+            return Err(LayoutErr { private: () });\n+        }\n+\n+        // (power-of-two implies align != 0.)\n+\n+        // Rounded up size is:\n+        //   size_rounded_up = (size + align - 1) & !(align - 1);\n+        //\n+        // We know from above that align != 0. If adding (align - 1)\n+        // does not overflow, then rounding up will be fine.\n+        //\n+        // Conversely, &-masking with !(align - 1) will subtract off\n+        // only low-order-bits. Thus if overflow occurs with the sum,\n+        // the &-mask cannot subtract enough to undo that overflow.\n+        //\n+        // Above implies that checking for summation overflow is both\n+        // necessary and sufficient.\n+        if size > usize::MAX - (align - 1) {\n+            return Err(LayoutErr { private: () });\n+        }\n+\n+        unsafe { Ok(Layout::from_size_align_unchecked(size, align)) }\n+    }\n+\n+    /// Creates a layout, bypassing all checks.\n+    ///\n+    /// # Safety\n+    ///\n+    /// This function is unsafe as it does not verify the preconditions from\n+    /// [`Layout::from_size_align`](#method.from_size_align).\n+    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+    #[rustc_const_stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+    #[inline]\n+    pub const unsafe fn from_size_align_unchecked(size: usize, align: usize) -> Self {\n+        Layout { size_: size, align_: NonZeroUsize::new_unchecked(align) }\n+    }\n+\n+    /// The minimum size in bytes for a memory block of this layout.\n+    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+    #[rustc_const_unstable(feature = \"const_alloc_layout\", issue = \"67521\")]\n+    #[inline]\n+    pub const fn size(&self) -> usize {\n+        self.size_\n+    }\n+\n+    /// The minimum byte alignment for a memory block of this layout.\n+    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+    #[rustc_const_unstable(feature = \"const_alloc_layout\", issue = \"67521\")]\n+    #[inline]\n+    pub const fn align(&self) -> usize {\n+        self.align_.get()\n+    }\n+\n+    /// Constructs a `Layout` suitable for holding a value of type `T`.\n+    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+    #[rustc_const_stable(feature = \"alloc_layout_const_new\", since = \"1.42.0\")]\n+    #[inline]\n+    pub const fn new<T>() -> Self {\n+        let (size, align) = size_align::<T>();\n+        // Note that the align is guaranteed by rustc to be a power of two and\n+        // the size+align combo is guaranteed to fit in our address space. As a\n+        // result use the unchecked constructor here to avoid inserting code\n+        // that panics if it isn't optimized well enough.\n+        unsafe { Layout::from_size_align_unchecked(size, align) }\n+    }\n+\n+    /// Produces layout describing a record that could be used to\n+    /// allocate backing structure for `T` (which could be a trait\n+    /// or other unsized type like a slice).\n+    #[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+    #[inline]\n+    pub fn for_value<T: ?Sized>(t: &T) -> Self {\n+        let (size, align) = (mem::size_of_val(t), mem::align_of_val(t));\n+        // See rationale in `new` for why this is using an unsafe variant below\n+        debug_assert!(Layout::from_size_align(size, align).is_ok());\n+        unsafe { Layout::from_size_align_unchecked(size, align) }\n+    }\n+\n+    /// Creates a `NonNull` that is dangling, but well-aligned for this Layout.\n+    ///\n+    /// Note that the pointer value may potentially represent a valid pointer,\n+    /// which means this must not be used as a \"not yet initialized\"\n+    /// sentinel value. Types that lazily allocate must track initialization by\n+    /// some other means.\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[inline]\n+    pub const fn dangling(&self) -> NonNull<u8> {\n+        // align is non-zero and a power of two\n+        unsafe { NonNull::new_unchecked(self.align() as *mut u8) }\n+    }\n+\n+    /// Creates a layout describing the record that can hold a value\n+    /// of the same layout as `self`, but that also is aligned to\n+    /// alignment `align` (measured in bytes).\n+    ///\n+    /// If `self` already meets the prescribed alignment, then returns\n+    /// `self`.\n+    ///\n+    /// Note that this method does not add any padding to the overall\n+    /// size, regardless of whether the returned layout has a different\n+    /// alignment. In other words, if `K` has size 16, `K.align_to(32)`\n+    /// will *still* have size 16.\n+    ///\n+    /// Returns an error if the combination of `self.size()` and the given\n+    /// `align` violates the conditions listed in\n+    /// [`Layout::from_size_align`](#method.from_size_align).\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[inline]\n+    pub fn align_to(&self, align: usize) -> Result<Self, LayoutErr> {\n+        Layout::from_size_align(self.size(), cmp::max(self.align(), align))\n+    }\n+\n+    /// Returns the amount of padding we must insert after `self`\n+    /// to ensure that the following address will satisfy `align`\n+    /// (measured in bytes).\n+    ///\n+    /// e.g., if `self.size()` is 9, then `self.padding_needed_for(4)`\n+    /// returns 3, because that is the minimum number of bytes of\n+    /// padding required to get a 4-aligned address (assuming that the\n+    /// corresponding memory block starts at a 4-aligned address).\n+    ///\n+    /// The return value of this function has no meaning if `align` is\n+    /// not a power-of-two.\n+    ///\n+    /// Note that the utility of the returned value requires `align`\n+    /// to be less than or equal to the alignment of the starting\n+    /// address for the whole allocated block of memory. One way to\n+    /// satisfy this constraint is to ensure `align <= self.align()`.\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[rustc_const_unstable(feature = \"const_alloc_layout\", issue = \"67521\")]\n+    #[inline]\n+    pub const fn padding_needed_for(&self, align: usize) -> usize {\n+        let len = self.size();\n+\n+        // Rounded up value is:\n+        //   len_rounded_up = (len + align - 1) & !(align - 1);\n+        // and then we return the padding difference: `len_rounded_up - len`.\n+        //\n+        // We use modular arithmetic throughout:\n+        //\n+        // 1. align is guaranteed to be > 0, so align - 1 is always\n+        //    valid.\n+        //\n+        // 2. `len + align - 1` can overflow by at most `align - 1`,\n+        //    so the &-mask with `!(align - 1)` will ensure that in the\n+        //    case of overflow, `len_rounded_up` will itself be 0.\n+        //    Thus the returned padding, when added to `len`, yields 0,\n+        //    which trivially satisfies the alignment `align`.\n+        //\n+        // (Of course, attempts to allocate blocks of memory whose\n+        // size and padding overflow in the above manner should cause\n+        // the allocator to yield an error anyway.)\n+\n+        let len_rounded_up = len.wrapping_add(align).wrapping_sub(1) & !align.wrapping_sub(1);\n+        len_rounded_up.wrapping_sub(len)\n+    }\n+\n+    /// Creates a layout by rounding the size of this layout up to a multiple\n+    /// of the layout's alignment.\n+    ///\n+    /// This is equivalent to adding the result of `padding_needed_for`\n+    /// to the layout's current size.\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[inline]\n+    pub fn pad_to_align(&self) -> Layout {\n+        let pad = self.padding_needed_for(self.align());\n+        // This cannot overflow. Quoting from the invariant of Layout:\n+        // > `size`, when rounded up to the nearest multiple of `align`,\n+        // > must not overflow (i.e., the rounded value must be less than\n+        // > `usize::MAX`)\n+        let new_size = self.size() + pad;\n+\n+        Layout::from_size_align(new_size, self.align()).unwrap()\n+    }\n+\n+    /// Creates a layout describing the record for `n` instances of\n+    /// `self`, with a suitable amount of padding between each to\n+    /// ensure that each instance is given its requested size and\n+    /// alignment. On success, returns `(k, offs)` where `k` is the\n+    /// layout of the array and `offs` is the distance between the start\n+    /// of each element in the array.\n+    ///\n+    /// On arithmetic overflow, returns `LayoutErr`.\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[inline]\n+    pub fn repeat(&self, n: usize) -> Result<(Self, usize), LayoutErr> {\n+        // This cannot overflow. Quoting from the invariant of Layout:\n+        // > `size`, when rounded up to the nearest multiple of `align`,\n+        // > must not overflow (i.e., the rounded value must be less than\n+        // > `usize::MAX`)\n+        let padded_size = self.size() + self.padding_needed_for(self.align());\n+        let alloc_size = padded_size.checked_mul(n).ok_or(LayoutErr { private: () })?;\n+\n+        unsafe {\n+            // self.align is already known to be valid and alloc_size has been\n+            // padded already.\n+            Ok((Layout::from_size_align_unchecked(alloc_size, self.align()), padded_size))\n+        }\n+    }\n+\n+    /// Creates a layout describing the record for `self` followed by\n+    /// `next`, including any necessary padding to ensure that `next`\n+    /// will be properly aligned. Note that the resulting layout will\n+    /// satisfy the alignment properties of both `self` and `next`.\n+    ///\n+    /// The resulting layout will be the same as that of a C struct containing\n+    /// two fields with the layouts of `self` and `next`, in that order.\n+    ///\n+    /// Returns `Some((k, offset))`, where `k` is layout of the concatenated\n+    /// record and `offset` is the relative location, in bytes, of the\n+    /// start of the `next` embedded within the concatenated record\n+    /// (assuming that the record itself starts at offset 0).\n+    ///\n+    /// On arithmetic overflow, returns `LayoutErr`.\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[inline]\n+    pub fn extend(&self, next: Self) -> Result<(Self, usize), LayoutErr> {\n+        let new_align = cmp::max(self.align(), next.align());\n+        let pad = self.padding_needed_for(next.align());\n+\n+        let offset = self.size().checked_add(pad).ok_or(LayoutErr { private: () })?;\n+        let new_size = offset.checked_add(next.size()).ok_or(LayoutErr { private: () })?;\n+\n+        let layout = Layout::from_size_align(new_size, new_align)?;\n+        Ok((layout, offset))\n+    }\n+\n+    /// Creates a layout describing the record for `n` instances of\n+    /// `self`, with no padding between each instance.\n+    ///\n+    /// Note that, unlike `repeat`, `repeat_packed` does not guarantee\n+    /// that the repeated instances of `self` will be properly\n+    /// aligned, even if a given instance of `self` is properly\n+    /// aligned. In other words, if the layout returned by\n+    /// `repeat_packed` is used to allocate an array, it is not\n+    /// guaranteed that all elements in the array will be properly\n+    /// aligned.\n+    ///\n+    /// On arithmetic overflow, returns `LayoutErr`.\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[inline]\n+    pub fn repeat_packed(&self, n: usize) -> Result<Self, LayoutErr> {\n+        let size = self.size().checked_mul(n).ok_or(LayoutErr { private: () })?;\n+        Layout::from_size_align(size, self.align())\n+    }\n+\n+    /// Creates a layout describing the record for `self` followed by\n+    /// `next` with no additional padding between the two. Since no\n+    /// padding is inserted, the alignment of `next` is irrelevant,\n+    /// and is not incorporated *at all* into the resulting layout.\n+    ///\n+    /// On arithmetic overflow, returns `LayoutErr`.\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[inline]\n+    pub fn extend_packed(&self, next: Self) -> Result<Self, LayoutErr> {\n+        let new_size = self.size().checked_add(next.size()).ok_or(LayoutErr { private: () })?;\n+        Layout::from_size_align(new_size, self.align())\n+    }\n+\n+    /// Creates a layout describing the record for a `[T; n]`.\n+    ///\n+    /// On arithmetic overflow, returns `LayoutErr`.\n+    #[unstable(feature = \"alloc_layout_extra\", issue = \"55724\")]\n+    #[inline]\n+    pub fn array<T>(n: usize) -> Result<Self, LayoutErr> {\n+        Layout::new::<T>().repeat(n).map(|(k, offs)| {\n+            debug_assert!(offs == mem::size_of::<T>());\n+            k\n+        })\n+    }\n+}\n+\n+/// The parameters given to `Layout::from_size_align`\n+/// or some other `Layout` constructor\n+/// do not satisfy its documented constraints.\n+#[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+#[derive(Clone, PartialEq, Eq, Debug)]\n+pub struct LayoutErr {\n+    private: (),\n+}\n+\n+// (we need this for downstream impl of trait Error)\n+#[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+impl fmt::Display for LayoutErr {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        f.write_str(\"invalid parameters to Layout::from_size_align\")\n+    }\n+}"}, {"sha": "e8c4b68c64890d259c0de425407ce2c3abbd9591", "filename": "src/libcore/alloc/mod.rs", "status": "added", "additions": 362, "deletions": 0, "changes": 362, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibcore%2Falloc%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibcore%2Falloc%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Falloc%2Fmod.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -0,0 +1,362 @@\n+//! Memory allocation APIs\n+\n+#![stable(feature = \"alloc_module\", since = \"1.28.0\")]\n+\n+mod global;\n+mod layout;\n+\n+#[stable(feature = \"global_alloc\", since = \"1.28.0\")]\n+pub use self::global::GlobalAlloc;\n+#[stable(feature = \"alloc_layout\", since = \"1.28.0\")]\n+pub use self::layout::{Layout, LayoutErr};\n+\n+use crate::fmt;\n+use crate::ptr::{self, NonNull};\n+\n+/// The `AllocErr` error indicates an allocation failure\n+/// that may be due to resource exhaustion or to\n+/// something wrong when combining the given input arguments with this\n+/// allocator.\n+#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n+#[derive(Clone, PartialEq, Eq, Debug)]\n+pub struct AllocErr;\n+\n+// (we need this for downstream impl of trait Error)\n+#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n+impl fmt::Display for AllocErr {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        f.write_str(\"memory allocation failed\")\n+    }\n+}\n+\n+/// A desired initial state for allocated memory.\n+#[derive(Debug, Copy, Clone, PartialEq, Eq)]\n+#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n+pub enum AllocInit {\n+    /// The contents of the new memory are undefined.\n+    ///\n+    /// Reading uninitialized memory is Undefined Behavior; it must be initialized before use.\n+    Uninitialized,\n+    /// The new memory is guaranteed to be zeroed.\n+    Zeroed,\n+}\n+\n+impl AllocInit {\n+    /// Initialize the specified memory block.\n+    ///\n+    /// This behaves like calling [`AllocInit::init_offset(memory, 0)`][off].\n+    ///\n+    /// [off]: AllocInit::init_offset\n+    ///\n+    /// # Safety\n+    ///\n+    /// * `memory.ptr` must be [valid] for writes of `memory.size` bytes.\n+    ///\n+    /// [valid]: ../../core/ptr/index.html#safety\n+    #[inline]\n+    #[unstable(feature = \"allocator_api\", issue = \"32838\")]\n+    pub unsafe fn init(self, memory: MemoryBlock) {\n+        self.init_offset(memory, 0)\n+    }\n+\n+    /// Initialize the memory block like specified by `init` at the specified `offset`.\n+    ///\n+    /// This is a no-op for [`AllocInit::Uninitialized`][] and writes zeroes for\n+    /// [`AllocInit::Zeroed`][] at `ptr + offset` until `ptr + layout.size()`.\n+    ///\n+    /// # Safety\n+    ///\n+    /// * `memory.ptr` must be [valid] for writes of `memory.size` bytes.\n+    /// * `offset` must be smaller than or equal to `memory.size`\n+    ///\n+    /// [valid]: ../../core/ptr/index.html#safety\n+    #[inline]\n+    #[unstable(feature = \"allocator_api\", issue = \"32838\")]\n+    pub unsafe fn init_offset(self, memory: MemoryBlock, offset: usize) {\n+        debug_assert!(\n+            offset <= memory.size,\n+            \"`offset` must be smaller than or equal to `memory.size`\"\n+        );\n+        match self {\n+            AllocInit::Uninitialized => (),\n+            AllocInit::Zeroed => {\n+                memory.ptr.as_ptr().add(offset).write_bytes(0, memory.size - offset)\n+            }\n+        }\n+    }\n+}\n+\n+/// Represents a block of allocated memory returned by an allocator.\n+#[derive(Debug, Copy, Clone)]\n+#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n+pub struct MemoryBlock {\n+    pub ptr: NonNull<u8>,\n+    pub size: usize,\n+}\n+\n+/// A placement constraint when growing or shrinking an existing allocation.\n+#[derive(Debug, Copy, Clone, PartialEq, Eq)]\n+#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n+pub enum ReallocPlacement {\n+    /// The allocator is allowed to move the allocation to a different memory address.\n+    // FIXME(wg-allocators#46): Add a section to the module documentation \"What is a legal\n+    //                          allocator\" and link it at \"valid location\".\n+    ///\n+    /// If the allocation _does_ move, it's the responsibility of the allocator\n+    /// to also move the data from the previous location to the new location.\n+    MayMove,\n+    /// The address of the new memory must not change.\n+    ///\n+    /// If the allocation would have to be moved to a new location to fit, the\n+    /// reallocation request will fail.\n+    InPlace,\n+}\n+\n+/// An implementation of `AllocRef` can allocate, grow, shrink, and deallocate arbitrary blocks of\n+/// data described via [`Layout`][].\n+///\n+/// `AllocRef` is designed to be implemented on ZSTs, references, or smart pointers because having\n+/// an allocator like `MyAlloc([u8; N])` cannot be moved, without updating the pointers to the\n+/// allocated memory.\n+///\n+/// Unlike [`GlobalAlloc`][], zero-sized allocations are allowed in `AllocRef`. If an underlying\n+/// allocator does not support this (like jemalloc) or return a null pointer (such as\n+/// `libc::malloc`), this case must be caught.\n+///\n+/// ### Currently allocated memory\n+///\n+/// Some of the methods require that a memory block be *currently allocated* via an allocator. This\n+/// means that:\n+///\n+/// * the starting address for that memory block was previously returned by [`alloc`], [`grow`], or\n+///   [`shrink`], and\n+///\n+/// * the memory block has not been subsequently deallocated, where blocks are either deallocated\n+///   directly by being passed to [`dealloc`] or were changed by being passed to [`grow`] or\n+///   [`shrink`] that returns `Ok`. If `grow` or `shrink` have returned `Err`, the passed pointer\n+///   remains valid.\n+///\n+/// [`alloc`]: AllocRef::alloc\n+/// [`grow`]: AllocRef::grow\n+/// [`shrink`]: AllocRef::shrink\n+/// [`dealloc`]: AllocRef::dealloc\n+///\n+/// ### Memory fitting\n+///\n+/// Some of the methods require that a layout *fit* a memory block. What it means for a layout to\n+/// \"fit\" a memory block means (or equivalently, for a memory block to \"fit\" a layout) is that the\n+/// following conditions must hold:\n+///\n+/// * The block must be allocated with the same alignment as [`layout.align()`], and\n+///\n+/// * The provided [`layout.size()`] must fall in the range `min ..= max`, where:\n+///   - `min` is the size of the layout most recently used to allocate the block, and\n+///   - `max` is the latest actual size returned from [`alloc`], [`grow`], or [`shrink`].\n+///\n+/// [`layout.align()`]: Layout::align\n+/// [`layout.size()`]: Layout::size\n+///\n+/// # Safety\n+///\n+/// * Memory blocks returned from an allocator must point to valid memory and retain their validity\n+///   until the instance and all of its clones are dropped, and\n+///\n+/// * cloning or moving the allocator must not invalidate memory blocks returned from this\n+///   allocator. A cloned allocator must behave like the same allocator.\n+///\n+/// * any pointer to a memory block which is [*currently allocated*] may be passed to any other\n+///   method of the allocator.\n+///\n+/// [*currently allocated*]: #currently-allocated-memory\n+#[unstable(feature = \"allocator_api\", issue = \"32838\")]\n+pub unsafe trait AllocRef {\n+    /// On success, returns a memory block meeting the size and alignment guarantees of `layout`.\n+    ///\n+    /// The returned block may have a larger size than specified by `layout.size()` and is\n+    /// initialized as specified by [`init`], all the way up to the returned size of the block.\n+    ///\n+    /// [`init`]: AllocInit\n+    ///\n+    /// # Errors\n+    ///\n+    /// Returning `Err` indicates that either memory is exhausted or `layout` does not meet\n+    /// allocator's size or alignment constraints.\n+    ///\n+    /// Implementations are encouraged to return `Err` on memory exhaustion rather than panicking or\n+    /// aborting, but this is not a strict requirement. (Specifically: it is *legal* to implement\n+    /// this trait atop an underlying native allocation library that aborts on memory exhaustion.)\n+    ///\n+    /// Clients wishing to abort computation in response to an allocation error are encouraged to\n+    /// call the [`handle_alloc_error`] function, rather than directly invoking `panic!` or similar.\n+    ///\n+    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n+    fn alloc(&mut self, layout: Layout, init: AllocInit) -> Result<MemoryBlock, AllocErr>;\n+\n+    /// Deallocates the memory denoted by `memory`.\n+    ///\n+    /// # Safety\n+    ///\n+    /// `memory` must be a memory block returned by this allocator.\n+    unsafe fn dealloc(&mut self, ptr: NonNull<u8>, layout: Layout);\n+\n+    /// Attempts to extend the memory block.\n+    ///\n+    /// Returns a new memory block containing a pointer and the actual size of the allocated\n+    /// block. The pointer is suitable for holding data described by a new layout with `layout`\u2019s\n+    /// alignment and a size given by `new_size`. To accomplish this, the allocator may extend the\n+    /// allocation referenced by `ptr` to fit the new layout. If the [`placement`] is\n+    /// [`InPlace`], the returned pointer is guaranteed to be the same as the passed `ptr`.\n+    ///\n+    /// If `ReallocPlacement::MayMove` is used then ownership of the memory block referenced by `ptr`\n+    /// is transferred to this allocator. The memory may or may not be freed, and should be\n+    /// considered unusable (unless of course it is transferred back to the caller again via the\n+    /// return value of this method).\n+    ///\n+    /// If this method returns `Err`, then ownership of the memory block has not been transferred to\n+    /// this allocator, and the contents of the memory block are unaltered.\n+    ///\n+    /// The memory block will contain the following contents after a successful call to `grow`:\n+    ///   * Bytes `0..layout.size()` are preserved from the original allocation.\n+    ///   * Bytes `layout.size()..old_size` will either be preserved or initialized according to\n+    ///     [`init`], depending on the allocator implementation. `old_size` refers to the size of\n+    ///     the `MemoryBlock` prior to the `grow` call, which may be larger than the size\n+    ///     that was originally requested when it was allocated.\n+    ///   * Bytes `old_size..new_size` are initialized according to [`init`]. `new_size` refers to\n+    ///     the size of the `MemoryBlock` returned by the `grow` call.\n+    ///\n+    /// [`InPlace`]: ReallocPlacement::InPlace\n+    /// [`placement`]: ReallocPlacement\n+    /// [`init`]: AllocInit\n+    ///\n+    /// # Safety\n+    ///\n+    /// * `ptr` must be [*currently allocated*] via this allocator,\n+    /// * `layout` must [*fit*] the `ptr`. (The `new_size` argument need not fit it.)\n+    // We can't require that `new_size` is strictly greater than `memory.size` because of ZSTs.\n+    // An alternative would be\n+    // * `new_size must be strictly greater than `memory.size` or both are zero\n+    /// * `new_size` must be greater than or equal to `layout.size()`\n+    /// * `new_size`, when rounded up to the nearest multiple of `layout.align()`, must not overflow\n+    ///   (i.e., the rounded value must be less than `usize::MAX`).\n+    ///\n+    /// [*currently allocated*]: #currently-allocated-memory\n+    /// [*fit*]: #memory-fitting\n+    ///\n+    /// # Errors\n+    ///\n+    /// Returns `Err` if the new layout does not meet the allocator's size and alignment\n+    /// constraints of the allocator, or if growing otherwise fails.\n+    ///\n+    /// Implementations are encouraged to return `Err` on memory exhaustion rather than panicking or\n+    /// aborting, but this is not a strict requirement. (Specifically: it is *legal* to implement\n+    /// this trait atop an underlying native allocation library that aborts on memory exhaustion.)\n+    ///\n+    /// Clients wishing to abort computation in response to an allocation error are encouraged to\n+    /// call the [`handle_alloc_error`] function, rather than directly invoking `panic!` or similar.\n+    ///\n+    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n+    unsafe fn grow(\n+        &mut self,\n+        ptr: NonNull<u8>,\n+        layout: Layout,\n+        new_size: usize,\n+        placement: ReallocPlacement,\n+        init: AllocInit,\n+    ) -> Result<MemoryBlock, AllocErr> {\n+        match placement {\n+            ReallocPlacement::InPlace => Err(AllocErr),\n+            ReallocPlacement::MayMove => {\n+                let size = layout.size();\n+                debug_assert!(\n+                    new_size >= size,\n+                    \"`new_size` must be greater than or equal to `layout.size()`\"\n+                );\n+\n+                if new_size == size {\n+                    return Ok(MemoryBlock { ptr, size });\n+                }\n+\n+                let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n+                let new_memory = self.alloc(new_layout, init)?;\n+                ptr::copy_nonoverlapping(ptr.as_ptr(), new_memory.ptr.as_ptr(), size);\n+                self.dealloc(ptr, layout);\n+                Ok(new_memory)\n+            }\n+        }\n+    }\n+\n+    /// Attempts to shrink the memory block.\n+    ///\n+    /// Returns a new memory block containing a pointer and the actual size of the allocated\n+    /// block. The pointer is suitable for holding data described by a new layout with `layout`\u2019s\n+    /// alignment and a size given by `new_size`. To accomplish this, the allocator may shrink the\n+    /// allocation referenced by `ptr` to fit the new layout. If the [`placement`] is\n+    /// [`InPlace`], the returned pointer is guaranteed to be the same as the passed `ptr`.\n+    ///\n+    /// If this returns `Ok`, then ownership of the memory block referenced by `ptr` has been\n+    /// transferred to this allocator. The memory may or may not have been freed, and should be\n+    /// considered unusable unless it was transferred back to the caller again via the\n+    /// return value of this method.\n+    ///\n+    /// If this method returns `Err`, then ownership of the memory block has not been transferred to\n+    /// this allocator, and the contents of the memory block are unaltered.\n+    ///\n+    /// The behavior of how the allocator tries to shrink the memory is specified by [`placement`].\n+    ///\n+    /// [`InPlace`]: ReallocPlacement::InPlace\n+    /// [`placement`]: ReallocPlacement\n+    ///\n+    /// # Safety\n+    ///\n+    /// * `ptr` must be [*currently allocated*] via this allocator,\n+    /// * `layout` must [*fit*] the `ptr`. (The `new_size` argument need not fit it.)\n+    // We can't require that `new_size` is strictly smaller than `memory.size` because of ZSTs.\n+    // An alternative would be\n+    // * `new_size must be strictly smaller than `memory.size` or both are zero\n+    /// * `new_size` must be smaller than or equal to `layout.size()`\n+    ///\n+    /// [*currently allocated*]: #currently-allocated-memory\n+    /// [*fit*]: #memory-fitting\n+    ///\n+    /// # Errors\n+    ///\n+    /// Returns `Err` if the new layout does not meet the allocator's size and alignment\n+    /// constraints of the allocator, or if growing otherwise fails.\n+    ///\n+    /// Implementations are encouraged to return `Err` on memory exhaustion rather than panicking or\n+    /// aborting, but this is not a strict requirement. (Specifically: it is *legal* to implement\n+    /// this trait atop an underlying native allocation library that aborts on memory exhaustion.)\n+    ///\n+    /// Clients wishing to abort computation in response to an allocation error are encouraged to\n+    /// call the [`handle_alloc_error`] function, rather than directly invoking `panic!` or similar.\n+    ///\n+    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n+    unsafe fn shrink(\n+        &mut self,\n+        ptr: NonNull<u8>,\n+        layout: Layout,\n+        new_size: usize,\n+        placement: ReallocPlacement,\n+    ) -> Result<MemoryBlock, AllocErr> {\n+        match placement {\n+            ReallocPlacement::InPlace => Err(AllocErr),\n+            ReallocPlacement::MayMove => {\n+                let size = layout.size();\n+                debug_assert!(\n+                    new_size <= size,\n+                    \"`new_size` must be smaller than or equal to `layout.size()`\"\n+                );\n+\n+                if new_size == size {\n+                    return Ok(MemoryBlock { ptr, size });\n+                }\n+\n+                let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n+                let new_memory = self.alloc(new_layout, AllocInit::Uninitialized)?;\n+                ptr::copy_nonoverlapping(ptr.as_ptr(), new_memory.ptr.as_ptr(), new_size);\n+                self.dealloc(ptr, layout);\n+                Ok(new_memory)\n+            }\n+        }\n+    }\n+}"}, {"sha": "bbbf6f7251e321ca70545e1d14c0879b040e30c2", "filename": "src/libstd/alloc.rs", "status": "modified", "additions": 73, "deletions": 34, "changes": 107, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibstd%2Falloc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibstd%2Falloc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Falloc.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -61,6 +61,7 @@\n \n #![stable(feature = \"alloc_module\", since = \"1.28.0\")]\n \n+use core::intrinsics;\n use core::ptr::NonNull;\n use core::sync::atomic::{AtomicPtr, Ordering};\n use core::{mem, ptr};\n@@ -138,59 +139,99 @@ pub struct System;\n #[unstable(feature = \"allocator_api\", issue = \"32838\")]\n unsafe impl AllocRef for System {\n     #[inline]\n-    fn alloc(&mut self, layout: Layout) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        if layout.size() == 0 {\n-            Ok((layout.dangling(), 0))\n-        } else {\n-            unsafe {\n-                NonNull::new(GlobalAlloc::alloc(self, layout))\n-                    .ok_or(AllocErr)\n-                    .map(|p| (p, layout.size()))\n+    fn alloc(&mut self, layout: Layout, init: AllocInit) -> Result<MemoryBlock, AllocErr> {\n+        unsafe {\n+            let size = layout.size();\n+            if size == 0 {\n+                Ok(MemoryBlock { ptr: layout.dangling(), size: 0 })\n+            } else {\n+                let raw_ptr = match init {\n+                    AllocInit::Uninitialized => GlobalAlloc::alloc(self, layout),\n+                    AllocInit::Zeroed => GlobalAlloc::alloc_zeroed(self, layout),\n+                };\n+                let ptr = NonNull::new(raw_ptr).ok_or(AllocErr)?;\n+                Ok(MemoryBlock { ptr, size })\n             }\n         }\n     }\n \n     #[inline]\n-    fn alloc_zeroed(&mut self, layout: Layout) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        if layout.size() == 0 {\n-            Ok((layout.dangling(), 0))\n-        } else {\n-            unsafe {\n-                NonNull::new(GlobalAlloc::alloc_zeroed(self, layout))\n-                    .ok_or(AllocErr)\n-                    .map(|p| (p, layout.size()))\n-            }\n+    unsafe fn dealloc(&mut self, ptr: NonNull<u8>, layout: Layout) {\n+        if layout.size() != 0 {\n+            GlobalAlloc::dealloc(self, ptr.as_ptr(), layout)\n         }\n     }\n \n     #[inline]\n-    unsafe fn dealloc(&mut self, ptr: NonNull<u8>, layout: Layout) {\n-        if layout.size() != 0 {\n-            GlobalAlloc::dealloc(self, ptr.as_ptr(), layout)\n+    unsafe fn grow(\n+        &mut self,\n+        ptr: NonNull<u8>,\n+        layout: Layout,\n+        new_size: usize,\n+        placement: ReallocPlacement,\n+        init: AllocInit,\n+    ) -> Result<MemoryBlock, AllocErr> {\n+        let size = layout.size();\n+        debug_assert!(\n+            new_size >= size,\n+            \"`new_size` must be greater than or equal to `memory.size()`\"\n+        );\n+\n+        if size == new_size {\n+            return Ok(MemoryBlock { ptr, size });\n+        }\n+\n+        match placement {\n+            ReallocPlacement::InPlace => Err(AllocErr),\n+            ReallocPlacement::MayMove if layout.size() == 0 => {\n+                let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n+                self.alloc(new_layout, init)\n+            }\n+            ReallocPlacement::MayMove => {\n+                // `realloc` probably checks for `new_size > size` or something similar.\n+                intrinsics::assume(new_size > size);\n+                let ptr = GlobalAlloc::realloc(self, ptr.as_ptr(), layout, new_size);\n+                let memory =\n+                    MemoryBlock { ptr: NonNull::new(ptr).ok_or(AllocErr)?, size: new_size };\n+                init.init_offset(memory, size);\n+                Ok(memory)\n+            }\n         }\n     }\n \n     #[inline]\n-    unsafe fn realloc(\n+    unsafe fn shrink(\n         &mut self,\n         ptr: NonNull<u8>,\n         layout: Layout,\n         new_size: usize,\n-    ) -> Result<(NonNull<u8>, usize), AllocErr> {\n-        match (layout.size(), new_size) {\n-            (0, 0) => Ok((layout.dangling(), 0)),\n-            (0, _) => self.alloc(Layout::from_size_align_unchecked(new_size, layout.align())),\n-            (_, 0) => {\n+        placement: ReallocPlacement,\n+    ) -> Result<MemoryBlock, AllocErr> {\n+        let size = layout.size();\n+        debug_assert!(\n+            new_size <= size,\n+            \"`new_size` must be smaller than or equal to `memory.size()`\"\n+        );\n+\n+        if size == new_size {\n+            return Ok(MemoryBlock { ptr, size });\n+        }\n+\n+        match placement {\n+            ReallocPlacement::InPlace => Err(AllocErr),\n+            ReallocPlacement::MayMove if new_size == 0 => {\n                 self.dealloc(ptr, layout);\n-                Ok((layout.dangling(), 0))\n+                Ok(MemoryBlock { ptr: layout.dangling(), size: 0 })\n+            }\n+            ReallocPlacement::MayMove => {\n+                // `realloc` probably checks for `new_size < size` or something similar.\n+                intrinsics::assume(new_size < size);\n+                let ptr = GlobalAlloc::realloc(self, ptr.as_ptr(), layout, new_size);\n+                Ok(MemoryBlock { ptr: NonNull::new(ptr).ok_or(AllocErr)?, size: new_size })\n             }\n-            (_, _) => NonNull::new(GlobalAlloc::realloc(self, ptr.as_ptr(), layout, new_size))\n-                .ok_or(AllocErr)\n-                .map(|p| (p, new_size)),\n         }\n     }\n }\n-\n static HOOK: AtomicPtr<()> = AtomicPtr::new(ptr::null_mut());\n \n /// Registers a custom allocation error hook, replacing any that was previously registered.\n@@ -238,9 +279,7 @@ pub fn rust_oom(layout: Layout) -> ! {\n     let hook: fn(Layout) =\n         if hook.is_null() { default_alloc_error_hook } else { unsafe { mem::transmute(hook) } };\n     hook(layout);\n-    unsafe {\n-        crate::sys::abort_internal();\n-    }\n+    unsafe { crate::sys::abort_internal() }\n }\n \n #[cfg(not(test))]"}, {"sha": "24b57f12e8df4eea7041f5542d729a5ef8157236", "filename": "src/libstd/error.rs", "status": "modified", "additions": 1, "deletions": 8, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibstd%2Ferror.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Flibstd%2Ferror.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Ferror.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -15,7 +15,7 @@\n \n use core::array;\n \n-use crate::alloc::{AllocErr, CannotReallocInPlace, LayoutErr};\n+use crate::alloc::{AllocErr, LayoutErr};\n use crate::any::TypeId;\n use crate::backtrace::Backtrace;\n use crate::borrow::Cow;\n@@ -409,13 +409,6 @@ impl Error for AllocErr {}\n )]\n impl Error for LayoutErr {}\n \n-#[unstable(\n-    feature = \"allocator_api\",\n-    reason = \"the precise API and guarantees it provides may be tweaked.\",\n-    issue = \"32838\"\n-)]\n-impl Error for CannotReallocInPlace {}\n-\n #[stable(feature = \"rust1\", since = \"1.0.0\")]\n impl Error for str::ParseBoolError {\n     #[allow(deprecated)]"}, {"sha": "184e4706a4c867077bf86a53495a1a4e54594095", "filename": "src/test/ui/allocator/custom.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Ftest%2Fui%2Fallocator%2Fcustom.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Ftest%2Fui%2Fallocator%2Fcustom.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fallocator%2Fcustom.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -7,7 +7,7 @@\n \n extern crate helper;\n \n-use std::alloc::{self, Global, AllocRef, System, Layout};\n+use std::alloc::{self, AllocInit, AllocRef, Global, Layout, System};\n use std::sync::atomic::{AtomicUsize, Ordering};\n \n static HITS: AtomicUsize = AtomicUsize::new(0);\n@@ -37,10 +37,10 @@ fn main() {\n     unsafe {\n         let layout = Layout::from_size_align(4, 2).unwrap();\n \n-        let (ptr, _) = Global.alloc(layout.clone()).unwrap();\n-        helper::work_with(&ptr);\n+        let memory = Global.alloc(layout.clone(), AllocInit::Uninitialized).unwrap();\n+        helper::work_with(&memory.ptr);\n         assert_eq!(HITS.load(Ordering::SeqCst), n + 1);\n-        Global.dealloc(ptr, layout.clone());\n+        Global.dealloc(memory.ptr, layout);\n         assert_eq!(HITS.load(Ordering::SeqCst), n + 2);\n \n         let s = String::with_capacity(10);\n@@ -49,10 +49,10 @@ fn main() {\n         drop(s);\n         assert_eq!(HITS.load(Ordering::SeqCst), n + 4);\n \n-        let (ptr, _) = System.alloc(layout.clone()).unwrap();\n+        let memory = System.alloc(layout.clone(), AllocInit::Uninitialized).unwrap();\n         assert_eq!(HITS.load(Ordering::SeqCst), n + 4);\n-        helper::work_with(&ptr);\n-        System.dealloc(ptr, layout);\n+        helper::work_with(&memory.ptr);\n+        System.dealloc(memory.ptr, layout);\n         assert_eq!(HITS.load(Ordering::SeqCst), n + 4);\n     }\n }"}, {"sha": "7de1ab7a5531508111999f8a662c2b11f5d5a191", "filename": "src/test/ui/allocator/xcrate-use.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Ftest%2Fui%2Fallocator%2Fxcrate-use.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Ftest%2Fui%2Fallocator%2Fxcrate-use.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fallocator%2Fxcrate-use.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -9,8 +9,8 @@\n extern crate custom;\n extern crate helper;\n \n-use std::alloc::{Global, AllocRef, System, Layout};\n-use std::sync::atomic::{Ordering, AtomicUsize};\n+use std::alloc::{AllocInit, AllocRef, Global, Layout, System};\n+use std::sync::atomic::{AtomicUsize, Ordering};\n \n #[global_allocator]\n static GLOBAL: custom::A = custom::A(AtomicUsize::new(0));\n@@ -20,16 +20,16 @@ fn main() {\n         let n = GLOBAL.0.load(Ordering::SeqCst);\n         let layout = Layout::from_size_align(4, 2).unwrap();\n \n-        let (ptr, _) = Global.alloc(layout.clone()).unwrap();\n-        helper::work_with(&ptr);\n+        let memory = Global.alloc(layout.clone(), AllocInit::Uninitialized).unwrap();\n+        helper::work_with(&memory.ptr);\n         assert_eq!(GLOBAL.0.load(Ordering::SeqCst), n + 1);\n-        Global.dealloc(ptr, layout.clone());\n+        Global.dealloc(memory.ptr, layout);\n         assert_eq!(GLOBAL.0.load(Ordering::SeqCst), n + 2);\n \n-        let (ptr, _) = System.alloc(layout.clone()).unwrap();\n+        let memory = System.alloc(layout.clone(), AllocInit::Uninitialized).unwrap();\n         assert_eq!(GLOBAL.0.load(Ordering::SeqCst), n + 2);\n-        helper::work_with(&ptr);\n-        System.dealloc(ptr, layout);\n+        helper::work_with(&memory.ptr);\n+        System.dealloc(memory.ptr, layout);\n         assert_eq!(GLOBAL.0.load(Ordering::SeqCst), n + 2);\n     }\n }"}, {"sha": "0687a9ce454ccd84362491a6cac5e2e033afaa71", "filename": "src/test/ui/realloc-16687.rs", "status": "modified", "additions": 51, "deletions": 37, "changes": 88, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Ftest%2Fui%2Frealloc-16687.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Ftest%2Fui%2Frealloc-16687.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Frealloc-16687.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -6,7 +6,7 @@\n \n #![feature(allocator_api)]\n \n-use std::alloc::{Global, AllocRef, Layout, handle_alloc_error};\n+use std::alloc::{handle_alloc_error, AllocInit, AllocRef, Global, Layout, ReallocPlacement};\n use std::ptr::{self, NonNull};\n \n fn main() {\n@@ -16,38 +16,40 @@ fn main() {\n }\n \n unsafe fn test_triangle() -> bool {\n-    static COUNT : usize = 16;\n+    static COUNT: usize = 16;\n     let mut ascend = vec![ptr::null_mut(); COUNT];\n     let ascend = &mut *ascend;\n-    static ALIGN : usize = 1;\n+    static ALIGN: usize = 1;\n \n     // Checks that `ascend` forms triangle of ascending size formed\n     // from pairs of rows (where each pair of rows is equally sized),\n     // and the elements of the triangle match their row-pair index.\n     unsafe fn sanity_check(ascend: &[*mut u8]) {\n         for i in 0..COUNT / 2 {\n-            let (p0, p1, size) = (ascend[2*i], ascend[2*i+1], idx_to_size(i));\n+            let (p0, p1, size) = (ascend[2 * i], ascend[2 * i + 1], idx_to_size(i));\n             for j in 0..size {\n                 assert_eq!(*p0.add(j), i as u8);\n                 assert_eq!(*p1.add(j), i as u8);\n             }\n         }\n     }\n \n-    static PRINT : bool = false;\n+    static PRINT: bool = false;\n \n     unsafe fn allocate(layout: Layout) -> *mut u8 {\n         if PRINT {\n             println!(\"allocate({:?})\", layout);\n         }\n \n-        let (ptr, _) = Global.alloc(layout).unwrap_or_else(|_| handle_alloc_error(layout));\n+        let memory = Global\n+            .alloc(layout, AllocInit::Uninitialized)\n+            .unwrap_or_else(|_| handle_alloc_error(layout));\n \n         if PRINT {\n-            println!(\"allocate({:?}) = {:?}\", layout, ptr);\n+            println!(\"allocate({:?}) = {:?}\", layout, memory.ptr);\n         }\n \n-        ptr.cast().as_ptr()\n+        memory.ptr.cast().as_ptr()\n     }\n \n     unsafe fn deallocate(ptr: *mut u8, layout: Layout) {\n@@ -63,33 +65,45 @@ unsafe fn test_triangle() -> bool {\n             println!(\"reallocate({:?}, old={:?}, new={:?})\", ptr, old, new);\n         }\n \n-        let (ptr, _) = Global.realloc(NonNull::new_unchecked(ptr), old, new.size())\n-            .unwrap_or_else(|_| handle_alloc_error(\n-                Layout::from_size_align_unchecked(new.size(), old.align())\n-            ));\n+        let memory = if new.size() > old.size() {\n+            Global.grow(\n+                NonNull::new_unchecked(ptr),\n+                old,\n+                new.size(),\n+                ReallocPlacement::MayMove,\n+                AllocInit::Uninitialized,\n+            )\n+        } else {\n+            Global.shrink(NonNull::new_unchecked(ptr), old, new.size(), ReallocPlacement::MayMove)\n+        };\n+\n+        let memory = memory.unwrap_or_else(|_| {\n+            handle_alloc_error(Layout::from_size_align_unchecked(new.size(), old.align()))\n+        });\n \n         if PRINT {\n-            println!(\"reallocate({:?}, old={:?}, new={:?}) = {:?}\",\n-                     ptr, old, new, ptr);\n+            println!(\"reallocate({:?}, old={:?}, new={:?}) = {:?}\", ptr, old, new, memory.ptr);\n         }\n-        ptr.cast().as_ptr()\n+        memory.ptr.cast().as_ptr()\n     }\n \n-    fn idx_to_size(i: usize) -> usize { (i+1) * 10 }\n+    fn idx_to_size(i: usize) -> usize {\n+        (i + 1) * 10\n+    }\n \n     // Allocate pairs of rows that form a triangle shape.  (Hope is\n     // that at least two rows will be allocated near each other, so\n     // that we trigger the bug (a buffer overrun) in an observable\n     // way.)\n     for i in 0..COUNT / 2 {\n         let size = idx_to_size(i);\n-        ascend[2*i]   = allocate(Layout::from_size_align(size, ALIGN).unwrap());\n-        ascend[2*i+1] = allocate(Layout::from_size_align(size, ALIGN).unwrap());\n+        ascend[2 * i] = allocate(Layout::from_size_align(size, ALIGN).unwrap());\n+        ascend[2 * i + 1] = allocate(Layout::from_size_align(size, ALIGN).unwrap());\n     }\n \n     // Initialize each pair of rows to distinct value.\n     for i in 0..COUNT / 2 {\n-        let (p0, p1, size) = (ascend[2*i], ascend[2*i+1], idx_to_size(i));\n+        let (p0, p1, size) = (ascend[2 * i], ascend[2 * i + 1], idx_to_size(i));\n         for j in 0..size {\n             *p0.add(j) = i as u8;\n             *p1.add(j) = i as u8;\n@@ -104,8 +118,8 @@ unsafe fn test_triangle() -> bool {\n \n     for i in 0..COUNT / 2 {\n         let size = idx_to_size(i);\n-        deallocate(ascend[2*i], Layout::from_size_align(size, ALIGN).unwrap());\n-        deallocate(ascend[2*i+1], Layout::from_size_align(size, ALIGN).unwrap());\n+        deallocate(ascend[2 * i], Layout::from_size_align(size, ALIGN).unwrap());\n+        deallocate(ascend[2 * i + 1], Layout::from_size_align(size, ALIGN).unwrap());\n     }\n \n     return true;\n@@ -115,68 +129,68 @@ unsafe fn test_triangle() -> bool {\n     // realloc'ing each row from top to bottom, and checking all the\n     // rows as we go.\n     unsafe fn test_1(ascend: &mut [*mut u8]) {\n-        let new_size = idx_to_size(COUNT-1);\n+        let new_size = idx_to_size(COUNT - 1);\n         let new = Layout::from_size_align(new_size, ALIGN).unwrap();\n         for i in 0..COUNT / 2 {\n-            let (p0, p1, old_size) = (ascend[2*i], ascend[2*i+1], idx_to_size(i));\n+            let (p0, p1, old_size) = (ascend[2 * i], ascend[2 * i + 1], idx_to_size(i));\n             assert!(old_size < new_size);\n             let old = Layout::from_size_align(old_size, ALIGN).unwrap();\n \n-            ascend[2*i] = reallocate(p0, old.clone(), new.clone());\n+            ascend[2 * i] = reallocate(p0, old.clone(), new.clone());\n             sanity_check(&*ascend);\n \n-            ascend[2*i+1] = reallocate(p1, old.clone(), new.clone());\n+            ascend[2 * i + 1] = reallocate(p1, old.clone(), new.clone());\n             sanity_check(&*ascend);\n         }\n     }\n \n     // Test 2: turn the square back into a triangle, top to bottom.\n     unsafe fn test_2(ascend: &mut [*mut u8]) {\n-        let old_size = idx_to_size(COUNT-1);\n+        let old_size = idx_to_size(COUNT - 1);\n         let old = Layout::from_size_align(old_size, ALIGN).unwrap();\n         for i in 0..COUNT / 2 {\n-            let (p0, p1, new_size) = (ascend[2*i], ascend[2*i+1], idx_to_size(i));\n+            let (p0, p1, new_size) = (ascend[2 * i], ascend[2 * i + 1], idx_to_size(i));\n             assert!(new_size < old_size);\n             let new = Layout::from_size_align(new_size, ALIGN).unwrap();\n \n-            ascend[2*i] = reallocate(p0, old.clone(), new.clone());\n+            ascend[2 * i] = reallocate(p0, old.clone(), new.clone());\n             sanity_check(&*ascend);\n \n-            ascend[2*i+1] = reallocate(p1, old.clone(), new.clone());\n+            ascend[2 * i + 1] = reallocate(p1, old.clone(), new.clone());\n             sanity_check(&*ascend);\n         }\n     }\n \n     // Test 3: turn triangle into a square, bottom to top.\n     unsafe fn test_3(ascend: &mut [*mut u8]) {\n-        let new_size = idx_to_size(COUNT-1);\n+        let new_size = idx_to_size(COUNT - 1);\n         let new = Layout::from_size_align(new_size, ALIGN).unwrap();\n         for i in (0..COUNT / 2).rev() {\n-            let (p0, p1, old_size) = (ascend[2*i], ascend[2*i+1], idx_to_size(i));\n+            let (p0, p1, old_size) = (ascend[2 * i], ascend[2 * i + 1], idx_to_size(i));\n             assert!(old_size < new_size);\n             let old = Layout::from_size_align(old_size, ALIGN).unwrap();\n \n-            ascend[2*i+1] = reallocate(p1, old.clone(), new.clone());\n+            ascend[2 * i + 1] = reallocate(p1, old.clone(), new.clone());\n             sanity_check(&*ascend);\n \n-            ascend[2*i] = reallocate(p0, old.clone(), new.clone());\n+            ascend[2 * i] = reallocate(p0, old.clone(), new.clone());\n             sanity_check(&*ascend);\n         }\n     }\n \n     // Test 4: turn the square back into a triangle, bottom to top.\n     unsafe fn test_4(ascend: &mut [*mut u8]) {\n-        let old_size = idx_to_size(COUNT-1);\n+        let old_size = idx_to_size(COUNT - 1);\n         let old = Layout::from_size_align(old_size, ALIGN).unwrap();\n         for i in (0..COUNT / 2).rev() {\n-            let (p0, p1, new_size) = (ascend[2*i], ascend[2*i+1], idx_to_size(i));\n+            let (p0, p1, new_size) = (ascend[2 * i], ascend[2 * i + 1], idx_to_size(i));\n             assert!(new_size < old_size);\n             let new = Layout::from_size_align(new_size, ALIGN).unwrap();\n \n-            ascend[2*i+1] = reallocate(p1, old.clone(), new.clone());\n+            ascend[2 * i + 1] = reallocate(p1, old.clone(), new.clone());\n             sanity_check(&*ascend);\n \n-            ascend[2*i] = reallocate(p0, old.clone(), new.clone());\n+            ascend[2 * i] = reallocate(p0, old.clone(), new.clone());\n             sanity_check(&*ascend);\n         }\n     }"}, {"sha": "380310190be0124a9f229d46b134eb8996d9c9a4", "filename": "src/test/ui/regions/regions-mock-codegen.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/127a11a344eb59b5aea1464e98257c262dcba967/src%2Ftest%2Fui%2Fregions%2Fregions-mock-codegen.rs", "raw_url": "https://github.com/rust-lang/rust/raw/127a11a344eb59b5aea1464e98257c262dcba967/src%2Ftest%2Fui%2Fregions%2Fregions-mock-codegen.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fregions%2Fregions-mock-codegen.rs?ref=127a11a344eb59b5aea1464e98257c262dcba967", "patch": "@@ -1,34 +1,34 @@\n // run-pass\n #![allow(dead_code)]\n #![allow(non_camel_case_types)]\n-\n // pretty-expanded FIXME #23616\n-\n #![feature(allocator_api)]\n \n-use std::alloc::{AllocRef, Global, Layout, handle_alloc_error};\n+use std::alloc::{handle_alloc_error, AllocInit, AllocRef, Global, Layout};\n use std::ptr::NonNull;\n \n struct arena(());\n \n struct Bcx<'a> {\n-    fcx: &'a Fcx<'a>\n+    fcx: &'a Fcx<'a>,\n }\n \n struct Fcx<'a> {\n     arena: &'a arena,\n-    ccx: &'a Ccx\n+    ccx: &'a Ccx,\n }\n \n struct Ccx {\n-    x: isize\n+    x: isize,\n }\n \n fn alloc(_bcx: &arena) -> &Bcx<'_> {\n     unsafe {\n         let layout = Layout::new::<Bcx>();\n-        let (ptr, _) = Global.alloc(layout).unwrap_or_else(|_| handle_alloc_error(layout));\n-        &*(ptr.as_ptr() as *const _)\n+        let memory = Global\n+            .alloc(layout, AllocInit::Uninitialized)\n+            .unwrap_or_else(|_| handle_alloc_error(layout));\n+        &*(memory.ptr.as_ptr() as *const _)\n     }\n }\n "}]}