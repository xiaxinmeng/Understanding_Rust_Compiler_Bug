{"sha": "dcb444af0a417766dde9b7b8f537fa1ba1568daf", "node_id": "C_kwDOAAsO6NoAKGRjYjQ0NGFmMGE0MTc3NjZkZGU5YjdiOGY1MzdmYTFiYTE1NjhkYWY", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-08-01T12:52:49Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-08-01T12:52:49Z"}, "message": "Auto merge of #99884 - nnethercote:lexer-improvements, r=matklad\n\nLexer improvements\n\nSome cleanups and small speed improvements.\n\nr? `@matklad`", "tree": {"sha": "e09469332bac5ee5f08ab9ac701cea19330ddd1e", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/e09469332bac5ee5f08ab9ac701cea19330ddd1e"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/dcb444af0a417766dde9b7b8f537fa1ba1568daf", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/dcb444af0a417766dde9b7b8f537fa1ba1568daf", "html_url": "https://github.com/rust-lang/rust/commit/dcb444af0a417766dde9b7b8f537fa1ba1568daf", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/dcb444af0a417766dde9b7b8f537fa1ba1568daf/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1f5d8d49eb6111931091f700d07518cd2b80bc18", "url": "https://api.github.com/repos/rust-lang/rust/commits/1f5d8d49eb6111931091f700d07518cd2b80bc18", "html_url": "https://github.com/rust-lang/rust/commit/1f5d8d49eb6111931091f700d07518cd2b80bc18"}, {"sha": "99f5c79d64c268e8603c6b00c88abda7319f26e2", "url": "https://api.github.com/repos/rust-lang/rust/commits/99f5c79d64c268e8603c6b00c88abda7319f26e2", "html_url": "https://github.com/rust-lang/rust/commit/99f5c79d64c268e8603c6b00c88abda7319f26e2"}], "stats": {"total": 245, "additions": 128, "deletions": 117}, "files": [{"sha": "c96474ccb428ae57e6d0ea222299e53ce20d56cd", "filename": "compiler/rustc_ast/src/util/comments.rs", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_ast%2Fsrc%2Futil%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_ast%2Fsrc%2Futil%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Futil%2Fcomments.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -194,7 +194,7 @@ pub fn gather_comments(sm: &SourceMap, path: FileName, src: String) -> Vec<Comme\n     }\n \n     for token in rustc_lexer::tokenize(&text[pos..]) {\n-        let token_text = &text[pos..pos + token.len];\n+        let token_text = &text[pos..pos + token.len as usize];\n         match token.kind {\n             rustc_lexer::TokenKind::Whitespace => {\n                 if let Some(mut idx) = token_text.find('\\n') {\n@@ -211,8 +211,10 @@ pub fn gather_comments(sm: &SourceMap, path: FileName, src: String) -> Vec<Comme\n             }\n             rustc_lexer::TokenKind::BlockComment { doc_style, .. } => {\n                 if doc_style.is_none() {\n-                    let code_to_the_right =\n-                        !matches!(text[pos + token.len..].chars().next(), Some('\\r' | '\\n'));\n+                    let code_to_the_right = !matches!(\n+                        text[pos + token.len as usize..].chars().next(),\n+                        Some('\\r' | '\\n')\n+                    );\n                     let style = match (code_to_the_left, code_to_the_right) {\n                         (_, true) => CommentStyle::Mixed,\n                         (false, false) => CommentStyle::Isolated,\n@@ -246,7 +248,7 @@ pub fn gather_comments(sm: &SourceMap, path: FileName, src: String) -> Vec<Comme\n                 code_to_the_left = true;\n             }\n         }\n-        pos += token.len;\n+        pos += token.len as usize;\n     }\n \n     comments"}, {"sha": "21557a9c8540126fb09d8724626c624d888ac9b8", "filename": "compiler/rustc_lexer/src/cursor.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -61,8 +61,8 @@ impl<'a> Cursor<'a> {\n     }\n \n     /// Returns amount of already consumed symbols.\n-    pub(crate) fn len_consumed(&self) -> usize {\n-        self.initial_len - self.chars.as_str().len()\n+    pub(crate) fn len_consumed(&self) -> u32 {\n+        (self.initial_len - self.chars.as_str().len()) as u32\n     }\n \n     /// Resets the number of bytes consumed to 0."}, {"sha": "6d311af9007b1684a1bb6e738b6b20a76d4773d6", "filename": "compiler/rustc_lexer/src/lib.rs", "status": "modified", "additions": 48, "deletions": 41, "changes": 89, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_lexer%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_lexer%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Flib.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -38,18 +38,17 @@ use std::convert::TryFrom;\n #[derive(Debug)]\n pub struct Token {\n     pub kind: TokenKind,\n-    pub len: usize,\n+    pub len: u32,\n }\n \n impl Token {\n-    fn new(kind: TokenKind, len: usize) -> Token {\n+    fn new(kind: TokenKind, len: u32) -> Token {\n         Token { kind, len }\n     }\n }\n \n /// Enum representing common lexeme types.\n-// perf note: Changing all `usize` to `u32` doesn't change performance. See #77629\n-#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]\n+#[derive(Clone, Copy, Debug, PartialEq, Eq)]\n pub enum TokenKind {\n     // Multi-char tokens:\n     /// \"// comment\"\n@@ -76,7 +75,7 @@ pub enum TokenKind {\n     /// tokens.\n     UnknownPrefix,\n     /// \"12_u8\", \"1.0e-40\", \"b\"123\"\". See `LiteralKind` for more details.\n-    Literal { kind: LiteralKind, suffix_start: usize },\n+    Literal { kind: LiteralKind, suffix_start: u32 },\n     /// \"'a\"\n     Lifetime { starts_with_number: bool },\n \n@@ -160,26 +159,24 @@ pub enum LiteralKind {\n     Str { terminated: bool },\n     /// \"b\"abc\"\", \"b\"abc\"\n     ByteStr { terminated: bool },\n-    /// \"r\"abc\"\", \"r#\"abc\"#\", \"r####\"ab\"###\"c\"####\", \"r#\"a\"\n-    RawStr { n_hashes: u8, err: Option<RawStrError> },\n-    /// \"br\"abc\"\", \"br#\"abc\"#\", \"br####\"ab\"###\"c\"####\", \"br#\"a\"\n-    RawByteStr { n_hashes: u8, err: Option<RawStrError> },\n+    /// \"r\"abc\"\", \"r#\"abc\"#\", \"r####\"ab\"###\"c\"####\", \"r#\"a\". `None` indicates\n+    /// an invalid literal.\n+    RawStr { n_hashes: Option<u8> },\n+    /// \"br\"abc\"\", \"br#\"abc\"#\", \"br####\"ab\"###\"c\"####\", \"br#\"a\". `None`\n+    /// indicates an invalid literal.\n+    RawByteStr { n_hashes: Option<u8> },\n }\n \n-/// Error produced validating a raw string. Represents cases like:\n-/// - `r##~\"abcde\"##`: `InvalidStarter`\n-/// - `r###\"abcde\"##`: `NoTerminator { expected: 3, found: 2, possible_terminator_offset: Some(11)`\n-/// - Too many `#`s (>255): `TooManyDelimiters`\n-// perf note: It doesn't matter that this makes `Token` 36 bytes bigger. See #77629\n #[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]\n pub enum RawStrError {\n-    /// Non `#` characters exist between `r` and `\"` eg. `r#~\"..`\n+    /// Non `#` characters exist between `r` and `\"`, e.g. `r##~\"abcde\"##`\n     InvalidStarter { bad_char: char },\n-    /// The string was never terminated. `possible_terminator_offset` is the number of characters after `r` or `br` where they\n-    /// may have intended to terminate it.\n-    NoTerminator { expected: usize, found: usize, possible_terminator_offset: Option<usize> },\n+    /// The string was not terminated, e.g. `r###\"abcde\"##`.\n+    /// `possible_terminator_offset` is the number of characters after `r` or\n+    /// `br` where they may have intended to terminate it.\n+    NoTerminator { expected: u32, found: u32, possible_terminator_offset: Option<u32> },\n     /// More than 255 `#`s exist.\n-    TooManyDelimiters { found: usize },\n+    TooManyDelimiters { found: u32 },\n }\n \n /// Base of numeric literal encoding according to its prefix.\n@@ -221,11 +218,25 @@ pub fn strip_shebang(input: &str) -> Option<usize> {\n }\n \n /// Parses the first token from the provided input string.\n+#[inline]\n pub fn first_token(input: &str) -> Token {\n     debug_assert!(!input.is_empty());\n     Cursor::new(input).advance_token()\n }\n \n+/// Validates a raw string literal. Used for getting more information about a\n+/// problem with a `RawStr`/`RawByteStr` with a `None` field.\n+#[inline]\n+pub fn validate_raw_str(input: &str, prefix_len: u32) -> Result<(), RawStrError> {\n+    debug_assert!(!input.is_empty());\n+    let mut cursor = Cursor::new(input);\n+    // Move past the leading `r` or `br`.\n+    for _ in 0..prefix_len {\n+        cursor.bump().unwrap();\n+    }\n+    cursor.raw_double_quoted_string(prefix_len).map(|_| ())\n+}\n+\n /// Creates an iterator that produces tokens from the input string.\n pub fn tokenize(input: &str) -> impl Iterator<Item = Token> + '_ {\n     let mut cursor = Cursor::new(input);\n@@ -315,12 +326,12 @@ impl Cursor<'_> {\n             'r' => match (self.first(), self.second()) {\n                 ('#', c1) if is_id_start(c1) => self.raw_ident(),\n                 ('#', _) | ('\"', _) => {\n-                    let (n_hashes, err) = self.raw_double_quoted_string(1);\n+                    let res = self.raw_double_quoted_string(1);\n                     let suffix_start = self.len_consumed();\n-                    if err.is_none() {\n+                    if res.is_ok() {\n                         self.eat_literal_suffix();\n                     }\n-                    let kind = RawStr { n_hashes, err };\n+                    let kind = RawStr { n_hashes: res.ok() };\n                     Literal { kind, suffix_start }\n                 }\n                 _ => self.ident_or_unknown_prefix(),\n@@ -350,12 +361,12 @@ impl Cursor<'_> {\n                 }\n                 ('r', '\"') | ('r', '#') => {\n                     self.bump();\n-                    let (n_hashes, err) = self.raw_double_quoted_string(2);\n+                    let res = self.raw_double_quoted_string(2);\n                     let suffix_start = self.len_consumed();\n-                    if err.is_none() {\n+                    if res.is_ok() {\n                         self.eat_literal_suffix();\n                     }\n-                    let kind = RawByteStr { n_hashes, err };\n+                    let kind = RawByteStr { n_hashes: res.ok() };\n                     Literal { kind, suffix_start }\n                 }\n                 _ => self.ident_or_unknown_prefix(),\n@@ -698,19 +709,18 @@ impl Cursor<'_> {\n     }\n \n     /// Eats the double-quoted string and returns `n_hashes` and an error if encountered.\n-    fn raw_double_quoted_string(&mut self, prefix_len: usize) -> (u8, Option<RawStrError>) {\n+    fn raw_double_quoted_string(&mut self, prefix_len: u32) -> Result<u8, RawStrError> {\n         // Wrap the actual function to handle the error with too many hashes.\n         // This way, it eats the whole raw string.\n-        let (n_hashes, err) = self.raw_string_unvalidated(prefix_len);\n+        let n_hashes = self.raw_string_unvalidated(prefix_len)?;\n         // Only up to 255 `#`s are allowed in raw strings\n         match u8::try_from(n_hashes) {\n-            Ok(num) => (num, err),\n-            // We lie about the number of hashes here :P\n-            Err(_) => (0, Some(RawStrError::TooManyDelimiters { found: n_hashes })),\n+            Ok(num) => Ok(num),\n+            Err(_) => Err(RawStrError::TooManyDelimiters { found: n_hashes }),\n         }\n     }\n \n-    fn raw_string_unvalidated(&mut self, prefix_len: usize) -> (usize, Option<RawStrError>) {\n+    fn raw_string_unvalidated(&mut self, prefix_len: u32) -> Result<u32, RawStrError> {\n         debug_assert!(self.prev() == 'r');\n         let start_pos = self.len_consumed();\n         let mut possible_terminator_offset = None;\n@@ -729,7 +739,7 @@ impl Cursor<'_> {\n             Some('\"') => (),\n             c => {\n                 let c = c.unwrap_or(EOF_CHAR);\n-                return (n_start_hashes, Some(RawStrError::InvalidStarter { bad_char: c }));\n+                return Err(RawStrError::InvalidStarter { bad_char: c });\n             }\n         }\n \n@@ -739,14 +749,11 @@ impl Cursor<'_> {\n             self.eat_while(|c| c != '\"');\n \n             if self.is_eof() {\n-                return (\n-                    n_start_hashes,\n-                    Some(RawStrError::NoTerminator {\n-                        expected: n_start_hashes,\n-                        found: max_hashes,\n-                        possible_terminator_offset,\n-                    }),\n-                );\n+                return Err(RawStrError::NoTerminator {\n+                    expected: n_start_hashes,\n+                    found: max_hashes,\n+                    possible_terminator_offset,\n+                });\n             }\n \n             // Eat closing double quote.\n@@ -764,7 +771,7 @@ impl Cursor<'_> {\n             }\n \n             if n_end_hashes == n_start_hashes {\n-                return (n_start_hashes, None);\n+                return Ok(n_start_hashes);\n             } else if n_end_hashes > max_hashes {\n                 // Keep track of possible terminators to give a hint about\n                 // where there might be a missing terminator"}, {"sha": "e4c1787f2ccef043e002d228346c72b8a2958655", "filename": "compiler/rustc_lexer/src/tests.rs", "status": "modified", "additions": 22, "deletions": 28, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -2,42 +2,39 @@ use super::*;\n \n use expect_test::{expect, Expect};\n \n-fn check_raw_str(s: &str, expected_hashes: u8, expected_err: Option<RawStrError>) {\n+fn check_raw_str(s: &str, expected: Result<u8, RawStrError>) {\n     let s = &format!(\"r{}\", s);\n     let mut cursor = Cursor::new(s);\n     cursor.bump();\n-    let (n_hashes, err) = cursor.raw_double_quoted_string(0);\n-    assert_eq!(n_hashes, expected_hashes);\n-    assert_eq!(err, expected_err);\n+    let res = cursor.raw_double_quoted_string(0);\n+    assert_eq!(res, expected);\n }\n \n #[test]\n fn test_naked_raw_str() {\n-    check_raw_str(r#\"\"abc\"\"#, 0, None);\n+    check_raw_str(r#\"\"abc\"\"#, Ok(0));\n }\n \n #[test]\n fn test_raw_no_start() {\n-    check_raw_str(r##\"\"abc\"#\"##, 0, None);\n+    check_raw_str(r##\"\"abc\"#\"##, Ok(0));\n }\n \n #[test]\n fn test_too_many_terminators() {\n     // this error is handled in the parser later\n-    check_raw_str(r###\"#\"abc\"##\"###, 1, None);\n+    check_raw_str(r###\"#\"abc\"##\"###, Ok(1));\n }\n \n #[test]\n fn test_unterminated() {\n     check_raw_str(\n         r#\"#\"abc\"#,\n-        1,\n-        Some(RawStrError::NoTerminator { expected: 1, found: 0, possible_terminator_offset: None }),\n+        Err(RawStrError::NoTerminator { expected: 1, found: 0, possible_terminator_offset: None }),\n     );\n     check_raw_str(\n         r###\"##\"abc\"#\"###,\n-        2,\n-        Some(RawStrError::NoTerminator {\n+        Err(RawStrError::NoTerminator {\n             expected: 2,\n             found: 1,\n             possible_terminator_offset: Some(7),\n@@ -46,41 +43,38 @@ fn test_unterminated() {\n     // We're looking for \"# not just any #\n     check_raw_str(\n         r###\"##\"abc#\"###,\n-        2,\n-        Some(RawStrError::NoTerminator { expected: 2, found: 0, possible_terminator_offset: None }),\n+        Err(RawStrError::NoTerminator { expected: 2, found: 0, possible_terminator_offset: None }),\n     )\n }\n \n #[test]\n fn test_invalid_start() {\n-    check_raw_str(r##\"#~\"abc\"#\"##, 1, Some(RawStrError::InvalidStarter { bad_char: '~' }));\n+    check_raw_str(r##\"#~\"abc\"#\"##, Err(RawStrError::InvalidStarter { bad_char: '~' }));\n }\n \n #[test]\n fn test_unterminated_no_pound() {\n     // https://github.com/rust-lang/rust/issues/70677\n     check_raw_str(\n         r#\"\"\"#,\n-        0,\n-        Some(RawStrError::NoTerminator { expected: 0, found: 0, possible_terminator_offset: None }),\n+        Err(RawStrError::NoTerminator { expected: 0, found: 0, possible_terminator_offset: None }),\n     );\n }\n \n #[test]\n fn test_too_many_hashes() {\n     let max_count = u8::MAX;\n-    let mut hashes: String = \"#\".repeat(max_count.into());\n+    let hashes1 = \"#\".repeat(max_count as usize);\n+    let hashes2 = \"#\".repeat(max_count as usize + 1);\n+    let middle = \"\\\"abc\\\"\";\n+    let s1 = [&hashes1, middle, &hashes1].join(\"\");\n+    let s2 = [&hashes2, middle, &hashes2].join(\"\");\n \n-    // Valid number of hashes (255 = 2^8 - 1 = u8::MAX), but invalid string.\n-    check_raw_str(&hashes, max_count, Some(RawStrError::InvalidStarter { bad_char: '\\u{0}' }));\n+    // Valid number of hashes (255 = 2^8 - 1 = u8::MAX).\n+    check_raw_str(&s1, Ok(255));\n \n     // One more hash sign (256 = 2^8) becomes too many.\n-    hashes.push('#');\n-    check_raw_str(\n-        &hashes,\n-        0,\n-        Some(RawStrError::TooManyDelimiters { found: usize::from(max_count) + 1 }),\n-    );\n+    check_raw_str(&s2, Err(RawStrError::TooManyDelimiters { found: u32::from(max_count) + 1 }));\n }\n \n #[test]\n@@ -251,7 +245,7 @@ fn raw_string() {\n     check_lexing(\n         \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\",\n         expect![[r#\"\n-            Token { kind: Literal { kind: RawStr { n_hashes: 3, err: None }, suffix_start: 17 }, len: 17 }\n+            Token { kind: Literal { kind: RawStr { n_hashes: Some(3) }, suffix_start: 17 }, len: 17 }\n         \"#]],\n     )\n }\n@@ -295,9 +289,9 @@ br###\"raw\"###suffix\n             Token { kind: Whitespace, len: 1 }\n             Token { kind: Literal { kind: Int { base: Decimal, empty_int: false }, suffix_start: 1 }, len: 3 }\n             Token { kind: Whitespace, len: 1 }\n-            Token { kind: Literal { kind: RawStr { n_hashes: 3, err: None }, suffix_start: 12 }, len: 18 }\n+            Token { kind: Literal { kind: RawStr { n_hashes: Some(3) }, suffix_start: 12 }, len: 18 }\n             Token { kind: Whitespace, len: 1 }\n-            Token { kind: Literal { kind: RawByteStr { n_hashes: 3, err: None }, suffix_start: 13 }, len: 19 }\n+            Token { kind: Literal { kind: RawByteStr { n_hashes: Some(3) }, suffix_start: 13 }, len: 19 }\n             Token { kind: Whitespace, len: 1 }\n         \"#]],\n     )"}, {"sha": "848e142e59ce9b4dcc47b41a37fb4d46b073d330", "filename": "compiler/rustc_parse/src/lexer/mod.rs", "status": "modified", "additions": 42, "deletions": 35, "changes": 77, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -22,6 +22,13 @@ mod unicode_chars;\n \n use unescape_error_reporting::{emit_unescape_error, escaped_char};\n \n+// This type is used a lot. Make sure it doesn't unintentionally get bigger.\n+//\n+// This assertion is in this crate, rather than in `rustc_lexer`, because that\n+// crate cannot depend on `rustc_data_structures`.\n+#[cfg(all(target_arch = \"x86_64\", target_pointer_width = \"64\"))]\n+rustc_data_structures::static_assert_size!(rustc_lexer::Token, 12);\n+\n #[derive(Clone, Debug)]\n pub struct UnmatchedBrace {\n     pub expected_delim: Delimiter,\n@@ -37,8 +44,7 @@ pub(crate) fn parse_token_trees<'a>(\n     start_pos: BytePos,\n     override_span: Option<Span>,\n ) -> (PResult<'a, TokenStream>, Vec<UnmatchedBrace>) {\n-    StringReader { sess, start_pos, pos: start_pos, end_src_index: src.len(), src, override_span }\n-        .into_token_trees()\n+    StringReader { sess, start_pos, pos: start_pos, src, override_span }.into_token_trees()\n }\n \n struct StringReader<'a> {\n@@ -47,8 +53,6 @@ struct StringReader<'a> {\n     start_pos: BytePos,\n     /// The absolute offset within the source_map of the current character.\n     pos: BytePos,\n-    /// Stop reading src at this index.\n-    end_src_index: usize,\n     /// Source text to tokenize.\n     src: &'a str,\n     override_span: Option<Span>,\n@@ -64,20 +68,17 @@ impl<'a> StringReader<'a> {\n         let mut spacing = Spacing::Joint;\n \n         // Skip `#!` at the start of the file\n-        let start_src_index = self.src_index(self.pos);\n-        let text: &str = &self.src[start_src_index..self.end_src_index];\n-        let is_beginning_of_file = self.pos == self.start_pos;\n-        if is_beginning_of_file {\n-            if let Some(shebang_len) = rustc_lexer::strip_shebang(text) {\n-                self.pos = self.pos + BytePos::from_usize(shebang_len);\n-                spacing = Spacing::Alone;\n-            }\n+        if self.pos == self.start_pos\n+            && let Some(shebang_len) = rustc_lexer::strip_shebang(self.src)\n+        {\n+            self.pos = self.pos + BytePos::from_usize(shebang_len);\n+            spacing = Spacing::Alone;\n         }\n \n         // Skip trivial (whitespace & comments) tokens\n         loop {\n             let start_src_index = self.src_index(self.pos);\n-            let text: &str = &self.src[start_src_index..self.end_src_index];\n+            let text: &str = &self.src[start_src_index..];\n \n             if text.is_empty() {\n                 let span = self.mk_sp(self.pos, self.pos);\n@@ -87,7 +88,7 @@ impl<'a> StringReader<'a> {\n             let token = rustc_lexer::first_token(text);\n \n             let start = self.pos;\n-            self.pos = self.pos + BytePos::from_usize(token.len);\n+            self.pos = self.pos + BytePos(token.len);\n \n             debug!(\"next_token: {:?}({:?})\", token.kind, self.str_from(start));\n \n@@ -239,7 +240,7 @@ impl<'a> StringReader<'a> {\n                 token::Ident(sym, false)\n             }\n             rustc_lexer::TokenKind::Literal { kind, suffix_start } => {\n-                let suffix_start = start + BytePos(suffix_start as u32);\n+                let suffix_start = start + BytePos(suffix_start);\n                 let (kind, symbol) = self.cook_lexer_literal(start, suffix_start, kind);\n                 let suffix = if suffix_start < self.pos {\n                     let string = self.str_from(suffix_start);\n@@ -404,15 +405,21 @@ impl<'a> StringReader<'a> {\n                 }\n                 (token::ByteStr, Mode::ByteStr, 2, 1) // b\" \"\n             }\n-            rustc_lexer::LiteralKind::RawStr { n_hashes, err } => {\n-                self.report_raw_str_error(start, err);\n-                let n = u32::from(n_hashes);\n-                (token::StrRaw(n_hashes), Mode::RawStr, 2 + n, 1 + n) // r##\" \"##\n+            rustc_lexer::LiteralKind::RawStr { n_hashes } => {\n+                if let Some(n_hashes) = n_hashes {\n+                    let n = u32::from(n_hashes);\n+                    (token::StrRaw(n_hashes), Mode::RawStr, 2 + n, 1 + n) // r##\" \"##\n+                } else {\n+                    self.report_raw_str_error(start, 1);\n+                }\n             }\n-            rustc_lexer::LiteralKind::RawByteStr { n_hashes, err } => {\n-                self.report_raw_str_error(start, err);\n-                let n = u32::from(n_hashes);\n-                (token::ByteStrRaw(n_hashes), Mode::RawByteStr, 3 + n, 1 + n) // br##\" \"##\n+            rustc_lexer::LiteralKind::RawByteStr { n_hashes } => {\n+                if let Some(n_hashes) = n_hashes {\n+                    let n = u32::from(n_hashes);\n+                    (token::ByteStrRaw(n_hashes), Mode::RawByteStr, 3 + n, 1 + n) // br##\" \"##\n+                } else {\n+                    self.report_raw_str_error(start, 2);\n+                }\n             }\n             rustc_lexer::LiteralKind::Int { base, empty_int } => {\n                 return if empty_int {\n@@ -483,17 +490,17 @@ impl<'a> StringReader<'a> {\n         &self.src[self.src_index(start)..self.src_index(end)]\n     }\n \n-    fn report_raw_str_error(&self, start: BytePos, opt_err: Option<RawStrError>) {\n-        match opt_err {\n-            Some(RawStrError::InvalidStarter { bad_char }) => {\n+    fn report_raw_str_error(&self, start: BytePos, prefix_len: u32) -> ! {\n+        match rustc_lexer::validate_raw_str(self.str_from(start), prefix_len) {\n+            Err(RawStrError::InvalidStarter { bad_char }) => {\n                 self.report_non_started_raw_string(start, bad_char)\n             }\n-            Some(RawStrError::NoTerminator { expected, found, possible_terminator_offset }) => self\n+            Err(RawStrError::NoTerminator { expected, found, possible_terminator_offset }) => self\n                 .report_unterminated_raw_string(start, expected, possible_terminator_offset, found),\n-            Some(RawStrError::TooManyDelimiters { found }) => {\n+            Err(RawStrError::TooManyDelimiters { found }) => {\n                 self.report_too_many_hashes(start, found)\n             }\n-            None => (),\n+            Ok(()) => panic!(\"no error found for supposedly invalid raw string literal\"),\n         }\n     }\n \n@@ -510,9 +517,9 @@ impl<'a> StringReader<'a> {\n     fn report_unterminated_raw_string(\n         &self,\n         start: BytePos,\n-        n_hashes: usize,\n-        possible_offset: Option<usize>,\n-        found_terminators: usize,\n+        n_hashes: u32,\n+        possible_offset: Option<u32>,\n+        found_terminators: u32,\n     ) -> ! {\n         let mut err = self.sess.span_diagnostic.struct_span_fatal_with_code(\n             self.mk_sp(start, start),\n@@ -525,7 +532,7 @@ impl<'a> StringReader<'a> {\n         if n_hashes > 0 {\n             err.note(&format!(\n                 \"this raw string should be terminated with `\\\"{}`\",\n-                \"#\".repeat(n_hashes)\n+                \"#\".repeat(n_hashes as usize)\n             ));\n         }\n \n@@ -536,7 +543,7 @@ impl<'a> StringReader<'a> {\n             err.span_suggestion(\n                 span,\n                 \"consider terminating the string here\",\n-                \"#\".repeat(n_hashes),\n+                \"#\".repeat(n_hashes as usize),\n                 Applicability::MaybeIncorrect,\n             );\n         }\n@@ -637,7 +644,7 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    fn report_too_many_hashes(&self, start: BytePos, found: usize) -> ! {\n+    fn report_too_many_hashes(&self, start: BytePos, found: u32) -> ! {\n         self.fatal_span_(\n             start,\n             self.pos,"}, {"sha": "aa70912dcde4c397c6c66baef1441025dbd95906", "filename": "compiler/rustc_parse/src/lexer/tokentrees.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -277,16 +277,17 @@ struct TokenStreamBuilder {\n }\n \n impl TokenStreamBuilder {\n+    #[inline(always)]\n     fn push(&mut self, tree: TokenTree) {\n         if let Some(TokenTree::Token(prev_token, Spacing::Joint)) = self.buf.last()\n             && let TokenTree::Token(token, joint) = &tree\n             && let Some(glued) = prev_token.glue(token)\n         {\n             self.buf.pop();\n             self.buf.push(TokenTree::Token(glued, *joint));\n-            return;\n+        } else {\n+            self.buf.push(tree)\n         }\n-        self.buf.push(tree);\n     }\n \n     fn into_token_stream(self) -> TokenStream {"}, {"sha": "05547ea1515c3fc276a3e697e55e0a951a7de54a", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -213,7 +213,7 @@ impl<'a> Iterator for TokenIter<'a> {\n             return None;\n         }\n         let token = rustc_lexer::first_token(self.src);\n-        let (text, rest) = self.src.split_at(token.len);\n+        let (text, rest) = self.src.split_at(token.len as usize);\n         self.src = rest;\n         Some((token.kind, text))\n     }"}, {"sha": "e9e13aece18f6eef16a9a0799847b25b4fe49822", "filename": "src/tools/clippy/clippy_lints/src/matches/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fmatches%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fmatches%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fmatches%2Fmod.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -1112,7 +1112,7 @@ fn span_contains_cfg(cx: &LateContext<'_>, s: Span) -> bool {\n     let mut pos = 0usize;\n     let mut iter = tokenize(&snip).map(|t| {\n         let start = pos;\n-        pos += t.len;\n+        pos += t.len as usize;\n         (t.kind, start..pos)\n     });\n "}, {"sha": "d2e675a783eaaf382517370b34526011b2cf2923", "filename": "src/tools/clippy/clippy_lints/src/undocumented_unsafe_blocks.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fundocumented_unsafe_blocks.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fundocumented_unsafe_blocks.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fundocumented_unsafe_blocks.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -345,7 +345,7 @@ fn text_has_safety_comment(src: &str, line_starts: &[BytePos], offset: usize) ->\n         if line.starts_with(\"/*\") {\n             let src = src[line_start..line_starts.last().unwrap().to_usize() - offset].trim_start();\n             let mut tokens = tokenize(src);\n-            return src[..tokens.next().unwrap().len]\n+            return src[..tokens.next().unwrap().len as usize]\n                 .to_ascii_uppercase()\n                 .contains(\"SAFETY:\")\n                 && tokens.all(|t| t.kind == TokenKind::Whitespace);"}, {"sha": "1834e2a2de8720b861a8690b57313f33c5297996", "filename": "src/tools/clippy/clippy_utils/src/hir_utils.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/dcb444af0a417766dde9b7b8f537fa1ba1568daf/src%2Ftools%2Fclippy%2Fclippy_utils%2Fsrc%2Fhir_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dcb444af0a417766dde9b7b8f537fa1ba1568daf/src%2Ftools%2Fclippy%2Fclippy_utils%2Fsrc%2Fhir_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fclippy%2Fclippy_utils%2Fsrc%2Fhir_utils.rs?ref=dcb444af0a417766dde9b7b8f537fa1ba1568daf", "patch": "@@ -141,7 +141,7 @@ impl HirEqInterExpr<'_, '_, '_> {\n                 let mut left_pos = 0;\n                 let left = tokenize(&left)\n                     .map(|t| {\n-                        let end = left_pos + t.len;\n+                        let end = left_pos + t.len as usize;\n                         let s = &left[left_pos..end];\n                         left_pos = end;\n                         (t, s)\n@@ -156,7 +156,7 @@ impl HirEqInterExpr<'_, '_, '_> {\n                 let mut right_pos = 0;\n                 let right = tokenize(&right)\n                     .map(|t| {\n-                        let end = right_pos + t.len;\n+                        let end = right_pos + t.len as usize;\n                         let s = &right[right_pos..end];\n                         right_pos = end;\n                         (t, s)"}]}