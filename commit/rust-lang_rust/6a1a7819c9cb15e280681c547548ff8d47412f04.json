{"sha": "6a1a7819c9cb15e280681c547548ff8d47412f04", "node_id": "MDY6Q29tbWl0NzI0NzEyOjZhMWE3ODE5YzljYjE1ZTI4MDY4MWM1NDc1NDhmZjhkNDc0MTJmMDQ=", "commit": {"author": {"name": "Eric Reed", "email": "ereed@mozilla.com", "date": "2013-07-02T23:55:56Z"}, "committer": {"name": "Eric Reed", "email": "ereed@mozilla.com", "date": "2013-07-02T23:55:56Z"}, "message": "Merge remote-tracking branch 'upstream/io' into io\n\nConflicts:\n\tsrc/libstd/rt/test.rs\n\tsrc/rt/rustrt.def.in", "tree": {"sha": "98e24ea7db8125c3ef0fb2ae70c7ff7eace03ea0", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/98e24ea7db8125c3ef0fb2ae70c7ff7eace03ea0"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6a1a7819c9cb15e280681c547548ff8d47412f04", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6a1a7819c9cb15e280681c547548ff8d47412f04", "html_url": "https://github.com/rust-lang/rust/commit/6a1a7819c9cb15e280681c547548ff8d47412f04", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6a1a7819c9cb15e280681c547548ff8d47412f04/comments", "author": null, "committer": null, "parents": [{"sha": "e6c57793be2cf7aabfa96aeada77935cc0351067", "url": "https://api.github.com/repos/rust-lang/rust/commits/e6c57793be2cf7aabfa96aeada77935cc0351067", "html_url": "https://github.com/rust-lang/rust/commit/e6c57793be2cf7aabfa96aeada77935cc0351067"}, {"sha": "f8a4d09f7efb618ca3f8b70374e158504cb33cb0", "url": "https://api.github.com/repos/rust-lang/rust/commits/f8a4d09f7efb618ca3f8b70374e158504cb33cb0", "html_url": "https://github.com/rust-lang/rust/commit/f8a4d09f7efb618ca3f8b70374e158504cb33cb0"}], "stats": {"total": 970, "additions": 405, "deletions": 565}, "files": [{"sha": "7608bc89e021ff2d587493e6f57278a8d9917f23", "filename": "src/libstd/rt/comm.rs", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fcomm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fcomm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fcomm.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -20,7 +20,8 @@ use cast;\n use util;\n use ops::Drop;\n use kinds::Owned;\n-use rt::sched::{Scheduler, Coroutine};\n+use rt::sched::{Scheduler};\n+use rt::task::Task;\n use rt::local::Local;\n use unstable::atomics::{AtomicUint, AtomicOption, SeqCst};\n use unstable::sync::UnsafeAtomicRcBox;\n@@ -136,7 +137,7 @@ impl<T> ChanOne<T> {\n                 }\n                 task_as_state => {\n                     // Port is blocked. Wake it up.\n-                    let recvr: ~Coroutine = cast::transmute(task_as_state);\n+                    let recvr: ~Task = cast::transmute(task_as_state);\n                     let mut sched = Local::take::<Scheduler>();\n                     rtdebug!(\"rendezvous send\");\n                     sched.metrics.rendezvous_sends += 1;\n@@ -192,7 +193,7 @@ impl<T> PortOne<T> {\n                         // NB: We have to drop back into the scheduler event loop here\n                         // instead of switching immediately back or we could end up\n                         // triggering infinite recursion on the scheduler's stack.\n-                        let task: ~Coroutine = cast::transmute(task_as_state);\n+                        let task: ~Task = cast::transmute(task_as_state);\n                         sched.enqueue_task(task);\n                     }\n                     _ => util::unreachable()\n@@ -257,7 +258,7 @@ impl<T> Drop for ChanOneHack<T> {\n                 task_as_state => {\n                     // The port is blocked waiting for a message we will never send. Wake it.\n                     assert!((*this.packet()).payload.is_none());\n-                    let recvr: ~Coroutine = cast::transmute(task_as_state);\n+                    let recvr: ~Task = cast::transmute(task_as_state);\n                     let sched = Local::take::<Scheduler>();\n                     sched.schedule_task(recvr);\n                 }\n@@ -554,6 +555,8 @@ mod test {\n                 { let _c = chan; }\n                 port.recv();\n             };\n+            // What is our res?\n+            rtdebug!(\"res is: %?\", res.is_err());\n             assert!(res.is_err());\n         }\n     }\n@@ -905,4 +908,5 @@ mod test {\n             }\n         }\n     }\n+\n }"}, {"sha": "79c0d5da9a4f5aa884c23e63dc339c801436a753", "filename": "src/libstd/rt/join_latch.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fjoin_latch.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fjoin_latch.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fjoin_latch.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -643,3 +643,4 @@ mod test {\n         }\n     }\n }\n+"}, {"sha": "374933ab281b87f5c37dfd55b74b1746386c825c", "filename": "src/libstd/rt/local.rs", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Flocal.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Flocal.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Flocal.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -13,6 +13,7 @@ use rt::sched::Scheduler;\n use rt::task::Task;\n use rt::local_ptr;\n use rt::rtio::{EventLoop, IoFactoryObject};\n+//use borrow::to_uint;\n \n pub trait Local {\n     fn put(value: ~Self);\n@@ -32,6 +33,7 @@ impl Local for Scheduler {\n         let res_ptr: *mut Option<T> = &mut res;\n         unsafe {\n             do local_ptr::borrow |sched| {\n+//                rtdebug!(\"successfully unsafe borrowed sched pointer\");\n                 let result = f(sched);\n                 *res_ptr = Some(result);\n             }\n@@ -51,9 +53,12 @@ impl Local for Task {\n     fn exists() -> bool { rtabort!(\"unimpl\") }\n     fn borrow<T>(f: &fn(&mut Task) -> T) -> T {\n         do Local::borrow::<Scheduler, T> |sched| {\n+//            rtdebug!(\"sched about to grab current_task\");\n             match sched.current_task {\n                 Some(~ref mut task) => {\n-                    f(&mut *task.task)\n+//                    rtdebug!(\"current task pointer: %x\", to_uint(task));\n+//                    rtdebug!(\"current task heap pointer: %x\", to_uint(&task.heap));\n+                    f(task)\n                 }\n                 None => {\n                     rtabort!(\"no scheduler\")\n@@ -64,7 +69,7 @@ impl Local for Task {\n     unsafe fn unsafe_borrow() -> *mut Task {\n         match (*Local::unsafe_borrow::<Scheduler>()).current_task {\n             Some(~ref mut task) => {\n-                let s: *mut Task = &mut *task.task;\n+                let s: *mut Task = &mut *task;\n                 return s;\n             }\n             None => {"}, {"sha": "aae194ae548383e81bc0b8cce2d8bf34ce1c47b7", "filename": "src/libstd/rt/mod.rs", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fmod.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -67,7 +67,7 @@ use iter::Times;\n use iterator::IteratorUtil;\n use option::Some;\n use ptr::RawPtr;\n-use rt::sched::{Scheduler, Coroutine, Shutdown};\n+use rt::sched::{Scheduler, Shutdown};\n use rt::sleeper_list::SleeperList;\n use rt::task::Task;\n use rt::thread::Thread;\n@@ -268,10 +268,9 @@ pub fn run(main: ~fn()) -> int {\n \n     // Create and enqueue the main task.\n     let main_cell = Cell::new(main);\n-    let mut new_task = ~Task::new_root();\n-    new_task.on_exit = Some(on_exit);\n-    let main_task = ~Coroutine::with_task(&mut scheds[0].stack_pool,\n-                                          new_task, main_cell.take());\n+    let mut main_task = ~Task::new_root(&mut scheds[0].stack_pool,\n+                                    main_cell.take());\n+    main_task.on_exit = Some(on_exit);\n     scheds[0].enqueue_task(main_task);\n \n     // Run each scheduler in a thread.\n@@ -348,15 +347,15 @@ pub fn context() -> RuntimeContext {\n #[test]\n fn test_context() {\n     use unstable::run_in_bare_thread;\n-    use self::sched::{Scheduler, Coroutine};\n+    use self::sched::{Scheduler};\n     use rt::local::Local;\n     use rt::test::new_test_uv_sched;\n \n     assert_eq!(context(), OldTaskContext);\n     do run_in_bare_thread {\n         assert_eq!(context(), GlobalContext);\n         let mut sched = ~new_test_uv_sched();\n-        let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n+        let task = ~do Task::new_root(&mut sched.stack_pool) {\n             assert_eq!(context(), TaskContext);\n             let sched = Local::take::<Scheduler>();\n             do sched.deschedule_running_task_and_then() |sched, task| {"}, {"sha": "7d8c673636e1b46879db6f2346c636e70ca715f9", "filename": "src/libstd/rt/sched.rs", "status": "modified", "additions": 85, "deletions": 355, "changes": 440, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fsched.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fsched.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fsched.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -11,24 +11,19 @@\n use option::*;\n use sys;\n use cast::transmute;\n-use cell::Cell;\n use clone::Clone;\n \n use super::sleeper_list::SleeperList;\n use super::work_queue::WorkQueue;\n-use super::stack::{StackPool, StackSegment};\n+use super::stack::{StackPool};\n use super::rtio::{EventLoop, EventLoopObject, RemoteCallbackObject};\n use super::context::Context;\n-use super::task::Task;\n+use super::task::{Task, AnySched, Sched};\n use super::message_queue::MessageQueue;\n use rt::local_ptr;\n use rt::local::Local;\n use rt::rtio::RemoteCallback;\n use rt::metrics::SchedMetrics;\n-\n-//use to_str::ToStr;\n-\n-/// To allow for using pointers as scheduler ids\n use borrow::{to_uint};\n \n /// The Scheduler is responsible for coordinating execution of Coroutines\n@@ -41,7 +36,7 @@ use borrow::{to_uint};\n pub struct Scheduler {\n     /// A queue of available work. Under a work-stealing policy there\n     /// is one per Scheduler.\n-    priv work_queue: WorkQueue<~Coroutine>,\n+    priv work_queue: WorkQueue<~Task>,\n     /// The queue of incoming messages from other schedulers.\n     /// These are enqueued by SchedHandles after which a remote callback\n     /// is triggered to handle the message.\n@@ -66,7 +61,7 @@ pub struct Scheduler {\n     /// Always valid when a task is executing, otherwise not\n     priv saved_context: Context,\n     /// The currently executing task\n-    current_task: Option<~Coroutine>,\n+    current_task: Option<~Task>,\n     /// An action performed after a context switch on behalf of the\n     /// code running before the context switch\n     priv cleanup_job: Option<CleanupJob>,\n@@ -81,33 +76,15 @@ pub struct SchedHandle {\n     sched_id: uint\n }\n \n-pub struct Coroutine {\n-    /// The segment of stack on which the task is currently running or,\n-    /// if the task is blocked, on which the task will resume execution\n-    priv current_stack_segment: StackSegment,\n-    /// These are always valid when the task is not running, unless\n-    /// the task is dead\n-    priv saved_context: Context,\n-    /// The heap, GC, unwinding, local storage, logging\n-    task: ~Task,\n-}\n-\n-// A scheduler home is either a handle to the home scheduler, or an\n-// explicit \"AnySched\".\n-pub enum SchedHome {\n-    AnySched,\n-    Sched(SchedHandle)\n-}\n-\n pub enum SchedMessage {\n     Wake,\n     Shutdown,\n-    PinnedTask(~Coroutine)\n+    PinnedTask(~Task)\n }\n \n enum CleanupJob {\n     DoNothing,\n-    GiveTask(~Coroutine, UnsafeTaskReceiver)\n+    GiveTask(~Task, UnsafeTaskReceiver)\n }\n \n impl Scheduler {\n@@ -116,7 +93,7 @@ impl Scheduler {\n     pub fn sched_id(&self) -> uint { to_uint(self) }\n \n     pub fn new(event_loop: ~EventLoopObject,\n-               work_queue: WorkQueue<~Coroutine>,\n+               work_queue: WorkQueue<~Task>,\n                sleeper_list: SleeperList)\n         -> Scheduler {\n \n@@ -125,7 +102,7 @@ impl Scheduler {\n     }\n \n     pub fn new_special(event_loop: ~EventLoopObject,\n-                       work_queue: WorkQueue<~Coroutine>,\n+                       work_queue: WorkQueue<~Task>,\n                        sleeper_list: SleeperList,\n                        run_anything: bool)\n         -> Scheduler {\n@@ -177,7 +154,7 @@ impl Scheduler {\n \n         rtdebug!(\"run taking sched\");\n         let sched = Local::take::<Scheduler>();\n-        // XXX: Reenable this once we're using a per-task queue. With a shared\n+        // XXX: Reenable this once we're using a per-scheduler queue. With a shared\n         // queue this is not true\n         //assert!(sched.work_queue.is_empty());\n         rtdebug!(\"scheduler metrics: %s\\n\", {\n@@ -213,29 +190,29 @@ impl Scheduler {\n         if sched.resume_task_from_queue() {\n             // We performed a scheduling action. There may be other work\n             // to do yet, so let's try again later.\n-            let mut sched = Local::take::<Scheduler>();\n-            sched.metrics.tasks_resumed_from_queue += 1;\n-            sched.event_loop.callback(Scheduler::run_sched_once);\n-            Local::put(sched);\n+            do Local::borrow::<Scheduler, ()> |sched| {\n+                sched.metrics.tasks_resumed_from_queue += 1;\n+                sched.event_loop.callback(Scheduler::run_sched_once);\n+            }\n             return;\n         }\n \n         // If we got here then there was no work to do.\n         // Generate a SchedHandle and push it to the sleeper list so\n         // somebody can wake us up later.\n         rtdebug!(\"no work to do\");\n-        let mut sched = Local::take::<Scheduler>();\n-        sched.metrics.wasted_turns += 1;\n-        if !sched.sleepy && !sched.no_sleep {\n-            rtdebug!(\"sleeping\");\n-            sched.metrics.sleepy_times += 1;\n-            sched.sleepy = true;\n-            let handle = sched.make_handle();\n-            sched.sleeper_list.push(handle);\n-        } else {\n-            rtdebug!(\"not sleeping\");\n+        do Local::borrow::<Scheduler, ()> |sched| {\n+            sched.metrics.wasted_turns += 1;\n+            if !sched.sleepy && !sched.no_sleep {\n+                rtdebug!(\"sleeping\");\n+                sched.metrics.sleepy_times += 1;\n+                sched.sleepy = true;\n+                let handle = sched.make_handle();\n+                sched.sleeper_list.push(handle);\n+            } else {\n+                rtdebug!(\"not sleeping\");\n+            }\n         }\n-        Local::put(sched);\n     }\n \n     pub fn make_handle(&mut self) -> SchedHandle {\n@@ -253,7 +230,7 @@ impl Scheduler {\n     /// Pushes the task onto the work stealing queue and tells the\n     /// event loop to run it later. Always use this instead of pushing\n     /// to the work queue directly.\n-    pub fn enqueue_task(&mut self, task: ~Coroutine) {\n+    pub fn enqueue_task(&mut self, task: ~Task) {\n \n         // We don't want to queue tasks that belong on other threads,\n         // so we send them home at enqueue time.\n@@ -307,7 +284,7 @@ impl Scheduler {\n                 rtdebug!(\"recv BiasedTask message in sched: %u\",\n                          this.sched_id());\n                 let mut task = task;\n-                task.task.home = Some(Sched(this.make_handle()));\n+                task.home = Some(Sched(this.make_handle()));\n                 this.resume_task_immediately(task);\n                 return true;\n             }\n@@ -349,9 +326,9 @@ impl Scheduler {\n     }\n \n     /// Given an input Coroutine sends it back to its home scheduler.\n-    fn send_task_home(task: ~Coroutine) {\n+    fn send_task_home(task: ~Task) {\n         let mut task = task;\n-        let mut home = task.task.home.swap_unwrap();\n+        let mut home = task.home.swap_unwrap();\n         match home {\n             Sched(ref mut home_handle) => {\n                 home_handle.send(PinnedTask(task));\n@@ -377,7 +354,7 @@ impl Scheduler {\n         match this.work_queue.pop() {\n             Some(task) => {\n                 let action_id = {\n-                    let home = &task.task.home;\n+                    let home = &task.home;\n                     match home {\n                         &Some(Sched(ref home_handle))\n                         if home_handle.sched_id != this.sched_id() => {\n@@ -440,14 +417,15 @@ impl Scheduler {\n         rtdebug!(\"ending running task\");\n \n         do self.deschedule_running_task_and_then |sched, dead_task| {\n-            let dead_task = Cell::new(dead_task);\n-            dead_task.take().recycle(&mut sched.stack_pool);\n+            let mut dead_task = dead_task;\n+            let coroutine = dead_task.coroutine.swap_unwrap();\n+            coroutine.recycle(&mut sched.stack_pool);\n         }\n \n         rtabort!(\"control reached end of task\");\n     }\n \n-    pub fn schedule_task(~self, task: ~Coroutine) {\n+    pub fn schedule_task(~self, task: ~Task) {\n         assert!(self.in_task_context());\n \n         // is the task home?\n@@ -462,8 +440,7 @@ impl Scheduler {\n             // here we know we are home, execute now OR we know we\n             // aren't homed, and that this sched doesn't care\n             do this.switch_running_tasks_and_then(task) |sched, last_task| {\n-                let last_task = Cell::new(last_task);\n-                sched.enqueue_task(last_task.take());\n+                sched.enqueue_task(last_task);\n             }\n         } else if !homed && !this.run_anything {\n             // the task isn't homed, but it can't be run here\n@@ -478,7 +455,7 @@ impl Scheduler {\n \n     // Core scheduling ops\n \n-    pub fn resume_task_immediately(~self, task: ~Coroutine) {\n+    pub fn resume_task_immediately(~self, task: ~Task) {\n         let mut this = self;\n         assert!(!this.in_task_context());\n \n@@ -521,7 +498,7 @@ impl Scheduler {\n     /// This passes a Scheduler pointer to the fn after the context switch\n     /// in order to prevent that fn from performing further scheduling operations.\n     /// Doing further scheduling could easily result in infinite recursion.\n-    pub fn deschedule_running_task_and_then(~self, f: &fn(&mut Scheduler, ~Coroutine)) {\n+    pub fn deschedule_running_task_and_then(~self, f: &fn(&mut Scheduler, ~Task)) {\n         let mut this = self;\n         assert!(this.in_task_context());\n \n@@ -530,8 +507,8 @@ impl Scheduler {\n \n         unsafe {\n             let blocked_task = this.current_task.swap_unwrap();\n-            let f_fake_region = transmute::<&fn(&mut Scheduler, ~Coroutine),\n-                                            &fn(&mut Scheduler, ~Coroutine)>(f);\n+            let f_fake_region = transmute::<&fn(&mut Scheduler, ~Task),\n+                                            &fn(&mut Scheduler, ~Task)>(f);\n             let f_opaque = ClosureConverter::from_fn(f_fake_region);\n             this.enqueue_cleanup_job(GiveTask(blocked_task, f_opaque));\n         }\n@@ -553,8 +530,8 @@ impl Scheduler {\n     /// Switch directly to another task, without going through the scheduler.\n     /// You would want to think hard about doing this, e.g. if there are\n     /// pending I/O events it would be a bad idea.\n-    pub fn switch_running_tasks_and_then(~self, next_task: ~Coroutine,\n-                                         f: &fn(&mut Scheduler, ~Coroutine)) {\n+    pub fn switch_running_tasks_and_then(~self, next_task: ~Task,\n+                                         f: &fn(&mut Scheduler, ~Task)) {\n         let mut this = self;\n         assert!(this.in_task_context());\n \n@@ -563,8 +540,8 @@ impl Scheduler {\n \n         let old_running_task = this.current_task.swap_unwrap();\n         let f_fake_region = unsafe {\n-            transmute::<&fn(&mut Scheduler, ~Coroutine),\n-                        &fn(&mut Scheduler, ~Coroutine)>(f)\n+            transmute::<&fn(&mut Scheduler, ~Task),\n+                        &fn(&mut Scheduler, ~Task)>(f)\n         };\n         let f_opaque = ClosureConverter::from_fn(f_fake_region);\n         this.enqueue_cleanup_job(GiveTask(old_running_task, f_opaque));\n@@ -631,12 +608,22 @@ impl Scheduler {\n         // because borrowck thinks the three patterns are conflicting\n         // borrows\n         unsafe {\n-            let last_task = transmute::<Option<&Coroutine>, Option<&mut Coroutine>>(last_task);\n+            let last_task = transmute::<Option<&Task>, Option<&mut Task>>(last_task);\n             let last_task_context = match last_task {\n-                Some(t) => Some(&mut t.saved_context), None => None\n+                Some(t) => {\n+                    Some(&mut t.coroutine.get_mut_ref().saved_context)\n+                }\n+                None => {\n+                    None\n+                }\n             };\n             let next_task_context = match self.current_task {\n-                Some(ref mut t) => Some(&mut t.saved_context), None => None\n+                Some(ref mut t) => {\n+                    Some(&mut t.coroutine.get_mut_ref().saved_context)\n+                }\n+                None => {\n+                    None\n+                }\n             };\n             // XXX: These transmutes can be removed after snapshot\n             return (transmute(&mut self.saved_context),\n@@ -661,186 +648,34 @@ impl SchedHandle {\n     }\n }\n \n-impl Coroutine {\n-\n-    /// This function checks that a coroutine is running \"home\".\n-    pub fn is_home(&self) -> bool {\n-        rtdebug!(\"checking if coroutine is home\");\n-        do Local::borrow::<Scheduler,bool> |sched| {\n-            match self.task.home {\n-                Some(AnySched) => { false }\n-                Some(Sched(SchedHandle { sched_id: ref id, _ })) => {\n-                    *id == sched.sched_id()\n-                }\n-                None => { rtabort!(\"error: homeless task!\"); }\n-            }\n-        }\n-    }\n-\n-    /// Without access to self, but with access to the \"expected home\n-    /// id\", see if we are home.\n-    fn is_home_using_id(id: uint) -> bool {\n-        rtdebug!(\"checking if coroutine is home using id\");\n-        do Local::borrow::<Scheduler,bool> |sched| {\n-            if sched.sched_id() == id {\n-                true\n-            } else {\n-                false\n-            }\n-        }\n-    }\n-\n-    /// Check if this coroutine has a home\n-    fn homed(&self) -> bool {\n-        rtdebug!(\"checking if this coroutine has a home\");\n-        match self.task.home {\n-            Some(AnySched) => { false }\n-            Some(Sched(_)) => { true }\n-            None => { rtabort!(\"error: homeless task!\");\n-                    }\n-        }\n-    }\n-\n-    /// A version of is_home that does not need to use TLS, it instead\n-    /// takes local scheduler as a parameter.\n-    fn is_home_no_tls(&self, sched: &~Scheduler) -> bool {\n-        rtdebug!(\"checking if coroutine is home without tls\");\n-        match self.task.home {\n-            Some(AnySched) => { true }\n-            Some(Sched(SchedHandle { sched_id: ref id, _})) => {\n-                *id == sched.sched_id()\n-            }\n-            None => { rtabort!(\"error: homeless task!\"); }\n-        }\n-    }\n-\n-    /// Check TLS for the scheduler to see if we are on a special\n-    /// scheduler.\n-    pub fn on_special() -> bool {\n-        rtdebug!(\"checking if coroutine is executing on special sched\");\n-        do Local::borrow::<Scheduler,bool>() |sched| {\n-            !sched.run_anything\n-        }\n-    }\n-\n-    // Created new variants of \"new\" that takes a home scheduler\n-    // parameter. The original with_task now calls with_task_homed\n-    // using the AnySched paramter.\n-\n-    pub fn new_homed(stack_pool: &mut StackPool, home: SchedHome, start: ~fn()) -> Coroutine {\n-        Coroutine::with_task_homed(stack_pool, ~Task::new_root(), start, home)\n-    }\n-\n-    pub fn new_root(stack_pool: &mut StackPool, start: ~fn()) -> Coroutine {\n-        Coroutine::with_task(stack_pool, ~Task::new_root(), start)\n-    }\n-\n-    pub fn with_task_homed(stack_pool: &mut StackPool,\n-                           task: ~Task,\n-                           start: ~fn(),\n-                           home: SchedHome) -> Coroutine {\n-\n-        static MIN_STACK_SIZE: uint = 1000000; // XXX: Too much stack\n-\n-        let start = Coroutine::build_start_wrapper(start);\n-        let mut stack = stack_pool.take_segment(MIN_STACK_SIZE);\n-        // NB: Context holds a pointer to that ~fn\n-        let initial_context = Context::new(start, &mut stack);\n-        let mut crt = Coroutine {\n-            current_stack_segment: stack,\n-            saved_context: initial_context,\n-            task: task,\n-        };\n-        crt.task.home = Some(home);\n-        return crt;\n-    }\n-\n-    pub fn with_task(stack_pool: &mut StackPool,\n-                 task: ~Task,\n-                 start: ~fn()) -> Coroutine {\n-        Coroutine::with_task_homed(stack_pool,\n-                                   task,\n-                                   start,\n-                                   AnySched)\n-    }\n-\n-    fn build_start_wrapper(start: ~fn()) -> ~fn() {\n-        // XXX: The old code didn't have this extra allocation\n-        let start_cell = Cell::new(start);\n-        let wrapper: ~fn() = || {\n-            // This is the first code to execute after the initial\n-            // context switch to the task. The previous context may\n-            // have asked us to do some cleanup.\n-            unsafe {\n-                let sched = Local::unsafe_borrow::<Scheduler>();\n-                (*sched).run_cleanup_job();\n-\n-                let sched = Local::unsafe_borrow::<Scheduler>();\n-                let task = (*sched).current_task.get_mut_ref();\n-                // FIXME #6141: shouldn't neet to put `start()` in\n-                // another closure\n-                let start_cell = Cell::new(start_cell.take());\n-                do task.task.run {\n-                    // N.B. Removing `start` from the start wrapper\n-                    // closure by emptying a cell is critical for\n-                    // correctness. The ~Task pointer, and in turn the\n-                    // closure used to initialize the first call\n-                    // frame, is destroyed in scheduler context, not\n-                    // task context.  So any captured closures must\n-                    // not contain user-definable dtors that expect to\n-                    // be in task context. By moving `start` out of\n-                    // the closure, all the user code goes out of\n-                    // scope while the task is still running.\n-                    let start = start_cell.take();\n-                    start();\n-                };\n-            }\n-\n-            let sched = Local::take::<Scheduler>();\n-            sched.terminate_current_task();\n-        };\n-        return wrapper;\n-    }\n-\n-    /// Destroy the task and try to reuse its components\n-    pub fn recycle(~self, stack_pool: &mut StackPool) {\n-        match self {\n-            ~Coroutine {current_stack_segment, _} => {\n-                stack_pool.give_segment(current_stack_segment);\n-            }\n-        }\n-    }\n-}\n-\n // XXX: Some hacks to put a &fn in Scheduler without borrowck\n // complaining\n type UnsafeTaskReceiver = sys::Closure;\n trait ClosureConverter {\n-    fn from_fn(&fn(&mut Scheduler, ~Coroutine)) -> Self;\n-    fn to_fn(self) -> &fn(&mut Scheduler, ~Coroutine);\n+    fn from_fn(&fn(&mut Scheduler, ~Task)) -> Self;\n+    fn to_fn(self) -> &fn(&mut Scheduler, ~Task);\n }\n impl ClosureConverter for UnsafeTaskReceiver {\n-    fn from_fn(f: &fn(&mut Scheduler, ~Coroutine)) -> UnsafeTaskReceiver { unsafe { transmute(f) } }\n-    fn to_fn(self) -> &fn(&mut Scheduler, ~Coroutine) { unsafe { transmute(self) } }\n+    fn from_fn(f: &fn(&mut Scheduler, ~Task)) -> UnsafeTaskReceiver { unsafe { transmute(f) } }\n+    fn to_fn(self) -> &fn(&mut Scheduler, ~Task) { unsafe { transmute(self) } }\n }\n \n+\n #[cfg(test)]\n mod test {\n     use int;\n     use cell::Cell;\n-    use iterator::IteratorUtil;\n     use unstable::run_in_bare_thread;\n     use task::spawn;\n     use rt::local::Local;\n     use rt::test::*;\n     use super::*;\n     use rt::thread::Thread;\n-    use ptr::to_uint;\n-    use vec::MutableVector;\n+    use borrow::to_uint;\n+    use rt::task::{Task,Sched};\n \n     // Confirm that a sched_id actually is the uint form of the\n     // pointer to the scheduler struct.\n-\n     #[test]\n     fn simple_sched_id_test() {\n         do run_in_bare_thread {\n@@ -851,7 +686,6 @@ mod test {\n \n     // Compare two scheduler ids that are different, this should never\n     // fail but may catch a mistake someday.\n-\n     #[test]\n     fn compare_sched_id_test() {\n         do run_in_bare_thread {\n@@ -863,7 +697,6 @@ mod test {\n \n     // A simple test to check if a homed task run on a single\n     // scheduler ends up executing while home.\n-\n     #[test]\n     fn test_home_sched() {\n         do run_in_bare_thread {\n@@ -874,8 +707,8 @@ mod test {\n             let sched_handle = sched.make_handle();\n             let sched_id = sched.sched_id();\n \n-            let task = ~do Coroutine::new_homed(&mut sched.stack_pool,\n-                                                Sched(sched_handle)) {\n+            let task = ~do Task::new_root_homed(&mut sched.stack_pool,\n+                                                 Sched(sched_handle)) {\n                 unsafe { *task_ran_ptr = true };\n                 let sched = Local::take::<Scheduler>();\n                 assert!(sched.sched_id() == sched_id);\n@@ -888,7 +721,6 @@ mod test {\n     }\n \n     // A test for each state of schedule_task\n-\n     #[test]\n     fn test_schedule_home_states() {\n \n@@ -898,7 +730,6 @@ mod test {\n         use rt::work_queue::WorkQueue;\n \n         do run_in_bare_thread {\n-//            let nthreads = 2;\n \n             let sleepers = SleeperList::new();\n             let work_queue = WorkQueue::new();\n@@ -924,33 +755,33 @@ mod test {\n             let t1_handle = special_sched.make_handle();\n             let t4_handle = special_sched.make_handle();\n \n-            let t1f = ~do Coroutine::new_homed(&mut special_sched.stack_pool,\n-                                            Sched(t1_handle)) {\n-                let is_home = Coroutine::is_home_using_id(special_id);\n+            let t1f = ~do Task::new_root_homed(&mut special_sched.stack_pool,\n+                                               Sched(t1_handle)) || {\n+                let is_home = Task::is_home_using_id(special_id);\n                 rtdebug!(\"t1 should be home: %b\", is_home);\n                 assert!(is_home);\n             };\n             let t1f = Cell::new(t1f);\n \n-            let t2f = ~do Coroutine::new_root(&mut normal_sched.stack_pool) {\n-                let on_special = Coroutine::on_special();\n+            let t2f = ~do Task::new_root(&mut normal_sched.stack_pool) {\n+                let on_special = Task::on_special();\n                 rtdebug!(\"t2 should not be on special: %b\", on_special);\n                 assert!(!on_special);\n             };\n             let t2f = Cell::new(t2f);\n \n-            let t3f = ~do Coroutine::new_root(&mut normal_sched.stack_pool) {\n+            let t3f = ~do Task::new_root(&mut normal_sched.stack_pool) {\n                 // not on special\n-                let on_special = Coroutine::on_special();\n+                let on_special = Task::on_special();\n                 rtdebug!(\"t3 should not be on special: %b\", on_special);\n                 assert!(!on_special);\n             };\n             let t3f = Cell::new(t3f);\n \n-            let t4f = ~do Coroutine::new_homed(&mut special_sched.stack_pool,\n-                                            Sched(t4_handle)) {\n+            let t4f = ~do Task::new_root_homed(&mut special_sched.stack_pool,\n+                                          Sched(t4_handle)) {\n                 // is home\n-                let home = Coroutine::is_home_using_id(special_id);\n+                let home = Task::is_home_using_id(special_id);\n                 rtdebug!(\"t4 should be home: %b\", home);\n                 assert!(home);\n             };\n@@ -988,7 +819,7 @@ mod test {\n             let t4 = Cell::new(t4);\n \n             // build a main task that runs our four tests\n-            let main_task = ~do Coroutine::new_root(&mut normal_sched.stack_pool) {\n+            let main_task = ~do Task::new_root(&mut normal_sched.stack_pool) {\n                 // the two tasks that require a normal start location\n                 t2.take()();\n                 t4.take()();\n@@ -997,7 +828,7 @@ mod test {\n             };\n \n             // task to run the two \"special start\" tests\n-            let special_task = ~do Coroutine::new_homed(\n+            let special_task = ~do Task::new_root_homed(\n                 &mut special_sched.stack_pool,\n                 Sched(special_handle2.take())) {\n                 t1.take()();\n@@ -1027,91 +858,7 @@ mod test {\n         }\n     }\n \n-    // The following test is a bit of a mess, but it trys to do\n-    // something tricky so I'm not sure how to get around this in the\n-    // short term.\n-\n-    // A number of schedulers are created, and then a task is created\n-    // and assigned a home scheduler. It is then \"started\" on a\n-    // different scheduler. The scheduler it is started on should\n-    // observe that the task is not home, and send it home.\n-\n-    // This test is light in that it does very little.\n-\n-    #[test]\n-    fn test_transfer_task_home() {\n-\n-        use rt::uv::uvio::UvEventLoop;\n-        use rt::sched::Shutdown;\n-        use rt::sleeper_list::SleeperList;\n-        use rt::work_queue::WorkQueue;\n-        use uint;\n-        use container::Container;\n-        use vec::OwnedVector;\n-\n-        do run_in_bare_thread {\n-\n-            static N: uint = 8;\n-\n-            let sleepers = SleeperList::new();\n-            let work_queue = WorkQueue::new();\n-\n-            let mut handles = ~[];\n-            let mut scheds = ~[];\n-\n-            for uint::range(0, N) |_| {\n-                let loop_ = ~UvEventLoop::new();\n-                let mut sched = ~Scheduler::new(loop_,\n-                                                work_queue.clone(),\n-                                                sleepers.clone());\n-                let handle = sched.make_handle();\n-                rtdebug!(\"sched id: %u\", handle.sched_id);\n-                handles.push(handle);\n-                scheds.push(sched);\n-            };\n-\n-            let handles = Cell::new(handles);\n-\n-            let home_handle = scheds[6].make_handle();\n-            let home_id = home_handle.sched_id;\n-            let home = Sched(home_handle);\n-\n-            let main_task = ~do Coroutine::new_homed(&mut scheds[1].stack_pool, home) {\n-\n-                // Here we check if the task is running on its home.\n-                let sched = Local::take::<Scheduler>();\n-                rtdebug!(\"run location scheduler id: %u, home: %u\",\n-                         sched.sched_id(),\n-                         home_id);\n-                assert!(sched.sched_id() == home_id);\n-                Local::put::<Scheduler>(sched);\n-\n-                let mut handles = handles.take();\n-                for handles.mut_iter().advance |handle| {\n-                    handle.send(Shutdown);\n-                }\n-            };\n-\n-            scheds[0].enqueue_task(main_task);\n-\n-            let mut threads = ~[];\n-\n-            while !scheds.is_empty() {\n-                let sched = scheds.pop();\n-                let sched_cell = Cell::new(sched);\n-                let thread = do Thread::start {\n-                    let sched = sched_cell.take();\n-                    sched.run();\n-                };\n-                threads.push(thread);\n-            }\n-\n-            let _threads = threads;\n-        }\n-    }\n-\n     // Do it a lot\n-\n     #[test]\n     fn test_stress_schedule_task_states() {\n         let n = stress_factor() * 120;\n@@ -1120,29 +867,14 @@ mod test {\n         }\n     }\n \n-    // The goal is that this is the high-stress test for making sure\n-    // homing is working. It allocates RUST_RT_STRESS tasks that\n-    // do nothing but assert that they are home at execution\n-    // time. These tasks are queued to random schedulers, so sometimes\n-    // they are home and sometimes not. It also runs RUST_RT_STRESS\n-    // times.\n-\n-    #[test]\n-    fn test_stress_homed_tasks() {\n-        let n = stress_factor();\n-        for int::range(0,n as int) |_| {\n-            run_in_mt_newsched_task_random_homed();\n-        }\n-    }\n-\n     #[test]\n     fn test_simple_scheduling() {\n         do run_in_bare_thread {\n             let mut task_ran = false;\n             let task_ran_ptr: *mut bool = &mut task_ran;\n \n             let mut sched = ~new_test_uv_sched();\n-            let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n+            let task = ~do Task::new_root(&mut sched.stack_pool) {\n                 unsafe { *task_ran_ptr = true; }\n             };\n             sched.enqueue_task(task);\n@@ -1160,7 +892,7 @@ mod test {\n \n             let mut sched = ~new_test_uv_sched();\n             for int::range(0, total) |_| {\n-                let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n+                let task = ~do Task::new_root(&mut sched.stack_pool) {\n                     unsafe { *task_count_ptr = *task_count_ptr + 1; }\n                 };\n                 sched.enqueue_task(task);\n@@ -1177,10 +909,10 @@ mod test {\n             let count_ptr: *mut int = &mut count;\n \n             let mut sched = ~new_test_uv_sched();\n-            let task1 = ~do Coroutine::new_root(&mut sched.stack_pool) {\n+            let task1 = ~do Task::new_root(&mut sched.stack_pool) {\n                 unsafe { *count_ptr = *count_ptr + 1; }\n                 let mut sched = Local::take::<Scheduler>();\n-                let task2 = ~do Coroutine::new_root(&mut sched.stack_pool) {\n+                let task2 = ~do Task::new_root(&mut sched.stack_pool) {\n                     unsafe { *count_ptr = *count_ptr + 1; }\n                 };\n                 // Context switch directly to the new task\n@@ -1205,7 +937,7 @@ mod test {\n \n             let mut sched = ~new_test_uv_sched();\n \n-            let start_task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n+            let start_task = ~do Task::new_root(&mut sched.stack_pool) {\n                 run_task(count_ptr);\n             };\n             sched.enqueue_task(start_task);\n@@ -1215,7 +947,7 @@ mod test {\n \n             fn run_task(count_ptr: *mut int) {\n                 do Local::borrow::<Scheduler, ()> |sched| {\n-                    let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n+                    let task = ~do Task::new_root(&mut sched.stack_pool) {\n                         unsafe {\n                             *count_ptr = *count_ptr + 1;\n                             if *count_ptr != MAX {\n@@ -1233,7 +965,7 @@ mod test {\n     fn test_block_task() {\n         do run_in_bare_thread {\n             let mut sched = ~new_test_uv_sched();\n-            let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n+            let task = ~do Task::new_root(&mut sched.stack_pool) {\n                 let sched = Local::take::<Scheduler>();\n                 assert!(sched.in_task_context());\n                 do sched.deschedule_running_task_and_then() |sched, task| {\n@@ -1280,13 +1012,13 @@ mod test {\n             let mut sched1 = ~new_test_uv_sched();\n             let handle1 = sched1.make_handle();\n             let handle1_cell = Cell::new(handle1);\n-            let task1 = ~do Coroutine::new_root(&mut sched1.stack_pool) {\n+            let task1 = ~do Task::new_root(&mut sched1.stack_pool) {\n                 chan_cell.take().send(());\n             };\n             sched1.enqueue_task(task1);\n \n             let mut sched2 = ~new_test_uv_sched();\n-            let task2 = ~do Coroutine::new_root(&mut sched2.stack_pool) {\n+            let task2 = ~do Task::new_root(&mut sched2.stack_pool) {\n                 port_cell.take().recv();\n                 // Release the other scheduler's handle so it can exit\n                 handle1_cell.take();\n@@ -1383,7 +1115,6 @@ mod test {\n                 }\n             }\n         }\n-\n     }\n \n     #[test]\n@@ -1408,5 +1139,4 @@ mod test {\n             }\n         }\n     }\n-\n }"}, {"sha": "b2e4f0d4716ff1f0996c092ed5b97b4e6393ba84", "filename": "src/libstd/rt/task.rs", "status": "modified", "additions": 166, "deletions": 10, "changes": 176, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Ftask.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Ftask.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Ftask.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -23,8 +23,11 @@ use option::{Option, Some, None};\n use rt::local::Local;\n use rt::logging::StdErrLogger;\n use super::local_heap::LocalHeap;\n-use rt::sched::{SchedHome, AnySched};\n+use rt::sched::{Scheduler, SchedHandle};\n use rt::join_latch::JoinLatch;\n+use rt::stack::{StackSegment, StackPool};\n+use rt::context::Context;\n+use cell::Cell;\n \n pub struct Task {\n     heap: LocalHeap,\n@@ -35,7 +38,22 @@ pub struct Task {\n     home: Option<SchedHome>,\n     join_latch: Option<~JoinLatch>,\n     on_exit: Option<~fn(bool)>,\n-    destroyed: bool\n+    destroyed: bool,\n+    coroutine: Option<~Coroutine>\n+}\n+\n+pub struct Coroutine {\n+    /// The segment of stack on which the task is currently running or\n+    /// if the task is blocked, on which the task will resume\n+    /// execution.\n+    priv current_stack_segment: StackSegment,\n+    /// Always valid if the task is alive and not running.\n+    saved_context: Context\n+}\n+\n+pub enum SchedHome {\n+    AnySched,\n+    Sched(SchedHandle)\n }\n \n pub struct GarbageCollector;\n@@ -46,31 +64,50 @@ pub struct Unwinder {\n }\n \n impl Task {\n-    pub fn new_root() -> Task {\n+\n+    pub fn new_root(stack_pool: &mut StackPool,\n+                    start: ~fn()) -> Task {\n+        Task::new_root_homed(stack_pool, AnySched, start)\n+    }\n+\n+    pub fn new_child(&mut self,\n+                     stack_pool: &mut StackPool,\n+                     start: ~fn()) -> Task {\n+        self.new_child_homed(stack_pool, AnySched, start)\n+    }\n+\n+    pub fn new_root_homed(stack_pool: &mut StackPool,\n+                          home: SchedHome,\n+                          start: ~fn()) -> Task {\n         Task {\n             heap: LocalHeap::new(),\n             gc: GarbageCollector,\n             storage: LocalStorage(ptr::null(), None),\n             logger: StdErrLogger,\n             unwinder: Unwinder { unwinding: false },\n-            home: Some(AnySched),\n+            home: Some(home),\n             join_latch: Some(JoinLatch::new_root()),\n             on_exit: None,\n-            destroyed: false\n+            destroyed: false,\n+            coroutine: Some(~Coroutine::new(stack_pool, start))\n         }\n     }\n \n-    pub fn new_child(&mut self) -> Task {\n+    pub fn new_child_homed(&mut self,\n+                           stack_pool: &mut StackPool,\n+                           home: SchedHome,\n+                           start: ~fn()) -> Task {\n         Task {\n             heap: LocalHeap::new(),\n             gc: GarbageCollector,\n             storage: LocalStorage(ptr::null(), None),\n             logger: StdErrLogger,\n-            home: Some(AnySched),\n+            home: Some(home),\n             unwinder: Unwinder { unwinding: false },\n             join_latch: Some(self.join_latch.get_mut_ref().new_child()),\n             on_exit: None,\n-            destroyed: false\n+            destroyed: false,\n+            coroutine: Some(~Coroutine::new(stack_pool, start))\n         }\n     }\n \n@@ -108,11 +145,11 @@ impl Task {\n     /// called unsafely, without removing Task from\n     /// thread-local-storage.\n     fn destroy(&mut self) {\n-        // This is just an assertion that `destroy` was called unsafely\n-        // and this instance of Task is still accessible.\n+\n         do Local::borrow::<Task, ()> |task| {\n             assert!(borrow::ref_eq(task, self));\n         }\n+\n         match self.storage {\n             LocalStorage(ptr, Some(ref dtor)) => {\n                 (*dtor)(ptr)\n@@ -125,12 +162,129 @@ impl Task {\n \n         self.destroyed = true;\n     }\n+\n+    /// Check if *task* is currently home.\n+    pub fn is_home(&self) -> bool {\n+        do Local::borrow::<Scheduler,bool> |sched| {\n+            match self.home {\n+                Some(AnySched) => { false }\n+                Some(Sched(SchedHandle { sched_id: ref id, _ })) => {\n+                    *id == sched.sched_id()\n+                }\n+                None => { rtabort!(\"task home of None\") }\n+            }\n+        }\n+    }\n+\n+    pub fn is_home_no_tls(&self, sched: &~Scheduler) -> bool {\n+        match self.home {\n+            Some(AnySched) => { false }\n+            Some(Sched(SchedHandle { sched_id: ref id, _ })) => {\n+                *id == sched.sched_id()\n+            }\n+            None => {rtabort!(\"task home of None\") }\n+        }\n+    }\n+\n+    pub fn is_home_using_id(sched_id: uint) -> bool {\n+        do Local::borrow::<Task,bool> |task| {\n+            match task.home {\n+                Some(Sched(SchedHandle { sched_id: ref id, _ })) => {\n+                    *id == sched_id\n+                }\n+                Some(AnySched) => { false }\n+                None => { rtabort!(\"task home of None\") }\n+            }\n+        }\n+    }\n+\n+    /// Check if this *task* has a home.\n+    pub fn homed(&self) -> bool {\n+        match self.home {\n+            Some(AnySched) => { false }\n+            Some(Sched(_)) => { true }\n+            None => {\n+                rtabort!(\"task home of None\")\n+            }\n+        }\n+    }\n+\n+    /// On a special scheduler?\n+    pub fn on_special() -> bool {\n+        do Local::borrow::<Scheduler,bool> |sched| {\n+            !sched.run_anything\n+        }\n+    }\n+\n }\n \n impl Drop for Task {\n     fn finalize(&self) { assert!(self.destroyed) }\n }\n \n+// Coroutines represent nothing more than a context and a stack\n+// segment.\n+\n+impl Coroutine {\n+\n+    pub fn new(stack_pool: &mut StackPool, start: ~fn()) -> Coroutine {\n+        static MIN_STACK_SIZE: uint = 100000; // XXX: Too much stack\n+\n+        let start = Coroutine::build_start_wrapper(start);\n+        let mut stack = stack_pool.take_segment(MIN_STACK_SIZE);\n+        let initial_context = Context::new(start, &mut stack);\n+        Coroutine {\n+            current_stack_segment: stack,\n+            saved_context: initial_context\n+        }\n+    }\n+\n+    fn build_start_wrapper(start: ~fn()) -> ~fn() {\n+        let start_cell = Cell::new(start);\n+        let wrapper: ~fn() = || {\n+            // First code after swap to this new context. Run our\n+            // cleanup job.\n+            unsafe {\n+                let sched = Local::unsafe_borrow::<Scheduler>();\n+                (*sched).run_cleanup_job();\n+\n+                let sched = Local::unsafe_borrow::<Scheduler>();\n+                let task = (*sched).current_task.get_mut_ref();\n+\n+                do task.run {\n+                    // N.B. Removing `start` from the start wrapper\n+                    // closure by emptying a cell is critical for\n+                    // correctness. The ~Task pointer, and in turn the\n+                    // closure used to initialize the first call\n+                    // frame, is destroyed in the scheduler context,\n+                    // not task context. So any captured closures must\n+                    // not contain user-definable dtors that expect to\n+                    // be in task context. By moving `start` out of\n+                    // the closure, all the user code goes our of\n+                    // scope while the task is still running.\n+                    let start = start_cell.take();\n+                    start();\n+                };\n+            }\n+\n+            let sched = Local::take::<Scheduler>();\n+            sched.terminate_current_task();\n+        };\n+        return wrapper;\n+    }\n+\n+    /// Destroy coroutine and try to reuse stack segment.\n+    pub fn recycle(~self, stack_pool: &mut StackPool) {\n+        match self {\n+            ~Coroutine { current_stack_segment, _ } => {\n+                stack_pool.give_segment(current_stack_segment);\n+            }\n+        }\n+    }\n+\n+}\n+\n+\n // Just a sanity check to make sure we are catching a Rust-thrown exception\n static UNWIND_TOKEN: uintptr_t = 839147;\n \n@@ -209,8 +363,10 @@ mod test {\n     fn unwind() {\n         do run_in_newsched_task() {\n             let result = spawntask_try(||());\n+            rtdebug!(\"trying first assert\");\n             assert!(result.is_ok());\n             let result = spawntask_try(|| fail!());\n+            rtdebug!(\"trying second assert\");\n             assert!(result.is_err());\n         }\n     }"}, {"sha": "a1483be8177239e5ab4c163d7c30a0a8caa6c6a9", "filename": "src/libstd/rt/test.rs", "status": "modified", "additions": 97, "deletions": 174, "changes": 271, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Ftest.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -15,23 +15,24 @@ use clone::Clone;\n use container::Container;\n use iterator::IteratorUtil;\n use vec::{OwnedVector, MutableVector};\n-use result::{Result, Ok, Err};\n-use unstable::run_in_bare_thread;\n use super::io::net::ip::{IpAddr, Ipv4, Ipv6};\n-use rt::comm::oneshot;\n-use rt::task::Task;\n-use rt::thread::Thread;\n+use rt::sched::Scheduler;\n use rt::local::Local;\n-use rt::sched::{Scheduler, Coroutine};\n-use rt::sleeper_list::SleeperList;\n+use unstable::run_in_bare_thread;\n+use rt::thread::Thread;\n+use rt::task::Task;\n+use rt::uv::uvio::UvEventLoop;\n use rt::work_queue::WorkQueue;\n+use rt::sleeper_list::SleeperList;\n+use rt::task::{Sched};\n+use rt::comm::oneshot;\n+use result::{Result, Ok, Err};\n \n pub fn new_test_uv_sched() -> Scheduler {\n-    use rt::uv::uvio::UvEventLoop;\n-    use rt::work_queue::WorkQueue;\n-    use rt::sleeper_list::SleeperList;\n \n-    let mut sched = Scheduler::new(~UvEventLoop::new(), WorkQueue::new(), SleeperList::new());\n+    let mut sched = Scheduler::new(~UvEventLoop::new(),\n+                                   WorkQueue::new(),\n+                                   SleeperList::new());\n     // Don't wait for the Shutdown message\n     sched.no_sleep = true;\n     return sched;\n@@ -41,19 +42,15 @@ pub fn new_test_uv_sched() -> Scheduler {\n /// then waits for the scheduler to exit. Failure of the task\n /// will abort the process.\n pub fn run_in_newsched_task(f: ~fn()) {\n-    use super::sched::*;\n-    use unstable::run_in_bare_thread;\n-\n     let f = Cell::new(f);\n \n     do run_in_bare_thread {\n         let mut sched = ~new_test_uv_sched();\n-        let mut new_task = ~Task::new_root();\n         let on_exit: ~fn(bool) = |exit_status| rtassert!(exit_status);\n-        new_task.on_exit = Some(on_exit);\n-        let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                         new_task,\n-                                         f.take());\n+        let mut task = ~Task::new_root(&mut sched.stack_pool,\n+                                       f.take());\n+        rtdebug!(\"newsched_task: %x\", to_uint(task));\n+        task.on_exit = Some(on_exit);\n         sched.enqueue_task(task);\n         sched.run();\n     }\n@@ -65,7 +62,6 @@ pub fn run_in_newsched_task(f: ~fn()) {\n pub fn run_in_mt_newsched_task(f: ~fn()) {\n     use os;\n     use from_str::FromStr;\n-    use rt::uv::uvio::UvEventLoop;\n     use rt::sched::Shutdown;\n     use rt::util;\n \n@@ -90,7 +86,9 @@ pub fn run_in_mt_newsched_task(f: ~fn()) {\n \n         for uint::range(0, nthreads) |_| {\n             let loop_ = ~UvEventLoop::new();\n-            let mut sched = ~Scheduler::new(loop_, work_queue.clone(), sleepers.clone());\n+            let mut sched = ~Scheduler::new(loop_,\n+                                            work_queue.clone(),\n+                                            sleepers.clone());\n             let handle = sched.make_handle();\n \n             handles.push(handle);\n@@ -99,9 +97,7 @@ pub fn run_in_mt_newsched_task(f: ~fn()) {\n \n         let f_cell = Cell::new(f_cell.take());\n         let handles = Cell::new(handles);\n-        let mut new_task = ~Task::new_root();\n         let on_exit: ~fn(bool) = |exit_status| {\n-\n             let mut handles = handles.take();\n             // Tell schedulers to exit\n             for handles.mut_iter().advance |handle| {\n@@ -110,9 +106,9 @@ pub fn run_in_mt_newsched_task(f: ~fn()) {\n \n             rtassert!(exit_status);\n         };\n-        new_task.on_exit = Some(on_exit);\n-        let main_task = ~Coroutine::with_task(&mut scheds[0].stack_pool,\n-                                              new_task, f_cell.take());\n+        let mut main_task = ~Task::new_root(&mut scheds[0].stack_pool,\n+                                        f_cell.take());\n+        main_task.on_exit = Some(on_exit);\n         scheds[0].enqueue_task(main_task);\n \n         let mut threads = ~[];\n@@ -134,144 +130,44 @@ pub fn run_in_mt_newsched_task(f: ~fn()) {\n \n }\n \n-// THIS IS AWFUL. Copy-pasted the above initialization function but\n-// with a number of hacks to make it spawn tasks on a variety of\n-// schedulers with a variety of homes using the new spawn.\n-\n-pub fn run_in_mt_newsched_task_random_homed() {\n-    use libc;\n-    use os;\n-    use from_str::FromStr;\n-    use rt::uv::uvio::UvEventLoop;\n-    use rt::sched::Shutdown;\n-\n-    do run_in_bare_thread {\n-        let nthreads = match os::getenv(\"RUST_TEST_THREADS\") {\n-            Some(nstr) => FromStr::from_str(nstr).get(),\n-            None => unsafe {\n-                // Using more threads than cores in test code to force\n-                // the OS to preempt them frequently.  Assuming that\n-                // this help stress test concurrent types.\n-                rust_get_num_cpus() * 2\n-            }\n-        };\n-\n-        let sleepers = SleeperList::new();\n-        let work_queue = WorkQueue::new();\n-\n-        let mut handles = ~[];\n-        let mut scheds = ~[];\n-\n-        // create a few special schedulers, those with even indicies\n-        // will be pinned-only\n-        for uint::range(0, nthreads) |i| {\n-            let special = (i % 2) == 0;\n-            let loop_ = ~UvEventLoop::new();\n-            let mut sched = ~Scheduler::new_special(\n-                loop_, work_queue.clone(), sleepers.clone(), special);\n-            let handle = sched.make_handle();\n-            handles.push(handle);\n-            scheds.push(sched);\n-        }\n-\n-        // Schedule a pile o tasks\n-        let n = 5*stress_factor();\n-        for uint::range(0,n) |_i| {\n-                rtdebug!(\"creating task: %u\", _i);\n-                let hf: ~fn() = || { assert!(true) };\n-                spawntask_homed(&mut scheds, hf);\n-            }\n-\n-        // Now we want another pile o tasks that do not ever run on a\n-        // special scheduler, because they are normal tasks. Because\n-        // we can we put these in the \"main\" task.\n-\n-        let n = 5*stress_factor();\n-\n-        let f: ~fn() = || {\n-            for uint::range(0,n) |_| {\n-                let f: ~fn()  = || {\n-                    // Borrow the scheduler we run on and check if it is\n-                    // privileged.\n-                    do Local::borrow::<Scheduler,()> |sched| {\n-                        assert!(sched.run_anything);\n-                    };\n-                };\n-                spawntask_random(f);\n-            };\n-        };\n-\n-        let f_cell = Cell::new(f);\n-        let handles = Cell::new(handles);\n-\n-        rtdebug!(\"creating main task\");\n-\n-        let main_task = ~do Coroutine::new_root(&mut scheds[0].stack_pool) {\n-            f_cell.take()();\n-            let mut handles = handles.take();\n-            // Tell schedulers to exit\n-            for handles.mut_iter().advance |handle| {\n-                handle.send(Shutdown);\n-            }\n-        };\n-\n-        rtdebug!(\"queuing main task\")\n-\n-        scheds[0].enqueue_task(main_task);\n-\n-        let mut threads = ~[];\n-\n-        while !scheds.is_empty() {\n-            let sched = scheds.pop();\n-            let sched_cell = Cell::new(sched);\n-            let thread = do Thread::start {\n-                let sched = sched_cell.take();\n-                rtdebug!(\"running sched: %u\", sched.sched_id());\n-                sched.run();\n-            };\n-\n-            threads.push(thread);\n-        }\n-\n-        rtdebug!(\"waiting on scheduler threads\");\n-\n-        // Wait for schedulers\n-        let _threads = threads;\n-    }\n-\n-    extern {\n-        fn rust_get_num_cpus() -> libc::uintptr_t;\n-    }\n-}\n-\n-\n /// Test tasks will abort on failure instead of unwinding\n pub fn spawntask(f: ~fn()) {\n     use super::sched::*;\n+    let f = Cell::new(f);\n+\n+    let task = unsafe {\n+        let sched = Local::unsafe_borrow::<Scheduler>();\n+        rtdebug!(\"spawntask taking the scheduler from TLS\");\n \n-    rtdebug!(\"spawntask taking the scheduler from TLS\")\n-    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n-        ~running_task.new_child()\n+\n+        do Local::borrow::<Task, ~Task>() |running_task| {\n+            ~running_task.new_child(&mut (*sched).stack_pool, f.take())\n+        }\n     };\n \n-    let mut sched = Local::take::<Scheduler>();\n-    let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     task, f);\n+    rtdebug!(\"new task pointer: %x\", to_uint(task));\n+\n+    let sched = Local::take::<Scheduler>();\n     rtdebug!(\"spawntask scheduling the new task\");\n     sched.schedule_task(task);\n }\n \n+\n /// Create a new task and run it right now. Aborts on failure\n pub fn spawntask_immediately(f: ~fn()) {\n     use super::sched::*;\n \n-    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n-        ~running_task.new_child()\n+    let f = Cell::new(f);\n+\n+    let task = unsafe {\n+        let sched = Local::unsafe_borrow::<Scheduler>();\n+        do Local::borrow::<Task, ~Task>() |running_task| {\n+            ~running_task.new_child(&mut (*sched).stack_pool,\n+                                    f.take())\n+        }\n     };\n \n-    let mut sched = Local::take::<Scheduler>();\n-    let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     task, f);\n+    let sched = Local::take::<Scheduler>();\n     do sched.switch_running_tasks_and_then(task) |sched, task| {\n         sched.enqueue_task(task);\n     }\n@@ -280,15 +176,16 @@ pub fn spawntask_immediately(f: ~fn()) {\n /// Create a new task and run it right now. Aborts on failure\n pub fn spawntask_later(f: ~fn()) {\n     use super::sched::*;\n+    let f = Cell::new(f);\n \n-    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n-        ~running_task.new_child()\n+    let task = unsafe {\n+        let sched = Local::unsafe_borrow::<Scheduler>();\n+        do Local::borrow::<Task, ~Task>() |running_task| {\n+            ~running_task.new_child(&mut (*sched).stack_pool, f.take())\n+        }\n     };\n \n     let mut sched = Local::take::<Scheduler>();\n-    let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     task, f);\n-\n     sched.enqueue_task(task);\n     Local::put(sched);\n }\n@@ -298,13 +195,18 @@ pub fn spawntask_random(f: ~fn()) {\n     use super::sched::*;\n     use rand::{Rand, rng};\n \n-    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n-        ~running_task.new_child()\n+    let f = Cell::new(f);\n+\n+    let task = unsafe {\n+        let sched = Local::unsafe_borrow::<Scheduler>();\n+        do Local::borrow::<Task, ~Task>() |running_task| {\n+            ~running_task.new_child(&mut (*sched).stack_pool,\n+                                    f.take())\n+\n+        }\n     };\n \n     let mut sched = Local::take::<Scheduler>();\n-    let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     task, f);\n \n     let mut rng = rng();\n     let run_now: bool = Rand::rand(&mut rng);\n@@ -343,33 +245,49 @@ pub fn spawntask_homed(scheds: &mut ~[~Scheduler], f: ~fn()) {\n             f()\n         };\n \n-        ~Coroutine::with_task_homed(&mut sched.stack_pool,\n-                                    ~Task::new_root(),\n-                                    af,\n-                                    Sched(handle))\n+        ~Task::new_root_homed(&mut sched.stack_pool,\n+                              Sched(handle),\n+                              af)\n     };\n     let dest_sched = &mut scheds[rng.gen_int_range(0,scheds.len() as int)];\n     // enqueue it for future execution\n     dest_sched.enqueue_task(task);\n }\n \n-/// Spawn a task and wait for it to finish, returning whether it completed successfully or failed\n+/// Spawn a task and wait for it to finish, returning whether it\n+/// completed successfully or failed\n pub fn spawntask_try(f: ~fn()) -> Result<(), ()> {\n     use cell::Cell;\n     use super::sched::*;\n \n+    let f = Cell::new(f);\n+\n     let (port, chan) = oneshot();\n     let chan = Cell::new(chan);\n-    let mut new_task = ~Task::new_root();\n     let on_exit: ~fn(bool) = |exit_status| chan.take().send(exit_status);\n+    let mut new_task = unsafe {\n+        let sched = Local::unsafe_borrow::<Scheduler>();\n+        do Local::borrow::<Task, ~Task> |_running_task| {\n+\n+            // I don't understand why using a child task here fails. I\n+            // think the fail status is propogating back up the task\n+            // tree and triggering a fail for the parent, which we\n+            // aren't correctly expecting.\n+\n+            // ~running_task.new_child(&mut (*sched).stack_pool,\n+            ~Task::new_root(&mut (*sched).stack_pool,\n+                           f.take())\n+        }\n+    };\n     new_task.on_exit = Some(on_exit);\n-    let mut sched = Local::take::<Scheduler>();\n-    let new_task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                         new_task, f);\n+\n+    let sched = Local::take::<Scheduler>();\n     do sched.switch_running_tasks_and_then(new_task) |sched, old_task| {\n         sched.enqueue_task(old_task);\n     }\n \n+    rtdebug!(\"enqueued the new task, now waiting on exit_status\");\n+\n     let exit_status = port.recv();\n     if exit_status { Ok(()) } else { Err(()) }\n }\n@@ -378,23 +296,27 @@ pub fn spawntask_try(f: ~fn()) -> Result<(), ()> {\n pub fn spawntask_thread(f: ~fn()) -> Thread {\n     use rt::sched::*;\n \n-    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n-        ~running_task.new_child()\n+    let f = Cell::new(f);\n+\n+    let task = unsafe {\n+        let sched = Local::unsafe_borrow::<Scheduler>();\n+        do Local::borrow::<Task, ~Task>() |running_task| {\n+            ~running_task.new_child(&mut (*sched).stack_pool,\n+                                    f.take())\n+        }\n     };\n \n     let task = Cell::new(task);\n-    let f = Cell::new(f);\n+\n     let thread = do Thread::start {\n         let mut sched = ~new_test_uv_sched();\n-        let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                         task.take(),\n-                                         f.take());\n-        sched.enqueue_task(task);\n+        sched.enqueue_task(task.take());\n         sched.run();\n     };\n     return thread;\n }\n \n+\n /// Get a port number, starting at 9600, for use in tests\n pub fn next_test_port() -> u16 {\n     unsafe {\n@@ -415,7 +337,8 @@ pub fn next_test_ip6() -> IpAddr {\n     Ipv6(0, 0, 0, 0, 0, 0, 0, 1, next_test_port())\n }\n \n-/// Get a constant that represents the number of times to repeat stress tests. Default 1.\n+/// Get a constant that represents the number of times to repeat\n+/// stress tests. Default 1.\n pub fn stress_factor() -> uint {\n     use os::getenv;\n "}, {"sha": "013eb438c3657f1d7a34da7d8f216a593b5f288c", "filename": "src/libstd/rt/tube.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Ftube.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Ftube.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Ftube.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -16,14 +16,15 @@\n use option::*;\n use clone::Clone;\n use super::rc::RC;\n-use rt::sched::{Scheduler, Coroutine};\n+use rt::sched::Scheduler;\n use rt::{context, TaskContext, SchedulerContext};\n use rt::local::Local;\n+use rt::task::Task;\n use vec::OwnedVector;\n use container::Container;\n \n struct TubeState<T> {\n-    blocked_task: Option<~Coroutine>,\n+    blocked_task: Option<~Task>,\n     buf: ~[T]\n }\n "}, {"sha": "aa1f0fbc1942a0c090b349442ffb151911dc494a", "filename": "src/libstd/rt/uv/uvio.rs", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fuv%2Fuvio.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Frt%2Fuv%2Fuvio.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fuv%2Fuvio.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -29,7 +29,10 @@ use unstable::sync::{Exclusive, exclusive};\n #[cfg(test)] use container::Container;\n #[cfg(test)] use uint;\n #[cfg(test)] use unstable::run_in_bare_thread;\n-#[cfg(test)] use rt::test::*;\n+#[cfg(test)] use rt::test::{spawntask_immediately,\n+                            next_test_ip4,\n+                            run_in_newsched_task};\n+\n \n pub struct UvEventLoop {\n     uvio: UvIoFactory"}, {"sha": "ad6f1d23c10ac231d1b9ae99a33af91b5fd05f0f", "filename": "src/libstd/sys.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Fsys.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Fsys.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsys.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -203,8 +203,8 @@ pub fn begin_unwind_(msg: *c_char, file: *c_char, line: size_t) -> ! {\n                 let msg = str::raw::from_c_str(msg);\n                 let file = str::raw::from_c_str(file);\n \n-                let outmsg = fmt!(\"task failed: '%s' at line %i of file %s\",\n-                                  msg, line as int, file);\n+                let outmsg = fmt!(\"task failed at '%s', %s:%i\",\n+                                  msg, file, line as int);\n \n                 // XXX: Logging doesn't work correctly in non-task context because it\n                 // invokes the local heap"}, {"sha": "b0fc6b2884f01e49e093036bdebb3b2f19853074", "filename": "src/libstd/task/mod.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Ftask%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Ftask%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Ftask%2Fmod.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -1182,3 +1182,4 @@ fn test_simple_newsched_spawn() {\n         spawn(||())\n     }\n }\n+"}, {"sha": "aea8cda6a21028f7f9e4d755dfe6b4f48cd1fd58", "filename": "src/libstd/task/spawn.rs", "status": "modified", "additions": 19, "deletions": 8, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Ftask%2Fspawn.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Ftask%2Fspawn.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Ftask%2Fspawn.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -581,13 +581,20 @@ pub fn spawn_raw(opts: TaskOpts, f: ~fn()) {\n fn spawn_raw_newsched(mut opts: TaskOpts, f: ~fn()) {\n     use rt::sched::*;\n \n-    let mut task = if opts.linked {\n-        do Local::borrow::<Task, ~Task>() |running_task| {\n-            ~running_task.new_child()\n+    let f = Cell::new(f);\n+\n+    let mut task = unsafe {\n+        let sched = Local::unsafe_borrow::<Scheduler>();\n+        rtdebug!(\"unsafe borrowed sched\");\n+\n+        if opts.linked {\n+            do Local::borrow::<Task, ~Task>() |running_task| {\n+                ~running_task.new_child(&mut (*sched).stack_pool, f.take())\n+            }\n+        } else {\n+            // An unlinked task is a new root in the task tree\n+            ~Task::new_root(&mut (*sched).stack_pool, f.take())\n         }\n-    } else {\n-        // An unlinked task is a new root in the task tree\n-        ~Task::new_root()\n     };\n \n     if opts.notify_chan.is_some() {\n@@ -601,9 +608,13 @@ fn spawn_raw_newsched(mut opts: TaskOpts, f: ~fn()) {\n         task.on_exit = Some(on_exit);\n     }\n \n+    rtdebug!(\"spawn about to take scheduler\");\n+\n     let mut sched = Local::take::<Scheduler>();\n-    let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     task, f);\n+    rtdebug!(\"took sched in spawn\");\n+//    let task = ~Coroutine::with_task(&mut sched.stack_pool,\n+//                                     task, f);\n+//    let task = ~Task::new_root(&mut sched.stack_pool, f);\n     sched.schedule_task(task);\n }\n "}, {"sha": "3a071af5d4cfd09dc93a78e1cc8d4c1ff6634cc6", "filename": "src/libstd/unstable/lang.rs", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Funstable%2Flang.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Flibstd%2Funstable%2Flang.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Flang.rs?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -23,6 +23,7 @@ use option::{Option, Some, None};\n use io;\n use rt::global_heap;\n use rt::borrowck;\n+use borrow::to_uint;\n \n #[allow(non_camel_case_types)]\n pub type rust_task = c_void;\n@@ -90,6 +91,9 @@ pub unsafe fn local_malloc(td: *c_char, size: uintptr_t) -> *c_char {\n         _ => {\n             let mut alloc = ::ptr::null();\n             do Local::borrow::<Task,()> |task| {\n+                rtdebug!(\"task pointer: %x, heap pointer: %x\",\n+                         to_uint(task),\n+                         to_uint(&task.heap));\n                 alloc = task.heap.alloc(td as *c_void, size as uint) as *c_char;\n             }\n             return alloc;"}, {"sha": "22b926b180056335603ad4ae4c533b3344f93c68", "filename": "src/rt/rustrt.def.in", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Frt%2Frustrt.def.in", "raw_url": "https://github.com/rust-lang/rust/raw/6a1a7819c9cb15e280681c547548ff8d47412f04/src%2Frt%2Frustrt.def.in", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Frustrt.def.in?ref=6a1a7819c9cb15e280681c547548ff8d47412f04", "patch": "@@ -269,3 +269,5 @@ rust_running_on_valgrind\n rust_get_num_cpus\n rust_get_global_args_ptr\n rust_current_boxed_region\n+rust_take_global_args_lock\n+rust_drop_global_args_lock"}]}