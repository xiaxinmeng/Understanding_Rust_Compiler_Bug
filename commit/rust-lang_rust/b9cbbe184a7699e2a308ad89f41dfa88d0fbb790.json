{"sha": "b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "node_id": "MDY6Q29tbWl0NzI0NzEyOmI5Y2JiZTE4NGE3Njk5ZTJhMzA4YWQ4OWY0MWRmYTg4ZDBmYmI3OTA=", "commit": {"author": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2016-11-04T23:49:31Z"}, "committer": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2016-11-05T17:50:24Z"}, "message": "Rollup merge of #37569 - jseyfried:improve_expansion_perf, r=eddyb\n\nmacros: improve expansion performance\n\nThis PR fixes that regression, further improves performance on recursive, `tt`-heavy workloads, and makes a variety of other improvements to parsing and expansion performance.\n\nExpansion performance improvements:\n\n| Test case      | Run-time | Memory usage |\n| -------------- | -------- | ------------ |\n| libsyntax      | 8%       | 10%          |\n| librustc       | 15%      | 6%           |\n| librustc_trans | 30%      | 6%           |\n| #37074         | 20%      | 15%          |\n| #34630         | 40%      | 8%           |\n\nr? @eddyb", "tree": {"sha": "137bfa09aaf68e50ac36de4aa272e68d3f6150a1", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/137bfa09aaf68e50ac36de4aa272e68d3f6150a1"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "html_url": "https://github.com/rust-lang/rust/commit/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/comments", "author": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "committer": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "9f882b997eaff8def0b31db22fe250776d38af60", "url": "https://api.github.com/repos/rust-lang/rust/commits/9f882b997eaff8def0b31db22fe250776d38af60", "html_url": "https://github.com/rust-lang/rust/commit/9f882b997eaff8def0b31db22fe250776d38af60"}, {"sha": "51104e5ca60177b9f646f0c906eac358050664b7", "url": "https://api.github.com/repos/rust-lang/rust/commits/51104e5ca60177b9f646f0c906eac358050664b7", "html_url": "https://github.com/rust-lang/rust/commit/51104e5ca60177b9f646f0c906eac358050664b7"}], "stats": {"total": 801, "additions": 353, "deletions": 448}, "files": [{"sha": "105a9e099b1a4e50af9c1f30cabd518f25f53a88", "filename": "src/librustc/hir/lowering.rs", "status": "modified", "additions": 24, "deletions": 30, "changes": 54, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibrustc%2Fhir%2Flowering.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibrustc%2Fhir%2Flowering.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Flowering.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -1208,36 +1208,30 @@ impl<'a> LoweringContext<'a> {\n                 ExprKind::Break(opt_ident) => hir::ExprBreak(self.lower_opt_sp_ident(opt_ident)),\n                 ExprKind::Continue(opt_ident) => hir::ExprAgain(self.lower_opt_sp_ident(opt_ident)),\n                 ExprKind::Ret(ref e) => hir::ExprRet(e.as_ref().map(|x| self.lower_expr(x))),\n-                ExprKind::InlineAsm(InlineAsm {\n-                        ref inputs,\n-                        ref outputs,\n-                        ref asm,\n-                        asm_str_style,\n-                        ref clobbers,\n-                        volatile,\n-                        alignstack,\n-                        dialect,\n-                        expn_id,\n-                    }) => hir::ExprInlineAsm(P(hir::InlineAsm {\n-                    inputs: inputs.iter().map(|&(ref c, _)| c.clone()).collect(),\n-                    outputs: outputs.iter()\n-                                    .map(|out| {\n-                                        hir::InlineAsmOutput {\n-                                            constraint: out.constraint.clone(),\n-                                            is_rw: out.is_rw,\n-                                            is_indirect: out.is_indirect,\n-                                        }\n-                                    })\n-                                    .collect(),\n-                    asm: asm.clone(),\n-                    asm_str_style: asm_str_style,\n-                    clobbers: clobbers.clone().into(),\n-                    volatile: volatile,\n-                    alignstack: alignstack,\n-                    dialect: dialect,\n-                    expn_id: expn_id,\n-                }), outputs.iter().map(|out| self.lower_expr(&out.expr)).collect(),\n-                   inputs.iter().map(|&(_, ref input)| self.lower_expr(input)).collect()),\n+                ExprKind::InlineAsm(ref asm) => {\n+                    let hir_asm = hir::InlineAsm {\n+                        inputs: asm.inputs.iter().map(|&(ref c, _)| c.clone()).collect(),\n+                        outputs: asm.outputs.iter().map(|out| {\n+                            hir::InlineAsmOutput {\n+                                constraint: out.constraint.clone(),\n+                                is_rw: out.is_rw,\n+                                is_indirect: out.is_indirect,\n+                            }\n+                        }).collect(),\n+                        asm: asm.asm.clone(),\n+                        asm_str_style: asm.asm_str_style,\n+                        clobbers: asm.clobbers.clone().into(),\n+                        volatile: asm.volatile,\n+                        alignstack: asm.alignstack,\n+                        dialect: asm.dialect,\n+                        expn_id: asm.expn_id,\n+                    };\n+                    let outputs =\n+                        asm.outputs.iter().map(|out| self.lower_expr(&out.expr)).collect();\n+                    let inputs =\n+                        asm.inputs.iter().map(|&(_, ref input)| self.lower_expr(input)).collect();\n+                    hir::ExprInlineAsm(P(hir_asm), outputs, inputs)\n+                }\n                 ExprKind::Struct(ref path, ref fields, ref maybe_expr) => {\n                     hir::ExprStruct(self.lower_path(path),\n                                     fields.iter().map(|x| self.lower_field(x)).collect(),"}, {"sha": "f7581924eb19fadb7770d73ef468420098ad767e", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -1050,7 +1050,7 @@ pub enum ExprKind {\n     Ret(Option<P<Expr>>),\n \n     /// Output of the `asm!()` macro\n-    InlineAsm(InlineAsm),\n+    InlineAsm(P<InlineAsm>),\n \n     /// A macro invocation; pre-expansion\n     Mac(Mac),"}, {"sha": "1f47a91fcc13e8372dcd621a7a7bac50cc65ca35", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -615,7 +615,9 @@ impl<'a> ExtCtxt<'a> {\n \n     pub fn new_parser_from_tts(&self, tts: &[tokenstream::TokenTree])\n         -> parser::Parser<'a> {\n-        parse::tts_to_parser(self.parse_sess, tts.to_vec())\n+        let mut parser = parse::tts_to_parser(self.parse_sess, tts.to_vec());\n+        parser.allow_interpolated_tts = false; // FIXME(jseyfried) `quote!` can't handle these yet\n+        parser\n     }\n     pub fn codemap(&self) -> &'a CodeMap { self.parse_sess.codemap() }\n     pub fn parse_sess(&self) -> &'a parse::ParseSess { self.parse_sess }"}, {"sha": "969cfa292ce8068315fea22fd6d587851647264f", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 32, "deletions": 22, "changes": 54, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -80,67 +80,71 @@ pub mod rt {\n \n     impl ToTokens for ast::Path {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(DUMMY_SP,\n-                                  token::Interpolated(token::NtPath(Box::new(self.clone()))))]\n+            let nt = token::NtPath(self.clone());\n+            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::Ty {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(self.span, token::Interpolated(token::NtTy(P(self.clone()))))]\n+            let nt = token::NtTy(P(self.clone()));\n+            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::Block {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(self.span, token::Interpolated(token::NtBlock(P(self.clone()))))]\n+            let nt = token::NtBlock(P(self.clone()));\n+            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::Generics {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(token::NtGenerics(self.clone())))]\n+            let nt = token::NtGenerics(self.clone());\n+            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::WhereClause {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(DUMMY_SP,\n-                                  token::Interpolated(token::NtWhereClause(self.clone())))]\n+            let nt = token::NtWhereClause(self.clone());\n+            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for P<ast::Item> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(self.span, token::Interpolated(token::NtItem(self.clone())))]\n+            let nt = token::NtItem(self.clone());\n+            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::ImplItem {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(self.span,\n-                                  token::Interpolated(token::NtImplItem(P(self.clone()))))]\n+            let nt = token::NtImplItem(self.clone());\n+            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for P<ast::ImplItem> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(self.span, token::Interpolated(token::NtImplItem(self.clone())))]\n+            let nt = token::NtImplItem((**self).clone());\n+            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::TraitItem {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(self.span,\n-                                  token::Interpolated(token::NtTraitItem(P(self.clone()))))]\n+            let nt = token::NtTraitItem(self.clone());\n+            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::Stmt {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            let mut tts = vec![\n-                TokenTree::Token(self.span, token::Interpolated(token::NtStmt(P(self.clone()))))\n-            ];\n+            let nt = token::NtStmt(self.clone());\n+            let mut tts = vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))];\n \n             // Some statements require a trailing semicolon.\n             if classify::stmt_ends_with_semi(&self.node) {\n@@ -153,31 +157,36 @@ pub mod rt {\n \n     impl ToTokens for P<ast::Expr> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(self.span, token::Interpolated(token::NtExpr(self.clone())))]\n+            let nt = token::NtExpr(self.clone());\n+            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for P<ast::Pat> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(self.span, token::Interpolated(token::NtPat(self.clone())))]\n+            let nt = token::NtPat(self.clone());\n+            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::Arm {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(token::NtArm(self.clone())))]\n+            let nt = token::NtArm(self.clone());\n+            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for ast::Arg {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(token::NtArg(self.clone())))]\n+            let nt = token::NtArg(self.clone());\n+            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n     impl ToTokens for P<ast::Block> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(token::NtBlock(self.clone())))]\n+            let nt = token::NtBlock(self.clone());\n+            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n \n@@ -204,7 +213,8 @@ pub mod rt {\n \n     impl ToTokens for P<ast::MetaItem> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(token::NtMeta(self.clone())))]\n+            let nt = token::NtMeta(self.clone());\n+            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n         }\n     }\n "}, {"sha": "1066646aa8e8a82a2c7830b6631502634e5d2401", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 32, "deletions": 29, "changes": 61, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -89,7 +89,6 @@ use parse::token::{DocComment, MatchNt, SubstNt};\n use parse::token::{Token, Nonterminal};\n use parse::token;\n use print::pprust;\n-use ptr::P;\n use tokenstream::{self, TokenTree};\n use util::small_vector::SmallVector;\n \n@@ -198,7 +197,7 @@ pub fn initial_matcher_pos(ms: Vec<TokenTree>, sep: Option<Token>, lo: BytePos)\n \n pub enum NamedMatch {\n     MatchedSeq(Vec<Rc<NamedMatch>>, syntax_pos::Span),\n-    MatchedNonterminal(Nonterminal)\n+    MatchedNonterminal(Rc<Nonterminal>)\n }\n \n pub fn nameize(p_s: &ParseSess, ms: &[TokenTree], res: &[Rc<NamedMatch>])\n@@ -279,17 +278,16 @@ pub fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n     }\n }\n \n-pub fn parse(sess: &ParseSess, mut rdr: TtReader, ms: &[TokenTree]) -> NamedParseResult {\n-    let mut cur_eis = SmallVector::one(initial_matcher_pos(ms.to_owned(),\n-                                                           None,\n-                                                           rdr.peek().sp.lo));\n+pub fn parse(sess: &ParseSess, rdr: TtReader, ms: &[TokenTree]) -> NamedParseResult {\n+    let mut parser = Parser::new_with_doc_flag(sess, Box::new(rdr), true);\n+    let mut cur_eis = SmallVector::one(initial_matcher_pos(ms.to_owned(), None, parser.span.lo));\n \n     loop {\n         let mut bb_eis = Vec::new(); // black-box parsed by parser.rs\n         let mut next_eis = Vec::new(); // or proceed normally\n         let mut eof_eis = Vec::new();\n \n-        let TokenAndSpan { tok, sp } = rdr.peek();\n+        let (sp, tok) = (parser.span, parser.token.clone());\n \n         /* we append new items to this while we go */\n         loop {\n@@ -474,23 +472,19 @@ pub fn parse(sess: &ParseSess, mut rdr: TtReader, ms: &[TokenTree]) -> NamedPars\n                 while !next_eis.is_empty() {\n                     cur_eis.push(next_eis.pop().unwrap());\n                 }\n-                rdr.next_token();\n+                parser.bump();\n             } else /* bb_eis.len() == 1 */ {\n-                rdr.next_tok = {\n-                    let mut rust_parser = Parser::new(sess, Box::new(&mut rdr));\n-                    let mut ei = bb_eis.pop().unwrap();\n-                    if let TokenTree::Token(span, MatchNt(_, ident)) = ei.top_elts.get_tt(ei.idx) {\n-                        let match_cur = ei.match_cur;\n-                        (&mut ei.matches[match_cur]).push(Rc::new(MatchedNonterminal(\n-                            parse_nt(&mut rust_parser, span, &ident.name.as_str()))));\n-                        ei.idx += 1;\n-                        ei.match_cur += 1;\n-                    } else {\n-                        unreachable!()\n-                    }\n-                    cur_eis.push(ei);\n-                    Some(TokenAndSpan { tok: rust_parser.token, sp: rust_parser.span })\n-                };\n+                let mut ei = bb_eis.pop().unwrap();\n+                if let TokenTree::Token(span, MatchNt(_, ident)) = ei.top_elts.get_tt(ei.idx) {\n+                    let match_cur = ei.match_cur;\n+                    (&mut ei.matches[match_cur]).push(Rc::new(MatchedNonterminal(\n+                        Rc::new(parse_nt(&mut parser, span, &ident.name.as_str())))));\n+                    ei.idx += 1;\n+                    ei.match_cur += 1;\n+                } else {\n+                    unreachable!()\n+                }\n+                cur_eis.push(ei);\n             }\n         }\n \n@@ -502,10 +496,19 @@ pub fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n     match name {\n         \"tt\" => {\n             p.quote_depth += 1; //but in theory, non-quoted tts might be useful\n-            let res: ::parse::PResult<'a, _> = p.parse_token_tree();\n-            let res = token::NtTT(P(panictry!(res)));\n+            let mut tt = panictry!(p.parse_token_tree());\n             p.quote_depth -= 1;\n-            return res;\n+            loop {\n+                let nt = match tt {\n+                    TokenTree::Token(_, token::Interpolated(ref nt)) => nt.clone(),\n+                    _ => break,\n+                };\n+                match *nt {\n+                    token::NtTT(ref sub_tt) => tt = sub_tt.clone(),\n+                    _ => break,\n+                }\n+            }\n+            return token::NtTT(tt);\n         }\n         _ => {}\n     }\n@@ -521,7 +524,7 @@ pub fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n         },\n         \"block\" => token::NtBlock(panictry!(p.parse_block())),\n         \"stmt\" => match panictry!(p.parse_stmt()) {\n-            Some(s) => token::NtStmt(P(s)),\n+            Some(s) => token::NtStmt(s),\n             None => {\n                 p.fatal(\"expected a statement\").emit();\n                 panic!(FatalError);\n@@ -534,7 +537,7 @@ pub fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n         \"ident\" => match p.token {\n             token::Ident(sn) => {\n                 p.bump();\n-                token::NtIdent(Box::new(Spanned::<Ident>{node: sn, span: p.span}))\n+                token::NtIdent(Spanned::<Ident>{node: sn, span: p.span})\n             }\n             _ => {\n                 let token_str = pprust::token_to_string(&p.token);\n@@ -544,7 +547,7 @@ pub fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n             }\n         },\n         \"path\" => {\n-            token::NtPath(Box::new(panictry!(p.parse_path(PathStyle::Type))))\n+            token::NtPath(panictry!(p.parse_path(PathStyle::Type)))\n         },\n         \"meta\" => token::NtMeta(panictry!(p.parse_meta_item())),\n         // this is not supposed to happen, since it has been checked"}, {"sha": "552d4de961740f4c59b749b30ba9fde36719d75e", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 14, "deletions": 8, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -236,22 +236,28 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n     // Extract the arguments:\n     let lhses = match **argument_map.get(&lhs_nm).unwrap() {\n         MatchedSeq(ref s, _) => {\n-            s.iter().map(|m| match **m {\n-                MatchedNonterminal(NtTT(ref tt)) => {\n-                    valid &= check_lhs_nt_follows(sess, tt);\n-                    (**tt).clone()\n+            s.iter().map(|m| {\n+                if let MatchedNonterminal(ref nt) = **m {\n+                    if let NtTT(ref tt) = **nt {\n+                        valid &= check_lhs_nt_follows(sess, tt);\n+                        return (*tt).clone();\n+                    }\n                 }\n-                _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n+                sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n             }).collect::<Vec<TokenTree>>()\n         }\n         _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n     };\n \n     let rhses = match **argument_map.get(&rhs_nm).unwrap() {\n         MatchedSeq(ref s, _) => {\n-            s.iter().map(|m| match **m {\n-                MatchedNonterminal(NtTT(ref tt)) => (**tt).clone(),\n-                _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured rhs\")\n+            s.iter().map(|m| {\n+                if let MatchedNonterminal(ref nt) = **m {\n+                    if let NtTT(ref tt) = **nt {\n+                        return (*tt).clone();\n+                    }\n+                }\n+                sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n             }).collect()\n         }\n         _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured rhs\")"}, {"sha": "37e329e5d3b29fb581e7d3b77838da3acda067d5", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 11, "deletions": 54, "changes": 65, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -12,9 +12,7 @@ use self::LockstepIterSize::*;\n use ast::Ident;\n use errors::{Handler, DiagnosticBuilder};\n use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n-use parse::token::{DocComment, MatchNt, SubstNt};\n-use parse::token::{Token, Interpolated, NtIdent, NtTT};\n-use parse::token;\n+use parse::token::{self, MatchNt, SubstNt, Token, NtIdent};\n use parse::lexer::TokenAndSpan;\n use syntax_pos::{Span, DUMMY_SP};\n use tokenstream::{self, TokenTree};\n@@ -46,9 +44,7 @@ pub struct TtReader<'a> {\n     /* cached: */\n     pub cur_tok: Token,\n     pub cur_span: Span,\n-    pub next_tok: Option<TokenAndSpan>,\n     /// Transform doc comments. Only useful in macro invocations\n-    pub desugar_doc_comments: bool,\n     pub fatal_errs: Vec<DiagnosticBuilder<'a>>,\n }\n \n@@ -59,20 +55,6 @@ pub fn new_tt_reader(sp_diag: &Handler,\n                      interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n                      src: Vec<tokenstream::TokenTree>)\n                      -> TtReader {\n-    new_tt_reader_with_doc_flag(sp_diag, interp, src, false)\n-}\n-\n-/// The extra `desugar_doc_comments` flag enables reading doc comments\n-/// like any other attribute which consists of `meta` and surrounding #[ ] tokens.\n-///\n-/// This can do Macro-By-Example transcription. On the other hand, if\n-/// `src` contains no `TokenTree::Sequence`s, `MatchNt`s or `SubstNt`s, `interp` can\n-/// (and should) be None.\n-pub fn new_tt_reader_with_doc_flag(sp_diag: &Handler,\n-                                   interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n-                                   src: Vec<tokenstream::TokenTree>,\n-                                   desugar_doc_comments: bool)\n-                                   -> TtReader {\n     let mut r = TtReader {\n         sp_diag: sp_diag,\n         stack: SmallVector::one(TtFrame {\n@@ -91,11 +73,9 @@ pub fn new_tt_reader_with_doc_flag(sp_diag: &Handler,\n         },\n         repeat_idx: Vec::new(),\n         repeat_len: Vec::new(),\n-        desugar_doc_comments: desugar_doc_comments,\n         /* dummy values, never read: */\n         cur_tok: token::Eof,\n         cur_span: DUMMY_SP,\n-        next_tok: None,\n         fatal_errs: Vec::new(),\n     };\n     tt_next_token(&mut r); /* get cur_tok and cur_span set up */\n@@ -174,9 +154,6 @@ fn lockstep_iter_size(t: &TokenTree, r: &TtReader) -> LockstepIterSize {\n /// Return the next token from the TtReader.\n /// EFFECT: advances the reader's token field\n pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n-    if let Some(tok) = r.next_tok.take() {\n-        return tok;\n-    }\n     // FIXME(pcwalton): Bad copy?\n     let ret_val = TokenAndSpan {\n         tok: r.cur_tok.clone(),\n@@ -269,47 +246,35 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n             }\n             // FIXME #2887: think about span stuff here\n             TokenTree::Token(sp, SubstNt(ident)) => {\n+                r.stack.last_mut().unwrap().idx += 1;\n                 match lookup_cur_matched(r, ident) {\n                     None => {\n-                        r.stack.last_mut().unwrap().idx += 1;\n                         r.cur_span = sp;\n                         r.cur_tok = SubstNt(ident);\n                         return ret_val;\n                         // this can't be 0 length, just like TokenTree::Delimited\n                     }\n-                    Some(cur_matched) => {\n-                        match *cur_matched {\n+                    Some(cur_matched) => if let MatchedNonterminal(ref nt) = *cur_matched {\n+                        match **nt {\n                             // sidestep the interpolation tricks for ident because\n                             // (a) idents can be in lots of places, so it'd be a pain\n                             // (b) we actually can, since it's a token.\n-                            MatchedNonterminal(NtIdent(ref sn)) => {\n-                                r.stack.last_mut().unwrap().idx += 1;\n+                            NtIdent(ref sn) => {\n                                 r.cur_span = sn.span;\n                                 r.cur_tok = token::Ident(sn.node);\n                                 return ret_val;\n                             }\n-                            MatchedNonterminal(NtTT(ref tt)) => {\n-                                r.stack.push(TtFrame {\n-                                    forest: TokenTree::Token(sp, Interpolated(NtTT(tt.clone()))),\n-                                    idx: 0,\n-                                    dotdotdoted: false,\n-                                    sep: None,\n-                                });\n-                            }\n-                            MatchedNonterminal(ref other_whole_nt) => {\n-                                r.stack.last_mut().unwrap().idx += 1;\n+                            _ => {\n                                 // FIXME(pcwalton): Bad copy.\n                                 r.cur_span = sp;\n-                                r.cur_tok = Interpolated((*other_whole_nt).clone());\n+                                r.cur_tok = token::Interpolated(nt.clone());\n                                 return ret_val;\n                             }\n-                            MatchedSeq(..) => {\n-                                panic!(r.sp_diag.span_fatal(\n-                                    sp, /* blame the macro writer */\n-                                    &format!(\"variable '{}' is still repeating at this depth\",\n-                                            ident)));\n-                            }\n                         }\n+                    } else {\n+                        panic!(r.sp_diag.span_fatal(\n+                            sp, /* blame the macro writer */\n+                            &format!(\"variable '{}' is still repeating at this depth\", ident)));\n                     }\n                 }\n             }\n@@ -324,14 +289,6 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                 });\n                 // if this could be 0-length, we'd need to potentially recur here\n             }\n-            TokenTree::Token(sp, DocComment(name)) if r.desugar_doc_comments => {\n-                r.stack.push(TtFrame {\n-                   forest: TokenTree::Token(sp, DocComment(name)),\n-                   idx: 0,\n-                   dotdotdoted: false,\n-                   sep: None\n-                });\n-            }\n             TokenTree::Token(sp, tok) => {\n                 r.cur_span = sp;\n                 r.cur_tok = tok;"}, {"sha": "1deeaf422316c29391b7dd63f8d0b6ebd9d8f817", "filename": "src/libsyntax/fold.rs", "status": "modified", "additions": 34, "deletions": 43, "changes": 77, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Ffold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Ffold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffold.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -576,7 +576,13 @@ pub fn noop_fold_token<T: Folder>(t: token::Token, fld: &mut T) -> token::Token\n     match t {\n         token::Ident(id) => token::Ident(fld.fold_ident(id)),\n         token::Lifetime(id) => token::Lifetime(fld.fold_ident(id)),\n-        token::Interpolated(nt) => token::Interpolated(fld.fold_interpolated(nt)),\n+        token::Interpolated(nt) => {\n+            let nt = match Rc::try_unwrap(nt) {\n+                Ok(nt) => nt,\n+                Err(nt) => (*nt).clone(),\n+            };\n+            token::Interpolated(Rc::new(fld.fold_interpolated(nt)))\n+        }\n         token::SubstNt(ident) => token::SubstNt(fld.fold_ident(ident)),\n         token::MatchNt(name, kind) => token::MatchNt(fld.fold_ident(name), fld.fold_ident(kind)),\n         _ => t\n@@ -614,26 +620,25 @@ pub fn noop_fold_interpolated<T: Folder>(nt: token::Nonterminal, fld: &mut T)\n                           .expect_one(\"expected fold to produce exactly one item\")),\n         token::NtBlock(block) => token::NtBlock(fld.fold_block(block)),\n         token::NtStmt(stmt) =>\n-            token::NtStmt(stmt.map(|stmt| fld.fold_stmt(stmt)\n+            token::NtStmt(fld.fold_stmt(stmt)\n                           // this is probably okay, because the only folds likely\n                           // to peek inside interpolated nodes will be renamings/markings,\n                           // which map single items to single items\n-                          .expect_one(\"expected fold to produce exactly one statement\"))),\n+                          .expect_one(\"expected fold to produce exactly one statement\")),\n         token::NtPat(pat) => token::NtPat(fld.fold_pat(pat)),\n         token::NtExpr(expr) => token::NtExpr(fld.fold_expr(expr)),\n         token::NtTy(ty) => token::NtTy(fld.fold_ty(ty)),\n-        token::NtIdent(id) =>\n-            token::NtIdent(Box::new(Spanned::<Ident>{node: fld.fold_ident(id.node), ..*id})),\n+        token::NtIdent(id) => token::NtIdent(Spanned::<Ident>{node: fld.fold_ident(id.node), ..id}),\n         token::NtMeta(meta_item) => token::NtMeta(fld.fold_meta_item(meta_item)),\n-        token::NtPath(path) => token::NtPath(Box::new(fld.fold_path(*path))),\n-        token::NtTT(tt) => token::NtTT(P(fld.fold_tt(&tt))),\n+        token::NtPath(path) => token::NtPath(fld.fold_path(path)),\n+        token::NtTT(tt) => token::NtTT(fld.fold_tt(&tt)),\n         token::NtArm(arm) => token::NtArm(fld.fold_arm(arm)),\n-        token::NtImplItem(arm) =>\n-            token::NtImplItem(arm.map(|arm| fld.fold_impl_item(arm)\n-                              .expect_one(\"expected fold to produce exactly one item\"))),\n-        token::NtTraitItem(arm) =>\n-            token::NtTraitItem(arm.map(|arm| fld.fold_trait_item(arm)\n-                               .expect_one(\"expected fold to produce exactly one item\"))),\n+        token::NtImplItem(item) =>\n+            token::NtImplItem(fld.fold_impl_item(item)\n+                              .expect_one(\"expected fold to produce exactly one item\")),\n+        token::NtTraitItem(item) =>\n+            token::NtTraitItem(fld.fold_trait_item(item)\n+                               .expect_one(\"expected fold to produce exactly one item\")),\n         token::NtGenerics(generics) => token::NtGenerics(fld.fold_generics(generics)),\n         token::NtWhereClause(where_clause) =>\n             token::NtWhereClause(fld.fold_where_clause(where_clause)),\n@@ -1244,36 +1249,22 @@ pub fn noop_fold_expr<T: Folder>(Expr {id, node, span, attrs}: Expr, folder: &mu\n                        folder.fold_ident(label.node)))\n             ),\n             ExprKind::Ret(e) => ExprKind::Ret(e.map(|x| folder.fold_expr(x))),\n-            ExprKind::InlineAsm(InlineAsm {\n-                inputs,\n-                outputs,\n-                asm,\n-                asm_str_style,\n-                clobbers,\n-                volatile,\n-                alignstack,\n-                dialect,\n-                expn_id,\n-            }) => ExprKind::InlineAsm(InlineAsm {\n-                inputs: inputs.move_map(|(c, input)| {\n-                    (c, folder.fold_expr(input))\n-                }),\n-                outputs: outputs.move_map(|out| {\n-                    InlineAsmOutput {\n-                        constraint: out.constraint,\n-                        expr: folder.fold_expr(out.expr),\n-                        is_rw: out.is_rw,\n-                        is_indirect: out.is_indirect,\n-                    }\n-                }),\n-                asm: asm,\n-                asm_str_style: asm_str_style,\n-                clobbers: clobbers,\n-                volatile: volatile,\n-                alignstack: alignstack,\n-                dialect: dialect,\n-                expn_id: expn_id,\n-            }),\n+            ExprKind::InlineAsm(asm) => ExprKind::InlineAsm(asm.map(|asm| {\n+                InlineAsm {\n+                    inputs: asm.inputs.move_map(|(c, input)| {\n+                        (c, folder.fold_expr(input))\n+                    }),\n+                    outputs: asm.outputs.move_map(|out| {\n+                        InlineAsmOutput {\n+                            constraint: out.constraint,\n+                            expr: folder.fold_expr(out.expr),\n+                            is_rw: out.is_rw,\n+                            is_indirect: out.is_indirect,\n+                        }\n+                    }),\n+                    ..asm\n+                }\n+            })),\n             ExprKind::Mac(mac) => ExprKind::Mac(folder.fold_mac(mac)),\n             ExprKind::Struct(path, fields, maybe_expr) => {\n                 ExprKind::Struct(folder.fold_path(path),"}, {"sha": "983c882eafca31298542d0a2a53ee7d44eca1dd8", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -215,7 +215,10 @@ impl<'a> Parser<'a> {\n     /// meta_item_inner : (meta_item | UNSUFFIXED_LIT) (',' meta_item_inner)? ;\n     pub fn parse_meta_item(&mut self) -> PResult<'a, P<ast::MetaItem>> {\n         let nt_meta = match self.token {\n-            token::Interpolated(token::NtMeta(ref e)) => Some(e.clone()),\n+            token::Interpolated(ref nt) => match **nt {\n+                token::NtMeta(ref e) => Some(e.clone()),\n+                _ => None,\n+            },\n             _ => None,\n         };\n "}, {"sha": "cf48c445c80ebf24f2be8bb15467ecba278aab70", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 3, "deletions": 24, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -22,7 +22,7 @@ use std::char;\n use std::mem::replace;\n use std::rc::Rc;\n \n-pub use ext::tt::transcribe::{TtReader, new_tt_reader, new_tt_reader_with_doc_flag};\n+pub use ext::tt::transcribe::{TtReader, new_tt_reader};\n \n pub mod comments;\n mod unicode_chars;\n@@ -171,31 +171,10 @@ impl<'a> Reader for TtReader<'a> {\n         self.fatal_errs.clear();\n     }\n     fn peek(&self) -> TokenAndSpan {\n-        self.next_tok.clone().unwrap_or(TokenAndSpan {\n+        TokenAndSpan {\n             tok: self.cur_tok.clone(),\n             sp: self.cur_span,\n-        })\n-    }\n-}\n-\n-impl<'a, 'b> Reader for &'b mut TtReader<'a> {\n-    fn is_eof(&self) -> bool {\n-        (**self).is_eof()\n-    }\n-    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n-        (**self).try_next_token()\n-    }\n-    fn fatal(&self, m: &str) -> FatalError {\n-        (**self).fatal(m)\n-    }\n-    fn err(&self, m: &str) {\n-        (**self).err(m)\n-    }\n-    fn emit_fatal_errors(&mut self) {\n-        (**self).emit_fatal_errors()\n-    }\n-    fn peek(&self) -> TokenAndSpan {\n-        (**self).peek()\n+        }\n     }\n }\n "}, {"sha": "b670a7384739bd30373abff886253c8888520f7b", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 134, "deletions": 181, "changes": 315, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -107,125 +107,41 @@ pub enum SemiColonMode {\n /// be. The important thing is to make sure that lookahead doesn't balk at\n /// `token::Interpolated` tokens.\n macro_rules! maybe_whole_expr {\n-    ($p:expr) => (\n-        {\n-            let found = match $p.token {\n-                token::Interpolated(token::NtExpr(ref e)) => {\n-                    Some((*e).clone())\n+    ($p:expr) => {\n+        if let token::Interpolated(nt) = $p.token.clone() {\n+            match *nt {\n+                token::NtExpr(ref e) => {\n+                    $p.bump();\n+                    return Ok((*e).clone());\n                 }\n-                token::Interpolated(token::NtPath(_)) => {\n-                    // FIXME: The following avoids an issue with lexical borrowck scopes,\n-                    // but the clone is unfortunate.\n-                    let pt = match $p.token {\n-                        token::Interpolated(token::NtPath(ref pt)) => (**pt).clone(),\n-                        _ => unreachable!()\n-                    };\n+                token::NtPath(ref path) => {\n+                    $p.bump();\n                     let span = $p.span;\n-                    Some($p.mk_expr(span.lo, span.hi, ExprKind::Path(None, pt), ThinVec::new()))\n+                    let kind = ExprKind::Path(None, (*path).clone());\n+                    return Ok($p.mk_expr(span.lo, span.hi, kind, ThinVec::new()));\n                 }\n-                token::Interpolated(token::NtBlock(_)) => {\n-                    // FIXME: The following avoids an issue with lexical borrowck scopes,\n-                    // but the clone is unfortunate.\n-                    let b = match $p.token {\n-                        token::Interpolated(token::NtBlock(ref b)) => (*b).clone(),\n-                        _ => unreachable!()\n-                    };\n+                token::NtBlock(ref block) => {\n+                    $p.bump();\n                     let span = $p.span;\n-                    Some($p.mk_expr(span.lo, span.hi, ExprKind::Block(b), ThinVec::new()))\n+                    let kind = ExprKind::Block((*block).clone());\n+                    return Ok($p.mk_expr(span.lo, span.hi, kind, ThinVec::new()));\n                 }\n-                _ => None\n+                _ => {},\n             };\n-            match found {\n-                Some(e) => {\n-                    $p.bump();\n-                    return Ok(e);\n-                }\n-                None => ()\n-            }\n         }\n-    )\n+    }\n }\n \n /// As maybe_whole_expr, but for things other than expressions\n macro_rules! maybe_whole {\n-    ($p:expr, $constructor:ident) => (\n-        {\n-            let found = match ($p).token {\n-                token::Interpolated(token::$constructor(_)) => {\n-                    Some(($p).bump_and_get())\n-                }\n-                _ => None\n-            };\n-            if let Some(token::Interpolated(token::$constructor(x))) = found {\n-                return Ok(x.clone());\n+    ($p:expr, $constructor:ident, |$x:ident| $e:expr) => {\n+        if let token::Interpolated(nt) = $p.token.clone() {\n+            if let token::$constructor($x) = (*nt).clone() {\n+                $p.bump();\n+                return Ok($e);\n             }\n         }\n-    );\n-    (no_clone $p:expr, $constructor:ident) => (\n-        {\n-            let found = match ($p).token {\n-                token::Interpolated(token::$constructor(_)) => {\n-                    Some(($p).bump_and_get())\n-                }\n-                _ => None\n-            };\n-            if let Some(token::Interpolated(token::$constructor(x))) = found {\n-                return Ok(x);\n-            }\n-        }\n-    );\n-    (no_clone_from_p $p:expr, $constructor:ident) => (\n-        {\n-            let found = match ($p).token {\n-                token::Interpolated(token::$constructor(_)) => {\n-                    Some(($p).bump_and_get())\n-                }\n-                _ => None\n-            };\n-            if let Some(token::Interpolated(token::$constructor(x))) = found {\n-                return Ok(x.unwrap());\n-            }\n-        }\n-    );\n-    (deref $p:expr, $constructor:ident) => (\n-        {\n-            let found = match ($p).token {\n-                token::Interpolated(token::$constructor(_)) => {\n-                    Some(($p).bump_and_get())\n-                }\n-                _ => None\n-            };\n-            if let Some(token::Interpolated(token::$constructor(x))) = found {\n-                return Ok((*x).clone());\n-            }\n-        }\n-    );\n-    (Some deref $p:expr, $constructor:ident) => (\n-        {\n-            let found = match ($p).token {\n-                token::Interpolated(token::$constructor(_)) => {\n-                    Some(($p).bump_and_get())\n-                }\n-                _ => None\n-            };\n-            if let Some(token::Interpolated(token::$constructor(x))) = found {\n-                return Ok(Some((*x).clone()));\n-            }\n-        }\n-    );\n-    (pair_empty $p:expr, $constructor:ident) => (\n-        {\n-            let found = match ($p).token {\n-                token::Interpolated(token::$constructor(_)) => {\n-                    Some(($p).bump_and_get())\n-                }\n-                _ => None\n-            };\n-            if let Some(token::Interpolated(token::$constructor(x))) = found {\n-                return Ok((Vec::new(), x));\n-            }\n-        }\n-    )\n+    };\n }\n \n fn maybe_append(mut lhs: Vec<Attribute>, rhs: Option<Vec<Attribute>>)\n@@ -294,6 +210,9 @@ pub struct Parser<'a> {\n     /// into modules, and sub-parsers have new values for this name.\n     pub root_module_name: Option<String>,\n     pub expected_tokens: Vec<TokenType>,\n+    pub tts: Vec<(TokenTree, usize)>,\n+    pub desugar_doc_comments: bool,\n+    pub allow_interpolated_tts: bool,\n }\n \n #[derive(PartialEq, Eq, Clone)]\n@@ -357,33 +276,82 @@ impl From<P<Expr>> for LhsExpr {\n }\n \n impl<'a> Parser<'a> {\n-    pub fn new(sess: &'a ParseSess, mut rdr: Box<Reader+'a>) -> Self {\n-        let tok0 = rdr.real_token();\n-        let span = tok0.sp;\n-        let mut directory = match span {\n-            syntax_pos::DUMMY_SP => PathBuf::new(),\n-            _ => PathBuf::from(sess.codemap().span_to_filename(span)),\n-        };\n-        directory.pop();\n+    pub fn new(sess: &'a ParseSess, rdr: Box<Reader+'a>) -> Self {\n+        Parser::new_with_doc_flag(sess, rdr, false)\n+    }\n \n-        Parser {\n+    pub fn new_with_doc_flag(sess: &'a ParseSess, rdr: Box<Reader+'a>, desugar_doc_comments: bool)\n+                             -> Self {\n+        let mut parser = Parser {\n             reader: rdr,\n             sess: sess,\n-            token: tok0.tok,\n-            span: span,\n-            prev_span: span,\n+            token: token::Underscore,\n+            span: syntax_pos::DUMMY_SP,\n+            prev_span: syntax_pos::DUMMY_SP,\n             prev_token_kind: PrevTokenKind::Other,\n             lookahead_buffer: Default::default(),\n             tokens_consumed: 0,\n             restrictions: Restrictions::empty(),\n             quote_depth: 0,\n             parsing_token_tree: false,\n             obsolete_set: HashSet::new(),\n-            directory: directory,\n+            directory: PathBuf::new(),\n             open_braces: Vec::new(),\n             owns_directory: true,\n             root_module_name: None,\n             expected_tokens: Vec::new(),\n+            tts: Vec::new(),\n+            desugar_doc_comments: desugar_doc_comments,\n+            allow_interpolated_tts: true,\n+        };\n+\n+        let tok = parser.next_tok();\n+        parser.token = tok.tok;\n+        parser.span = tok.sp;\n+        if parser.span != syntax_pos::DUMMY_SP {\n+            parser.directory = PathBuf::from(sess.codemap().span_to_filename(parser.span));\n+            parser.directory.pop();\n+        }\n+        parser\n+    }\n+\n+    fn next_tok(&mut self) -> TokenAndSpan {\n+        'outer: loop {\n+            let mut tok = if let Some((tts, i)) = self.tts.pop() {\n+                let tt = tts.get_tt(i);\n+                if i + 1 < tts.len() {\n+                    self.tts.push((tts, i + 1));\n+                }\n+                if let TokenTree::Token(sp, tok) = tt {\n+                    TokenAndSpan { tok: tok, sp: sp }\n+                } else {\n+                    self.tts.push((tt, 0));\n+                    continue\n+                }\n+            } else {\n+                self.reader.real_token()\n+            };\n+\n+            loop {\n+                let nt = match tok.tok {\n+                    token::Interpolated(ref nt) => nt.clone(),\n+                    token::DocComment(name) if self.desugar_doc_comments => {\n+                        self.tts.push((TokenTree::Token(tok.sp, token::DocComment(name)), 0));\n+                        continue 'outer\n+                    }\n+                    _ => return tok,\n+                };\n+                match *nt {\n+                    token::NtTT(TokenTree::Token(sp, ref t)) => {\n+                        tok = TokenAndSpan { tok: t.clone(), sp: sp };\n+                    }\n+                    token::NtTT(ref tt) => {\n+                        self.tts.push((tt.clone(), 0));\n+                        continue 'outer\n+                    }\n+                    _ => return tok,\n+                }\n+            }\n         }\n     }\n \n@@ -516,9 +484,6 @@ impl<'a> Parser<'a> {\n                 self.bump();\n                 Ok(i)\n             }\n-            token::Interpolated(token::NtIdent(..)) => {\n-                self.bug(\"ident interpolation not converted to real token\");\n-            }\n             _ => {\n                 Err(if self.prev_token_kind == PrevTokenKind::DocComment {\n                     self.span_fatal_help(self.prev_span,\n@@ -935,7 +900,7 @@ impl<'a> Parser<'a> {\n         };\n \n         let next = if self.lookahead_buffer.start == self.lookahead_buffer.end {\n-            self.reader.real_token()\n+            self.next_tok()\n         } else {\n             // Avoid token copies with `replace`.\n             let old_start = self.lookahead_buffer.start;\n@@ -980,7 +945,7 @@ impl<'a> Parser<'a> {\n             f(&self.token)\n         } else if dist < LOOKAHEAD_BUFFER_CAPACITY {\n             while self.lookahead_buffer.len() < dist {\n-                self.lookahead_buffer.buffer[self.lookahead_buffer.end] = self.reader.real_token();\n+                self.lookahead_buffer.buffer[self.lookahead_buffer.end] = self.next_tok();\n                 self.lookahead_buffer.end =\n                     (self.lookahead_buffer.end + 1) % LOOKAHEAD_BUFFER_CAPACITY;\n             }\n@@ -1162,7 +1127,7 @@ impl<'a> Parser<'a> {\n \n     /// Parse the items in a trait declaration\n     pub fn parse_trait_item(&mut self) -> PResult<'a, TraitItem> {\n-        maybe_whole!(no_clone_from_p self, NtTraitItem);\n+        maybe_whole!(self, NtTraitItem, |x| x);\n         let mut attrs = self.parse_outer_attributes()?;\n         let lo = self.span.lo;\n \n@@ -1331,7 +1296,7 @@ impl<'a> Parser<'a> {\n \n     /// Parse a type.\n     pub fn parse_ty(&mut self) -> PResult<'a, P<Ty>> {\n-        maybe_whole!(no_clone self, NtTy);\n+        maybe_whole!(self, NtTy, |x| x);\n \n         let lo = self.span.lo;\n \n@@ -1476,7 +1441,7 @@ impl<'a> Parser<'a> {\n     /// This version of parse arg doesn't necessarily require\n     /// identifier names.\n     pub fn parse_arg_general(&mut self, require_name: bool) -> PResult<'a, Arg> {\n-        maybe_whole!(no_clone self, NtArg);\n+        maybe_whole!(self, NtArg, |x| x);\n \n         let pat = if require_name || self.is_named_argument() {\n             debug!(\"parse_arg_general parse_pat (require_name:{})\",\n@@ -1542,12 +1507,13 @@ impl<'a> Parser<'a> {\n     /// Matches token_lit = LIT_INTEGER | ...\n     pub fn parse_lit_token(&mut self) -> PResult<'a, LitKind> {\n         let out = match self.token {\n-            token::Interpolated(token::NtExpr(ref v)) => {\n-                match v.node {\n+            token::Interpolated(ref nt) => match **nt {\n+                token::NtExpr(ref v) => match v.node {\n                     ExprKind::Lit(ref lit) => { lit.node.clone() }\n                     _ => { return self.unexpected_last(&self.token); }\n-                }\n-            }\n+                },\n+                _ => { return self.unexpected_last(&self.token); }\n+            },\n             token::Literal(lit, suf) => {\n                 let (suffix_illegal, out) = match lit {\n                     token::Byte(i) => (true, LitKind::Byte(parse::byte_lit(&i.as_str()).0)),\n@@ -1703,14 +1669,7 @@ impl<'a> Parser<'a> {\n     /// bounds are permitted and whether `::` must precede type parameter\n     /// groups.\n     pub fn parse_path(&mut self, mode: PathStyle) -> PResult<'a, ast::Path> {\n-        // Check for a whole path...\n-        let found = match self.token {\n-            token::Interpolated(token::NtPath(_)) => Some(self.bump_and_get()),\n-            _ => None,\n-        };\n-        if let Some(token::Interpolated(token::NtPath(path))) = found {\n-            return Ok(*path);\n-        }\n+        maybe_whole!(self, NtPath, |x| x);\n \n         let lo = self.span.lo;\n         let is_global = self.eat(&token::ModSep);\n@@ -2746,8 +2705,6 @@ impl<'a> Parser<'a> {\n         // and token::SubstNt's; it's too early to know yet\n         // whether something will be a nonterminal or a seq\n         // yet.\n-        maybe_whole!(deref self, NtTT);\n-\n         match self.token {\n             token::Eof => {\n                 let mut err: DiagnosticBuilder<'a> =\n@@ -2760,6 +2717,17 @@ impl<'a> Parser<'a> {\n                 Err(err)\n             },\n             token::OpenDelim(delim) => {\n+                if self.tts.last().map(|&(_, i)| i == 1).unwrap_or(false) {\n+                    let tt = self.tts.pop().unwrap().0;\n+                    self.bump();\n+                    return Ok(if self.allow_interpolated_tts {\n+                        // avoid needlessly reparsing token trees in recursive macro expansions\n+                        TokenTree::Token(tt.span(), token::Interpolated(Rc::new(token::NtTT(tt))))\n+                    } else {\n+                        tt\n+                    });\n+                }\n+\n                 let parsing_token_tree = ::std::mem::replace(&mut self.parsing_token_tree, true);\n                 // The span for beginning of the delimited section\n                 let pre_span = self.span;\n@@ -2833,29 +2801,20 @@ impl<'a> Parser<'a> {\n                     close_span: close_span,\n                 })))\n             },\n+            token::CloseDelim(_) => {\n+                // An unexpected closing delimiter (i.e., there is no\n+                // matching opening delimiter).\n+                let token_str = self.this_token_to_string();\n+                let err = self.diagnostic().struct_span_err(self.span,\n+                    &format!(\"unexpected close delimiter: `{}`\", token_str));\n+                Err(err)\n+            },\n+            /* we ought to allow different depths of unquotation */\n+            token::Dollar | token::SubstNt(..) if self.quote_depth > 0 => {\n+                self.parse_unquoted()\n+            }\n             _ => {\n-                // invariants: the current token is not a left-delimiter,\n-                // not an EOF, and not the desired right-delimiter (if\n-                // it were, parse_seq_to_before_end would have prevented\n-                // reaching this point).\n-                maybe_whole!(deref self, NtTT);\n-                match self.token {\n-                    token::CloseDelim(_) => {\n-                        // An unexpected closing delimiter (i.e., there is no\n-                        // matching opening delimiter).\n-                        let token_str = self.this_token_to_string();\n-                        let err = self.diagnostic().struct_span_err(self.span,\n-                            &format!(\"unexpected close delimiter: `{}`\", token_str));\n-                        Err(err)\n-                    },\n-                    /* we ought to allow different depths of unquotation */\n-                    token::Dollar | token::SubstNt(..) if self.quote_depth > 0 => {\n-                        self.parse_unquoted()\n-                    }\n-                    _ => {\n-                        Ok(TokenTree::Token(self.span, self.bump_and_get()))\n-                    }\n-                }\n+                Ok(TokenTree::Token(self.span, self.bump_and_get()))\n             }\n         }\n     }\n@@ -3336,7 +3295,7 @@ impl<'a> Parser<'a> {\n     }\n \n     pub fn parse_arm(&mut self) -> PResult<'a, Arm> {\n-        maybe_whole!(no_clone self, NtArm);\n+        maybe_whole!(self, NtArm, |x| x);\n \n         let attrs = self.parse_outer_attributes()?;\n         let pats = self.parse_pats()?;\n@@ -3592,7 +3551,7 @@ impl<'a> Parser<'a> {\n \n     /// Parse a pattern.\n     pub fn parse_pat(&mut self) -> PResult<'a, P<Pat>> {\n-        maybe_whole!(self, NtPat);\n+        maybe_whole!(self, NtPat, |x| x);\n \n         let lo = self.span.lo;\n         let pat;\n@@ -3897,7 +3856,7 @@ impl<'a> Parser<'a> {\n     fn parse_stmt_without_recovery(&mut self,\n                                    macro_legacy_warnings: bool)\n                                    -> PResult<'a, Option<Stmt>> {\n-        maybe_whole!(Some deref self, NtStmt);\n+        maybe_whole!(self, NtStmt, |x| Some(x));\n \n         let attrs = self.parse_outer_attributes()?;\n         let lo = self.span.lo;\n@@ -4086,7 +4045,7 @@ impl<'a> Parser<'a> {\n \n     /// Parse a block. No inner attrs are allowed.\n     pub fn parse_block(&mut self) -> PResult<'a, P<Block>> {\n-        maybe_whole!(no_clone self, NtBlock);\n+        maybe_whole!(self, NtBlock, |x| x);\n \n         let lo = self.span.lo;\n \n@@ -4124,7 +4083,7 @@ impl<'a> Parser<'a> {\n \n     /// Parse a block. Inner attrs are allowed.\n     fn parse_inner_attrs_and_block(&mut self) -> PResult<'a, (Vec<Attribute>, P<Block>)> {\n-        maybe_whole!(pair_empty self, NtBlock);\n+        maybe_whole!(self, NtBlock, |x| (Vec::new(), x));\n \n         let lo = self.span.lo;\n         self.expect(&token::OpenDelim(token::Brace))?;\n@@ -4299,7 +4258,7 @@ impl<'a> Parser<'a> {\n     ///                  | ( < lifetimes , typaramseq ( , )? > )\n     /// where   typaramseq = ( typaram ) | ( typaram , typaramseq )\n     pub fn parse_generics(&mut self) -> PResult<'a, ast::Generics> {\n-        maybe_whole!(self, NtGenerics);\n+        maybe_whole!(self, NtGenerics, |x| x);\n         let span_lo = self.span.lo;\n \n         if self.eat(&token::Lt) {\n@@ -4440,7 +4399,7 @@ impl<'a> Parser<'a> {\n     /// where T : Trait<U, V> + 'b, 'a : 'b\n     /// ```\n     pub fn parse_where_clause(&mut self) -> PResult<'a, ast::WhereClause> {\n-        maybe_whole!(self, NtWhereClause);\n+        maybe_whole!(self, NtWhereClause, |x| x);\n \n         let mut where_clause = WhereClause {\n             id: ast::DUMMY_NODE_ID,\n@@ -4848,7 +4807,7 @@ impl<'a> Parser<'a> {\n \n     /// Parse an impl item.\n     pub fn parse_impl_item(&mut self) -> PResult<'a, ImplItem> {\n-        maybe_whole!(no_clone_from_p self, NtImplItem);\n+        maybe_whole!(self, NtImplItem, |x| x);\n \n         let mut attrs = self.parse_outer_attributes()?;\n         let lo = self.span.lo;\n@@ -5716,19 +5675,13 @@ impl<'a> Parser<'a> {\n     /// extern crate.\n     fn parse_item_(&mut self, attrs: Vec<Attribute>,\n                    macros_allowed: bool, attributes_allowed: bool) -> PResult<'a, Option<P<Item>>> {\n-        let nt_item = match self.token {\n-            token::Interpolated(token::NtItem(ref item)) => {\n-                Some((**item).clone())\n-            }\n-            _ => None\n-        };\n-        if let Some(mut item) = nt_item {\n-            self.bump();\n+        maybe_whole!(self, NtItem, |item| {\n+            let mut item = item.unwrap();\n             let mut attrs = attrs;\n             mem::swap(&mut item.attrs, &mut attrs);\n             item.attrs.extend(attrs);\n-            return Ok(Some(P(item)));\n-        }\n+            Some(P(item))\n+        });\n \n         let lo = self.span.lo;\n "}, {"sha": "0198ee073d23982c3534787ab2c950fa2a44076e", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 20, "deletions": 15, "changes": 35, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -123,7 +123,7 @@ pub enum Token {\n     Lifetime(ast::Ident),\n \n     /* For interpolation */\n-    Interpolated(Nonterminal),\n+    Interpolated(Rc<Nonterminal>),\n     // Can be expanded into several tokens.\n     /// Doc comment\n     DocComment(ast::Name),\n@@ -172,12 +172,15 @@ impl Token {\n             DotDot | DotDotDot          => true, // range notation\n             Lt | BinOp(Shl)             => true, // associated path\n             ModSep                      => true,\n-            Interpolated(NtExpr(..))    => true,\n-            Interpolated(NtIdent(..))   => true,\n-            Interpolated(NtBlock(..))   => true,\n-            Interpolated(NtPath(..))    => true,\n             Pound                       => true, // for expression attributes\n-            _                           => false,\n+            Interpolated(ref nt) => match **nt {\n+                NtExpr(..) => true,\n+                NtIdent(..) => true,\n+                NtBlock(..) => true,\n+                NtPath(..) => true,\n+                _ => false,\n+            },\n+            _ => false,\n         }\n     }\n \n@@ -215,10 +218,12 @@ impl Token {\n \n     /// Returns `true` if the token is an interpolated path.\n     pub fn is_path(&self) -> bool {\n-        match *self {\n-            Interpolated(NtPath(..))    => true,\n-            _                           => false,\n+        if let Interpolated(ref nt) = *self {\n+            if let NtPath(..) = **nt {\n+                return true;\n+            }\n         }\n+        false\n     }\n \n     /// Returns `true` if the token is a lifetime.\n@@ -290,19 +295,19 @@ impl Token {\n pub enum Nonterminal {\n     NtItem(P<ast::Item>),\n     NtBlock(P<ast::Block>),\n-    NtStmt(P<ast::Stmt>),\n+    NtStmt(ast::Stmt),\n     NtPat(P<ast::Pat>),\n     NtExpr(P<ast::Expr>),\n     NtTy(P<ast::Ty>),\n-    NtIdent(Box<ast::SpannedIdent>),\n+    NtIdent(ast::SpannedIdent),\n     /// Stuff inside brackets for attributes\n     NtMeta(P<ast::MetaItem>),\n-    NtPath(Box<ast::Path>),\n-    NtTT(P<tokenstream::TokenTree>), // needs P'ed to break a circularity\n+    NtPath(ast::Path),\n+    NtTT(tokenstream::TokenTree),\n     // These are not exposed to macros, but are used by quasiquote.\n     NtArm(ast::Arm),\n-    NtImplItem(P<ast::ImplItem>),\n-    NtTraitItem(P<ast::TraitItem>),\n+    NtImplItem(ast::ImplItem),\n+    NtTraitItem(ast::TraitItem),\n     NtGenerics(ast::Generics),\n     NtWhereClause(ast::WhereClause),\n     NtArg(ast::Arg),"}, {"sha": "7352792a8a25253ec950d07722089a53c2b4232b", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -285,7 +285,7 @@ pub fn token_to_string(tok: &Token) -> String {\n         token::Comment              => \"/* */\".to_string(),\n         token::Shebang(s)           => format!(\"/* shebang: {}*/\", s),\n \n-        token::Interpolated(ref nt) => match *nt {\n+        token::Interpolated(ref nt) => match **nt {\n             token::NtExpr(ref e)        => expr_to_string(&e),\n             token::NtMeta(ref e)        => meta_item_to_string(&e),\n             token::NtTy(ref e)          => ty_to_string(&e),"}, {"sha": "9ef6c07e489dce7de227d349cbeef5b313358422", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 5, "deletions": 8, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -134,8 +134,10 @@ impl TokenTree {\n                     AttrStyle::Inner => 3,\n                 }\n             }\n+            TokenTree::Token(_, token::Interpolated(ref nt)) => {\n+                if let Nonterminal::NtTT(..) = **nt { 1 } else { 0 }\n+            },\n             TokenTree::Token(_, token::MatchNt(..)) => 3,\n-            TokenTree::Token(_, token::Interpolated(Nonterminal::NtTT(..))) => 1,\n             TokenTree::Delimited(_, ref delimed) => delimed.tts.len() + 2,\n             TokenTree::Sequence(_, ref seq) => seq.tts.len(),\n             TokenTree::Token(..) => 0,\n@@ -193,9 +195,6 @@ impl TokenTree {\n                          TokenTree::Token(sp, token::Ident(kind))];\n                 v[index].clone()\n             }\n-            (&TokenTree::Token(_, token::Interpolated(Nonterminal::NtTT(ref tt))), _) => {\n-                tt.clone().unwrap()\n-            }\n             (&TokenTree::Sequence(_, ref seq), _) => seq.tts[index].clone(),\n             _ => panic!(\"Cannot expand a token tree\"),\n         }\n@@ -215,11 +214,9 @@ impl TokenTree {\n                  mtch: &[TokenTree],\n                  tts: &[TokenTree])\n                  -> macro_parser::NamedParseResult {\n+        let diag = &cx.parse_sess().span_diagnostic;\n         // `None` is because we're not interpolating\n-        let arg_rdr = lexer::new_tt_reader_with_doc_flag(&cx.parse_sess().span_diagnostic,\n-                                                         None,\n-                                                         tts.iter().cloned().collect(),\n-                                                         true);\n+        let arg_rdr = lexer::new_tt_reader(diag, None, tts.iter().cloned().collect());\n         macro_parser::parse(cx.parse_sess(), arg_rdr, mtch)\n     }\n "}, {"sha": "e4d0cb74046037a9407e421640445adaaacb8d16", "filename": "src/libsyntax_ext/asm.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax_ext%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Flibsyntax_ext%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fasm.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -250,7 +250,7 @@ pub fn expand_asm<'cx>(cx: &'cx mut ExtCtxt,\n \n     MacEager::expr(P(ast::Expr {\n         id: ast::DUMMY_NODE_ID,\n-        node: ast::ExprKind::InlineAsm(ast::InlineAsm {\n+        node: ast::ExprKind::InlineAsm(P(ast::InlineAsm {\n             asm: token::intern_and_get_ident(&asm),\n             asm_str_style: asm_str_style.unwrap(),\n             outputs: outputs,\n@@ -260,7 +260,7 @@ pub fn expand_asm<'cx>(cx: &'cx mut ExtCtxt,\n             alignstack: alignstack,\n             dialect: dialect,\n             expn_id: expn_id,\n-        }),\n+        })),\n         span: sp,\n         attrs: ast::ThinVec::new(),\n     }))"}, {"sha": "6ac0d5ad1a3bc89e9f145548c16511f60afbebbb", "filename": "src/test/run-pass-fulldeps/auxiliary/procedural_mbe_matching.rs", "status": "modified", "additions": 33, "deletions": 28, "changes": 61, "blob_url": "https://github.com/rust-lang/rust/blob/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9cbbe184a7699e2a308ad89f41dfa88d0fbb790/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs?ref=b9cbbe184a7699e2a308ad89f41dfa88d0fbb790", "patch": "@@ -18,10 +18,10 @@ extern crate syntax_pos;\n extern crate rustc;\n extern crate rustc_plugin;\n \n-use syntax::parse::token::{self, str_to_ident, NtExpr, NtPat};\n+use syntax::parse::token::{str_to_ident, NtExpr, NtPat};\n use syntax::ast::{Pat};\n use syntax::tokenstream::{TokenTree};\n-use syntax::ext::base::{ExtCtxt, MacResult, DummyResult, MacEager};\n+use syntax::ext::base::{ExtCtxt, MacResult, MacEager};\n use syntax::ext::build::AstBuilder;\n use syntax::ext::tt::macro_parser::{MatchedSeq, MatchedNonterminal};\n use syntax::ext::tt::macro_parser::{Success, Failure, Error};\n@@ -30,35 +30,12 @@ use syntax::ptr::P;\n use syntax_pos::Span;\n use rustc_plugin::Registry;\n \n-fn expand_mbe_matches(cx: &mut ExtCtxt, sp: Span, args: &[TokenTree])\n+fn expand_mbe_matches(cx: &mut ExtCtxt, _: Span, args: &[TokenTree])\n         -> Box<MacResult + 'static> {\n \n     let mbe_matcher = quote_matcher!(cx, $matched:expr, $($pat:pat)|+);\n-\n-    let mac_expr = match TokenTree::parse(cx, &mbe_matcher[..], args) {\n-        Success(map) => {\n-            match (&*map[&str_to_ident(\"matched\")], &*map[&str_to_ident(\"pat\")]) {\n-                (&MatchedNonterminal(NtExpr(ref matched_expr)),\n-                 &MatchedSeq(ref pats, seq_sp)) => {\n-                    let pats: Vec<P<Pat>> = pats.iter().map(|pat_nt|\n-                        if let &MatchedNonterminal(NtPat(ref pat)) = &**pat_nt {\n-                            pat.clone()\n-                        } else {\n-                            unreachable!()\n-                        }\n-                    ).collect();\n-                    let arm = cx.arm(seq_sp, pats, cx.expr_bool(seq_sp, true));\n-\n-                    quote_expr!(cx,\n-                        match $matched_expr {\n-                            $arm\n-                            _ => false\n-                        }\n-                    )\n-                }\n-                _ => unreachable!()\n-            }\n-        }\n+    let map = match TokenTree::parse(cx, &mbe_matcher, args) {\n+        Success(map) => map,\n         Failure(_, tok) => {\n             panic!(\"expected Success, but got Failure: {}\", parse_failure_msg(tok));\n         }\n@@ -67,6 +44,34 @@ fn expand_mbe_matches(cx: &mut ExtCtxt, sp: Span, args: &[TokenTree])\n         }\n     };\n \n+    let matched_nt = match *map[&str_to_ident(\"matched\")] {\n+        MatchedNonterminal(ref nt) => nt.clone(),\n+        _ => unreachable!(),\n+    };\n+\n+    let mac_expr = match (&*matched_nt, &*map[&str_to_ident(\"pat\")]) {\n+        (&NtExpr(ref matched_expr), &MatchedSeq(ref pats, seq_sp)) => {\n+            let pats: Vec<P<Pat>> = pats.iter().map(|pat_nt| {\n+                match **pat_nt {\n+                    MatchedNonterminal(ref nt) => match **nt {\n+                        NtPat(ref pat) => pat.clone(),\n+                        _ => unreachable!(),\n+                    },\n+                    _ => unreachable!(),\n+                }\n+            }).collect();\n+            let arm = cx.arm(seq_sp, pats, cx.expr_bool(seq_sp, true));\n+\n+            quote_expr!(cx,\n+                match $matched_expr {\n+                    $arm\n+                    _ => false\n+                }\n+            )\n+        }\n+        _ => unreachable!()\n+    };\n+\n     MacEager::expr(mac_expr)\n }\n "}]}