{"sha": "7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "node_id": "C_kwDOAAsO6NoAKDdhMTdmYjljNDNiMmNkMGE4YTJmZjNkOTNiOWQ0MzZmYTI4MTUzZDY", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2022-02-12T12:48:46Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2022-02-12T12:48:46Z"}, "message": "Merge #11444\n\n11444: feat: Fix up syntax errors in attribute macro inputs to make completion work more often r=flodiebold a=flodiebold\n\nThis implements the \"fix up syntax nodes\" workaround mentioned in #11014. It isn't much more than a proof of concept; I have only implemented a few cases, but it already helps quite a bit.\r\n\r\nSome notes:\r\n - I'm not super happy about how much the fixup procedure needs to interact with the syntax node -> token tree conversion code (e.g. needing to share the token map). This could maybe be simplified with some refactoring of that code.\r\n - It would maybe be nice to have the fixup procedure reuse or share information with the parser, though I'm not really sure how much that would actually help.\n\nCo-authored-by: Florian Diebold <flodiebold@gmail.com>", "tree": {"sha": "17094b7e227b4780068bb850be25eb2667c3d6a7", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/17094b7e227b4780068bb850be25eb2667c3d6a7"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJiB6yuCRBK7hj4Ov3rIwAAqdcIAGsQ59OtKc0z0oGWygWJzvMe\nUAcb1iCIvkGPoi6ILA4EMZwTQ8k5EP75y5g5H1ZUc1oUSxltEtFRXdzrHjCUiDOx\nES3Yw+EQ01h0mO4rvF0QWcVMFAVB87we0UtvAn50Rt4lJ87EFqycYiOnJKO3Ghey\nFTbcZl/o3b/ehzOARFVi3IZPJxr1Y1NyBMCSosOyDzOO+05RCR4h9k7n9jmTAfSZ\nueq5T7Y/l9Fzzaqam3w+K5thIR70lddyuh7UQscpMqSZcEC4maDuApf3Rj5nt9zy\nFbQazS2jUy4w3TPXeTGlk9JOHQyok8mYDsTwvXKXi/0LAN9iQvd8KgFHfD/Rt/o=\n=SrF8\n-----END PGP SIGNATURE-----\n", "payload": "tree 17094b7e227b4780068bb850be25eb2667c3d6a7\nparent 4449a336f6965ebdfa9b7408e6ff40a6a990a43d\nparent ccb789b94ab2b4bebb3da8be046efb13028380f1\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1644670126 +0000\ncommitter GitHub <noreply@github.com> 1644670126 +0000\n\nMerge #11444\n\n11444: feat: Fix up syntax errors in attribute macro inputs to make completion work more often r=flodiebold a=flodiebold\n\nThis implements the \"fix up syntax nodes\" workaround mentioned in #11014. It isn't much more than a proof of concept; I have only implemented a few cases, but it already helps quite a bit.\r\n\r\nSome notes:\r\n - I'm not super happy about how much the fixup procedure needs to interact with the syntax node -> token tree conversion code (e.g. needing to share the token map). This could maybe be simplified with some refactoring of that code.\r\n - It would maybe be nice to have the fixup procedure reuse or share information with the parser, though I'm not really sure how much that would actually help.\n\nCo-authored-by: Florian Diebold <flodiebold@gmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "html_url": "https://github.com/rust-lang/rust/commit/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "4449a336f6965ebdfa9b7408e6ff40a6a990a43d", "url": "https://api.github.com/repos/rust-lang/rust/commits/4449a336f6965ebdfa9b7408e6ff40a6a990a43d", "html_url": "https://github.com/rust-lang/rust/commit/4449a336f6965ebdfa9b7408e6ff40a6a990a43d"}, {"sha": "ccb789b94ab2b4bebb3da8be046efb13028380f1", "url": "https://api.github.com/repos/rust-lang/rust/commits/ccb789b94ab2b4bebb3da8be046efb13028380f1", "html_url": "https://github.com/rust-lang/rust/commit/ccb789b94ab2b4bebb3da8be046efb13028380f1"}], "stats": {"total": 741, "additions": 646, "deletions": 95}, "files": [{"sha": "5ccc61a83c1d4eecf540c486336ec396313cf32d", "filename": "Cargo.lock", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -539,6 +539,7 @@ dependencies = [\n  \"cfg\",\n  \"cov-mark\",\n  \"either\",\n+ \"expect-test\",\n  \"hashbrown 0.12.0\",\n  \"itertools\",\n  \"la-arena\","}, {"sha": "af825a2e00b872ed4381df03bf04b5e183139aa0", "filename": "crates/base_db/src/fixture.rs", "status": "modified", "additions": 83, "deletions": 32, "changes": 115, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fbase_db%2Fsrc%2Ffixture.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fbase_db%2Fsrc%2Ffixture.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fbase_db%2Fsrc%2Ffixture.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -43,6 +43,17 @@ pub trait WithFixture: Default + SourceDatabaseExt + 'static {\n         db\n     }\n \n+    fn with_files_extra_proc_macros(\n+        ra_fixture: &str,\n+        proc_macros: Vec<(String, ProcMacro)>,\n+    ) -> Self {\n+        let fixture = ChangeFixture::parse_with_proc_macros(ra_fixture, proc_macros);\n+        let mut db = Self::default();\n+        fixture.change.apply(&mut db);\n+        assert!(fixture.file_position.is_none());\n+        db\n+    }\n+\n     fn with_position(ra_fixture: &str) -> (Self, FilePosition) {\n         let (db, file_id, range_or_offset) = Self::with_range_or_offset(ra_fixture);\n         let offset = range_or_offset.expect_offset();\n@@ -84,7 +95,14 @@ pub struct ChangeFixture {\n \n impl ChangeFixture {\n     pub fn parse(ra_fixture: &str) -> ChangeFixture {\n-        let (mini_core, proc_macros, fixture) = Fixture::parse(ra_fixture);\n+        Self::parse_with_proc_macros(ra_fixture, Vec::new())\n+    }\n+\n+    pub fn parse_with_proc_macros(\n+        ra_fixture: &str,\n+        mut proc_macros: Vec<(String, ProcMacro)>,\n+    ) -> ChangeFixture {\n+        let (mini_core, proc_macro_names, fixture) = Fixture::parse(ra_fixture);\n         let mut change = Change::new();\n \n         let mut files = Vec::new();\n@@ -222,11 +240,12 @@ impl ChangeFixture {\n             }\n         }\n \n-        if !proc_macros.is_empty() {\n+        if !proc_macro_names.is_empty() {\n             let proc_lib_file = file_id;\n             file_id.0 += 1;\n \n-            let (proc_macro, source) = test_proc_macros(&proc_macros);\n+            proc_macros.extend(default_test_proc_macros());\n+            let (proc_macro, source) = filter_test_proc_macros(&proc_macro_names, proc_macros);\n             let mut fs = FileSet::default();\n             fs.insert(\n                 proc_lib_file,\n@@ -272,52 +291,84 @@ impl ChangeFixture {\n     }\n }\n \n-fn test_proc_macros(proc_macros: &[String]) -> (Vec<ProcMacro>, String) {\n-    // The source here is only required so that paths to the macros exist and are resolvable.\n-    let source = r#\"\n+fn default_test_proc_macros() -> [(String, ProcMacro); 4] {\n+    [\n+        (\n+            r#\"\n #[proc_macro_attribute]\n pub fn identity(_attr: TokenStream, item: TokenStream) -> TokenStream {\n     item\n }\n+\"#\n+            .into(),\n+            ProcMacro {\n+                name: \"identity\".into(),\n+                kind: crate::ProcMacroKind::Attr,\n+                expander: Arc::new(IdentityProcMacroExpander),\n+            },\n+        ),\n+        (\n+            r#\"\n #[proc_macro_derive(DeriveIdentity)]\n pub fn derive_identity(item: TokenStream) -> TokenStream {\n     item\n }\n+\"#\n+            .into(),\n+            ProcMacro {\n+                name: \"DeriveIdentity\".into(),\n+                kind: crate::ProcMacroKind::CustomDerive,\n+                expander: Arc::new(IdentityProcMacroExpander),\n+            },\n+        ),\n+        (\n+            r#\"\n #[proc_macro_attribute]\n pub fn input_replace(attr: TokenStream, _item: TokenStream) -> TokenStream {\n     attr\n }\n+\"#\n+            .into(),\n+            ProcMacro {\n+                name: \"input_replace\".into(),\n+                kind: crate::ProcMacroKind::Attr,\n+                expander: Arc::new(AttributeInputReplaceProcMacroExpander),\n+            },\n+        ),\n+        (\n+            r#\"\n #[proc_macro]\n pub fn mirror(input: TokenStream) -> TokenStream {\n     input\n }\n-\"#;\n-    let proc_macros = [\n-        ProcMacro {\n-            name: \"identity\".into(),\n-            kind: crate::ProcMacroKind::Attr,\n-            expander: Arc::new(IdentityProcMacroExpander),\n-        },\n-        ProcMacro {\n-            name: \"DeriveIdentity\".into(),\n-            kind: crate::ProcMacroKind::CustomDerive,\n-            expander: Arc::new(IdentityProcMacroExpander),\n-        },\n-        ProcMacro {\n-            name: \"input_replace\".into(),\n-            kind: crate::ProcMacroKind::Attr,\n-            expander: Arc::new(AttributeInputReplaceProcMacroExpander),\n-        },\n-        ProcMacro {\n-            name: \"mirror\".into(),\n-            kind: crate::ProcMacroKind::FuncLike,\n-            expander: Arc::new(MirrorProcMacroExpander),\n-        },\n+\"#\n+            .into(),\n+            ProcMacro {\n+                name: \"mirror\".into(),\n+                kind: crate::ProcMacroKind::FuncLike,\n+                expander: Arc::new(MirrorProcMacroExpander),\n+            },\n+        ),\n     ]\n-    .into_iter()\n-    .filter(|pm| proc_macros.iter().any(|name| name == &stdx::to_lower_snake_case(&pm.name)))\n-    .collect();\n-    (proc_macros, source.into())\n+}\n+\n+fn filter_test_proc_macros(\n+    proc_macro_names: &[String],\n+    proc_macro_defs: Vec<(String, ProcMacro)>,\n+) -> (Vec<ProcMacro>, String) {\n+    // The source here is only required so that paths to the macros exist and are resolvable.\n+    let mut source = String::new();\n+    let mut proc_macros = Vec::new();\n+\n+    for (c, p) in proc_macro_defs {\n+        if !proc_macro_names.iter().any(|name| name == &stdx::to_lower_snake_case(&p.name)) {\n+            continue;\n+        }\n+        proc_macros.push(p);\n+        source += &c;\n+    }\n+\n+    (proc_macros, source)\n }\n \n #[derive(Debug, Clone, Copy)]"}, {"sha": "1a7d9aa8411263ed4004ce40636e6a3a0b2284ce", "filename": "crates/hir_def/src/macro_expansion_tests.rs", "status": "modified", "additions": 52, "deletions": 7, "changes": 59, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -14,10 +14,10 @@ mod builtin_fn_macro;\n mod builtin_derive_macro;\n mod proc_macros;\n \n-use std::{iter, ops::Range};\n+use std::{iter, ops::Range, sync::Arc};\n \n use ::mbe::TokenMap;\n-use base_db::{fixture::WithFixture, SourceDatabase};\n+use base_db::{fixture::WithFixture, ProcMacro, SourceDatabase};\n use expect_test::Expect;\n use hir_expand::{\n     db::{AstDatabase, TokenExpander},\n@@ -39,7 +39,21 @@ use crate::{\n \n #[track_caller]\n fn check(ra_fixture: &str, mut expect: Expect) {\n-    let db = TestDB::with_files(ra_fixture);\n+    let extra_proc_macros = vec![(\n+        r#\"\n+#[proc_macro_attribute]\n+pub fn identity_when_valid(_attr: TokenStream, item: TokenStream) -> TokenStream {\n+    item\n+}\n+\"#\n+        .into(),\n+        ProcMacro {\n+            name: \"identity_when_valid\".into(),\n+            kind: base_db::ProcMacroKind::Attr,\n+            expander: Arc::new(IdentityWhenValidProcMacroExpander),\n+        },\n+    )];\n+    let db = TestDB::with_files_extra_proc_macros(ra_fixture, extra_proc_macros);\n     let krate = db.crate_graph().iter().next().unwrap();\n     let def_map = db.crate_def_map(krate);\n     let local_id = def_map.root();\n@@ -172,7 +186,7 @@ fn check(ra_fixture: &str, mut expect: Expect) {\n         let range: Range<usize> = range.into();\n \n         if show_token_ids {\n-            if let Some((tree, map)) = arg.as_deref() {\n+            if let Some((tree, map, _)) = arg.as_deref() {\n                 let tt_range = call.token_tree().unwrap().syntax().text_range();\n                 let mut ranges = Vec::new();\n                 extract_id_ranges(&mut ranges, &map, &tree);\n@@ -201,10 +215,19 @@ fn check(ra_fixture: &str, mut expect: Expect) {\n     }\n \n     for decl_id in def_map[local_id].scope.declarations() {\n-        if let ModuleDefId::AdtId(AdtId::StructId(struct_id)) = decl_id {\n-            let src = struct_id.lookup(&db).source(&db);\n+        // FIXME: I'm sure there's already better way to do this\n+        let src = match decl_id {\n+            ModuleDefId::AdtId(AdtId::StructId(struct_id)) => {\n+                Some(struct_id.lookup(&db).source(&db).syntax().cloned())\n+            }\n+            ModuleDefId::FunctionId(function_id) => {\n+                Some(function_id.lookup(&db).source(&db).syntax().cloned())\n+            }\n+            _ => None,\n+        };\n+        if let Some(src) = src {\n             if src.file_id.is_attr_macro(&db) || src.file_id.is_custom_derive(&db) {\n-                let pp = pretty_print_macro_expansion(src.value.syntax().clone(), None);\n+                let pp = pretty_print_macro_expansion(src.value, None);\n                 format_to!(expanded_text, \"\\n{}\", pp)\n             }\n         }\n@@ -304,3 +327,25 @@ fn pretty_print_macro_expansion(expn: SyntaxNode, map: Option<&TokenMap>) -> Str\n     }\n     res\n }\n+\n+// Identity mapping, but only works when the input is syntactically valid. This\n+// simulates common proc macros that unnecessarily parse their input and return\n+// compile errors.\n+#[derive(Debug)]\n+struct IdentityWhenValidProcMacroExpander;\n+impl base_db::ProcMacroExpander for IdentityWhenValidProcMacroExpander {\n+    fn expand(\n+        &self,\n+        subtree: &Subtree,\n+        _: Option<&Subtree>,\n+        _: &base_db::Env,\n+    ) -> Result<Subtree, base_db::ProcMacroExpansionError> {\n+        let (parse, _) =\n+            ::mbe::token_tree_to_syntax_node(subtree, ::mbe::TopEntryPoint::MacroItems);\n+        if parse.errors().is_empty() {\n+            Ok(subtree.clone())\n+        } else {\n+            panic!(\"got invalid macro input: {:?}\", parse.errors());\n+        }\n+    }\n+}"}, {"sha": "0ca30fb79928517788345819f3fa05a2656955af", "filename": "crates/hir_def/src/macro_expansion_tests/proc_macros.rs", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -52,3 +52,43 @@ struct S;\n #[attr2] struct S;\"##]],\n     );\n }\n+\n+#[test]\n+fn attribute_macro_syntax_completion_1() {\n+    // this is just the case where the input is actually valid\n+    check(\n+        r#\"\n+//- proc_macros: identity_when_valid\n+#[proc_macros::identity_when_valid]\n+fn foo() { bar.baz(); blub }\n+\"#,\n+        expect![[r##\"\n+#[proc_macros::identity_when_valid]\n+fn foo() { bar.baz(); blub }\n+\n+fn foo() {\n+    bar.baz();\n+    blub\n+}\"##]],\n+    );\n+}\n+\n+#[test]\n+fn attribute_macro_syntax_completion_2() {\n+    // common case of dot completion while typing\n+    check(\n+        r#\"\n+//- proc_macros: identity_when_valid\n+#[proc_macros::identity_when_valid]\n+fn foo() { bar.; blub }\n+\"#,\n+        expect![[r##\"\n+#[proc_macros::identity_when_valid]\n+fn foo() { bar.; blub }\n+\n+fn foo() {\n+    bar. ;\n+    blub\n+}\"##]],\n+    );\n+}"}, {"sha": "d7b4cbf82e2e83ce5284c8d53755b5abb4943ec8", "filename": "crates/hir_expand/Cargo.toml", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2FCargo.toml?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -27,3 +27,6 @@ profile = { path = \"../profile\", version = \"0.0.0\" }\n tt = { path = \"../tt\", version = \"0.0.0\" }\n mbe = { path = \"../mbe\", version = \"0.0.0\" }\n limit = { path = \"../limit\", version = \"0.0.0\" }\n+\n+[dev-dependencies]\n+expect-test = \"1.2.0-pre.1\""}, {"sha": "75766a54a74f7920e16746860ed39bf750bb8507", "filename": "crates/hir_expand/src/db.rs", "status": "modified", "additions": 33, "deletions": 10, "changes": 43, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fdb.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -14,7 +14,7 @@ use syntax::{\n };\n \n use crate::{\n-    ast_id_map::AstIdMap, hygiene::HygieneFrame, BuiltinAttrExpander, BuiltinDeriveExpander,\n+    ast_id_map::AstIdMap, fixup, hygiene::HygieneFrame, BuiltinAttrExpander, BuiltinDeriveExpander,\n     BuiltinFnLikeExpander, ExpandTo, HirFileId, HirFileIdRepr, MacroCallId, MacroCallKind,\n     MacroCallLoc, MacroDefId, MacroDefKind, MacroFile, ProcMacroExpander,\n };\n@@ -108,7 +108,10 @@ pub trait AstDatabase: SourceDatabase {\n \n     /// Lowers syntactic macro call to a token tree representation.\n     #[salsa::transparent]\n-    fn macro_arg(&self, id: MacroCallId) -> Option<Arc<(tt::Subtree, mbe::TokenMap)>>;\n+    fn macro_arg(\n+        &self,\n+        id: MacroCallId,\n+    ) -> Option<Arc<(tt::Subtree, mbe::TokenMap, fixup::SyntaxFixupUndoInfo)>>;\n     /// Extracts syntax node, corresponding to a macro call. That's a firewall\n     /// query, only typing in the macro call itself changes the returned\n     /// subtree.\n@@ -146,8 +149,15 @@ pub fn expand_speculative(\n \n     // Build the subtree and token mapping for the speculative args\n     let censor = censor_for_macro_input(&loc, &speculative_args);\n-    let (mut tt, spec_args_tmap) =\n-        mbe::syntax_node_to_token_tree_censored(&speculative_args, &censor);\n+    let mut fixups = fixup::fixup_syntax(&speculative_args);\n+    fixups.replace.extend(censor.into_iter().map(|node| (node, Vec::new())));\n+    let (mut tt, spec_args_tmap, _) = mbe::syntax_node_to_token_tree_with_modifications(\n+        &speculative_args,\n+        fixups.token_map,\n+        fixups.next_id,\n+        fixups.replace,\n+        fixups.append,\n+    );\n \n     let (attr_arg, token_id) = match loc.kind {\n         MacroCallKind::Attr { invoc_attr_index, .. } => {\n@@ -194,14 +204,15 @@ pub fn expand_speculative(\n \n     // Do the actual expansion, we need to directly expand the proc macro due to the attribute args\n     // Otherwise the expand query will fetch the non speculative attribute args and pass those instead.\n-    let speculative_expansion = if let MacroDefKind::ProcMacro(expander, ..) = loc.def.kind {\n+    let mut speculative_expansion = if let MacroDefKind::ProcMacro(expander, ..) = loc.def.kind {\n         tt.delimiter = None;\n         expander.expand(db, loc.krate, &tt, attr_arg.as_ref())\n     } else {\n         macro_def.expand(db, actual_macro_call, &tt)\n     };\n \n     let expand_to = macro_expand_to(db, actual_macro_call);\n+    fixup::reverse_fixups(&mut speculative_expansion.value, &spec_args_tmap, &fixups.undo_info);\n     let (node, rev_tmap) = token_tree_to_syntax_node(&speculative_expansion.value, expand_to);\n \n     let range = rev_tmap.first_range_by_token(token_id, token_to_map.kind())?;\n@@ -289,20 +300,31 @@ fn parse_macro_expansion(\n     }\n }\n \n-fn macro_arg(db: &dyn AstDatabase, id: MacroCallId) -> Option<Arc<(tt::Subtree, mbe::TokenMap)>> {\n+fn macro_arg(\n+    db: &dyn AstDatabase,\n+    id: MacroCallId,\n+) -> Option<Arc<(tt::Subtree, mbe::TokenMap, fixup::SyntaxFixupUndoInfo)>> {\n     let arg = db.macro_arg_text(id)?;\n     let loc = db.lookup_intern_macro_call(id);\n \n     let node = SyntaxNode::new_root(arg);\n     let censor = censor_for_macro_input(&loc, &node);\n-    let (mut tt, tmap) = mbe::syntax_node_to_token_tree_censored(&node, &censor);\n+    let mut fixups = fixup::fixup_syntax(&node);\n+    fixups.replace.extend(censor.into_iter().map(|node| (node, Vec::new())));\n+    let (mut tt, tmap, _) = mbe::syntax_node_to_token_tree_with_modifications(\n+        &node,\n+        fixups.token_map,\n+        fixups.next_id,\n+        fixups.replace,\n+        fixups.append,\n+    );\n \n     if loc.def.is_proc_macro() {\n         // proc macros expect their inputs without parentheses, MBEs expect it with them included\n         tt.delimiter = None;\n     }\n \n-    Some(Arc::new((tt, tmap)))\n+    Some(Arc::new((tt, tmap, fixups.undo_info)))\n }\n \n fn censor_for_macro_input(loc: &MacroCallLoc, node: &SyntaxNode) -> FxHashSet<SyntaxNode> {\n@@ -419,10 +441,9 @@ fn macro_expand(db: &dyn AstDatabase, id: MacroCallId) -> ExpandResult<Option<Ar\n         // be reported at the definition site (when we construct a def map).\n         Err(err) => return ExpandResult::str_err(format!(\"invalid macro definition: {}\", err)),\n     };\n-    let ExpandResult { value: tt, err } = expander.expand(db, id, &macro_arg.0);\n+    let ExpandResult { value: mut tt, err } = expander.expand(db, id, &macro_arg.0);\n     // Set a hard limit for the expanded tt\n     let count = tt.count();\n-    // XXX: Make ExpandResult a real error and use .map_err instead?\n     if TOKEN_LIMIT.check(count).is_err() {\n         return ExpandResult::str_err(format!(\n             \"macro invocation exceeds token limit: produced {} tokens, limit is {}\",\n@@ -431,6 +452,8 @@ fn macro_expand(db: &dyn AstDatabase, id: MacroCallId) -> ExpandResult<Option<Ar\n         ));\n     }\n \n+    fixup::reverse_fixups(&mut tt, &macro_arg.1, &macro_arg.2);\n+\n     ExpandResult { value: Some(Arc::new(tt)), err }\n }\n "}, {"sha": "2eb3da79dc615a41094de1f36b9180ea7e2084bc", "filename": "crates/hir_expand/src/fixup.rs", "status": "added", "additions": 261, "deletions": 0, "changes": 261, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2Fsrc%2Ffixup.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2Fsrc%2Ffixup.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Ffixup.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -0,0 +1,261 @@\n+//! To make attribute macros work reliably when typing, we need to take care to\n+//! fix up syntax errors in the code we're passing to them.\n+use std::mem;\n+\n+use mbe::{SyntheticToken, SyntheticTokenId, TokenMap};\n+use rustc_hash::FxHashMap;\n+use syntax::{\n+    ast::{self, AstNode},\n+    match_ast, SyntaxKind, SyntaxNode, TextRange,\n+};\n+use tt::Subtree;\n+\n+/// The result of calculating fixes for a syntax node -- a bunch of changes\n+/// (appending to and replacing nodes), the information that is needed to\n+/// reverse those changes afterwards, and a token map.\n+#[derive(Debug)]\n+pub(crate) struct SyntaxFixups {\n+    pub(crate) append: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    pub(crate) replace: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    pub(crate) undo_info: SyntaxFixupUndoInfo,\n+    pub(crate) token_map: TokenMap,\n+    pub(crate) next_id: u32,\n+}\n+\n+/// This is the information needed to reverse the fixups.\n+#[derive(Debug, PartialEq, Eq)]\n+pub struct SyntaxFixupUndoInfo {\n+    original: Vec<Subtree>,\n+}\n+\n+const EMPTY_ID: SyntheticTokenId = SyntheticTokenId(!0);\n+\n+pub(crate) fn fixup_syntax(node: &SyntaxNode) -> SyntaxFixups {\n+    let mut append = FxHashMap::default();\n+    let mut replace = FxHashMap::default();\n+    let mut preorder = node.preorder();\n+    let mut original = Vec::new();\n+    let mut token_map = TokenMap::default();\n+    let mut next_id = 0;\n+    while let Some(event) = preorder.next() {\n+        let node = match event {\n+            syntax::WalkEvent::Enter(node) => node,\n+            syntax::WalkEvent::Leave(_) => continue,\n+        };\n+\n+        if can_handle_error(&node) && has_error_to_handle(&node) {\n+            // the node contains an error node, we have to completely replace it by something valid\n+            let (original_tree, new_tmap, new_next_id) =\n+                mbe::syntax_node_to_token_tree_with_modifications(\n+                    &node,\n+                    mem::take(&mut token_map),\n+                    next_id,\n+                    Default::default(),\n+                    Default::default(),\n+                );\n+            token_map = new_tmap;\n+            next_id = new_next_id;\n+            let idx = original.len() as u32;\n+            original.push(original_tree);\n+            let replacement = SyntheticToken {\n+                kind: SyntaxKind::IDENT,\n+                text: \"__ra_fixup\".into(),\n+                range: node.text_range(),\n+                id: SyntheticTokenId(idx),\n+            };\n+            replace.insert(node.clone(), vec![replacement]);\n+            preorder.skip_subtree();\n+            continue;\n+        }\n+\n+        // In some other situations, we can fix things by just appending some tokens.\n+        let end_range = TextRange::empty(node.text_range().end());\n+        match_ast! {\n+            match node {\n+                ast::FieldExpr(it) => {\n+                    if it.name_ref().is_none() {\n+                        // incomplete field access: some_expr.|\n+                        append.insert(node.clone(), vec![\n+                            SyntheticToken {\n+                                kind: SyntaxKind::IDENT,\n+                                text: \"__ra_fixup\".into(),\n+                                range: end_range,\n+                                id: EMPTY_ID,\n+                            },\n+                        ]);\n+                    }\n+                },\n+                ast::ExprStmt(it) => {\n+                    if it.semicolon_token().is_none() {\n+                        append.insert(node.clone(), vec![\n+                            SyntheticToken {\n+                                kind: SyntaxKind::SEMICOLON,\n+                                text: \";\".into(),\n+                                range: end_range,\n+                                id: EMPTY_ID,\n+                            },\n+                        ]);\n+                    }\n+                },\n+                _ => (),\n+            }\n+        }\n+    }\n+    SyntaxFixups {\n+        append,\n+        replace,\n+        token_map,\n+        next_id,\n+        undo_info: SyntaxFixupUndoInfo { original },\n+    }\n+}\n+\n+fn has_error(node: &SyntaxNode) -> bool {\n+    node.children().any(|c| c.kind() == SyntaxKind::ERROR)\n+}\n+\n+fn can_handle_error(node: &SyntaxNode) -> bool {\n+    ast::Expr::can_cast(node.kind())\n+}\n+\n+fn has_error_to_handle(node: &SyntaxNode) -> bool {\n+    has_error(node) || node.children().any(|c| !can_handle_error(&c) && has_error_to_handle(&c))\n+}\n+\n+pub(crate) fn reverse_fixups(\n+    tt: &mut Subtree,\n+    token_map: &TokenMap,\n+    undo_info: &SyntaxFixupUndoInfo,\n+) {\n+    tt.token_trees.retain(|tt| match tt {\n+        tt::TokenTree::Leaf(leaf) => {\n+            token_map.synthetic_token_id(leaf.id()).is_none()\n+                || token_map.synthetic_token_id(leaf.id()) != Some(EMPTY_ID)\n+        }\n+        _ => true,\n+    });\n+    tt.token_trees.iter_mut().for_each(|tt| match tt {\n+        tt::TokenTree::Subtree(tt) => reverse_fixups(tt, token_map, undo_info),\n+        tt::TokenTree::Leaf(leaf) => {\n+            if let Some(id) = token_map.synthetic_token_id(leaf.id()) {\n+                let original = &undo_info.original[id.0 as usize];\n+                *tt = tt::TokenTree::Subtree(original.clone());\n+            }\n+        }\n+    });\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use expect_test::{expect, Expect};\n+\n+    use super::reverse_fixups;\n+\n+    #[track_caller]\n+    fn check(ra_fixture: &str, mut expect: Expect) {\n+        let parsed = syntax::SourceFile::parse(ra_fixture);\n+        let fixups = super::fixup_syntax(&parsed.syntax_node());\n+        let (mut tt, tmap, _) = mbe::syntax_node_to_token_tree_with_modifications(\n+            &parsed.syntax_node(),\n+            fixups.token_map,\n+            fixups.next_id,\n+            fixups.replace,\n+            fixups.append,\n+        );\n+\n+        let mut actual = tt.to_string();\n+        actual.push_str(\"\\n\");\n+\n+        expect.indent(false);\n+        expect.assert_eq(&actual);\n+\n+        // the fixed-up tree should be syntactically valid\n+        let (parse, _) = mbe::token_tree_to_syntax_node(&tt, ::mbe::TopEntryPoint::MacroItems);\n+        assert_eq!(\n+            parse.errors(),\n+            &[],\n+            \"parse has syntax errors. parse tree:\\n{:#?}\",\n+            parse.syntax_node()\n+        );\n+\n+        reverse_fixups(&mut tt, &tmap, &fixups.undo_info);\n+\n+        // the fixed-up + reversed version should be equivalent to the original input\n+        // (but token IDs don't matter)\n+        let (original_as_tt, _) = mbe::syntax_node_to_token_tree(&parsed.syntax_node());\n+        assert_eq!(tt.to_string(), original_as_tt.to_string());\n+    }\n+\n+    #[test]\n+    fn incomplete_field_expr_1() {\n+        check(\n+            r#\"\n+fn foo() {\n+    a.\n+}\n+\"#,\n+            expect![[r#\"\n+fn foo () {a . __ra_fixup}\n+\"#]],\n+        )\n+    }\n+\n+    #[test]\n+    fn incomplete_field_expr_2() {\n+        check(\n+            r#\"\n+fn foo() {\n+    a. ;\n+}\n+\"#,\n+            expect![[r#\"\n+fn foo () {a . __ra_fixup ;}\n+\"#]],\n+        )\n+    }\n+\n+    #[test]\n+    fn incomplete_field_expr_3() {\n+        check(\n+            r#\"\n+fn foo() {\n+    a. ;\n+    bar();\n+}\n+\"#,\n+            expect![[r#\"\n+fn foo () {a . __ra_fixup ; bar () ;}\n+\"#]],\n+        )\n+    }\n+\n+    #[test]\n+    fn field_expr_before_call() {\n+        // another case that easily happens while typing\n+        check(\n+            r#\"\n+fn foo() {\n+    a.b\n+    bar();\n+}\n+\"#,\n+            expect![[r#\"\n+fn foo () {a . b ; bar () ;}\n+\"#]],\n+        )\n+    }\n+\n+    #[test]\n+    fn extraneous_comma() {\n+        check(\n+            r#\"\n+fn foo() {\n+    bar(,);\n+}\n+\"#,\n+            expect![[r#\"\n+fn foo () {__ra_fixup ;}\n+\"#]],\n+        )\n+    }\n+}"}, {"sha": "d60734372c0cedd9589a2b789bb4fabfae4e2098", "filename": "crates/hir_expand/src/hygiene.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -15,6 +15,7 @@ use syntax::{\n \n use crate::{\n     db::{self, AstDatabase},\n+    fixup,\n     name::{AsName, Name},\n     HirFileId, HirFileIdRepr, InFile, MacroCallKind, MacroCallLoc, MacroDefKind, MacroFile,\n };\n@@ -127,7 +128,7 @@ struct HygieneInfo {\n     attr_input_or_mac_def_start: Option<InFile<TextSize>>,\n \n     macro_def: Arc<TokenExpander>,\n-    macro_arg: Arc<(tt::Subtree, mbe::TokenMap)>,\n+    macro_arg: Arc<(tt::Subtree, mbe::TokenMap, fixup::SyntaxFixupUndoInfo)>,\n     macro_arg_shift: mbe::Shift,\n     exp_map: Arc<mbe::TokenMap>,\n }"}, {"sha": "279fdc61dca9de48c73f6e35f5c5b80a04559240", "filename": "crates/hir_expand/src/lib.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fhir_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Flib.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -15,6 +15,7 @@ pub mod proc_macro;\n pub mod quote;\n pub mod eager;\n pub mod mod_path;\n+mod fixup;\n \n pub use mbe::{ExpandError, ExpandResult, Origin};\n \n@@ -426,7 +427,7 @@ pub struct ExpansionInfo {\n     attr_input_or_mac_def: Option<InFile<ast::TokenTree>>,\n \n     macro_def: Arc<TokenExpander>,\n-    macro_arg: Arc<(tt::Subtree, mbe::TokenMap)>,\n+    macro_arg: Arc<(tt::Subtree, mbe::TokenMap, fixup::SyntaxFixupUndoInfo)>,\n     /// A shift built from `macro_arg`'s subtree, relevant for attributes as the item is the macro arg\n     /// and as such we need to shift tokens if they are part of an attributes input instead of their item.\n     macro_arg_shift: mbe::Shift,"}, {"sha": "2e643453afc60e051fe57d513e42d858aec7d629", "filename": "crates/ide_completion/src/tests/attribute.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fide_completion%2Fsrc%2Ftests%2Fattribute.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fide_completion%2Fsrc%2Ftests%2Fattribute.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_completion%2Fsrc%2Ftests%2Fattribute.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -62,8 +62,7 @@ fn proc_macros_qualified() {\n struct Foo;\n \"#,\n         expect![[r#\"\n-            at input_replace pub macro input_replace\n-            at identity      pub macro identity\n+            at identity pub macro identity\n         \"#]],\n     )\n }"}, {"sha": "07b7f4d1a5e8d5fcaf8472de4dd522c9d2d8a868", "filename": "crates/mbe/src/lib.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fmbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fmbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Flib.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -30,7 +30,8 @@ pub use tt::{Delimiter, DelimiterKind, Punct};\n pub use crate::{\n     syntax_bridge::{\n         parse_exprs_with_sep, parse_to_token_tree, syntax_node_to_token_tree,\n-        syntax_node_to_token_tree_censored, token_tree_to_syntax_node,\n+        syntax_node_to_token_tree_with_modifications, token_tree_to_syntax_node, SyntheticToken,\n+        SyntheticTokenId,\n     },\n     token_map::TokenMap,\n };"}, {"sha": "7e12647cd86230d4703d2d844744914f28f058cb", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 144, "deletions": 41, "changes": 185, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -1,7 +1,7 @@\n //! Conversions between [`SyntaxNode`] and [`tt::TokenTree`].\n \n-use rustc_hash::{FxHashMap, FxHashSet};\n-use stdx::non_empty_vec::NonEmptyVec;\n+use rustc_hash::FxHashMap;\n+use stdx::{always, non_empty_vec::NonEmptyVec};\n use syntax::{\n     ast::{self, make::tokens::doc_comment},\n     AstToken, Parse, PreorderWithTokens, SmolStr, SyntaxElement, SyntaxKind,\n@@ -15,20 +15,43 @@ use crate::{to_parser_input::to_parser_input, tt_iter::TtIter, TokenMap};\n /// Convert the syntax node to a `TokenTree` (what macro\n /// will consume).\n pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> (tt::Subtree, TokenMap) {\n-    syntax_node_to_token_tree_censored(node, &Default::default())\n+    let (subtree, token_map, _) = syntax_node_to_token_tree_with_modifications(\n+        node,\n+        Default::default(),\n+        0,\n+        Default::default(),\n+        Default::default(),\n+    );\n+    (subtree, token_map)\n }\n \n /// Convert the syntax node to a `TokenTree` (what macro will consume)\n /// with the censored range excluded.\n-pub fn syntax_node_to_token_tree_censored(\n+pub fn syntax_node_to_token_tree_with_modifications(\n     node: &SyntaxNode,\n-    censor: &FxHashSet<SyntaxNode>,\n-) -> (tt::Subtree, TokenMap) {\n+    existing_token_map: TokenMap,\n+    next_id: u32,\n+    replace: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    append: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+) -> (tt::Subtree, TokenMap, u32) {\n     let global_offset = node.text_range().start();\n-    let mut c = Convertor::new(node, global_offset, censor);\n+    let mut c = Convertor::new(node, global_offset, existing_token_map, next_id, replace, append);\n     let subtree = convert_tokens(&mut c);\n     c.id_alloc.map.shrink_to_fit();\n-    (subtree, c.id_alloc.map)\n+    always!(c.replace.is_empty(), \"replace: {:?}\", c.replace);\n+    always!(c.append.is_empty(), \"append: {:?}\", c.append);\n+    (subtree, c.id_alloc.map, c.id_alloc.next_id)\n+}\n+\n+#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]\n+pub struct SyntheticTokenId(pub u32);\n+\n+#[derive(Debug, Clone)]\n+pub struct SyntheticToken {\n+    pub kind: SyntaxKind,\n+    pub text: SmolStr,\n+    pub range: TextRange,\n+    pub id: SyntheticTokenId,\n }\n \n // The following items are what `rustc` macro can be parsed into :\n@@ -147,13 +170,14 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n             Some(it) => it,\n             None => break,\n         };\n+        let synth_id = token.synthetic_id(&conv);\n \n         let kind = token.kind(&conv);\n         if kind == COMMENT {\n             if let Some(tokens) = conv.convert_doc_comment(&token) {\n                 // FIXME: There has to be a better way to do this\n                 // Add the comments token id to the converted doc string\n-                let id = conv.id_alloc().alloc(range);\n+                let id = conv.id_alloc().alloc(range, synth_id);\n                 result.extend(tokens.into_iter().map(|mut tt| {\n                     if let tt::TokenTree::Subtree(sub) = &mut tt {\n                         if let Some(tt::TokenTree::Leaf(tt::Leaf::Literal(lit))) =\n@@ -168,7 +192,9 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n             continue;\n         }\n         let tt = if kind.is_punct() && kind != UNDERSCORE {\n-            assert_eq!(range.len(), TextSize::of('.'));\n+            if synth_id.is_none() {\n+                assert_eq!(range.len(), TextSize::of('.'));\n+            }\n \n             if let Some(delim) = subtree.delimiter {\n                 let expected = match delim.kind {\n@@ -220,11 +246,13 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n                     panic!(\"Token from lexer must be single char: token = {:#?}\", token);\n                 }\n             };\n-            tt::Leaf::from(tt::Punct { char, spacing, id: conv.id_alloc().alloc(range) }).into()\n+            tt::Leaf::from(tt::Punct { char, spacing, id: conv.id_alloc().alloc(range, synth_id) })\n+                .into()\n         } else {\n             macro_rules! make_leaf {\n                 ($i:ident) => {\n-                    tt::$i { id: conv.id_alloc().alloc(range), text: token.to_text(conv) }.into()\n+                    tt::$i { id: conv.id_alloc().alloc(range, synth_id), text: token.to_text(conv) }\n+                        .into()\n                 };\n             }\n             let leaf: tt::Leaf = match kind {\n@@ -239,14 +267,14 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n                     let apostrophe = tt::Leaf::from(tt::Punct {\n                         char: '\\'',\n                         spacing: tt::Spacing::Joint,\n-                        id: conv.id_alloc().alloc(r),\n+                        id: conv.id_alloc().alloc(r, synth_id),\n                     });\n                     result.push(apostrophe.into());\n \n                     let r = TextRange::at(range.start() + char_unit, range.len() - char_unit);\n                     let ident = tt::Leaf::from(tt::Ident {\n                         text: SmolStr::new(&token.to_text(conv)[1..]),\n-                        id: conv.id_alloc().alloc(r),\n+                        id: conv.id_alloc().alloc(r, synth_id),\n                     });\n                     result.push(ident.into());\n                     continue;\n@@ -267,7 +295,7 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n \n         conv.id_alloc().close_delim(entry.idx, None);\n         let leaf: tt::Leaf = tt::Punct {\n-            id: conv.id_alloc().alloc(entry.open_range),\n+            id: conv.id_alloc().alloc(entry.open_range, None),\n             char: match entry.subtree.delimiter.unwrap().kind {\n                 tt::DelimiterKind::Parenthesis => '(',\n                 tt::DelimiterKind::Brace => '{',\n@@ -361,11 +389,18 @@ struct TokenIdAlloc {\n }\n \n impl TokenIdAlloc {\n-    fn alloc(&mut self, absolute_range: TextRange) -> tt::TokenId {\n+    fn alloc(\n+        &mut self,\n+        absolute_range: TextRange,\n+        synthetic_id: Option<SyntheticTokenId>,\n+    ) -> tt::TokenId {\n         let relative_range = absolute_range - self.global_offset;\n         let token_id = tt::TokenId(self.next_id);\n         self.next_id += 1;\n         self.map.insert(token_id, relative_range);\n+        if let Some(id) = synthetic_id {\n+            self.map.insert_synthetic(token_id, id);\n+        }\n         token_id\n     }\n \n@@ -405,6 +440,8 @@ trait SrcToken<Ctx>: std::fmt::Debug {\n     fn to_char(&self, ctx: &Ctx) -> Option<char>;\n \n     fn to_text(&self, ctx: &Ctx) -> SmolStr;\n+\n+    fn synthetic_id(&self, ctx: &Ctx) -> Option<SyntheticTokenId>;\n }\n \n trait TokenConvertor: Sized {\n@@ -431,6 +468,10 @@ impl<'a> SrcToken<RawConvertor<'a>> for usize {\n     fn to_text(&self, ctx: &RawConvertor<'_>) -> SmolStr {\n         ctx.lexed.text(*self).into()\n     }\n+\n+    fn synthetic_id(&self, _ctx: &RawConvertor<'a>) -> Option<SyntheticTokenId> {\n+        None\n+    }\n }\n \n impl<'a> TokenConvertor for RawConvertor<'a> {\n@@ -465,86 +506,130 @@ impl<'a> TokenConvertor for RawConvertor<'a> {\n     }\n }\n \n-struct Convertor<'c> {\n+struct Convertor {\n     id_alloc: TokenIdAlloc,\n     current: Option<SyntaxToken>,\n+    current_synthetic: Vec<SyntheticToken>,\n     preorder: PreorderWithTokens,\n-    censor: &'c FxHashSet<SyntaxNode>,\n+    replace: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    append: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n     range: TextRange,\n     punct_offset: Option<(SyntaxToken, TextSize)>,\n }\n \n-impl<'c> Convertor<'c> {\n+impl Convertor {\n     fn new(\n         node: &SyntaxNode,\n         global_offset: TextSize,\n-        censor: &'c FxHashSet<SyntaxNode>,\n-    ) -> Convertor<'c> {\n+        existing_token_map: TokenMap,\n+        next_id: u32,\n+        mut replace: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+        mut append: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    ) -> Convertor {\n         let range = node.text_range();\n         let mut preorder = node.preorder_with_tokens();\n-        let first = Self::next_token(&mut preorder, censor);\n+        let (first, synthetic) = Self::next_token(&mut preorder, &mut replace, &mut append);\n         Convertor {\n-            id_alloc: { TokenIdAlloc { map: TokenMap::default(), global_offset, next_id: 0 } },\n+            id_alloc: { TokenIdAlloc { map: existing_token_map, global_offset, next_id } },\n             current: first,\n+            current_synthetic: synthetic,\n             preorder,\n             range,\n-            censor,\n+            replace,\n+            append,\n             punct_offset: None,\n         }\n     }\n \n     fn next_token(\n         preorder: &mut PreorderWithTokens,\n-        censor: &FxHashSet<SyntaxNode>,\n-    ) -> Option<SyntaxToken> {\n+        replace: &mut FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+        append: &mut FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    ) -> (Option<SyntaxToken>, Vec<SyntheticToken>) {\n         while let Some(ev) = preorder.next() {\n             let ele = match ev {\n                 WalkEvent::Enter(ele) => ele,\n+                WalkEvent::Leave(SyntaxElement::Node(node)) => {\n+                    if let Some(mut v) = append.remove(&node) {\n+                        if !v.is_empty() {\n+                            v.reverse();\n+                            return (None, v);\n+                        }\n+                    }\n+                    continue;\n+                }\n                 _ => continue,\n             };\n             match ele {\n-                SyntaxElement::Token(t) => return Some(t),\n-                SyntaxElement::Node(node) if censor.contains(&node) => preorder.skip_subtree(),\n-                SyntaxElement::Node(_) => (),\n+                SyntaxElement::Token(t) => return (Some(t), Vec::new()),\n+                SyntaxElement::Node(node) => {\n+                    if let Some(mut v) = replace.remove(&node) {\n+                        preorder.skip_subtree();\n+                        if !v.is_empty() {\n+                            v.reverse();\n+                            return (None, v);\n+                        }\n+                    }\n+                }\n             }\n         }\n-        None\n+        (None, Vec::new())\n     }\n }\n \n #[derive(Debug)]\n enum SynToken {\n     Ordinary(SyntaxToken),\n+    // FIXME is this supposed to be `Punct`?\n     Punch(SyntaxToken, TextSize),\n+    Synthetic(SyntheticToken),\n }\n \n impl SynToken {\n-    fn token(&self) -> &SyntaxToken {\n+    fn token(&self) -> Option<&SyntaxToken> {\n         match self {\n-            SynToken::Ordinary(it) | SynToken::Punch(it, _) => it,\n+            SynToken::Ordinary(it) | SynToken::Punch(it, _) => Some(it),\n+            SynToken::Synthetic(_) => None,\n         }\n     }\n }\n \n-impl<'a> SrcToken<Convertor<'a>> for SynToken {\n-    fn kind(&self, _ctx: &Convertor<'a>) -> SyntaxKind {\n-        self.token().kind()\n+impl SrcToken<Convertor> for SynToken {\n+    fn kind(&self, _ctx: &Convertor) -> SyntaxKind {\n+        match self {\n+            SynToken::Ordinary(token) => token.kind(),\n+            SynToken::Punch(token, _) => token.kind(),\n+            SynToken::Synthetic(token) => token.kind,\n+        }\n     }\n-    fn to_char(&self, _ctx: &Convertor<'a>) -> Option<char> {\n+    fn to_char(&self, _ctx: &Convertor) -> Option<char> {\n         match self {\n             SynToken::Ordinary(_) => None,\n             SynToken::Punch(it, i) => it.text().chars().nth((*i).into()),\n+            SynToken::Synthetic(token) if token.text.len() == 1 => token.text.chars().next(),\n+            SynToken::Synthetic(_) => None,\n         }\n     }\n-    fn to_text(&self, _ctx: &Convertor<'a>) -> SmolStr {\n-        self.token().text().into()\n+    fn to_text(&self, _ctx: &Convertor) -> SmolStr {\n+        match self {\n+            SynToken::Ordinary(token) => token.text().into(),\n+            SynToken::Punch(token, _) => token.text().into(),\n+            SynToken::Synthetic(token) => token.text.clone(),\n+        }\n+    }\n+\n+    fn synthetic_id(&self, _ctx: &Convertor) -> Option<SyntheticTokenId> {\n+        match self {\n+            SynToken::Synthetic(token) => Some(token.id),\n+            _ => None,\n+        }\n     }\n }\n \n-impl TokenConvertor for Convertor<'_> {\n+impl TokenConvertor for Convertor {\n     type Token = SynToken;\n     fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n-        convert_doc_comment(token.token())\n+        convert_doc_comment(token.token()?)\n     }\n \n     fn bump(&mut self) -> Option<(Self::Token, TextRange)> {\n@@ -558,11 +643,25 @@ impl TokenConvertor for Convertor<'_> {\n             }\n         }\n \n+        if let Some(synth_token) = self.current_synthetic.pop() {\n+            if self.current_synthetic.is_empty() {\n+                let (new_current, new_synth) =\n+                    Self::next_token(&mut self.preorder, &mut self.replace, &mut self.append);\n+                self.current = new_current;\n+                self.current_synthetic = new_synth;\n+            }\n+            let range = synth_token.range;\n+            return Some((SynToken::Synthetic(synth_token), range));\n+        }\n+\n         let curr = self.current.clone()?;\n         if !&self.range.contains_range(curr.text_range()) {\n             return None;\n         }\n-        self.current = Self::next_token(&mut self.preorder, self.censor);\n+        let (new_current, new_synth) =\n+            Self::next_token(&mut self.preorder, &mut self.replace, &mut self.append);\n+        self.current = new_current;\n+        self.current_synthetic = new_synth;\n         let token = if curr.kind().is_punct() {\n             self.punct_offset = Some((curr.clone(), 0.into()));\n             let range = curr.text_range();\n@@ -585,6 +684,10 @@ impl TokenConvertor for Convertor<'_> {\n             }\n         }\n \n+        if let Some(synth_token) = self.current_synthetic.last() {\n+            return Some(SynToken::Synthetic(synth_token.clone()));\n+        }\n+\n         let curr = self.current.clone()?;\n         if !self.range.contains_range(curr.text_range()) {\n             return None;"}, {"sha": "c923e7a69a1bcf8f18d4d3a9a7eba6fb7bb6612d", "filename": "crates/mbe/src/token_map.rs", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fmbe%2Fsrc%2Ftoken_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Fmbe%2Fsrc%2Ftoken_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Ftoken_map.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -5,6 +5,8 @@ use std::hash::Hash;\n use parser::{SyntaxKind, T};\n use syntax::{TextRange, TextSize};\n \n+use crate::syntax_bridge::SyntheticTokenId;\n+\n #[derive(Debug, PartialEq, Eq, Clone, Copy, Hash)]\n enum TokenTextRange {\n     Token(TextRange),\n@@ -31,6 +33,7 @@ impl TokenTextRange {\n pub struct TokenMap {\n     /// Maps `tt::TokenId` to the *relative* source range.\n     entries: Vec<(tt::TokenId, TokenTextRange)>,\n+    pub synthetic_entries: Vec<(tt::TokenId, SyntheticTokenId)>,\n }\n \n impl TokenMap {\n@@ -57,6 +60,10 @@ impl TokenMap {\n             .filter_map(move |(_, range)| range.by_kind(kind))\n     }\n \n+    pub fn synthetic_token_id(&self, token_id: tt::TokenId) -> Option<SyntheticTokenId> {\n+        self.synthetic_entries.iter().find(|(tid, _)| *tid == token_id).map(|(_, id)| *id)\n+    }\n+\n     pub fn first_range_by_token(\n         &self,\n         token_id: tt::TokenId,\n@@ -67,12 +74,17 @@ impl TokenMap {\n \n     pub(crate) fn shrink_to_fit(&mut self) {\n         self.entries.shrink_to_fit();\n+        self.synthetic_entries.shrink_to_fit();\n     }\n \n     pub(crate) fn insert(&mut self, token_id: tt::TokenId, relative_range: TextRange) {\n         self.entries.push((token_id, TokenTextRange::Token(relative_range)));\n     }\n \n+    pub(crate) fn insert_synthetic(&mut self, token_id: tt::TokenId, id: SyntheticTokenId) {\n+        self.synthetic_entries.push((token_id, id));\n+    }\n+\n     pub(crate) fn insert_delim(\n         &mut self,\n         token_id: tt::TokenId,"}, {"sha": "0316b15038cec0cc7f5170324517e50e62ab51a4", "filename": "crates/tt/src/lib.rs", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Ftt%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6/crates%2Ftt%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Ftt%2Fsrc%2Flib.rs?ref=7a17fb9c43b2cd0a8a2ff3d93b9d436fa28153d6", "patch": "@@ -87,6 +87,16 @@ pub struct Ident {\n     pub id: TokenId,\n }\n \n+impl Leaf {\n+    pub fn id(&self) -> TokenId {\n+        match self {\n+            Leaf::Literal(l) => l.id,\n+            Leaf::Punct(p) => p.id,\n+            Leaf::Ident(i) => i.id,\n+        }\n+    }\n+}\n+\n fn print_debug_subtree(f: &mut fmt::Formatter<'_>, subtree: &Subtree, level: usize) -> fmt::Result {\n     let align = \"  \".repeat(level);\n "}]}