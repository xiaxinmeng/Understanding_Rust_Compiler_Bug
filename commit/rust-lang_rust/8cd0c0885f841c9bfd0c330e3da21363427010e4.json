{"sha": "8cd0c0885f841c9bfd0c330e3da21363427010e4", "node_id": "MDY6Q29tbWl0NzI0NzEyOjhjZDBjMDg4NWY4NDFjOWJmZDBjMzMwZTNkYTIxMzYzNDI3MDEwZTQ=", "commit": {"author": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2017-02-20T05:44:06Z"}, "committer": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2017-03-03T02:05:57Z"}, "message": "Introduce `syntax::parse::parser::TokenCursor`.", "tree": {"sha": "f3deee85d2261de9e0b13c8b3dce7eb59e034111", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f3deee85d2261de9e0b13c8b3dce7eb59e034111"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/8cd0c0885f841c9bfd0c330e3da21363427010e4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/8cd0c0885f841c9bfd0c330e3da21363427010e4", "html_url": "https://github.com/rust-lang/rust/commit/8cd0c0885f841c9bfd0c330e3da21363427010e4", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/8cd0c0885f841c9bfd0c330e3da21363427010e4/comments", "author": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "8dca72be9be71fbccae9370b45af8efb946dedc1", "url": "https://api.github.com/repos/rust-lang/rust/commits/8dca72be9be71fbccae9370b45af8efb946dedc1", "html_url": "https://github.com/rust-lang/rust/commit/8dca72be9be71fbccae9370b45af8efb946dedc1"}], "stats": {"total": 288, "additions": 173, "deletions": 115}, "files": [{"sha": "b12b0c03267010e30b2050a101451934bc551f7c", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 127, "deletions": 47, "changes": 174, "blob_url": "https://github.com/rust-lang/rust/blob/8cd0c0885f841c9bfd0c330e3da21363427010e4/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8cd0c0885f841c9bfd0c330e3da21363427010e4/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=8cd0c0885f841c9bfd0c330e3da21363427010e4", "patch": "@@ -9,7 +9,7 @@\n // except according to those terms.\n \n use abi::{self, Abi};\n-use ast::BareFnTy;\n+use ast::{AttrStyle, BareFnTy};\n use ast::{RegionTyParamBound, TraitTyParamBound, TraitBoundModifier};\n use ast::Unsafety;\n use ast::{Mod, Arg, Arm, Attribute, BindingMode, TraitItemKind};\n@@ -46,21 +46,21 @@ use errors::{self, DiagnosticBuilder};\n use parse::{self, classify, token};\n use parse::common::SeqSep;\n use parse::lexer::TokenAndSpan;\n+use parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n use parse::obsolete::ObsoleteSyntax;\n use parse::{new_sub_parser_from_file, ParseSess, Directory, DirectoryOwnership};\n use util::parser::{AssocOp, Fixity};\n use print::pprust;\n use ptr::P;\n use parse::PResult;\n-use tokenstream::{Delimited, TokenTree};\n+use tokenstream::{self, Delimited, TokenTree, TokenStream};\n use symbol::{Symbol, keywords};\n use util::ThinVec;\n \n use std::collections::HashSet;\n-use std::mem;\n+use std::{cmp, mem, slice};\n use std::path::{Path, PathBuf};\n use std::rc::Rc;\n-use std::slice;\n \n bitflags! {\n     flags Restrictions: u8 {\n@@ -175,12 +175,108 @@ pub struct Parser<'a> {\n     /// into modules, and sub-parsers have new values for this name.\n     pub root_module_name: Option<String>,\n     pub expected_tokens: Vec<TokenType>,\n-    pub tts: Vec<(TokenTree, usize)>,\n+    token_cursor: TokenCursor,\n     pub desugar_doc_comments: bool,\n     /// Whether we should configure out of line modules as we parse.\n     pub cfg_mods: bool,\n }\n \n+struct TokenCursor {\n+    frame: TokenCursorFrame,\n+    stack: Vec<TokenCursorFrame>,\n+}\n+\n+struct TokenCursorFrame {\n+    delim: token::DelimToken,\n+    span: Span,\n+    open_delim: bool,\n+    tree_cursor: tokenstream::Cursor,\n+    close_delim: bool,\n+}\n+\n+impl TokenCursorFrame {\n+    fn new(sp: Span, delimited: &Delimited) -> Self {\n+        TokenCursorFrame {\n+            delim: delimited.delim,\n+            span: sp,\n+            open_delim: delimited.delim == token::NoDelim,\n+            tree_cursor: delimited.tts.iter().cloned().collect::<TokenStream>().into_trees(),\n+            close_delim: delimited.delim == token::NoDelim,\n+        }\n+    }\n+}\n+\n+impl TokenCursor {\n+    fn next(&mut self) -> TokenAndSpan {\n+        loop {\n+            let tree = if !self.frame.open_delim {\n+                self.frame.open_delim = true;\n+                Delimited { delim: self.frame.delim, tts: Vec::new() }.open_tt(self.frame.span)\n+            } else if let Some(tree) = self.frame.tree_cursor.next() {\n+                tree\n+            } else if !self.frame.close_delim {\n+                self.frame.close_delim = true;\n+                Delimited { delim: self.frame.delim, tts: Vec::new() }.close_tt(self.frame.span)\n+            } else if let Some(frame) = self.stack.pop() {\n+                self.frame = frame;\n+                continue\n+            } else {\n+                return TokenAndSpan { tok: token::Eof, sp: self.frame.span }\n+            };\n+\n+            match tree {\n+                TokenTree::Token(sp, tok) => return TokenAndSpan { tok: tok, sp: sp },\n+                TokenTree::Delimited(sp, ref delimited) => {\n+                    let frame = TokenCursorFrame::new(sp, delimited);\n+                    self.stack.push(mem::replace(&mut self.frame, frame));\n+                }\n+            }\n+        }\n+    }\n+\n+    fn next_desugared(&mut self) -> TokenAndSpan {\n+        let (sp, name) = match self.next() {\n+            TokenAndSpan { sp, tok: token::DocComment(name) } => (sp, name),\n+            tok @ _ => return tok,\n+        };\n+\n+        let stripped = strip_doc_comment_decoration(&name.as_str());\n+\n+        // Searches for the occurrences of `\"#*` and returns the minimum number of `#`s\n+        // required to wrap the text.\n+        let mut num_of_hashes = 0;\n+        let mut count = 0;\n+        for ch in stripped.chars() {\n+            count = match ch {\n+                '\"' => 1,\n+                '#' if count > 0 => count + 1,\n+                _ => 0,\n+            };\n+            num_of_hashes = cmp::max(num_of_hashes, count);\n+        }\n+\n+        let body = TokenTree::Delimited(sp, Rc::new(Delimited {\n+            delim: token::Bracket,\n+            tts: vec![TokenTree::Token(sp, token::Ident(ast::Ident::from_str(\"doc\"))),\n+                      TokenTree::Token(sp, token::Eq),\n+                      TokenTree::Token(sp, token::Literal(\n+                          token::StrRaw(Symbol::intern(&stripped), num_of_hashes), None))],\n+        }));\n+\n+        self.stack.push(mem::replace(&mut self.frame, TokenCursorFrame::new(sp, &Delimited {\n+            delim: token::NoDelim,\n+            tts: if doc_comment_style(&name.as_str()) == AttrStyle::Inner {\n+                [TokenTree::Token(sp, token::Pound), TokenTree::Token(sp, token::Not), body]\n+                    .iter().cloned().collect()\n+            } else {\n+                [TokenTree::Token(sp, token::Pound), body].iter().cloned().collect()\n+            },\n+        })));\n+\n+        self.next()\n+    }\n+}\n+\n #[derive(PartialEq, Eq, Clone)]\n pub enum TokenType {\n     Token(token::Token),\n@@ -313,10 +409,6 @@ impl<'a> Parser<'a> {\n                directory: Option<Directory>,\n                desugar_doc_comments: bool)\n                -> Self {\n-        let tt = TokenTree::Delimited(syntax_pos::DUMMY_SP, Rc::new(Delimited {\n-            delim: token::NoDelim,\n-            tts: tokens,\n-        }));\n         let mut parser = Parser {\n             sess: sess,\n             token: token::Underscore,\n@@ -328,7 +420,13 @@ impl<'a> Parser<'a> {\n             directory: Directory { path: PathBuf::new(), ownership: DirectoryOwnership::Owned },\n             root_module_name: None,\n             expected_tokens: Vec::new(),\n-            tts: if tt.len() > 0 { vec![(tt, 0)] } else { Vec::new() },\n+            token_cursor: TokenCursor {\n+                frame: TokenCursorFrame::new(syntax_pos::DUMMY_SP, &Delimited {\n+                    delim: token::NoDelim,\n+                    tts: tokens,\n+                }),\n+                stack: Vec::new(),\n+            },\n             desugar_doc_comments: desugar_doc_comments,\n             cfg_mods: true,\n         };\n@@ -346,28 +444,9 @@ impl<'a> Parser<'a> {\n     }\n \n     fn next_tok(&mut self) -> TokenAndSpan {\n-        loop {\n-            let tok = if let Some((tts, i)) = self.tts.pop() {\n-                let tt = tts.get_tt(i);\n-                if i + 1 < tts.len() {\n-                    self.tts.push((tts, i + 1));\n-                }\n-                if let TokenTree::Token(sp, tok) = tt {\n-                    TokenAndSpan { tok: tok, sp: sp }\n-                } else {\n-                    self.tts.push((tt, 0));\n-                    continue\n-                }\n-            } else {\n-                TokenAndSpan { tok: token::Eof, sp: self.span }\n-            };\n-\n-            match tok.tok {\n-                token::DocComment(name) if self.desugar_doc_comments => {\n-                    self.tts.push((TokenTree::Token(tok.sp, token::DocComment(name)), 0));\n-                }\n-                _ => return tok,\n-            }\n+        match self.desugar_doc_comments {\n+            true => self.token_cursor.next_desugared(),\n+            false => self.token_cursor.next(),\n         }\n     }\n \n@@ -972,19 +1051,16 @@ impl<'a> Parser<'a> {\n         F: FnOnce(&token::Token) -> R,\n     {\n         if dist == 0 {\n-            return f(&self.token);\n-        }\n-        let mut tok = token::Eof;\n-        if let Some(&(ref tts, mut i)) = self.tts.last() {\n-            i += dist - 1;\n-            if i < tts.len() {\n-                tok = match tts.get_tt(i) {\n-                    TokenTree::Token(_, tok) => tok,\n-                    TokenTree::Delimited(_, delimited) => token::OpenDelim(delimited.delim),\n-                };\n-            }\n+            return f(&self.token)\n         }\n-        f(&tok)\n+\n+        f(&match self.token_cursor.frame.tree_cursor.look_ahead(dist - 1) {\n+            Some(tree) => match tree {\n+                TokenTree::Token(_, tok) => tok,\n+                TokenTree::Delimited(_, delimited) => token::OpenDelim(delimited.delim),\n+            },\n+            None => token::CloseDelim(self.token_cursor.frame.delim),\n+        })\n     }\n     pub fn fatal(&self, m: &str) -> DiagnosticBuilder<'a> {\n         self.sess.span_diagnostic.struct_span_fatal(self.span, m)\n@@ -2569,10 +2645,14 @@ impl<'a> Parser<'a> {\n     pub fn parse_token_tree(&mut self) -> PResult<'a, TokenTree> {\n         match self.token {\n             token::OpenDelim(..) => {\n-                let tt = self.tts.pop().unwrap().0;\n-                self.span = tt.span();\n+                let frame = mem::replace(&mut self.token_cursor.frame,\n+                                         self.token_cursor.stack.pop().unwrap());\n+                self.span = frame.span;\n                 self.bump();\n-                return Ok(tt);\n+                return Ok(TokenTree::Delimited(frame.span, Rc::new(Delimited {\n+                    delim: frame.delim,\n+                    tts: frame.tree_cursor.original_stream().trees().collect(),\n+                })));\n             },\n             token::CloseDelim(_) | token::Eof => unreachable!(),\n             _ => Ok(TokenTree::Token(self.span, self.bump_and_get())),"}, {"sha": "083435a04336201542a00469119d5fde4846c3e6", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 46, "deletions": 68, "changes": 114, "blob_url": "https://github.com/rust-lang/rust/blob/8cd0c0885f841c9bfd0c330e3da21363427010e4/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8cd0c0885f841c9bfd0c330e3da21363427010e4/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=8cd0c0885f841c9bfd0c330e3da21363427010e4", "patch": "@@ -22,12 +22,11 @@\n //! and a borrowed TokenStream is sufficient to build an owned TokenStream without taking\n //! ownership of the original.\n \n-use ast::{self, AttrStyle, LitKind};\n+use ast::{self, LitKind};\n use syntax_pos::{BytePos, Span, DUMMY_SP};\n use codemap::Spanned;\n use ext::base;\n use ext::tt::{macro_parser, quoted};\n-use parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n use parse::{self, Directory};\n use parse::token::{self, Token, Lit};\n use print::pprust;\n@@ -103,72 +102,6 @@ pub enum TokenTree {\n }\n \n impl TokenTree {\n-    pub fn len(&self) -> usize {\n-        match *self {\n-            TokenTree::Token(_, token::DocComment(name)) => {\n-                match doc_comment_style(&name.as_str()) {\n-                    AttrStyle::Outer => 2,\n-                    AttrStyle::Inner => 3,\n-                }\n-            }\n-            TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n-                token::NoDelim => delimed.tts.len(),\n-                _ => delimed.tts.len() + 2,\n-            },\n-            TokenTree::Token(..) => 0,\n-        }\n-    }\n-\n-    pub fn get_tt(&self, index: usize) -> TokenTree {\n-        match (self, index) {\n-            (&TokenTree::Token(sp, token::DocComment(_)), 0) => TokenTree::Token(sp, token::Pound),\n-            (&TokenTree::Token(sp, token::DocComment(name)), 1)\n-                if doc_comment_style(&name.as_str()) == AttrStyle::Inner => {\n-                TokenTree::Token(sp, token::Not)\n-            }\n-            (&TokenTree::Token(sp, token::DocComment(name)), _) => {\n-                let stripped = strip_doc_comment_decoration(&name.as_str());\n-\n-                // Searches for the occurrences of `\"#*` and returns the minimum number of `#`s\n-                // required to wrap the text.\n-                let num_of_hashes = stripped.chars()\n-                    .scan(0, |cnt, x| {\n-                        *cnt = if x == '\"' {\n-                            1\n-                        } else if *cnt != 0 && x == '#' {\n-                            *cnt + 1\n-                        } else {\n-                            0\n-                        };\n-                        Some(*cnt)\n-                    })\n-                    .max()\n-                    .unwrap_or(0);\n-\n-                TokenTree::Delimited(sp, Rc::new(Delimited {\n-                    delim: token::Bracket,\n-                    tts: vec![TokenTree::Token(sp, token::Ident(ast::Ident::from_str(\"doc\"))),\n-                              TokenTree::Token(sp, token::Eq),\n-                              TokenTree::Token(sp, token::Literal(\n-                                  token::StrRaw(Symbol::intern(&stripped), num_of_hashes), None))],\n-                }))\n-            }\n-            (&TokenTree::Delimited(_, ref delimed), _) if delimed.delim == token::NoDelim => {\n-                delimed.tts[index].clone()\n-            }\n-            (&TokenTree::Delimited(span, ref delimed), _) => {\n-                if index == 0 {\n-                    return delimed.open_tt(span);\n-                }\n-                if index == delimed.tts.len() + 1 {\n-                    return delimed.close_tt(span);\n-                }\n-                delimed.tts[index - 1].clone()\n-            }\n-            _ => panic!(\"Cannot expand a token tree\"),\n-        }\n-    }\n-\n     /// Use this token tree as a matcher to parse given tts.\n     pub fn parse(cx: &base::ExtCtxt, mtch: &[quoted::TokenTree], tts: &[TokenTree])\n                  -> macro_parser::NamedParseResult {\n@@ -416,6 +349,51 @@ impl Cursor {\n             }\n         })\n     }\n+\n+    pub fn original_stream(self) -> TokenStream {\n+        match self.0 {\n+            CursorKind::Empty => TokenStream::empty(),\n+            CursorKind::Tree(tree, _) => tree.into(),\n+            CursorKind::Stream(cursor) => TokenStream::concat_rc_slice({\n+                cursor.stack.get(0).cloned().map(|(stream, _)| stream).unwrap_or(cursor.stream)\n+            }),\n+        }\n+    }\n+\n+    pub fn look_ahead(&self, n: usize) -> Option<TokenTree> {\n+        fn look_ahead(streams: &[TokenStream], mut n: usize) -> Result<TokenTree, usize> {\n+            for stream in streams {\n+                n = match stream.kind {\n+                    TokenStreamKind::Tree(ref tree) if n == 0 => return Ok(tree.clone()),\n+                    TokenStreamKind::Tree(..) => n - 1,\n+                    TokenStreamKind::Stream(ref stream) => match look_ahead(stream, n) {\n+                        Ok(tree) => return Ok(tree),\n+                        Err(n) => n,\n+                    },\n+                    _ => n,\n+                };\n+            }\n+\n+            Err(n)\n+        }\n+\n+        match self.0 {\n+            CursorKind::Empty | CursorKind::Tree(_, true) => Err(n),\n+            CursorKind::Tree(ref tree, false) => look_ahead(&[tree.clone().into()], n),\n+            CursorKind::Stream(ref cursor) => {\n+                look_ahead(&cursor.stream[cursor.index ..], n).or_else(|mut n| {\n+                    for &(ref stream, index) in cursor.stack.iter().rev() {\n+                        n = match look_ahead(&stream[index..], n) {\n+                            Ok(tree) => return Ok(tree),\n+                            Err(n) => n,\n+                        }\n+                    }\n+\n+                    Err(n)\n+                })\n+            }\n+        }.ok()\n+    }\n }\n \n impl fmt::Display for TokenStream {"}]}