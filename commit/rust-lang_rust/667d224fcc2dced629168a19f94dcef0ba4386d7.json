{"sha": "667d224fcc2dced629168a19f94dcef0ba4386d7", "node_id": "MDY6Q29tbWl0NzI0NzEyOjY2N2QyMjRmY2MyZGNlZDYyOTE2OGExOWY5NGRjZWYwYmE0Mzg2ZDc=", "commit": {"author": {"name": "Veetaha", "email": "veetaha2@gmail.com", "date": "2020-06-14T09:48:16Z"}, "committer": {"name": "Veetaha", "email": "veetaha2@gmail.com", "date": "2020-06-14T10:12:52Z"}, "message": "Reduce the usage of bare subscript operator", "tree": {"sha": "00eb2752abef5e67ac95a56273d5d27caf0fb747", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/00eb2752abef5e67ac95a56273d5d27caf0fb747"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/667d224fcc2dced629168a19f94dcef0ba4386d7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/667d224fcc2dced629168a19f94dcef0ba4386d7", "html_url": "https://github.com/rust-lang/rust/commit/667d224fcc2dced629168a19f94dcef0ba4386d7", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/667d224fcc2dced629168a19f94dcef0ba4386d7/comments", "author": {"login": "Veetaha", "id": 36276403, "node_id": "MDQ6VXNlcjM2Mjc2NDAz", "avatar_url": "https://avatars.githubusercontent.com/u/36276403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Veetaha", "html_url": "https://github.com/Veetaha", "followers_url": "https://api.github.com/users/Veetaha/followers", "following_url": "https://api.github.com/users/Veetaha/following{/other_user}", "gists_url": "https://api.github.com/users/Veetaha/gists{/gist_id}", "starred_url": "https://api.github.com/users/Veetaha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Veetaha/subscriptions", "organizations_url": "https://api.github.com/users/Veetaha/orgs", "repos_url": "https://api.github.com/users/Veetaha/repos", "events_url": "https://api.github.com/users/Veetaha/events{/privacy}", "received_events_url": "https://api.github.com/users/Veetaha/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Veetaha", "id": 36276403, "node_id": "MDQ6VXNlcjM2Mjc2NDAz", "avatar_url": "https://avatars.githubusercontent.com/u/36276403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Veetaha", "html_url": "https://github.com/Veetaha", "followers_url": "https://api.github.com/users/Veetaha/followers", "following_url": "https://api.github.com/users/Veetaha/following{/other_user}", "gists_url": "https://api.github.com/users/Veetaha/gists{/gist_id}", "starred_url": "https://api.github.com/users/Veetaha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Veetaha/subscriptions", "organizations_url": "https://api.github.com/users/Veetaha/orgs", "repos_url": "https://api.github.com/users/Veetaha/repos", "events_url": "https://api.github.com/users/Veetaha/events{/privacy}", "received_events_url": "https://api.github.com/users/Veetaha/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "246c66a7f7fd3f85d7d6e47a36f17bafb0ccb08a", "url": "https://api.github.com/repos/rust-lang/rust/commits/246c66a7f7fd3f85d7d6e47a36f17bafb0ccb08a", "html_url": "https://github.com/rust-lang/rust/commit/246c66a7f7fd3f85d7d6e47a36f17bafb0ccb08a"}], "stats": {"total": 89, "additions": 43, "deletions": 46}, "files": [{"sha": "97aa3e7951ca270ba460e06b4d0d5959a00e8a63", "filename": "crates/ra_syntax/src/parsing/text_token_source.rs", "status": "modified", "additions": 43, "deletions": 46, "changes": 89, "blob_url": "https://github.com/rust-lang/rust/blob/667d224fcc2dced629168a19f94dcef0ba4386d7/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_token_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/667d224fcc2dced629168a19f94dcef0ba4386d7/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_token_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_token_source.rs?ref=667d224fcc2dced629168a19f94dcef0ba4386d7", "patch": "@@ -1,40 +1,35 @@\n-//! FIXME: write short doc here\n+//! See `TextTokenSource` docs.\n \n-use ra_parser::Token as PToken;\n use ra_parser::TokenSource;\n \n use crate::{parsing::lexer::Token, SyntaxKind::EOF, TextRange, TextSize};\n \n+/// Implementation of `ra_parser::TokenSource` that takes tokens from source code text.\n pub(crate) struct TextTokenSource<'t> {\n     text: &'t str,\n-    /// start position of each token(expect whitespace and comment)\n+    /// token and its start position (non-whitespace/comment tokens)\n     /// ```non-rust\n     ///  struct Foo;\n-    /// ^------^---\n-    /// |      |  ^-\n-    /// 0      7  10\n+    ///  ^------^--^-\n+    ///  |      |    \\________\n+    ///  |      \\____         \\\n+    ///  |           \\         |\n+    ///  (struct, 0) (Foo, 7) (;, 10)\n     /// ```\n-    /// (token, start_offset): `[(struct, 0), (Foo, 7), (;, 10)]`\n-    start_offsets: Vec<TextSize>,\n-    /// non-whitespace/comment tokens\n-    /// ```non-rust\n-    /// struct Foo {}\n-    /// ^^^^^^ ^^^ ^^\n-    /// ```\n-    /// tokens: `[struct, Foo, {, }]`\n-    tokens: Vec<Token>,\n+    /// `[(struct, 0), (Foo, 7), (;, 10)]`\n+    token_offset_pairs: Vec<(Token, TextSize)>,\n \n     /// Current token and position\n-    curr: (PToken, usize),\n+    curr: (ra_parser::Token, usize),\n }\n \n impl<'t> TokenSource for TextTokenSource<'t> {\n-    fn current(&self) -> PToken {\n+    fn current(&self) -> ra_parser::Token {\n         self.curr.0\n     }\n \n-    fn lookahead_nth(&self, n: usize) -> PToken {\n-        mk_token(self.curr.1 + n, &self.start_offsets, &self.tokens)\n+    fn lookahead_nth(&self, n: usize) -> ra_parser::Token {\n+        mk_token(self.curr.1 + n, &self.token_offset_pairs)\n     }\n \n     fn bump(&mut self) {\n@@ -43,45 +38,47 @@ impl<'t> TokenSource for TextTokenSource<'t> {\n         }\n \n         let pos = self.curr.1 + 1;\n-        self.curr = (mk_token(pos, &self.start_offsets, &self.tokens), pos);\n+        self.curr = (mk_token(pos, &self.token_offset_pairs), pos);\n     }\n \n     fn is_keyword(&self, kw: &str) -> bool {\n-        let pos = self.curr.1;\n-        if pos >= self.tokens.len() {\n-            return false;\n-        }\n-        let range = TextRange::at(self.start_offsets[pos], self.tokens[pos].len);\n-        self.text[range] == *kw\n+        self.token_offset_pairs\n+            .get(self.curr.1)\n+            .map(|(token, offset)| &self.text[TextRange::at(*offset, token.len)] == kw)\n+            .unwrap_or(false)\n     }\n }\n \n-fn mk_token(pos: usize, start_offsets: &[TextSize], tokens: &[Token]) -> PToken {\n-    let kind = tokens.get(pos).map(|t| t.kind).unwrap_or(EOF);\n-    let is_jointed_to_next = if pos + 1 < start_offsets.len() {\n-        start_offsets[pos] + tokens[pos].len == start_offsets[pos + 1]\n-    } else {\n-        false\n+fn mk_token(pos: usize, token_offset_pairs: &[(Token, TextSize)]) -> ra_parser::Token {\n+    let (kind, is_jointed_to_next) = match token_offset_pairs.get(pos) {\n+        Some((token, offset)) => (\n+            token.kind,\n+            token_offset_pairs\n+                .get(pos + 1)\n+                .map(|(_, next_offset)| offset + token.len == *next_offset)\n+                .unwrap_or(false),\n+        ),\n+        None => (EOF, false),\n     };\n-\n-    PToken { kind, is_jointed_to_next }\n+    ra_parser::Token { kind, is_jointed_to_next }\n }\n \n impl<'t> TextTokenSource<'t> {\n     /// Generate input from tokens(expect comment and whitespace).\n     pub fn new(text: &'t str, raw_tokens: &'t [Token]) -> TextTokenSource<'t> {\n-        let mut tokens = Vec::new();\n-        let mut start_offsets = Vec::new();\n-        let mut len = 0.into();\n-        for &token in raw_tokens.iter() {\n-            if !token.kind.is_trivia() {\n-                tokens.push(token);\n-                start_offsets.push(len);\n-            }\n-            len += token.len;\n-        }\n+        let token_offset_pairs: Vec<_> = raw_tokens\n+            .iter()\n+            .filter_map({\n+                let mut len = 0.into();\n+                move |token| {\n+                    let pair = if token.kind.is_trivia() { None } else { Some((*token, len)) };\n+                    len += token.len;\n+                    pair\n+                }\n+            })\n+            .collect();\n \n-        let first = mk_token(0, &start_offsets, &tokens);\n-        TextTokenSource { text, start_offsets, tokens, curr: (first, 0) }\n+        let first = mk_token(0, &token_offset_pairs);\n+        TextTokenSource { text, token_offset_pairs, curr: (first, 0) }\n     }\n }"}]}