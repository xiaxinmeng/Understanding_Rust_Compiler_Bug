{"sha": "a55c57284d8341ee5b22c5372e77ac0af9479dc5", "node_id": "MDY6Q29tbWl0NzI0NzEyOmE1NWM1NzI4NGQ4MzQxZWU1YjIyYzUzNzJlNzdhYzBhZjk0NzlkYzU=", "commit": {"author": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2013-12-13T01:27:37Z"}, "committer": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2013-12-24T22:42:00Z"}, "message": "std: Introduce std::sync\n\nFor now, this moves the following modules to std::sync\n\n* UnsafeArc (also removed unwrap method)\n* mpsc_queue\n* spsc_queue\n* atomics\n* mpmc_bounded_queue\n* deque\n\nWe may want to remove some of the queues, but for now this moves things out of\nstd::rt into std::sync", "tree": {"sha": "73d06098fa02bf822a6709761f67342e8ba1fe6a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/73d06098fa02bf822a6709761f67342e8ba1fe6a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a55c57284d8341ee5b22c5372e77ac0af9479dc5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a55c57284d8341ee5b22c5372e77ac0af9479dc5", "html_url": "https://github.com/rust-lang/rust/commit/a55c57284d8341ee5b22c5372e77ac0af9479dc5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a55c57284d8341ee5b22c5372e77ac0af9479dc5/comments", "author": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "committer": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "dafb310ba131b34a8819566201dc8d0af9bbd406", "url": "https://api.github.com/repos/rust-lang/rust/commits/dafb310ba131b34a8819566201dc8d0af9bbd406", "html_url": "https://github.com/rust-lang/rust/commit/dafb310ba131b34a8819566201dc8d0af9bbd406"}], "stats": {"total": 866, "additions": 288, "deletions": 578}, "files": [{"sha": "a411c4e9185b10d489161a6f124627870b5cd95b", "filename": "src/libextra/arc.rs", "status": "modified", "additions": 1, "deletions": 64, "changes": 65, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibextra%2Farc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibextra%2Farc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Farc.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -45,7 +45,7 @@ use sync;\n use sync::{Mutex, RWLock};\n \n use std::cast;\n-use std::unstable::sync::UnsafeArc;\n+use std::sync::arc::UnsafeArc;\n use std::task;\n use std::borrow;\n \n@@ -127,20 +127,6 @@ impl<T:Freeze+Send> Arc<T> {\n     pub fn get<'a>(&'a self) -> &'a T {\n         unsafe { &*self.x.get_immut() }\n     }\n-\n-    /**\n-     * Retrieve the data back out of the Arc. This function blocks until the\n-     * reference given to it is the last existing one, and then unwrap the data\n-     * instead of destroying it.\n-     *\n-     * If multiple tasks call unwrap, all but the first will fail. Do not call\n-     * unwrap from a task that holds another reference to the same Arc; it is\n-     * guaranteed to deadlock.\n-     */\n-    pub fn unwrap(self) -> T {\n-        let Arc { x: x } = self;\n-        x.unwrap()\n-    }\n }\n \n impl<T:Freeze + Send> Clone for Arc<T> {\n@@ -247,22 +233,6 @@ impl<T:Send> MutexArc<T> {\n                           cond: cond })\n         })\n     }\n-\n-    /**\n-     * Retrieves the data, blocking until all other references are dropped,\n-     * exactly as arc::unwrap.\n-     *\n-     * Will additionally fail if another task has failed while accessing the arc.\n-     */\n-    pub fn unwrap(self) -> T {\n-        let MutexArc { x: x } = self;\n-        let inner = x.unwrap();\n-        let MutexArcInner { failed: failed, data: data, .. } = inner;\n-        if failed {\n-            fail!(\"Can't unwrap poisoned MutexArc - another task failed inside!\");\n-        }\n-        data\n-    }\n }\n \n impl<T:Freeze + Send> MutexArc<T> {\n@@ -503,23 +473,6 @@ impl<T:Freeze + Send> RWArc<T> {\n             }\n         }\n     }\n-\n-    /**\n-     * Retrieves the data, blocking until all other references are dropped,\n-     * exactly as arc::unwrap.\n-     *\n-     * Will additionally fail if another task has failed while accessing the arc\n-     * in write mode.\n-     */\n-    pub fn unwrap(self) -> T {\n-        let RWArc { x: x, .. } = self;\n-        let inner = x.unwrap();\n-        let RWArcInner { failed: failed, data: data, .. } = inner;\n-        if failed {\n-            fail!(\"Can't unwrap poisoned RWArc - another task failed inside!\")\n-        }\n-        data\n-    }\n }\n \n // Borrowck rightly complains about immutably aliasing the rwlock in order to\n@@ -689,22 +642,6 @@ mod tests {\n         })\n     }\n \n-    #[test] #[should_fail]\n-    pub fn test_mutex_arc_unwrap_poison() {\n-        let arc = MutexArc::new(1);\n-        let arc2 = ~(&arc).clone();\n-        let (p, c) = Chan::new();\n-        do task::spawn {\n-            arc2.access(|one| {\n-                c.send(());\n-                assert!(*one == 2);\n-            })\n-        }\n-        let _ = p.recv();\n-        let one = arc.unwrap();\n-        assert!(one == 1);\n-    }\n-\n     #[test]\n     fn test_unsafe_mutex_arc_nested() {\n         unsafe {"}, {"sha": "fb11eb6a3c46eb6226a8c65b95e3338e08214622", "filename": "src/libextra/sync.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibextra%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibextra%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fsync.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -19,8 +19,9 @@\n \n \n use std::borrow;\n-use std::unstable::sync::{Exclusive, UnsafeArc};\n-use std::unstable::atomics;\n+use std::unstable::sync::Exclusive;\n+use std::sync::arc::UnsafeArc;\n+use std::sync::atomics;\n use std::unstable::finally::Finally;\n use std::util;\n use std::util::NonCopyable;"}, {"sha": "61fadb7e2365476d1722511808cfff3cd7d16ee8", "filename": "src/librustc/middle/trans/debuginfo.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibrustc%2Fmiddle%2Ftrans%2Fdebuginfo.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibrustc%2Fmiddle%2Ftrans%2Fdebuginfo.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fdebuginfo.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -146,7 +146,7 @@ use std::hashmap::HashMap;\n use std::hashmap::HashSet;\n use std::libc::{c_uint, c_ulonglong, c_longlong};\n use std::ptr;\n-use std::unstable::atomics;\n+use std::sync::atomics;\n use std::vec;\n use syntax::codemap::{Span, Pos};\n use syntax::{ast, codemap, ast_util, ast_map, opt_vec};"}, {"sha": "200e4e632616c5e7f9b72f8531483e9588a2457a", "filename": "src/libstd/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Flib.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -159,6 +159,7 @@ pub mod trie;\n pub mod task;\n pub mod comm;\n pub mod local_data;\n+pub mod sync;\n \n \n /* Runtime and platform support */"}, {"sha": "7632ec6cf2999fd689b5b407fd890bc93fe28b5b", "filename": "src/libstd/sync/arc.rs", "status": "added", "additions": 153, "deletions": 0, "changes": 153, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Farc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Farc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Farc.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -0,0 +1,153 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! Atomically reference counted data\n+//!\n+//! This modules contains the implementation of an atomically reference counted\n+//! pointer for the purpose of sharing data between tasks. This is obviously a\n+//! very unsafe primitive to use, but it has its use cases when implementing\n+//! concurrent data structures and similar tasks.\n+//!\n+//! Great care must be taken to ensure that data races do not arise through the\n+//! usage of `UnsafeArc`, and this often requires some form of external\n+//! synchronization. The only guarantee provided to you by this class is that\n+//! the underlying data will remain valid (not free'd) so long as the reference\n+//! count is greater than one.\n+\n+use cast;\n+use clone::Clone;\n+use kinds::Send;\n+use ops::Drop;\n+use ptr::RawPtr;\n+use sync::atomics::{AtomicUint, SeqCst, Relaxed, Acquire};\n+use vec;\n+\n+/// An atomically reference counted pointer.\n+///\n+/// Enforces no shared-memory safety.\n+#[unsafe_no_drop_flag]\n+pub struct UnsafeArc<T> {\n+    priv data: *mut ArcData<T>,\n+}\n+\n+struct ArcData<T> {\n+    count: AtomicUint,\n+    data: T,\n+}\n+\n+unsafe fn new_inner<T: Send>(data: T, refcount: uint) -> *mut ArcData<T> {\n+    let data = ~ArcData { count: AtomicUint::new(refcount), data: data };\n+    cast::transmute(data)\n+}\n+\n+impl<T: Send> UnsafeArc<T> {\n+    /// Creates a new `UnsafeArc` which wraps the given data.\n+    pub fn new(data: T) -> UnsafeArc<T> {\n+        unsafe { UnsafeArc { data: new_inner(data, 1) } }\n+    }\n+\n+    /// As new(), but returns an extra pre-cloned handle.\n+    pub fn new2(data: T) -> (UnsafeArc<T>, UnsafeArc<T>) {\n+        unsafe {\n+            let ptr = new_inner(data, 2);\n+            (UnsafeArc { data: ptr }, UnsafeArc { data: ptr })\n+        }\n+    }\n+\n+    /// As new(), but returns a vector of as many pre-cloned handles as\n+    /// requested.\n+    pub fn newN(data: T, num_handles: uint) -> ~[UnsafeArc<T>] {\n+        unsafe {\n+            if num_handles == 0 {\n+                ~[] // need to free data here\n+            } else {\n+                let ptr = new_inner(data, num_handles);\n+                vec::from_fn(num_handles, |_| UnsafeArc { data: ptr })\n+            }\n+        }\n+    }\n+\n+    /// Gets a pointer to the inner shared data. Note that care must be taken to\n+    /// ensure that the outer `UnsafeArc` does not fall out of scope while this\n+    /// pointer is in use, otherwise it could possibly contain a use-after-free.\n+    #[inline]\n+    pub fn get(&self) -> *mut T {\n+        unsafe {\n+            assert!((*self.data).count.load(Relaxed) > 0);\n+            return &mut (*self.data).data as *mut T;\n+        }\n+    }\n+\n+    /// Gets an immutable pointer to the inner shared data. This has the same\n+    /// caveats as the `get` method.\n+    #[inline]\n+    pub fn get_immut(&self) -> *T {\n+        unsafe {\n+            assert!((*self.data).count.load(Relaxed) > 0);\n+            return &(*self.data).data as *T;\n+        }\n+    }\n+}\n+\n+impl<T: Send> Clone for UnsafeArc<T> {\n+    fn clone(&self) -> UnsafeArc<T> {\n+        unsafe {\n+            // This barrier might be unnecessary, but I'm not sure...\n+            let old_count = (*self.data).count.fetch_add(1, Acquire);\n+            assert!(old_count >= 1);\n+            return UnsafeArc { data: self.data };\n+        }\n+    }\n+}\n+\n+#[unsafe_destructor]\n+impl<T> Drop for UnsafeArc<T>{\n+    fn drop(&mut self) {\n+        unsafe {\n+            // Happens when destructing an unwrapper's handle and from\n+            // `#[unsafe_no_drop_flag]`\n+            if self.data.is_null() {\n+                return\n+            }\n+            // Must be acquire+release, not just release, to make sure this\n+            // doesn't get reordered to after the unwrapper pointer load.\n+            let old_count = (*self.data).count.fetch_sub(1, SeqCst);\n+            assert!(old_count >= 1);\n+            if old_count == 1 {\n+                let _: ~ArcData<T> = cast::transmute(self.data);\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use prelude::*;\n+    use super::UnsafeArc;\n+    use task;\n+    use mem::size_of;\n+\n+    #[test]\n+    fn test_size() {\n+        assert_eq!(size_of::<UnsafeArc<[int, ..10]>>(), size_of::<*[int, ..10]>());\n+    }\n+\n+    #[test]\n+    fn arclike_newN() {\n+        // Tests that the many-refcounts-at-once constructors don't leak.\n+        let _ = UnsafeArc::new2(~~\"hello\");\n+        let x = UnsafeArc::newN(~~\"hello\", 0);\n+        assert_eq!(x.len(), 0)\n+        let x = UnsafeArc::newN(~~\"hello\", 1);\n+        assert_eq!(x.len(), 1)\n+        let x = UnsafeArc::newN(~~\"hello\", 10);\n+        assert_eq!(x.len(), 10)\n+    }\n+}"}, {"sha": "bc9d99c0f37d7f85b3cad745aa085f447a2cc95a", "filename": "src/libstd/sync/atomics.rs", "status": "renamed", "additions": 6, "deletions": 3, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fatomics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fatomics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fatomics.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -11,13 +11,16 @@\n /*!\n  * Atomic types\n  *\n- * Basic atomic types supporting atomic operations. Each method takes an `Ordering` which\n- * represents the strength of the memory barrier for that operation. These orderings are the same\n- * as C++11 atomic orderings [http://gcc.gnu.org/wiki/Atomic/GCCMM/AtomicSync]\n+ * Basic atomic types supporting atomic operations. Each method takes an\n+ * `Ordering` which represents the strength of the memory barrier for that\n+ * operation. These orderings are the same as C++11 atomic orderings\n+ * [http://gcc.gnu.org/wiki/Atomic/GCCMM/AtomicSync]\n  *\n  * All atomic types are a single word in size.\n  */\n \n+#[allow(missing_doc)];\n+\n use unstable::intrinsics;\n use cast;\n use option::{Option,Some,None};", "previous_filename": "src/libstd/unstable/atomics.rs"}, {"sha": "4d0efcd6ee10ada30cffce16e1300cb34bfa1651", "filename": "src/libstd/sync/deque.rs", "status": "renamed", "additions": 8, "deletions": 5, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fdeque.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fdeque.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fdeque.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -50,15 +50,18 @@\n \n use cast;\n use clone::Clone;\n-use iter::range;\n+use iter::{range, Iterator};\n use kinds::Send;\n use libc;\n use mem;\n use ops::Drop;\n use option::{Option, Some, None};\n use ptr;\n-use unstable::atomics::{AtomicInt, AtomicPtr, SeqCst};\n-use unstable::sync::{UnsafeArc, Exclusive};\n+use ptr::RawPtr;\n+use sync::arc::UnsafeArc;\n+use sync::atomics::{AtomicInt, AtomicPtr, SeqCst};\n+use unstable::sync::Exclusive;\n+use vec::{OwnedVector, ImmutableVector};\n \n // Once the queue is less than 1/K full, then it will be downsized. Note that\n // the deque requires that this number be less than 2.\n@@ -399,8 +402,8 @@ mod tests {\n     use rt::thread::Thread;\n     use rand;\n     use rand::Rng;\n-    use unstable::atomics::{AtomicBool, INIT_ATOMIC_BOOL, SeqCst,\n-                            AtomicUint, INIT_ATOMIC_UINT};\n+    use sync::atomics::{AtomicBool, INIT_ATOMIC_BOOL, SeqCst,\n+                        AtomicUint, INIT_ATOMIC_UINT};\n     use vec;\n \n     #[test]", "previous_filename": "src/libstd/rt/deque.rs"}, {"sha": "3213c538152c676ce8bbfd659e6205b5662a6442", "filename": "src/libstd/sync/mod.rs", "status": "added", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fmod.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -0,0 +1,23 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! Useful synchronization primitives\n+//!\n+//! This modules contains useful safe and unsafe synchronization primitives.\n+//! Most of the primitives in this module do not provide any sort of locking\n+//! and/or blocking at all, but rather provide the necessary tools to build\n+//! other types of concurrent primitives.\n+\n+pub mod arc;\n+pub mod atomics;\n+pub mod deque;\n+pub mod mpmc_bounded_queue;\n+pub mod mpsc_queue;\n+pub mod spsc_queue;"}, {"sha": "b623976306d846ebfb3eac0b77eb250f1a182afd", "filename": "src/libstd/sync/mpmc_bounded_queue.rs", "status": "renamed", "additions": 6, "deletions": 4, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fmpmc_bounded_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fmpmc_bounded_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fmpmc_bounded_queue.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -25,15 +25,17 @@\n  * policies, either expressed or implied, of Dmitry Vyukov.\n  */\n \n+#[allow(missing_doc, dead_code)];\n+\n // http://www.1024cores.net/home/lock-free-algorithms/queues/bounded-mpmc-queue\n \n-use unstable::sync::UnsafeArc;\n-use unstable::atomics::{AtomicUint,Relaxed,Release,Acquire};\n-use option::*;\n-use vec;\n use clone::Clone;\n use kinds::Send;\n use num::{Exponential,Algebraic,Round};\n+use option::{Option, Some, None};\n+use sync::arc::UnsafeArc;\n+use sync::atomics::{AtomicUint,Relaxed,Release,Acquire};\n+use vec;\n \n struct Node<T> {\n     sequence: AtomicUint,", "previous_filename": "src/libstd/rt/mpmc_bounded_queue.rs"}, {"sha": "89e56e3fa67c02b549aed946965e9d57235b1f8c", "filename": "src/libstd/sync/mpsc_queue.rs", "status": "renamed", "additions": 40, "deletions": 10, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fmpsc_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fmpsc_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fmpsc_queue.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -26,6 +26,14 @@\n  */\n \n //! A mostly lock-free multi-producer, single consumer queue.\n+//!\n+//! This module contains an implementation of a concurrent MPSC queue. This\n+//! queue can be used to share data between tasks, and is also used as the\n+//! building block of channels in rust.\n+//!\n+//! Note that the current implementation of this queue has a caveat of the `pop`\n+//! method, and see the method for more information about it. Due to this\n+//! caveat, this queue may not be appropriate for all use-cases.\n \n // http://www.1024cores.net/home/lock-free-algorithms\n //                         /queues/non-intrusive-mpsc-node-based-queue\n@@ -35,9 +43,11 @@ use clone::Clone;\n use kinds::Send;\n use ops::Drop;\n use option::{Option, None, Some};\n-use unstable::atomics::{AtomicPtr, Release, Acquire, AcqRel, Relaxed};\n-use unstable::sync::UnsafeArc;\n+use ptr::RawPtr;\n+use sync::arc::UnsafeArc;\n+use sync::atomics::{AtomicPtr, Release, Acquire, AcqRel, Relaxed};\n \n+/// A result of the `pop` function.\n pub enum PopResult<T> {\n     /// Some data has been popped\n     Data(T),\n@@ -61,10 +71,14 @@ struct State<T, P> {\n     packet: P,\n }\n \n+/// The consumer half of this concurrent queue. This half is used to receive\n+/// data from the producers.\n pub struct Consumer<T, P> {\n     priv state: UnsafeArc<State<T, P>>,\n }\n \n+/// The production half of the concurrent queue. This handle may be cloned in\n+/// order to make handles for new producers.\n pub struct Producer<T, P> {\n     priv state: UnsafeArc<State<T, P>>,\n }\n@@ -75,6 +89,11 @@ impl<T: Send, P: Send> Clone for Producer<T, P> {\n     }\n }\n \n+/// Creates a new MPSC queue. The given argument `p` is a user-defined \"packet\"\n+/// of information which will be shared by the consumer and the producer which\n+/// can be re-acquired via the `packet` function. This is helpful when extra\n+/// state is shared between the producer and consumer, but note that there is no\n+/// synchronization performed of this data.\n pub fn queue<T: Send, P: Send>(p: P) -> (Consumer<T, P>, Producer<T, P>) {\n     unsafe {\n         let (a, b) = UnsafeArc::new2(State::new(p));\n@@ -92,7 +111,7 @@ impl<T> Node<T> {\n }\n \n impl<T: Send, P: Send> State<T, P> {\n-    pub unsafe fn new(p: P) -> State<T, P> {\n+    unsafe fn new(p: P) -> State<T, P> {\n         let stub = Node::new(None);\n         State {\n             head: AtomicPtr::new(stub),\n@@ -122,10 +141,6 @@ impl<T: Send, P: Send> State<T, P> {\n \n         if self.head.load(Acquire) == tail {Empty} else {Inconsistent}\n     }\n-\n-    unsafe fn is_empty(&mut self) -> bool {\n-        return (*self.tail).next.load(Acquire).is_null();\n-    }\n }\n \n #[unsafe_destructor]\n@@ -143,27 +158,42 @@ impl<T: Send, P: Send> Drop for State<T, P> {\n }\n \n impl<T: Send, P: Send> Producer<T, P> {\n+    /// Pushes a new value onto this queue.\n     pub fn push(&mut self, value: T) {\n         unsafe { (*self.state.get()).push(value) }\n     }\n-    pub fn is_empty(&self) -> bool {\n-        unsafe{ (*self.state.get()).is_empty() }\n-    }\n+    /// Gets an unsafe pointer to the user-defined packet shared by the\n+    /// producers and the consumer. Note that care must be taken to ensure that\n+    /// the lifetime of the queue outlives the usage of the returned pointer.\n     pub unsafe fn packet(&self) -> *mut P {\n         &mut (*self.state.get()).packet as *mut P\n     }\n }\n \n impl<T: Send, P: Send> Consumer<T, P> {\n+    /// Pops some data from this queue.\n+    ///\n+    /// Note that the current implementation means that this function cannot\n+    /// return `Option<T>`. It is possible for this queue to be in an\n+    /// inconsistent state where many pushes have suceeded and completely\n+    /// finished, but pops cannot return `Some(t)`. This inconsistent state\n+    /// happens when a pusher is pre-empted at an inopportune moment.\n+    ///\n+    /// This inconsistent state means that this queue does indeed have data, but\n+    /// it does not currently have access to it at this time.\n     pub fn pop(&mut self) -> PopResult<T> {\n         unsafe { (*self.state.get()).pop() }\n     }\n+    /// Attempts to pop data from this queue, but doesn't attempt too hard. This\n+    /// will canonicalize inconsistent states to a `None` value.\n     pub fn casual_pop(&mut self) -> Option<T> {\n         match self.pop() {\n             Data(t) => Some(t),\n             Empty | Inconsistent => None,\n         }\n     }\n+    /// Gets an unsafe pointer to the underlying user-defined packet. See\n+    /// `Producer.packet` for more information.\n     pub unsafe fn packet(&self) -> *mut P {\n         &mut (*self.state.get()).packet as *mut P\n     }", "previous_filename": "src/libstd/rt/mpsc_queue.rs"}, {"sha": "c4abba04659c01a5da63c70ef26190b7ed3c40a9", "filename": "src/libstd/sync/spsc_queue.rs", "status": "renamed", "additions": 40, "deletions": 2, "changes": 42, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fspsc_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Fsync%2Fspsc_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fspsc_queue.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -26,12 +26,20 @@\n  */\n \n // http://www.1024cores.net/home/lock-free-algorithms/queues/unbounded-spsc-queue\n+\n+//! A single-producer single-consumer concurrent queue\n+//!\n+//! This module contains the implementation of an SPSC queue which can be used\n+//! concurrently between two tasks. This data structure is safe to use and\n+//! enforces the semantics that there is one pusher and one popper.\n+\n use cast;\n use kinds::Send;\n use ops::Drop;\n use option::{Some, None, Option};\n-use unstable::atomics::{AtomicPtr, Relaxed, AtomicUint, Acquire, Release};\n-use unstable::sync::UnsafeArc;\n+use ptr::RawPtr;\n+use sync::arc::UnsafeArc;\n+use sync::atomics::{AtomicPtr, Relaxed, AtomicUint, Acquire, Release};\n \n // Node within the linked list queue of messages to send\n struct Node<T> {\n@@ -64,14 +72,34 @@ struct State<T, P> {\n     packet: P,\n }\n \n+/// Producer half of this queue. This handle is used to push data to the\n+/// consumer.\n pub struct Producer<T, P> {\n     priv state: UnsafeArc<State<T, P>>,\n }\n \n+/// Consumer half of this queue. This handle is used to receive data from the\n+/// producer.\n pub struct Consumer<T, P> {\n     priv state: UnsafeArc<State<T, P>>,\n }\n \n+/// Creates a new queue. The producer returned is connected to the consumer to\n+/// push all data to the consumer.\n+///\n+/// # Arguments\n+///\n+///   * `bound` - This queue implementation is implemented with a linked list,\n+///               and this means that a push is always a malloc. In order to\n+///               amortize this cost, an internal cache of nodes is maintained\n+///               to prevent a malloc from always being necessary. This bound is\n+///               the limit on the size of the cache (if desired). If the value\n+///               is 0, then the cache has no bound. Otherwise, the cache will\n+///               never grow larger than `bound` (although the queue itself\n+///               could be much larger.\n+///\n+///   * `p` - This is the user-defined packet of data which will also be shared\n+///           between the producer and consumer.\n pub fn queue<T: Send, P: Send>(bound: uint,\n                                p: P) -> (Consumer<T, P>, Producer<T, P>)\n {\n@@ -105,21 +133,31 @@ impl<T: Send> Node<T> {\n }\n \n impl<T: Send, P: Send> Producer<T, P> {\n+    /// Pushes data onto the queue\n     pub fn push(&mut self, t: T) {\n         unsafe { (*self.state.get()).push(t) }\n     }\n+    /// Tests whether the queue is empty. Note that if this function returns\n+    /// `false`, the return value is significant, but if the return value is\n+    /// `true` then almost no meaning can be attached to the return value.\n     pub fn is_empty(&self) -> bool {\n         unsafe { (*self.state.get()).is_empty() }\n     }\n+    /// Acquires an unsafe pointer to the underlying user-defined packet. Note\n+    /// that care must be taken to ensure that the queue outlives the usage of\n+    /// the packet (because it is an unsafe pointer).\n     pub unsafe fn packet(&self) -> *mut P {\n         &mut (*self.state.get()).packet as *mut P\n     }\n }\n \n impl<T: Send, P: Send> Consumer<T, P> {\n+    /// Pops some data from this queue, returning `None` when the queue is\n+    /// empty.\n     pub fn pop(&mut self) -> Option<T> {\n         unsafe { (*self.state.get()).pop() }\n     }\n+    /// Same function as the producer's `packet` method.\n     pub unsafe fn packet(&self) -> *mut P {\n         &mut (*self.state.get()).packet as *mut P\n     }", "previous_filename": "src/libstd/rt/spsc_queue.rs"}, {"sha": "0569fe32c58b37f1a781db84ca2b125b98b7ae6b", "filename": "src/libstd/unstable/dynamic_lib.rs", "status": "modified", "additions": 1, "deletions": 8, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Funstable%2Fdynamic_lib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Funstable%2Fdynamic_lib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Fdynamic_lib.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -140,7 +140,6 @@ pub mod dl {\n     use path;\n     use ptr;\n     use str;\n-    use unstable::sync::atomic;\n     use result::*;\n \n     pub unsafe fn open_external(filename: &path::Path) -> *libc::c_void {\n@@ -158,11 +157,7 @@ pub mod dl {\n         static mut lock: Mutex = MUTEX_INIT;\n         unsafe {\n             // dlerror isn't thread safe, so we need to lock around this entire\n-            // sequence. `atomic` asserts that we don't do anything that\n-            // would cause this task to be descheduled, which could deadlock\n-            // the scheduler if it happens while the lock is held.\n-            // FIXME #9105 use a Rust mutex instead of C++ mutexes.\n-            let _guard = atomic();\n+            // sequence\n             lock.lock();\n             let _old_error = dlerror();\n \n@@ -208,7 +203,6 @@ pub mod dl {\n     use libc;\n     use path;\n     use ptr;\n-    use unstable::sync::atomic;\n     use result::*;\n \n     pub unsafe fn open_external(filename: &path::Path) -> *libc::c_void {\n@@ -225,7 +219,6 @@ pub mod dl {\n \n     pub fn check_for_errors_in<T>(f: || -> T) -> Result<T, ~str> {\n         unsafe {\n-            let _guard = atomic();\n             SetLastError(0);\n \n             let result = f();"}, {"sha": "f70d0b5169fffc38714169a7c29d2f8543282e9f", "filename": "src/libstd/unstable/mod.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Funstable%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Funstable%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Fmod.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -22,7 +22,6 @@ pub mod simd;\n pub mod lang;\n pub mod sync;\n pub mod mutex;\n-pub mod atomics;\n pub mod raw;\n \n /**"}, {"sha": "eaf716f27265d6903beb8fd803c58b2c10d814c5", "filename": "src/libstd/unstable/mutex.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Funstable%2Fmutex.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Funstable%2Fmutex.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Fmutex.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -48,7 +48,7 @@\n #[allow(non_camel_case_types)];\n \n use libc::c_void;\n-use unstable::atomics;\n+use sync::atomics;\n \n pub struct Mutex {\n     // pointers for the lock/cond handles, atomically updated"}, {"sha": "ad36f71cdeac5c91210c69edfc3363f5fcd58ac5", "filename": "src/libstd/unstable/sync.rs", "status": "modified", "additions": 4, "deletions": 477, "changes": 481, "blob_url": "https://github.com/rust-lang/rust/blob/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Funstable%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a55c57284d8341ee5b22c5372e77ac0af9479dc5/src%2Flibstd%2Funstable%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Fsync.rs?ref=a55c57284d8341ee5b22c5372e77ac0af9479dc5", "patch": "@@ -8,353 +8,12 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-use cast;\n-use comm::{Chan, Port};\n-use ptr;\n-use option::{Option,Some,None};\n-use task;\n-use unstable::atomics::{AtomicOption,AtomicUint,Acquire,Release,Relaxed,SeqCst};\n-use unstable::mutex::Mutex;\n-use ops::Drop;\n use clone::Clone;\n use kinds::Send;\n-use vec;\n-\n-/// An atomically reference counted pointer.\n-///\n-/// Enforces no shared-memory safety.\n-//#[unsafe_no_drop_flag] FIXME: #9758\n-pub struct UnsafeArc<T> {\n-    data: *mut ArcData<T>,\n-}\n-\n-pub enum UnsafeArcUnwrap<T> {\n-    UnsafeArcSelf(UnsafeArc<T>),\n-    UnsafeArcT(T)\n-}\n-\n-#[cfg(test)]\n-impl<T> UnsafeArcUnwrap<T> {\n-    fn expect_t(self, msg: &'static str) -> T {\n-        match self {\n-            UnsafeArcSelf(_) => fail!(msg),\n-            UnsafeArcT(t) => t\n-        }\n-    }\n-\n-    fn is_self(&self) -> bool {\n-        match *self {\n-            UnsafeArcSelf(_) => true,\n-            UnsafeArcT(_) => false\n-        }\n-    }\n-}\n-\n-struct ArcData<T> {\n-    count: AtomicUint,\n-    // An unwrapper uses this protocol to communicate with the \"other\" task that\n-    // drops the last refcount on an arc. Unfortunately this can't be a proper\n-    // pipe protocol because the unwrapper has to access both stages at once.\n-    // FIXME(#7544): Maybe use AtomicPtr instead (to avoid xchg in take() later)?\n-    unwrapper: AtomicOption<(Chan<()>, Port<bool>)>,\n-    // FIXME(#3224) should be able to make this non-option to save memory\n-    data: Option<T>,\n-}\n-\n-unsafe fn new_inner<T: Send>(data: T, refcount: uint) -> *mut ArcData<T> {\n-    let data = ~ArcData { count: AtomicUint::new(refcount),\n-                          unwrapper: AtomicOption::empty(),\n-                          data: Some(data) };\n-    cast::transmute(data)\n-}\n-\n-/// A helper object used by `UnsafeArc::unwrap`.\n-struct ChannelAndDataGuard<T> {\n-    channel: Option<Chan<bool>>,\n-    data: Option<~ArcData<T>>,\n-}\n-\n-#[unsafe_destructor]\n-impl<T> Drop for ChannelAndDataGuard<T> {\n-    fn drop(&mut self) {\n-        if task::failing() {\n-            // Killed during wait. Because this might happen while\n-            // someone else still holds a reference, we can't free\n-            // the data now; the \"other\" last refcount will free it.\n-            unsafe {\n-                let channel = self.channel.take_unwrap();\n-                let data = self.data.take_unwrap();\n-                channel.send(false);\n-                cast::forget(data);\n-            }\n-        }\n-    }\n-}\n-\n-impl<T> ChannelAndDataGuard<T> {\n-    fn unwrap(mut self) -> (Chan<bool>, ~ArcData<T>) {\n-        (self.channel.take_unwrap(), self.data.take_unwrap())\n-    }\n-}\n-\n-impl<T: Send> UnsafeArc<T> {\n-    pub fn new(data: T) -> UnsafeArc<T> {\n-        unsafe { UnsafeArc { data: new_inner(data, 1) } }\n-    }\n-\n-    /// As new(), but returns an extra pre-cloned handle.\n-    pub fn new2(data: T) -> (UnsafeArc<T>, UnsafeArc<T>) {\n-        unsafe {\n-            let ptr = new_inner(data, 2);\n-            (UnsafeArc { data: ptr }, UnsafeArc { data: ptr })\n-        }\n-    }\n-\n-    /// As new(), but returns a vector of as many pre-cloned handles as requested.\n-    pub fn newN(data: T, num_handles: uint) -> ~[UnsafeArc<T>] {\n-        unsafe {\n-            if num_handles == 0 {\n-                ~[] // need to free data here\n-            } else {\n-                let ptr = new_inner(data, num_handles);\n-                vec::from_fn(num_handles, |_| UnsafeArc { data: ptr })\n-            }\n-        }\n-    }\n-\n-    /// As newN(), but from an already-existing handle. Uses one xadd.\n-    pub fn cloneN(self, num_handles: uint) -> ~[UnsafeArc<T>] {\n-        if num_handles == 0 {\n-            ~[] // The \"num_handles - 1\" trick (below) fails in the 0 case.\n-        } else {\n-            unsafe {\n-                // Minus one because we are recycling the given handle's refcount.\n-                let old_count = (*self.data).count.fetch_add(num_handles - 1, Acquire);\n-                // let old_count = (*self.data).count.fetch_add(num_handles, Acquire);\n-                assert!(old_count >= 1);\n-                let ptr = self.data;\n-                cast::forget(self); // Don't run the destructor on this handle.\n-                vec::from_fn(num_handles, |_| UnsafeArc { data: ptr })\n-            }\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn get(&self) -> *mut T {\n-        unsafe {\n-            assert!((*self.data).count.load(Relaxed) > 0);\n-            let r: *mut T = (*self.data).data.get_mut_ref();\n-            return r;\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn get_immut(&self) -> *T {\n-        unsafe {\n-            assert!((*self.data).count.load(Relaxed) > 0);\n-            let r: *T = (*self.data).data.get_ref();\n-            return r;\n-        }\n-    }\n-\n-    /// Wait until all other handles are dropped, then retrieve the enclosed\n-    /// data. See extra::arc::Arc for specific semantics documentation.\n-    /// If called when the task is already unkillable, unwrap will unkillably\n-    /// block; otherwise, an unwrapping task can be killed by linked failure.\n-    pub fn unwrap(self) -> T {\n-        unsafe {\n-            let mut this = self;\n-            // The ~ dtor needs to run if this code succeeds.\n-            let mut data: ~ArcData<T> = cast::transmute(this.data);\n-            // Set up the unwrap protocol.\n-            let (p1,c1) = Chan::new(); // ()\n-            let (p2,c2) = Chan::new(); // bool\n-            // Try to put our server end in the unwrapper slot.\n-            // This needs no barrier -- it's protected by the release barrier on\n-            // the xadd, and the acquire+release barrier in the destructor's xadd.\n-            if data.unwrapper.fill(~(c1,p2), Relaxed).is_none() {\n-                // Got in. Tell this handle's destructor not to run (we are now it).\n-                this.data = ptr::mut_null();\n-                // Drop our own reference.\n-                let old_count = data.count.fetch_sub(1, Release);\n-                assert!(old_count >= 1);\n-                if old_count == 1 {\n-                    // We were the last owner. Can unwrap immediately.\n-                    // AtomicOption's destructor will free the server endpoint.\n-                    // FIXME(#3224): it should be like this\n-                    // let ~ArcData { data: user_data, _ } = data;\n-                    // user_data\n-                    data.data.take_unwrap()\n-                } else {\n-                    // The *next* person who sees the refcount hit 0 will wake us.\n-                    let c2_and_data = ChannelAndDataGuard {\n-                        channel: Some(c2),\n-                        data: Some(data),\n-                    };\n-                    p1.recv();\n-                    // Got here. Back in the 'unkillable' without getting killed.\n-                    let (c2, data) = c2_and_data.unwrap();\n-                    c2.send(true);\n-                    // FIXME(#3224): it should be like this\n-                    // let ~ArcData { data: user_data, _ } = data;\n-                    // user_data\n-                    let mut data = data;\n-                    data.data.take_unwrap()\n-                }\n-            } else {\n-                // If 'put' returns the server end back to us, we were rejected;\n-                // someone else was trying to unwrap. Avoid guaranteed deadlock.\n-                cast::forget(data);\n-                fail!(\"Another task is already unwrapping this Arc!\");\n-            }\n-        }\n-    }\n-\n-    /// As unwrap above, but without blocking. Returns 'UnsafeArcSelf(self)' if this is\n-    /// not the last reference; 'UnsafeArcT(unwrapped_data)' if so.\n-    pub fn try_unwrap(mut self) -> UnsafeArcUnwrap<T> {\n-        unsafe {\n-            // The ~ dtor needs to run if this code succeeds.\n-            let mut data: ~ArcData<T> = cast::transmute(self.data);\n-            // This can of course race with anybody else who has a handle, but in\n-            // such a case, the returned count will always be at least 2. If we\n-            // see 1, no race was possible. All that matters is 1 or not-1.\n-            let count = data.count.load(Acquire);\n-            assert!(count >= 1);\n-            // The more interesting race is one with an unwrapper. They may have\n-            // already dropped their count -- but if so, the unwrapper pointer\n-            // will have been set first, which the barriers ensure we will see.\n-            // (Note: using is_empty(), not take(), to not free the unwrapper.)\n-            if count == 1 && data.unwrapper.is_empty(Acquire) {\n-                // Tell this handle's destructor not to run (we are now it).\n-                self.data = ptr::mut_null();\n-                // FIXME(#3224) as above\n-                UnsafeArcT(data.data.take_unwrap())\n-            } else {\n-                cast::forget(data);\n-                UnsafeArcSelf(self)\n-            }\n-        }\n-    }\n-}\n-\n-impl<T: Send> Clone for UnsafeArc<T> {\n-    fn clone(&self) -> UnsafeArc<T> {\n-        unsafe {\n-            // This barrier might be unnecessary, but I'm not sure...\n-            let old_count = (*self.data).count.fetch_add(1, Acquire);\n-            assert!(old_count >= 1);\n-            return UnsafeArc { data: self.data };\n-        }\n-    }\n-}\n-\n-#[unsafe_destructor]\n-impl<T> Drop for UnsafeArc<T>{\n-    fn drop(&mut self) {\n-        unsafe {\n-            // Happens when destructing an unwrapper's handle and from `#[unsafe_no_drop_flag]`\n-            if self.data.is_null() {\n-                return\n-            }\n-            let mut data: ~ArcData<T> = cast::transmute(self.data);\n-            // Must be acquire+release, not just release, to make sure this\n-            // doesn't get reordered to after the unwrapper pointer load.\n-            let old_count = data.count.fetch_sub(1, SeqCst);\n-            assert!(old_count >= 1);\n-            if old_count == 1 {\n-                // Were we really last, or should we hand off to an\n-                // unwrapper? It's safe to not xchg because the unwrapper\n-                // will set the unwrap lock *before* dropping his/her\n-                // reference. In effect, being here means we're the only\n-                // *awake* task with the data.\n-                match data.unwrapper.take(Acquire) {\n-                    Some(~(message, response)) => {\n-                        // Send 'ready' and wait for a response.\n-                        message.send(());\n-                        // Unkillable wait. Message guaranteed to come.\n-                        if response.recv() {\n-                            // Other task got the data.\n-                            cast::forget(data);\n-                        } else {\n-                            // Other task was killed. drop glue takes over.\n-                        }\n-                    }\n-                    None => {\n-                        // drop glue takes over.\n-                    }\n-                }\n-            } else {\n-                cast::forget(data);\n-            }\n-        }\n-    }\n-}\n-\n-\n-/****************************************************************************/\n-\n-pub struct AtomicGuard {\n-    on: bool,\n-}\n-\n-impl Drop for AtomicGuard {\n-    fn drop(&mut self) {\n-        use rt::task::{Task, GreenTask, SchedTask};\n-        use rt::local::Local;\n-\n-        if self.on {\n-            unsafe {\n-                let task_opt: Option<*mut Task> = Local::try_unsafe_borrow();\n-                match task_opt {\n-                    Some(t) => {\n-                        match (*t).task_type {\n-                            GreenTask(_) => (*t).death.allow_deschedule(),\n-                            SchedTask => {}\n-                        }\n-                    }\n-                    None => {}\n-                }\n-            }\n-        }\n-    }\n-}\n-\n-/**\n- * Enables a runtime assertion that no operation while the returned guard is\n- * live uses scheduler operations (deschedule, recv, spawn, etc). This is for\n- * use with pthread mutexes, which may block the entire scheduler thread,\n- * rather than just one task, and is hence prone to deadlocks if mixed with\n- * descheduling.\n- *\n- * NOTE: THIS DOES NOT PROVIDE LOCKING, or any sort of critical-section\n- * synchronization whatsoever. It only makes sense to use for CPU-local issues.\n- */\n-// FIXME(#8140) should not be pub\n-pub unsafe fn atomic() -> AtomicGuard {\n-    use rt::task::{Task, GreenTask, SchedTask};\n-    use rt::local::Local;\n-\n-    let task_opt: Option<*mut Task> = Local::try_unsafe_borrow();\n-    match task_opt {\n-        Some(t) => {\n-            match (*t).task_type {\n-                GreenTask(_) => {\n-                    (*t).death.inhibit_deschedule();\n-                    return AtomicGuard {\n-                        on: true,\n-                    };\n-                }\n-                SchedTask => {}\n-            }\n-        }\n-        None => {}\n-    }\n-\n-    AtomicGuard {\n-        on: false,\n-    }\n-}\n+use ops::Drop;\n+use option::{Option,Some,None};\n+use sync::arc::UnsafeArc;\n+use unstable::mutex::Mutex;\n \n pub struct LittleLock {\n     priv l: Mutex,\n@@ -496,14 +155,6 @@ impl<T:Send> Exclusive<T> {\n             l.wait();\n         }\n     }\n-\n-    pub fn unwrap(self) -> T {\n-        let Exclusive { x: x } = self;\n-        // Someday we might need to unkillably unwrap an Exclusive, but not today.\n-        let inner = x.unwrap();\n-        let ExData { data: user_data, .. } = inner; // will destroy the LittleLock\n-        user_data\n-    }\n }\n \n #[cfg(test)]\n@@ -514,20 +165,6 @@ mod tests {\n     use task;\n     use mem::size_of;\n \n-    //#[unsafe_no_drop_flag] FIXME: #9758\n-    #[ignore]\n-    #[test]\n-    fn test_size() {\n-        assert_eq!(size_of::<UnsafeArc<[int, ..10]>>(), size_of::<*[int, ..10]>());\n-    }\n-\n-    #[test]\n-    fn test_atomic() {\n-        // NB. The whole runtime will abort on an 'atomic-sleep' violation,\n-        // so we can't really test for the converse behaviour.\n-        unsafe { let _ = atomic(); } // oughtn't fail\n-    }\n-\n     #[test]\n     fn exclusive_new_arc() {\n         unsafe {\n@@ -570,114 +207,4 @@ mod tests {\n             x.with(|one| assert_eq!(*one, 1));\n         }\n     }\n-\n-    #[test]\n-    fn arclike_newN() {\n-        // Tests that the many-refcounts-at-once constructors don't leak.\n-        let _ = UnsafeArc::new2(~~\"hello\");\n-        let x = UnsafeArc::newN(~~\"hello\", 0);\n-        assert_eq!(x.len(), 0)\n-        let x = UnsafeArc::newN(~~\"hello\", 1);\n-        assert_eq!(x.len(), 1)\n-        let x = UnsafeArc::newN(~~\"hello\", 10);\n-        assert_eq!(x.len(), 10)\n-    }\n-\n-    #[test]\n-    fn arclike_cloneN() {\n-        // Tests that the many-refcounts-at-once special-clone doesn't leak.\n-        let x = UnsafeArc::new(~~\"hello\");\n-        let x = x.cloneN(0);\n-        assert_eq!(x.len(), 0);\n-        let x = UnsafeArc::new(~~\"hello\");\n-        let x = x.cloneN(1);\n-        assert_eq!(x.len(), 1);\n-        let x = UnsafeArc::new(~~\"hello\");\n-        let x = x.cloneN(10);\n-        assert_eq!(x.len(), 10);\n-    }\n-\n-    #[test]\n-    fn arclike_unwrap_basic() {\n-        let x = UnsafeArc::new(~~\"hello\");\n-        assert!(x.unwrap() == ~~\"hello\");\n-    }\n-\n-    #[test]\n-    fn arclike_try_unwrap() {\n-        let x = UnsafeArc::new(~~\"hello\");\n-        assert!(x.try_unwrap().expect_t(\"try_unwrap failed\") == ~~\"hello\");\n-    }\n-\n-    #[test]\n-    fn arclike_try_unwrap_fail() {\n-        let x = UnsafeArc::new(~~\"hello\");\n-        let x2 = x.clone();\n-        let left_x = x.try_unwrap();\n-        assert!(left_x.is_self());\n-        drop(left_x);\n-        assert!(x2.try_unwrap().expect_t(\"try_unwrap none\") == ~~\"hello\");\n-    }\n-\n-    #[test]\n-    fn arclike_try_unwrap_unwrap_race() {\n-        // When an unwrap and a try_unwrap race, the unwrapper should always win.\n-        let x = UnsafeArc::new(~~\"hello\");\n-        let x2 = x.clone();\n-        let (p,c) = Chan::new();\n-        do task::spawn {\n-            c.send(());\n-            assert!(x2.unwrap() == ~~\"hello\");\n-            c.send(());\n-        }\n-        p.recv();\n-        task::deschedule(); // Try to make the unwrapper get blocked first.\n-        let left_x = x.try_unwrap();\n-        assert!(left_x.is_self());\n-        drop(left_x);\n-        p.recv();\n-    }\n-\n-    #[test]\n-    fn exclusive_new_unwrap_basic() {\n-        // Unlike the above, also tests no double-freeing of the LittleLock.\n-        let x = Exclusive::new(~~\"hello\");\n-        assert!(x.unwrap() == ~~\"hello\");\n-    }\n-\n-    #[test]\n-    fn exclusive_new_unwrap_contended() {\n-        let x = Exclusive::new(~~\"hello\");\n-        let x2 = x.clone();\n-        do task::spawn {\n-            unsafe { x2.with(|_hello| ()); }\n-            task::deschedule();\n-        }\n-        assert!(x.unwrap() == ~~\"hello\");\n-\n-        // Now try the same thing, but with the child task blocking.\n-        let x = Exclusive::new(~~\"hello\");\n-        let x2 = x.clone();\n-        let mut builder = task::task();\n-        let res = builder.future_result();\n-        do builder.spawn {\n-            assert!(x2.unwrap() == ~~\"hello\");\n-        }\n-        // Have to get rid of our reference before blocking.\n-        drop(x);\n-        res.recv();\n-    }\n-\n-    #[test] #[should_fail]\n-    fn exclusive_new_unwrap_conflict() {\n-        let x = Exclusive::new(~~\"hello\");\n-        let x2 = x.clone();\n-        let mut builder = task::task();\n-        let res = builder.future_result();\n-        do builder.spawn {\n-            assert!(x2.unwrap() == ~~\"hello\");\n-        }\n-        assert!(x.unwrap() == ~~\"hello\");\n-        assert!(res.recv().is_ok());\n-    }\n }"}]}