{"sha": "27fb904d680996fe48e04aef65d4d655bdab843b", "node_id": "C_kwDOAAsO6NoAKDI3ZmI5MDRkNjgwOTk2ZmU0OGUwNGFlZjY1ZDRkNjU1YmRhYjg0M2I", "commit": {"author": {"name": "hkalbasi", "email": "hamidrezakalbasi@protonmail.com", "date": "2022-11-01T16:20:30Z"}, "committer": {"name": "hkalbasi", "email": "hamidrezakalbasi@protonmail.com", "date": "2022-11-24T12:56:12Z"}, "message": "move some layout logic to rustc_target::abi::layout", "tree": {"sha": "48ed33a5a63e23d06770dc5a86ec6532e254cdc3", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/48ed33a5a63e23d06770dc5a86ec6532e254cdc3"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/27fb904d680996fe48e04aef65d4d655bdab843b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/27fb904d680996fe48e04aef65d4d655bdab843b", "html_url": "https://github.com/rust-lang/rust/commit/27fb904d680996fe48e04aef65d4d655bdab843b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/27fb904d680996fe48e04aef65d4d655bdab843b/comments", "author": {"login": "HKalbasi", "id": 45197576, "node_id": "MDQ6VXNlcjQ1MTk3NTc2", "avatar_url": "https://avatars.githubusercontent.com/u/45197576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HKalbasi", "html_url": "https://github.com/HKalbasi", "followers_url": "https://api.github.com/users/HKalbasi/followers", "following_url": "https://api.github.com/users/HKalbasi/following{/other_user}", "gists_url": "https://api.github.com/users/HKalbasi/gists{/gist_id}", "starred_url": "https://api.github.com/users/HKalbasi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HKalbasi/subscriptions", "organizations_url": "https://api.github.com/users/HKalbasi/orgs", "repos_url": "https://api.github.com/users/HKalbasi/repos", "events_url": "https://api.github.com/users/HKalbasi/events{/privacy}", "received_events_url": "https://api.github.com/users/HKalbasi/received_events", "type": "User", "site_admin": false}, "committer": {"login": "HKalbasi", "id": 45197576, "node_id": "MDQ6VXNlcjQ1MTk3NTc2", "avatar_url": "https://avatars.githubusercontent.com/u/45197576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HKalbasi", "html_url": "https://github.com/HKalbasi", "followers_url": "https://api.github.com/users/HKalbasi/followers", "following_url": "https://api.github.com/users/HKalbasi/following{/other_user}", "gists_url": "https://api.github.com/users/HKalbasi/gists{/gist_id}", "starred_url": "https://api.github.com/users/HKalbasi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HKalbasi/subscriptions", "organizations_url": "https://api.github.com/users/HKalbasi/orgs", "repos_url": "https://api.github.com/users/HKalbasi/repos", "events_url": "https://api.github.com/users/HKalbasi/events{/privacy}", "received_events_url": "https://api.github.com/users/HKalbasi/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "09a384643ee4188fd1e837ab25856a7e8ff62d07", "url": "https://api.github.com/repos/rust-lang/rust/commits/09a384643ee4188fd1e837ab25856a7e8ff62d07", "html_url": "https://github.com/rust-lang/rust/commit/09a384643ee4188fd1e837ab25856a7e8ff62d07"}], "stats": {"total": 2388, "additions": 1231, "deletions": 1157}, "files": [{"sha": "13d37c9337535c0aa423cdd46525ca124bf98bdc", "filename": "Cargo.lock", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -4281,6 +4281,8 @@ name = \"rustc_target\"\n version = \"0.0.0\"\n dependencies = [\n  \"bitflags\",\n+ \"rand 0.8.5\",\n+ \"rand_xoshiro\",\n  \"rustc_data_structures\",\n  \"rustc_feature\",\n  \"rustc_index\",\n@@ -4336,6 +4338,7 @@ dependencies = [\n  \"rustc_infer\",\n  \"rustc_middle\",\n  \"rustc_span\",\n+ \"rustc_target\",\n  \"rustc_trait_selection\",\n  \"smallvec\",\n  \"tracing\","}, {"sha": "6bdd5511459943b0dbf9de6ea5ab4115779b576e", "filename": "compiler/rustc_hir_analysis/src/collect.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -32,8 +32,8 @@ use rustc_middle::hir::nested_filter;\n use rustc_middle::middle::codegen_fn_attrs::{CodegenFnAttrFlags, CodegenFnAttrs};\n use rustc_middle::mir::mono::Linkage;\n use rustc_middle::ty::query::Providers;\n+use rustc_middle::ty::repr_options_of_def;\n use rustc_middle::ty::util::{Discr, IntTypeExt};\n-use rustc_middle::ty::ReprOptions;\n use rustc_middle::ty::{self, AdtKind, Const, DefIdTree, IsSuggestable, Ty, TyCtxt};\n use rustc_session::lint;\n use rustc_session::parse::feature_err;\n@@ -860,7 +860,7 @@ fn adt_def<'tcx>(tcx: TyCtxt<'tcx>, def_id: DefId) -> ty::AdtDef<'tcx> {\n         bug!();\n     };\n \n-    let repr = ReprOptions::new(tcx, def_id.to_def_id());\n+    let repr = repr_options_of_def(tcx, def_id.to_def_id());\n     let (kind, variants) = match item.kind {\n         ItemKind::Enum(ref def, _) => {\n             let mut distance_from_explicit = 0;"}, {"sha": "fadd47eed723c33bb506a535f461d78b0f34c47a", "filename": "compiler/rustc_lint/src/types.rs", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -12,7 +12,7 @@ use rustc_middle::ty::{self, AdtKind, DefIdTree, Ty, TyCtxt, TypeSuperVisitable,\n use rustc_span::source_map;\n use rustc_span::symbol::sym;\n use rustc_span::{Span, Symbol};\n-use rustc_target::abi::{Abi, WrappingRange};\n+use rustc_target::abi::{Abi, Size, WrappingRange};\n use rustc_target::abi::{Integer, TagEncoding, Variants};\n use rustc_target::spec::abi::Abi as SpecAbi;\n \n@@ -225,11 +225,11 @@ fn report_bin_hex_error(\n     cx: &LateContext<'_>,\n     expr: &hir::Expr<'_>,\n     ty: attr::IntType,\n+    size: Size,\n     repr_str: String,\n     val: u128,\n     negative: bool,\n ) {\n-    let size = Integer::from_attr(&cx.tcx, ty).size();\n     cx.struct_span_lint(\n         OVERFLOWING_LITERALS,\n         expr.span,\n@@ -352,6 +352,7 @@ fn lint_int_literal<'tcx>(\n                 cx,\n                 e,\n                 attr::IntType::SignedInt(ty::ast_int_ty(t)),\n+                Integer::from_int_ty(cx, t).size(),\n                 repr_str,\n                 v,\n                 negative,\n@@ -437,6 +438,7 @@ fn lint_uint_literal<'tcx>(\n                 cx,\n                 e,\n                 attr::IntType::UnsignedInt(ty::ast_uint_ty(t)),\n+                Integer::from_uint_ty(cx, t).size(),\n                 repr_str,\n                 lit_val,\n                 false,"}, {"sha": "d3d667f68407fde123e0407e22ba4caa1d0a1095", "filename": "compiler/rustc_middle/src/ty/adt.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_middle%2Fsrc%2Fty%2Fadt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_middle%2Fsrc%2Fty%2Fadt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Fadt.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -14,17 +14,15 @@ use rustc_index::vec::{Idx, IndexVec};\n use rustc_query_system::ich::StableHashingContext;\n use rustc_session::DataTypeKind;\n use rustc_span::symbol::sym;\n-use rustc_target::abi::VariantIdx;\n+use rustc_target::abi::{ReprOptions, VariantIdx};\n \n use std::cell::RefCell;\n use std::cmp::Ordering;\n use std::hash::{Hash, Hasher};\n use std::ops::Range;\n use std::str;\n \n-use super::{\n-    Destructor, FieldDef, GenericPredicates, ReprOptions, Ty, TyCtxt, VariantDef, VariantDiscr,\n-};\n+use super::{Destructor, FieldDef, GenericPredicates, Ty, TyCtxt, VariantDef, VariantDiscr};\n \n bitflags! {\n     #[derive(HashStable, TyEncodable, TyDecodable)]"}, {"sha": "488fd567846a3a850f973c7a7f3328ae21f28694", "filename": "compiler/rustc_middle/src/ty/layout.rs", "status": "modified", "additions": 12, "deletions": 19, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_middle%2Fsrc%2Fty%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_middle%2Fsrc%2Fty%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Flayout.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -1,8 +1,6 @@\n use crate::middle::codegen_fn_attrs::CodegenFnAttrFlags;\n use crate::ty::normalize_erasing_regions::NormalizationError;\n use crate::ty::{self, ReprOptions, Ty, TyCtxt, TypeVisitable};\n-use rustc_ast as ast;\n-use rustc_attr as attr;\n use rustc_errors::{DiagnosticBuilder, Handler, IntoDiagnostic};\n use rustc_hir as hir;\n use rustc_hir::def_id::DefId;\n@@ -20,7 +18,6 @@ use std::ops::Bound;\n \n pub trait IntegerExt {\n     fn to_ty<'tcx>(&self, tcx: TyCtxt<'tcx>, signed: bool) -> Ty<'tcx>;\n-    fn from_attr<C: HasDataLayout>(cx: &C, ity: attr::IntType) -> Integer;\n     fn from_int_ty<C: HasDataLayout>(cx: &C, ity: ty::IntTy) -> Integer;\n     fn from_uint_ty<C: HasDataLayout>(cx: &C, uty: ty::UintTy) -> Integer;\n     fn repr_discr<'tcx>(\n@@ -49,22 +46,6 @@ impl IntegerExt for Integer {\n         }\n     }\n \n-    /// Gets the Integer type from an attr::IntType.\n-    fn from_attr<C: HasDataLayout>(cx: &C, ity: attr::IntType) -> Integer {\n-        let dl = cx.data_layout();\n-\n-        match ity {\n-            attr::SignedInt(ast::IntTy::I8) | attr::UnsignedInt(ast::UintTy::U8) => I8,\n-            attr::SignedInt(ast::IntTy::I16) | attr::UnsignedInt(ast::UintTy::U16) => I16,\n-            attr::SignedInt(ast::IntTy::I32) | attr::UnsignedInt(ast::UintTy::U32) => I32,\n-            attr::SignedInt(ast::IntTy::I64) | attr::UnsignedInt(ast::UintTy::U64) => I64,\n-            attr::SignedInt(ast::IntTy::I128) | attr::UnsignedInt(ast::UintTy::U128) => I128,\n-            attr::SignedInt(ast::IntTy::Isize) | attr::UnsignedInt(ast::UintTy::Usize) => {\n-                dl.ptr_sized_integer()\n-            }\n-        }\n-    }\n-\n     fn from_int_ty<C: HasDataLayout>(cx: &C, ity: ty::IntTy) -> Integer {\n         match ity {\n             ty::IntTy::I8 => I8,\n@@ -237,6 +218,18 @@ pub struct LayoutCx<'tcx, C> {\n     pub param_env: ty::ParamEnv<'tcx>,\n }\n \n+impl<'tcx> LayoutCalculator for LayoutCx<'tcx, TyCtxt<'tcx>> {\n+    type TargetDataLayoutRef = &'tcx TargetDataLayout;\n+\n+    fn delay_bug(&self, txt: &str) {\n+        self.tcx.sess.delay_span_bug(DUMMY_SP, txt);\n+    }\n+\n+    fn current_data_layout(&self) -> Self::TargetDataLayoutRef {\n+        &self.tcx.data_layout\n+    }\n+}\n+\n /// Type size \"skeleton\", i.e., the only information determining a type's size.\n /// While this is conservative, (aside from constant sizes, only pointers,\n /// newtypes thereof and null pointer optimized enums are allowed), it is"}, {"sha": "e3421ab9ce0bf65a335fa151fd6a8042cb1f9bb8", "filename": "compiler/rustc_middle/src/ty/mod.rs", "status": "modified", "additions": 62, "deletions": 146, "changes": 208, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -48,7 +48,8 @@ use rustc_session::cstore::CrateStoreDyn;\n use rustc_span::hygiene::MacroKind;\n use rustc_span::symbol::{kw, sym, Ident, Symbol};\n use rustc_span::{ExpnId, Span};\n-use rustc_target::abi::{Align, VariantIdx};\n+use rustc_target::abi::{Align, Integer, IntegerType, VariantIdx};\n+pub use rustc_target::abi::{ReprFlags, ReprOptions};\n pub use subst::*;\n pub use vtable::*;\n \n@@ -1994,161 +1995,76 @@ impl Hash for FieldDef {\n     }\n }\n \n-bitflags! {\n-    #[derive(TyEncodable, TyDecodable, Default, HashStable)]\n-    pub struct ReprFlags: u8 {\n-        const IS_C               = 1 << 0;\n-        const IS_SIMD            = 1 << 1;\n-        const IS_TRANSPARENT     = 1 << 2;\n-        // Internal only for now. If true, don't reorder fields.\n-        const IS_LINEAR          = 1 << 3;\n-        // If true, the type's layout can be randomized using\n-        // the seed stored in `ReprOptions.layout_seed`\n-        const RANDOMIZE_LAYOUT   = 1 << 4;\n-        // Any of these flags being set prevent field reordering optimisation.\n-        const IS_UNOPTIMISABLE   = ReprFlags::IS_C.bits\n-                                 | ReprFlags::IS_SIMD.bits\n-                                 | ReprFlags::IS_LINEAR.bits;\n-    }\n-}\n-\n-/// Represents the repr options provided by the user,\n-#[derive(Copy, Clone, Debug, Eq, PartialEq, TyEncodable, TyDecodable, Default, HashStable)]\n-pub struct ReprOptions {\n-    pub int: Option<attr::IntType>,\n-    pub align: Option<Align>,\n-    pub pack: Option<Align>,\n-    pub flags: ReprFlags,\n-    /// The seed to be used for randomizing a type's layout\n-    ///\n-    /// Note: This could technically be a `[u8; 16]` (a `u128`) which would\n-    /// be the \"most accurate\" hash as it'd encompass the item and crate\n-    /// hash without loss, but it does pay the price of being larger.\n-    /// Everything's a tradeoff, a `u64` seed should be sufficient for our\n-    /// purposes (primarily `-Z randomize-layout`)\n-    pub field_shuffle_seed: u64,\n-}\n-\n-impl ReprOptions {\n-    pub fn new(tcx: TyCtxt<'_>, did: DefId) -> ReprOptions {\n-        let mut flags = ReprFlags::empty();\n-        let mut size = None;\n-        let mut max_align: Option<Align> = None;\n-        let mut min_pack: Option<Align> = None;\n-\n-        // Generate a deterministically-derived seed from the item's path hash\n-        // to allow for cross-crate compilation to actually work\n-        let mut field_shuffle_seed = tcx.def_path_hash(did).0.to_smaller_hash();\n-\n-        // If the user defined a custom seed for layout randomization, xor the item's\n-        // path hash with the user defined seed, this will allowing determinism while\n-        // still allowing users to further randomize layout generation for e.g. fuzzing\n-        if let Some(user_seed) = tcx.sess.opts.unstable_opts.layout_seed {\n-            field_shuffle_seed ^= user_seed;\n-        }\n-\n-        for attr in tcx.get_attrs(did, sym::repr) {\n-            for r in attr::parse_repr_attr(&tcx.sess, attr) {\n-                flags.insert(match r {\n-                    attr::ReprC => ReprFlags::IS_C,\n-                    attr::ReprPacked(pack) => {\n-                        let pack = Align::from_bytes(pack as u64).unwrap();\n-                        min_pack = Some(if let Some(min_pack) = min_pack {\n-                            min_pack.min(pack)\n-                        } else {\n-                            pack\n-                        });\n-                        ReprFlags::empty()\n-                    }\n-                    attr::ReprTransparent => ReprFlags::IS_TRANSPARENT,\n-                    attr::ReprSimd => ReprFlags::IS_SIMD,\n-                    attr::ReprInt(i) => {\n-                        size = Some(i);\n-                        ReprFlags::empty()\n-                    }\n-                    attr::ReprAlign(align) => {\n-                        max_align = max_align.max(Some(Align::from_bytes(align as u64).unwrap()));\n-                        ReprFlags::empty()\n-                    }\n-                });\n-            }\n-        }\n+pub fn repr_options_of_def(tcx: TyCtxt<'_>, did: DefId) -> ReprOptions {\n+    let mut flags = ReprFlags::empty();\n+    let mut size = None;\n+    let mut max_align: Option<Align> = None;\n+    let mut min_pack: Option<Align> = None;\n \n-        // If `-Z randomize-layout` was enabled for the type definition then we can\n-        // consider performing layout randomization\n-        if tcx.sess.opts.unstable_opts.randomize_layout {\n-            flags.insert(ReprFlags::RANDOMIZE_LAYOUT);\n-        }\n+    // Generate a deterministically-derived seed from the item's path hash\n+    // to allow for cross-crate compilation to actually work\n+    let mut field_shuffle_seed = tcx.def_path_hash(did).0.to_smaller_hash();\n \n-        // This is here instead of layout because the choice must make it into metadata.\n-        if !tcx.consider_optimizing(|| format!(\"Reorder fields of {:?}\", tcx.def_path_str(did))) {\n-            flags.insert(ReprFlags::IS_LINEAR);\n-        }\n-\n-        Self { int: size, align: max_align, pack: min_pack, flags, field_shuffle_seed }\n-    }\n-\n-    #[inline]\n-    pub fn simd(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_SIMD)\n+    // If the user defined a custom seed for layout randomization, xor the item's\n+    // path hash with the user defined seed, this will allowing determinism while\n+    // still allowing users to further randomize layout generation for e.g. fuzzing\n+    if let Some(user_seed) = tcx.sess.opts.unstable_opts.layout_seed {\n+        field_shuffle_seed ^= user_seed;\n     }\n \n-    #[inline]\n-    pub fn c(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_C)\n-    }\n-\n-    #[inline]\n-    pub fn packed(&self) -> bool {\n-        self.pack.is_some()\n-    }\n-\n-    #[inline]\n-    pub fn transparent(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_TRANSPARENT)\n-    }\n-\n-    #[inline]\n-    pub fn linear(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_LINEAR)\n-    }\n-\n-    /// Returns the discriminant type, given these `repr` options.\n-    /// This must only be called on enums!\n-    pub fn discr_type(&self) -> attr::IntType {\n-        self.int.unwrap_or(attr::SignedInt(ast::IntTy::Isize))\n-    }\n-\n-    /// Returns `true` if this `#[repr()]` should inhabit \"smart enum\n-    /// layout\" optimizations, such as representing `Foo<&T>` as a\n-    /// single pointer.\n-    pub fn inhibit_enum_layout_opt(&self) -> bool {\n-        self.c() || self.int.is_some()\n-    }\n-\n-    /// Returns `true` if this `#[repr()]` should inhibit struct field reordering\n-    /// optimizations, such as with `repr(C)`, `repr(packed(1))`, or `repr(<int>)`.\n-    pub fn inhibit_struct_field_reordering_opt(&self) -> bool {\n-        if let Some(pack) = self.pack {\n-            if pack.bytes() == 1 {\n-                return true;\n-            }\n+    for attr in tcx.get_attrs(did, sym::repr) {\n+        for r in attr::parse_repr_attr(&tcx.sess, attr) {\n+            flags.insert(match r {\n+                attr::ReprC => ReprFlags::IS_C,\n+                attr::ReprPacked(pack) => {\n+                    let pack = Align::from_bytes(pack as u64).unwrap();\n+                    min_pack =\n+                        Some(if let Some(min_pack) = min_pack { min_pack.min(pack) } else { pack });\n+                    ReprFlags::empty()\n+                }\n+                attr::ReprTransparent => ReprFlags::IS_TRANSPARENT,\n+                attr::ReprSimd => ReprFlags::IS_SIMD,\n+                attr::ReprInt(i) => {\n+                    size = Some(match i {\n+                        attr::IntType::SignedInt(x) => match x {\n+                            ast::IntTy::Isize => IntegerType::Pointer(true),\n+                            ast::IntTy::I8 => IntegerType::Fixed(Integer::I8, true),\n+                            ast::IntTy::I16 => IntegerType::Fixed(Integer::I16, true),\n+                            ast::IntTy::I32 => IntegerType::Fixed(Integer::I32, true),\n+                            ast::IntTy::I64 => IntegerType::Fixed(Integer::I64, true),\n+                            ast::IntTy::I128 => IntegerType::Fixed(Integer::I128, true),\n+                        },\n+                        attr::IntType::UnsignedInt(x) => match x {\n+                            ast::UintTy::Usize => IntegerType::Pointer(false),\n+                            ast::UintTy::U8 => IntegerType::Fixed(Integer::I8, false),\n+                            ast::UintTy::U16 => IntegerType::Fixed(Integer::I16, false),\n+                            ast::UintTy::U32 => IntegerType::Fixed(Integer::I32, false),\n+                            ast::UintTy::U64 => IntegerType::Fixed(Integer::I64, false),\n+                            ast::UintTy::U128 => IntegerType::Fixed(Integer::I128, false),\n+                        },\n+                    });\n+                    ReprFlags::empty()\n+                }\n+                attr::ReprAlign(align) => {\n+                    max_align = max_align.max(Some(Align::from_bytes(align as u64).unwrap()));\n+                    ReprFlags::empty()\n+                }\n+            });\n         }\n-\n-        self.flags.intersects(ReprFlags::IS_UNOPTIMISABLE) || self.int.is_some()\n     }\n \n-    /// Returns `true` if this type is valid for reordering and `-Z randomize-layout`\n-    /// was enabled for its declaration crate\n-    pub fn can_randomize_type_layout(&self) -> bool {\n-        !self.inhibit_struct_field_reordering_opt()\n-            && self.flags.contains(ReprFlags::RANDOMIZE_LAYOUT)\n+    // If `-Z randomize-layout` was enabled for the type definition then we can\n+    // consider performing layout randomization\n+    if tcx.sess.opts.unstable_opts.randomize_layout {\n+        flags.insert(ReprFlags::RANDOMIZE_LAYOUT);\n     }\n \n-    /// Returns `true` if this `#[repr()]` should inhibit union ABI optimisations.\n-    pub fn inhibit_union_abi_opt(&self) -> bool {\n-        self.c()\n+    // This is here instead of layout because the choice must make it into metadata.\n+    if !tcx.consider_optimizing(|| format!(\"Reorder fields of {:?}\", tcx.def_path_str(did))) {\n+        flags.insert(ReprFlags::IS_LINEAR);\n     }\n+\n+    ReprOptions { int: size, align: max_align, pack: min_pack, flags, field_shuffle_seed }\n }\n \n impl<'tcx> FieldDef {"}, {"sha": "6561c4c278d0ee1032560d1ba7ae50341dfe7444", "filename": "compiler/rustc_middle/src/ty/util.rs", "status": "modified", "additions": 6, "deletions": 17, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_middle%2Fsrc%2Fty%2Futil.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_middle%2Fsrc%2Fty%2Futil.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Futil.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -8,8 +8,6 @@ use crate::ty::{\n };\n use crate::ty::{GenericArgKind, SubstsRef};\n use rustc_apfloat::Float as _;\n-use rustc_ast as ast;\n-use rustc_attr::{self as attr, SignedInt, UnsignedInt};\n use rustc_data_structures::fx::{FxHashMap, FxHashSet};\n use rustc_data_structures::stable_hasher::{HashStable, StableHasher};\n use rustc_errors::ErrorGuaranteed;\n@@ -19,7 +17,7 @@ use rustc_hir::def_id::DefId;\n use rustc_index::bit_set::GrowableBitSet;\n use rustc_macros::HashStable;\n use rustc_span::{sym, DUMMY_SP};\n-use rustc_target::abi::{Integer, Size, TargetDataLayout};\n+use rustc_target::abi::{Integer, IntegerType, Size, TargetDataLayout};\n use rustc_target::spec::abi::Abi;\n use smallvec::SmallVec;\n use std::{fmt, iter};\n@@ -104,21 +102,12 @@ pub trait IntTypeExt {\n     fn initial_discriminant<'tcx>(&self, tcx: TyCtxt<'tcx>) -> Discr<'tcx>;\n }\n \n-impl IntTypeExt for attr::IntType {\n+impl IntTypeExt for IntegerType {\n     fn to_ty<'tcx>(&self, tcx: TyCtxt<'tcx>) -> Ty<'tcx> {\n-        match *self {\n-            SignedInt(ast::IntTy::I8) => tcx.types.i8,\n-            SignedInt(ast::IntTy::I16) => tcx.types.i16,\n-            SignedInt(ast::IntTy::I32) => tcx.types.i32,\n-            SignedInt(ast::IntTy::I64) => tcx.types.i64,\n-            SignedInt(ast::IntTy::I128) => tcx.types.i128,\n-            SignedInt(ast::IntTy::Isize) => tcx.types.isize,\n-            UnsignedInt(ast::UintTy::U8) => tcx.types.u8,\n-            UnsignedInt(ast::UintTy::U16) => tcx.types.u16,\n-            UnsignedInt(ast::UintTy::U32) => tcx.types.u32,\n-            UnsignedInt(ast::UintTy::U64) => tcx.types.u64,\n-            UnsignedInt(ast::UintTy::U128) => tcx.types.u128,\n-            UnsignedInt(ast::UintTy::Usize) => tcx.types.usize,\n+        match self {\n+            IntegerType::Pointer(true) => tcx.types.isize,\n+            IntegerType::Pointer(false) => tcx.types.usize,\n+            IntegerType::Fixed(i, s) => i.to_ty(tcx, *s),\n         }\n     }\n "}, {"sha": "f2e21078b446b42d5acf4a23252025d9798ea2f9", "filename": "compiler/rustc_target/Cargo.toml", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_target%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_target%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2FCargo.toml?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -6,6 +6,8 @@ edition = \"2021\"\n [dependencies]\n bitflags = \"1.2.1\"\n tracing = \"0.1\"\n+rand = \"0.8.4\"\n+rand_xoshiro = \"0.6.0\"\n serde_json = \"1.0.59\"\n rustc_data_structures = { path = \"../rustc_data_structures\", optional = true  }\n rustc_feature = { path = \"../rustc_feature\", optional = true }\n@@ -23,4 +25,4 @@ nightly = [\n     \"rustc_macros\",\n     \"rustc_serialize\",\n     \"rustc_span\",\n-]\n\\ No newline at end of file\n+]"}, {"sha": "cf4843e9d6cb55ce822a2c08adfe01f3bb721809", "filename": "compiler/rustc_target/src/abi/layout.rs", "status": "added", "additions": 943, "deletions": 0, "changes": 943, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_target%2Fsrc%2Fabi%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_target%2Fsrc%2Fabi%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Flayout.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -0,0 +1,943 @@\n+use super::*;\n+use std::{\n+    borrow::Borrow,\n+    cmp,\n+    fmt::Debug,\n+    iter,\n+    ops::{Bound, Deref},\n+};\n+\n+use rand::{seq::SliceRandom, SeedableRng};\n+use rand_xoshiro::Xoshiro128StarStar;\n+\n+use tracing::debug;\n+\n+// Invert a bijective mapping, i.e. `invert(map)[y] = x` if `map[x] = y`.\n+// This is used to go between `memory_index` (source field order to memory order)\n+// and `inverse_memory_index` (memory order to source field order).\n+// See also `FieldsShape::Arbitrary::memory_index` for more details.\n+// FIXME(eddyb) build a better abstraction for permutations, if possible.\n+fn invert_mapping(map: &[u32]) -> Vec<u32> {\n+    let mut inverse = vec![0; map.len()];\n+    for i in 0..map.len() {\n+        inverse[map[i] as usize] = i as u32;\n+    }\n+    inverse\n+}\n+\n+pub trait LayoutCalculator {\n+    type TargetDataLayoutRef: Borrow<TargetDataLayout>;\n+\n+    fn delay_bug(&self, txt: &str);\n+    fn current_data_layout(&self) -> Self::TargetDataLayoutRef;\n+\n+    fn scalar_pair<V: Idx>(&self, a: Scalar, b: Scalar) -> LayoutS<V> {\n+        let dl = self.current_data_layout();\n+        let dl = dl.borrow();\n+        let b_align = b.align(dl);\n+        let align = a.align(dl).max(b_align).max(dl.aggregate_align);\n+        let b_offset = a.size(dl).align_to(b_align.abi);\n+        let size = (b_offset + b.size(dl)).align_to(align.abi);\n+\n+        // HACK(nox): We iter on `b` and then `a` because `max_by_key`\n+        // returns the last maximum.\n+        let largest_niche = Niche::from_scalar(dl, b_offset, b)\n+            .into_iter()\n+            .chain(Niche::from_scalar(dl, Size::ZERO, a))\n+            .max_by_key(|niche| niche.available(dl));\n+\n+        LayoutS {\n+            variants: Variants::Single { index: V::new(0) },\n+            fields: FieldsShape::Arbitrary {\n+                offsets: vec![Size::ZERO, b_offset],\n+                memory_index: vec![0, 1],\n+            },\n+            abi: Abi::ScalarPair(a, b),\n+            largest_niche,\n+            align,\n+            size,\n+        }\n+    }\n+\n+    fn univariant<'a, V: Idx, F: Deref<Target = &'a LayoutS<V>> + Debug>(\n+        &self,\n+        dl: &TargetDataLayout,\n+        fields: &[F],\n+        repr: &ReprOptions,\n+        kind: StructKind,\n+    ) -> Option<LayoutS<V>> {\n+        let pack = repr.pack;\n+        let mut align = if pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n+        let mut inverse_memory_index: Vec<u32> = (0..fields.len() as u32).collect();\n+        let optimize = !repr.inhibit_struct_field_reordering_opt();\n+        if optimize {\n+            let end =\n+                if let StructKind::MaybeUnsized = kind { fields.len() - 1 } else { fields.len() };\n+            let optimizing = &mut inverse_memory_index[..end];\n+            let effective_field_align = |f: &F| {\n+                if let Some(pack) = pack {\n+                    // return the packed alignment in bytes\n+                    f.align.abi.min(pack).bytes()\n+                } else {\n+                    // returns log2(effective-align).\n+                    // This is ok since `pack` applies to all fields equally.\n+                    // The calculation assumes that size is an integer multiple of align, except for ZSTs.\n+                    //\n+                    // group [u8; 4] with align-4 or [u8; 6] with align-2 fields\n+                    f.align.abi.bytes().max(f.size.bytes()).trailing_zeros() as u64\n+                }\n+            };\n+\n+            // If `-Z randomize-layout` was enabled for the type definition we can shuffle\n+            // the field ordering to try and catch some code making assumptions about layouts\n+            // we don't guarantee\n+            if repr.can_randomize_type_layout() {\n+                // `ReprOptions.layout_seed` is a deterministic seed that we can use to\n+                // randomize field ordering with\n+                let mut rng = Xoshiro128StarStar::seed_from_u64(repr.field_shuffle_seed);\n+\n+                // Shuffle the ordering of the fields\n+                optimizing.shuffle(&mut rng);\n+\n+                // Otherwise we just leave things alone and actually optimize the type's fields\n+            } else {\n+                match kind {\n+                    StructKind::AlwaysSized | StructKind::MaybeUnsized => {\n+                        optimizing.sort_by_key(|&x| {\n+                            // Place ZSTs first to avoid \"interesting offsets\",\n+                            // especially with only one or two non-ZST fields.\n+                            // Then place largest alignments first, largest niches within an alignment group last\n+                            let f = &fields[x as usize];\n+                            let niche_size = f.largest_niche.map_or(0, |n| n.available(dl));\n+                            (!f.is_zst(), cmp::Reverse(effective_field_align(f)), niche_size)\n+                        });\n+                    }\n+\n+                    StructKind::Prefixed(..) => {\n+                        // Sort in ascending alignment so that the layout stays optimal\n+                        // regardless of the prefix.\n+                        // And put the largest niche in an alignment group at the end\n+                        // so it can be used as discriminant in jagged enums\n+                        optimizing.sort_by_key(|&x| {\n+                            let f = &fields[x as usize];\n+                            let niche_size = f.largest_niche.map_or(0, |n| n.available(dl));\n+                            (effective_field_align(f), niche_size)\n+                        });\n+                    }\n+                }\n+\n+                // FIXME(Kixiron): We can always shuffle fields within a given alignment class\n+                //                 regardless of the status of `-Z randomize-layout`\n+            }\n+        }\n+        // inverse_memory_index holds field indices by increasing memory offset.\n+        // That is, if field 5 has offset 0, the first element of inverse_memory_index is 5.\n+        // We now write field offsets to the corresponding offset slot;\n+        // field 5 with offset 0 puts 0 in offsets[5].\n+        // At the bottom of this function, we invert `inverse_memory_index` to\n+        // produce `memory_index` (see `invert_mapping`).\n+        let mut sized = true;\n+        let mut offsets = vec![Size::ZERO; fields.len()];\n+        let mut offset = Size::ZERO;\n+        let mut largest_niche = None;\n+        let mut largest_niche_available = 0;\n+        if let StructKind::Prefixed(prefix_size, prefix_align) = kind {\n+            let prefix_align =\n+                if let Some(pack) = pack { prefix_align.min(pack) } else { prefix_align };\n+            align = align.max(AbiAndPrefAlign::new(prefix_align));\n+            offset = prefix_size.align_to(prefix_align);\n+        }\n+        for &i in &inverse_memory_index {\n+            let field = &fields[i as usize];\n+            if !sized {\n+                self.delay_bug(&format!(\n+                    \"univariant: field #{} comes after unsized field\",\n+                    offsets.len(),\n+                ));\n+            }\n+\n+            if field.is_unsized() {\n+                sized = false;\n+            }\n+\n+            // Invariant: offset < dl.obj_size_bound() <= 1<<61\n+            let field_align = if let Some(pack) = pack {\n+                field.align.min(AbiAndPrefAlign::new(pack))\n+            } else {\n+                field.align\n+            };\n+            offset = offset.align_to(field_align.abi);\n+            align = align.max(field_align);\n+\n+            debug!(\"univariant offset: {:?} field: {:#?}\", offset, field);\n+            offsets[i as usize] = offset;\n+\n+            if let Some(mut niche) = field.largest_niche {\n+                let available = niche.available(dl);\n+                if available > largest_niche_available {\n+                    largest_niche_available = available;\n+                    niche.offset += offset;\n+                    largest_niche = Some(niche);\n+                }\n+            }\n+\n+            offset = offset.checked_add(field.size, dl)?;\n+        }\n+        if let Some(repr_align) = repr.align {\n+            align = align.max(AbiAndPrefAlign::new(repr_align));\n+        }\n+        debug!(\"univariant min_size: {:?}\", offset);\n+        let min_size = offset;\n+        // As stated above, inverse_memory_index holds field indices by increasing offset.\n+        // This makes it an already-sorted view of the offsets vec.\n+        // To invert it, consider:\n+        // If field 5 has offset 0, offsets[0] is 5, and memory_index[5] should be 0.\n+        // Field 5 would be the first element, so memory_index is i:\n+        // Note: if we didn't optimize, it's already right.\n+        let memory_index =\n+            if optimize { invert_mapping(&inverse_memory_index) } else { inverse_memory_index };\n+        let size = min_size.align_to(align.abi);\n+        let mut abi = Abi::Aggregate { sized };\n+        // Unpack newtype ABIs and find scalar pairs.\n+        if sized && size.bytes() > 0 {\n+            // All other fields must be ZSTs.\n+            let mut non_zst_fields = fields.iter().enumerate().filter(|&(_, f)| !f.is_zst());\n+\n+            match (non_zst_fields.next(), non_zst_fields.next(), non_zst_fields.next()) {\n+                // We have exactly one non-ZST field.\n+                (Some((i, field)), None, None) => {\n+                    // Field fills the struct and it has a scalar or scalar pair ABI.\n+                    if offsets[i].bytes() == 0 && align.abi == field.align.abi && size == field.size\n+                    {\n+                        match field.abi {\n+                            // For plain scalars, or vectors of them, we can't unpack\n+                            // newtypes for `#[repr(C)]`, as that affects C ABIs.\n+                            Abi::Scalar(_) | Abi::Vector { .. } if optimize => {\n+                                abi = field.abi;\n+                            }\n+                            // But scalar pairs are Rust-specific and get\n+                            // treated as aggregates by C ABIs anyway.\n+                            Abi::ScalarPair(..) => {\n+                                abi = field.abi;\n+                            }\n+                            _ => {}\n+                        }\n+                    }\n+                }\n+\n+                // Two non-ZST fields, and they're both scalars.\n+                (Some((i, a)), Some((j, b)), None) => {\n+                    match (a.abi, b.abi) {\n+                        (Abi::Scalar(a), Abi::Scalar(b)) => {\n+                            // Order by the memory placement, not source order.\n+                            let ((i, a), (j, b)) = if offsets[i] < offsets[j] {\n+                                ((i, a), (j, b))\n+                            } else {\n+                                ((j, b), (i, a))\n+                            };\n+                            let pair = self.scalar_pair::<V>(a, b);\n+                            let pair_offsets = match pair.fields {\n+                                FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n+                                    assert_eq!(memory_index, &[0, 1]);\n+                                    offsets\n+                                }\n+                                _ => panic!(),\n+                            };\n+                            if offsets[i] == pair_offsets[0]\n+                                && offsets[j] == pair_offsets[1]\n+                                && align == pair.align\n+                                && size == pair.size\n+                            {\n+                                // We can use `ScalarPair` only when it matches our\n+                                // already computed layout (including `#[repr(C)]`).\n+                                abi = pair.abi;\n+                            }\n+                        }\n+                        _ => {}\n+                    }\n+                }\n+\n+                _ => {}\n+            }\n+        }\n+        if fields.iter().any(|f| f.abi.is_uninhabited()) {\n+            abi = Abi::Uninhabited;\n+        }\n+        Some(LayoutS {\n+            variants: Variants::Single { index: V::new(0) },\n+            fields: FieldsShape::Arbitrary { offsets, memory_index },\n+            abi,\n+            largest_niche,\n+            align,\n+            size,\n+        })\n+    }\n+\n+    fn layout_of_never_type<V: Idx>(&self) -> LayoutS<V> {\n+        let dl = self.current_data_layout();\n+        let dl = dl.borrow();\n+        LayoutS {\n+            variants: Variants::Single { index: V::new(0) },\n+            fields: FieldsShape::Primitive,\n+            abi: Abi::Uninhabited,\n+            largest_niche: None,\n+            align: dl.i8_align,\n+            size: Size::ZERO,\n+        }\n+    }\n+\n+    fn layout_of_struct_or_enum<'a, V: Idx, F: Deref<Target = &'a LayoutS<V>> + Debug>(\n+        &self,\n+        repr: &ReprOptions,\n+        variants: &IndexVec<V, Vec<F>>,\n+        is_enum: bool,\n+        is_unsafe_cell: bool,\n+        scalar_valid_range: (Bound<u128>, Bound<u128>),\n+        discr_range_of_repr: impl Fn(i128, i128) -> (Integer, bool),\n+        discriminants: impl Iterator<Item = (V, i128)>,\n+        niche_optimize_enum: bool,\n+        always_sized: bool,\n+    ) -> Option<LayoutS<V>> {\n+        let dl = self.current_data_layout();\n+        let dl = dl.borrow();\n+\n+        let scalar_unit = |value: Primitive| {\n+            let size = value.size(dl);\n+            assert!(size.bits() <= 128);\n+            Scalar::Initialized { value, valid_range: WrappingRange::full(size) }\n+        };\n+\n+        // A variant is absent if it's uninhabited and only has ZST fields.\n+        // Present uninhabited variants only require space for their fields,\n+        // but *not* an encoding of the discriminant (e.g., a tag value).\n+        // See issue #49298 for more details on the need to leave space\n+        // for non-ZST uninhabited data (mostly partial initialization).\n+        let absent = |fields: &[F]| {\n+            let uninhabited = fields.iter().any(|f| f.abi.is_uninhabited());\n+            let is_zst = fields.iter().all(|f| f.is_zst());\n+            uninhabited && is_zst\n+        };\n+        let (present_first, present_second) = {\n+            let mut present_variants = variants\n+                .iter_enumerated()\n+                .filter_map(|(i, v)| if absent(v) { None } else { Some(i) });\n+            (present_variants.next(), present_variants.next())\n+        };\n+        let present_first = match present_first {\n+            Some(present_first) => present_first,\n+            // Uninhabited because it has no variants, or only absent ones.\n+            None if is_enum => {\n+                return Some(self.layout_of_never_type());\n+            }\n+            // If it's a struct, still compute a layout so that we can still compute the\n+            // field offsets.\n+            None => V::new(0),\n+        };\n+\n+        let is_struct = !is_enum ||\n+                    // Only one variant is present.\n+                    (present_second.is_none() &&\n+                        // Representation optimizations are allowed.\n+                        !repr.inhibit_enum_layout_opt());\n+        if is_struct {\n+            // Struct, or univariant enum equivalent to a struct.\n+            // (Typechecking will reject discriminant-sizing attrs.)\n+\n+            let v = present_first;\n+            let kind = if is_enum || variants[v].is_empty() {\n+                StructKind::AlwaysSized\n+            } else {\n+                if !always_sized { StructKind::MaybeUnsized } else { StructKind::AlwaysSized }\n+            };\n+\n+            let mut st = self.univariant(dl, &variants[v], &repr, kind)?;\n+            st.variants = Variants::Single { index: v };\n+\n+            if is_unsafe_cell {\n+                let hide_niches = |scalar: &mut _| match scalar {\n+                    Scalar::Initialized { value, valid_range } => {\n+                        *valid_range = WrappingRange::full(value.size(dl))\n+                    }\n+                    // Already doesn't have any niches\n+                    Scalar::Union { .. } => {}\n+                };\n+                match &mut st.abi {\n+                    Abi::Uninhabited => {}\n+                    Abi::Scalar(scalar) => hide_niches(scalar),\n+                    Abi::ScalarPair(a, b) => {\n+                        hide_niches(a);\n+                        hide_niches(b);\n+                    }\n+                    Abi::Vector { element, count: _ } => hide_niches(element),\n+                    Abi::Aggregate { sized: _ } => {}\n+                }\n+                st.largest_niche = None;\n+                return Some(st);\n+            }\n+\n+            let (start, end) = scalar_valid_range;\n+            match st.abi {\n+                Abi::Scalar(ref mut scalar) | Abi::ScalarPair(ref mut scalar, _) => {\n+                    // the asserts ensure that we are not using the\n+                    // `#[rustc_layout_scalar_valid_range(n)]`\n+                    // attribute to widen the range of anything as that would probably\n+                    // result in UB somewhere\n+                    // FIXME(eddyb) the asserts are probably not needed,\n+                    // as larger validity ranges would result in missed\n+                    // optimizations, *not* wrongly assuming the inner\n+                    // value is valid. e.g. unions enlarge validity ranges,\n+                    // because the values may be uninitialized.\n+                    if let Bound::Included(start) = start {\n+                        // FIXME(eddyb) this might be incorrect - it doesn't\n+                        // account for wrap-around (end < start) ranges.\n+                        let valid_range = scalar.valid_range_mut();\n+                        assert!(valid_range.start <= start);\n+                        valid_range.start = start;\n+                    }\n+                    if let Bound::Included(end) = end {\n+                        // FIXME(eddyb) this might be incorrect - it doesn't\n+                        // account for wrap-around (end < start) ranges.\n+                        let valid_range = scalar.valid_range_mut();\n+                        assert!(valid_range.end >= end);\n+                        valid_range.end = end;\n+                    }\n+\n+                    // Update `largest_niche` if we have introduced a larger niche.\n+                    let niche = Niche::from_scalar(dl, Size::ZERO, *scalar);\n+                    if let Some(niche) = niche {\n+                        match st.largest_niche {\n+                            Some(largest_niche) => {\n+                                // Replace the existing niche even if they're equal,\n+                                // because this one is at a lower offset.\n+                                if largest_niche.available(dl) <= niche.available(dl) {\n+                                    st.largest_niche = Some(niche);\n+                                }\n+                            }\n+                            None => st.largest_niche = Some(niche),\n+                        }\n+                    }\n+                }\n+                _ => assert!(\n+                    start == Bound::Unbounded && end == Bound::Unbounded,\n+                    \"nonscalar layout for layout_scalar_valid_range type: {:#?}\",\n+                    st,\n+                ),\n+            }\n+\n+            return Some(st);\n+        }\n+\n+        // At this point, we have handled all unions and\n+        // structs. (We have also handled univariant enums\n+        // that allow representation optimization.)\n+        assert!(is_enum);\n+\n+        // Until we've decided whether to use the tagged or\n+        // niche filling LayoutS, we don't want to intern the\n+        // variant layouts, so we can't store them in the\n+        // overall LayoutS. Store the overall LayoutS\n+        // and the variant LayoutSs here until then.\n+        struct TmpLayout<V: Idx> {\n+            layout: LayoutS<V>,\n+            variants: IndexVec<V, LayoutS<V>>,\n+        }\n+\n+        let calculate_niche_filling_layout = || -> Option<TmpLayout<V>> {\n+            if niche_optimize_enum {\n+                return None;\n+            }\n+\n+            if variants.len() < 2 {\n+                return None;\n+            }\n+\n+            let mut align = dl.aggregate_align;\n+            let mut variant_layouts = variants\n+                .iter_enumerated()\n+                .map(|(j, v)| {\n+                    let mut st = self.univariant(dl, v, &repr, StructKind::AlwaysSized)?;\n+                    st.variants = Variants::Single { index: j };\n+\n+                    align = align.max(st.align);\n+\n+                    Some(st)\n+                })\n+                .collect::<Option<IndexVec<V, _>>>()?;\n+\n+            let largest_variant_index = variant_layouts\n+                .iter_enumerated()\n+                .max_by_key(|(_i, layout)| layout.size.bytes())\n+                .map(|(i, _layout)| i)?;\n+\n+            let all_indices = (0..=variants.len() - 1).map(V::new);\n+            let needs_disc = |index: V| index != largest_variant_index && !absent(&variants[index]);\n+            let niche_variants = all_indices.clone().find(|v| needs_disc(*v)).unwrap().index()\n+                ..=all_indices.rev().find(|v| needs_disc(*v)).unwrap().index();\n+\n+            let count = niche_variants.size_hint().1.unwrap() as u128;\n+\n+            // Find the field with the largest niche\n+            let (field_index, niche, (niche_start, niche_scalar)) = variants[largest_variant_index]\n+                .iter()\n+                .enumerate()\n+                .filter_map(|(j, field)| Some((j, field.largest_niche?)))\n+                .max_by_key(|(_, niche)| niche.available(dl))\n+                .and_then(|(j, niche)| Some((j, niche, niche.reserve(dl, count)?)))?;\n+            let niche_offset =\n+                niche.offset + variant_layouts[largest_variant_index].fields.offset(field_index);\n+            let niche_size = niche.value.size(dl);\n+            let size = variant_layouts[largest_variant_index].size.align_to(align.abi);\n+\n+            let all_variants_fit = variant_layouts.iter_enumerated_mut().all(|(i, layout)| {\n+                if i == largest_variant_index {\n+                    return true;\n+                }\n+\n+                layout.largest_niche = None;\n+\n+                if layout.size <= niche_offset {\n+                    // This variant will fit before the niche.\n+                    return true;\n+                }\n+\n+                // Determine if it'll fit after the niche.\n+                let this_align = layout.align.abi;\n+                let this_offset = (niche_offset + niche_size).align_to(this_align);\n+\n+                if this_offset + layout.size > size {\n+                    return false;\n+                }\n+\n+                // It'll fit, but we need to make some adjustments.\n+                match layout.fields {\n+                    FieldsShape::Arbitrary { ref mut offsets, .. } => {\n+                        for (j, offset) in offsets.iter_mut().enumerate() {\n+                            if !variants[i][j].is_zst() {\n+                                *offset += this_offset;\n+                            }\n+                        }\n+                    }\n+                    _ => {\n+                        panic!(\"Layout of fields should be Arbitrary for variants\")\n+                    }\n+                }\n+\n+                // It can't be a Scalar or ScalarPair because the offset isn't 0.\n+                if !layout.abi.is_uninhabited() {\n+                    layout.abi = Abi::Aggregate { sized: true };\n+                }\n+                layout.size += this_offset;\n+\n+                true\n+            });\n+\n+            if !all_variants_fit {\n+                return None;\n+            }\n+\n+            let largest_niche = Niche::from_scalar(dl, niche_offset, niche_scalar);\n+\n+            let others_zst = variant_layouts\n+                .iter_enumerated()\n+                .all(|(i, layout)| i == largest_variant_index || layout.size == Size::ZERO);\n+            let same_size = size == variant_layouts[largest_variant_index].size;\n+            let same_align = align == variant_layouts[largest_variant_index].align;\n+\n+            let abi = if variant_layouts.iter().all(|v| v.abi.is_uninhabited()) {\n+                Abi::Uninhabited\n+            } else if same_size && same_align && others_zst {\n+                match variant_layouts[largest_variant_index].abi {\n+                    // When the total alignment and size match, we can use the\n+                    // same ABI as the scalar variant with the reserved niche.\n+                    Abi::Scalar(_) => Abi::Scalar(niche_scalar),\n+                    Abi::ScalarPair(first, second) => {\n+                        // Only the niche is guaranteed to be initialised,\n+                        // so use union layouts for the other primitive.\n+                        if niche_offset == Size::ZERO {\n+                            Abi::ScalarPair(niche_scalar, second.to_union())\n+                        } else {\n+                            Abi::ScalarPair(first.to_union(), niche_scalar)\n+                        }\n+                    }\n+                    _ => Abi::Aggregate { sized: true },\n+                }\n+            } else {\n+                Abi::Aggregate { sized: true }\n+            };\n+\n+            let layout = LayoutS {\n+                variants: Variants::Multiple {\n+                    tag: niche_scalar,\n+                    tag_encoding: TagEncoding::Niche {\n+                        untagged_variant: largest_variant_index,\n+                        niche_variants: (V::new(*niche_variants.start())\n+                            ..=V::new(*niche_variants.end())),\n+                        niche_start,\n+                    },\n+                    tag_field: 0,\n+                    variants: IndexVec::new(),\n+                },\n+                fields: FieldsShape::Arbitrary {\n+                    offsets: vec![niche_offset],\n+                    memory_index: vec![0],\n+                },\n+                abi,\n+                largest_niche,\n+                size,\n+                align,\n+            };\n+\n+            Some(TmpLayout { layout, variants: variant_layouts })\n+        };\n+\n+        let niche_filling_layout = calculate_niche_filling_layout();\n+\n+        let (mut min, mut max) = (i128::MAX, i128::MIN);\n+        let discr_type = repr.discr_type();\n+        let bits = Integer::from_attr(dl, discr_type).size().bits();\n+        for (i, mut val) in discriminants {\n+            if variants[i].iter().any(|f| f.abi.is_uninhabited()) {\n+                continue;\n+            }\n+            if discr_type.is_signed() {\n+                // sign extend the raw representation to be an i128\n+                val = (val << (128 - bits)) >> (128 - bits);\n+            }\n+            if val < min {\n+                min = val;\n+            }\n+            if val > max {\n+                max = val;\n+            }\n+        }\n+        // We might have no inhabited variants, so pretend there's at least one.\n+        if (min, max) == (i128::MAX, i128::MIN) {\n+            min = 0;\n+            max = 0;\n+        }\n+        assert!(min <= max, \"discriminant range is {}...{}\", min, max);\n+        let (min_ity, signed) = discr_range_of_repr(min, max); //Integer::repr_discr(tcx, ty, &repr, min, max);\n+\n+        let mut align = dl.aggregate_align;\n+        let mut size = Size::ZERO;\n+\n+        // We're interested in the smallest alignment, so start large.\n+        let mut start_align = Align::from_bytes(256).unwrap();\n+        assert_eq!(Integer::for_align(dl, start_align), None);\n+\n+        // repr(C) on an enum tells us to make a (tag, union) layout,\n+        // so we need to grow the prefix alignment to be at least\n+        // the alignment of the union. (This value is used both for\n+        // determining the alignment of the overall enum, and the\n+        // determining the alignment of the payload after the tag.)\n+        let mut prefix_align = min_ity.align(dl).abi;\n+        if repr.c() {\n+            for fields in variants {\n+                for field in fields {\n+                    prefix_align = prefix_align.max(field.align.abi);\n+                }\n+            }\n+        }\n+\n+        // Create the set of structs that represent each variant.\n+        let mut layout_variants = variants\n+            .iter_enumerated()\n+            .map(|(i, field_layouts)| {\n+                let mut st = self.univariant(\n+                    dl,\n+                    &field_layouts,\n+                    &repr,\n+                    StructKind::Prefixed(min_ity.size(), prefix_align),\n+                )?;\n+                st.variants = Variants::Single { index: i };\n+                // Find the first field we can't move later\n+                // to make room for a larger discriminant.\n+                for field in st.fields.index_by_increasing_offset().map(|j| &field_layouts[j]) {\n+                    if !field.is_zst() || field.align.abi.bytes() != 1 {\n+                        start_align = start_align.min(field.align.abi);\n+                        break;\n+                    }\n+                }\n+                size = cmp::max(size, st.size);\n+                align = align.max(st.align);\n+                Some(st)\n+            })\n+            .collect::<Option<IndexVec<V, _>>>()?;\n+\n+        // Align the maximum variant size to the largest alignment.\n+        size = size.align_to(align.abi);\n+\n+        if size.bytes() >= dl.obj_size_bound() {\n+            return None;\n+        }\n+\n+        let typeck_ity = Integer::from_attr(dl, repr.discr_type());\n+        if typeck_ity < min_ity {\n+            // It is a bug if Layout decided on a greater discriminant size than typeck for\n+            // some reason at this point (based on values discriminant can take on). Mostly\n+            // because this discriminant will be loaded, and then stored into variable of\n+            // type calculated by typeck. Consider such case (a bug): typeck decided on\n+            // byte-sized discriminant, but layout thinks we need a 16-bit to store all\n+            // discriminant values. That would be a bug, because then, in codegen, in order\n+            // to store this 16-bit discriminant into 8-bit sized temporary some of the\n+            // space necessary to represent would have to be discarded (or layout is wrong\n+            // on thinking it needs 16 bits)\n+            panic!(\n+                \"layout decided on a larger discriminant type ({:?}) than typeck ({:?})\",\n+                min_ity, typeck_ity\n+            );\n+            // However, it is fine to make discr type however large (as an optimisation)\n+            // after this point \u2013 we\u2019ll just truncate the value we load in codegen.\n+        }\n+\n+        // Check to see if we should use a different type for the\n+        // discriminant. We can safely use a type with the same size\n+        // as the alignment of the first field of each variant.\n+        // We increase the size of the discriminant to avoid LLVM copying\n+        // padding when it doesn't need to. This normally causes unaligned\n+        // load/stores and excessive memcpy/memset operations. By using a\n+        // bigger integer size, LLVM can be sure about its contents and\n+        // won't be so conservative.\n+\n+        // Use the initial field alignment\n+        let mut ity = if repr.c() || repr.int.is_some() {\n+            min_ity\n+        } else {\n+            Integer::for_align(dl, start_align).unwrap_or(min_ity)\n+        };\n+\n+        // If the alignment is not larger than the chosen discriminant size,\n+        // don't use the alignment as the final size.\n+        if ity <= min_ity {\n+            ity = min_ity;\n+        } else {\n+            // Patch up the variants' first few fields.\n+            let old_ity_size = min_ity.size();\n+            let new_ity_size = ity.size();\n+            for variant in &mut layout_variants {\n+                match variant.fields {\n+                    FieldsShape::Arbitrary { ref mut offsets, .. } => {\n+                        for i in offsets {\n+                            if *i <= old_ity_size {\n+                                assert_eq!(*i, old_ity_size);\n+                                *i = new_ity_size;\n+                            }\n+                        }\n+                        // We might be making the struct larger.\n+                        if variant.size <= old_ity_size {\n+                            variant.size = new_ity_size;\n+                        }\n+                    }\n+                    _ => panic!(),\n+                }\n+            }\n+        }\n+\n+        let tag_mask = ity.size().unsigned_int_max();\n+        let tag = Scalar::Initialized {\n+            value: Int(ity, signed),\n+            valid_range: WrappingRange {\n+                start: (min as u128 & tag_mask),\n+                end: (max as u128 & tag_mask),\n+            },\n+        };\n+        let mut abi = Abi::Aggregate { sized: true };\n+\n+        if layout_variants.iter().all(|v| v.abi.is_uninhabited()) {\n+            abi = Abi::Uninhabited;\n+        } else if tag.size(dl) == size {\n+            // Make sure we only use scalar layout when the enum is entirely its\n+            // own tag (i.e. it has no padding nor any non-ZST variant fields).\n+            abi = Abi::Scalar(tag);\n+        } else {\n+            // Try to use a ScalarPair for all tagged enums.\n+            let mut common_prim = None;\n+            let mut common_prim_initialized_in_all_variants = true;\n+            for (field_layouts, layout_variant) in iter::zip(&*variants, &layout_variants) {\n+                let FieldsShape::Arbitrary { ref offsets, .. } = layout_variant.fields else {\n+                    panic!();\n+                };\n+                let mut fields = iter::zip(field_layouts, offsets).filter(|p| !p.0.is_zst());\n+                let (field, offset) = match (fields.next(), fields.next()) {\n+                    (None, None) => {\n+                        common_prim_initialized_in_all_variants = false;\n+                        continue;\n+                    }\n+                    (Some(pair), None) => pair,\n+                    _ => {\n+                        common_prim = None;\n+                        break;\n+                    }\n+                };\n+                let prim = match field.abi {\n+                    Abi::Scalar(scalar) => {\n+                        common_prim_initialized_in_all_variants &=\n+                            matches!(scalar, Scalar::Initialized { .. });\n+                        scalar.primitive()\n+                    }\n+                    _ => {\n+                        common_prim = None;\n+                        break;\n+                    }\n+                };\n+                if let Some(pair) = common_prim {\n+                    // This is pretty conservative. We could go fancier\n+                    // by conflating things like i32 and u32, or even\n+                    // realising that (u8, u8) could just cohabit with\n+                    // u16 or even u32.\n+                    if pair != (prim, offset) {\n+                        common_prim = None;\n+                        break;\n+                    }\n+                } else {\n+                    common_prim = Some((prim, offset));\n+                }\n+            }\n+            if let Some((prim, offset)) = common_prim {\n+                let prim_scalar = if common_prim_initialized_in_all_variants {\n+                    scalar_unit(prim)\n+                } else {\n+                    // Common prim might be uninit.\n+                    Scalar::Union { value: prim }\n+                };\n+                let pair = self.scalar_pair::<V>(tag, prim_scalar);\n+                let pair_offsets = match pair.fields {\n+                    FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n+                        assert_eq!(memory_index, &[0, 1]);\n+                        offsets\n+                    }\n+                    _ => panic!(),\n+                };\n+                if pair_offsets[0] == Size::ZERO\n+                    && pair_offsets[1] == *offset\n+                    && align == pair.align\n+                    && size == pair.size\n+                {\n+                    // We can use `ScalarPair` only when it matches our\n+                    // already computed layout (including `#[repr(C)]`).\n+                    abi = pair.abi;\n+                }\n+            }\n+        }\n+\n+        // If we pick a \"clever\" (by-value) ABI, we might have to adjust the ABI of the\n+        // variants to ensure they are consistent. This is because a downcast is\n+        // semantically a NOP, and thus should not affect layout.\n+        if matches!(abi, Abi::Scalar(..) | Abi::ScalarPair(..)) {\n+            for variant in &mut layout_variants {\n+                // We only do this for variants with fields; the others are not accessed anyway.\n+                // Also do not overwrite any already existing \"clever\" ABIs.\n+                if variant.fields.count() > 0 && matches!(variant.abi, Abi::Aggregate { .. }) {\n+                    variant.abi = abi;\n+                    // Also need to bump up the size and alignment, so that the entire value fits in here.\n+                    variant.size = cmp::max(variant.size, size);\n+                    variant.align.abi = cmp::max(variant.align.abi, align.abi);\n+                }\n+            }\n+        }\n+\n+        let largest_niche = Niche::from_scalar(dl, Size::ZERO, tag);\n+\n+        let tagged_layout = LayoutS {\n+            variants: Variants::Multiple {\n+                tag,\n+                tag_encoding: TagEncoding::Direct,\n+                tag_field: 0,\n+                variants: IndexVec::new(),\n+            },\n+            fields: FieldsShape::Arbitrary { offsets: vec![Size::ZERO], memory_index: vec![0] },\n+            largest_niche,\n+            abi,\n+            align,\n+            size,\n+        };\n+\n+        let tagged_layout = TmpLayout { layout: tagged_layout, variants: layout_variants };\n+\n+        let mut best_layout = match (tagged_layout, niche_filling_layout) {\n+            (tl, Some(nl)) => {\n+                // Pick the smaller layout; otherwise,\n+                // pick the layout with the larger niche; otherwise,\n+                // pick tagged as it has simpler codegen.\n+                use cmp::Ordering::*;\n+                let niche_size = |tmp_l: &TmpLayout<V>| {\n+                    tmp_l.layout.largest_niche.map_or(0, |n| n.available(dl))\n+                };\n+                match (tl.layout.size.cmp(&nl.layout.size), niche_size(&tl).cmp(&niche_size(&nl))) {\n+                    (Greater, _) => nl,\n+                    (Equal, Less) => nl,\n+                    _ => tl,\n+                }\n+            }\n+            (tl, None) => tl,\n+        };\n+\n+        // Now we can intern the variant layouts and store them in the enum layout.\n+        best_layout.layout.variants = match best_layout.layout.variants {\n+            Variants::Multiple { tag, tag_encoding, tag_field, .. } => {\n+                Variants::Multiple { tag, tag_encoding, tag_field, variants: best_layout.variants }\n+            }\n+            _ => panic!(),\n+        };\n+        Some(best_layout.layout)\n+    }\n+\n+    fn layout_of_union<'a, V: Idx, F: Deref<Target = &'a LayoutS<V>> + Debug>(\n+        &self,\n+        repr: &ReprOptions,\n+        variants: &IndexVec<V, Vec<F>>,\n+    ) -> Option<LayoutS<V>> {\n+        let dl = self.current_data_layout();\n+        let dl = dl.borrow();\n+        let mut align = if repr.pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n+\n+        if let Some(repr_align) = repr.align {\n+            align = align.max(AbiAndPrefAlign::new(repr_align));\n+        }\n+\n+        let optimize = !repr.inhibit_union_abi_opt();\n+        let mut size = Size::ZERO;\n+        let mut abi = Abi::Aggregate { sized: true };\n+        let index = V::new(0);\n+        for field in &variants[index] {\n+            assert!(!field.is_unsized());\n+            align = align.max(field.align);\n+\n+            // If all non-ZST fields have the same ABI, forward this ABI\n+            if optimize && !field.is_zst() {\n+                // Discard valid range information and allow undef\n+                let field_abi = match field.abi {\n+                    Abi::Scalar(x) => Abi::Scalar(x.to_union()),\n+                    Abi::ScalarPair(x, y) => Abi::ScalarPair(x.to_union(), y.to_union()),\n+                    Abi::Vector { element: x, count } => {\n+                        Abi::Vector { element: x.to_union(), count }\n+                    }\n+                    Abi::Uninhabited | Abi::Aggregate { .. } => Abi::Aggregate { sized: true },\n+                };\n+\n+                if size == Size::ZERO {\n+                    // first non ZST: initialize 'abi'\n+                    abi = field_abi;\n+                } else if abi != field_abi {\n+                    // different fields have different ABI: reset to Aggregate\n+                    abi = Abi::Aggregate { sized: true };\n+                }\n+            }\n+\n+            size = cmp::max(size, field.size);\n+        }\n+\n+        if let Some(pack) = repr.pack {\n+            align = align.min(AbiAndPrefAlign::new(pack));\n+        }\n+\n+        Some(LayoutS {\n+            variants: Variants::Single { index },\n+            fields: FieldsShape::Union(NonZeroUsize::new(variants[index].len())?),\n+            abi,\n+            largest_niche: None,\n+            align,\n+            size: size.align_to(align.abi),\n+        })\n+    }\n+}"}, {"sha": "b6972d914a0eeda527e7ac997abcda7246f33aad", "filename": "compiler/rustc_target/src/abi/mod.rs", "status": "modified", "additions": 144, "deletions": 7, "changes": 151, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -13,6 +13,7 @@ use std::num::{NonZeroUsize, ParseIntError};\n use std::ops::{Add, AddAssign, Deref, Mul, RangeInclusive, Sub};\n use std::str::FromStr;\n \n+use bitflags::bitflags;\n #[cfg(feature = \"nightly\")]\n use rustc_data_structures::intern::Interned;\n use rustc_index::vec::{Idx, IndexVec};\n@@ -22,6 +23,127 @@ use rustc_macros::HashStable_Generic;\n #[cfg(feature = \"nightly\")]\n pub mod call;\n \n+mod layout;\n+\n+pub use layout::LayoutCalculator;\n+\n+bitflags! {\n+    #[derive(Default)]\n+    #[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+    pub struct ReprFlags: u8 {\n+        const IS_C               = 1 << 0;\n+        const IS_SIMD            = 1 << 1;\n+        const IS_TRANSPARENT     = 1 << 2;\n+        // Internal only for now. If true, don't reorder fields.\n+        const IS_LINEAR          = 1 << 3;\n+        // If true, the type's layout can be randomized using\n+        // the seed stored in `ReprOptions.layout_seed`\n+        const RANDOMIZE_LAYOUT   = 1 << 4;\n+        // Any of these flags being set prevent field reordering optimisation.\n+        const IS_UNOPTIMISABLE   = ReprFlags::IS_C.bits\n+                                 | ReprFlags::IS_SIMD.bits\n+                                 | ReprFlags::IS_LINEAR.bits;\n+    }\n+}\n+\n+#[derive(Copy, Clone, Debug, Eq, PartialEq)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub enum IntegerType {\n+    Pointer(bool),\n+    Fixed(Integer, bool),\n+}\n+\n+impl IntegerType {\n+    pub fn is_signed(&self) -> bool {\n+        match self {\n+            IntegerType::Pointer(b) => *b,\n+            IntegerType::Fixed(_, b) => *b,\n+        }\n+    }\n+}\n+\n+/// Represents the repr options provided by the user,\n+#[derive(Copy, Clone, Debug, Eq, PartialEq, Default)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub struct ReprOptions {\n+    pub int: Option<IntegerType>,\n+    pub align: Option<Align>,\n+    pub pack: Option<Align>,\n+    pub flags: ReprFlags,\n+    /// The seed to be used for randomizing a type's layout\n+    ///\n+    /// Note: This could technically be a `[u8; 16]` (a `u128`) which would\n+    /// be the \"most accurate\" hash as it'd encompass the item and crate\n+    /// hash without loss, but it does pay the price of being larger.\n+    /// Everything's a tradeoff, a `u64` seed should be sufficient for our\n+    /// purposes (primarily `-Z randomize-layout`)\n+    pub field_shuffle_seed: u64,\n+}\n+\n+impl ReprOptions {\n+    #[inline]\n+    pub fn simd(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_SIMD)\n+    }\n+\n+    #[inline]\n+    pub fn c(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_C)\n+    }\n+\n+    #[inline]\n+    pub fn packed(&self) -> bool {\n+        self.pack.is_some()\n+    }\n+\n+    #[inline]\n+    pub fn transparent(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_TRANSPARENT)\n+    }\n+\n+    #[inline]\n+    pub fn linear(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_LINEAR)\n+    }\n+\n+    /// Returns the discriminant type, given these `repr` options.\n+    /// This must only be called on enums!\n+    pub fn discr_type(&self) -> IntegerType {\n+        self.int.unwrap_or(IntegerType::Pointer(true))\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhabit \"smart enum\n+    /// layout\" optimizations, such as representing `Foo<&T>` as a\n+    /// single pointer.\n+    pub fn inhibit_enum_layout_opt(&self) -> bool {\n+        self.c() || self.int.is_some()\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhibit struct field reordering\n+    /// optimizations, such as with `repr(C)`, `repr(packed(1))`, or `repr(<int>)`.\n+    pub fn inhibit_struct_field_reordering_opt(&self) -> bool {\n+        if let Some(pack) = self.pack {\n+            if pack.bytes() == 1 {\n+                return true;\n+            }\n+        }\n+\n+        self.flags.intersects(ReprFlags::IS_UNOPTIMISABLE) || self.int.is_some()\n+    }\n+\n+    /// Returns `true` if this type is valid for reordering and `-Z randomize-layout`\n+    /// was enabled for its declaration crate\n+    pub fn can_randomize_type_layout(&self) -> bool {\n+        !self.inhibit_struct_field_reordering_opt()\n+            && self.flags.contains(ReprFlags::RANDOMIZE_LAYOUT)\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhibit union ABI optimisations.\n+    pub fn inhibit_union_abi_opt(&self) -> bool {\n+        self.c()\n+    }\n+}\n+\n /// Parsed [Data layout](https://llvm.org/docs/LangRef.html#data-layout)\n /// for a target, which contains everything needed to compute layouts.\n #[derive(Debug, PartialEq, Eq)]\n@@ -622,7 +744,7 @@ impl AbiAndPrefAlign {\n \n /// Integers, also used for enum discriminants.\n #[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n \n pub enum Integer {\n     I8,\n@@ -644,6 +766,16 @@ impl Integer {\n         }\n     }\n \n+    /// Gets the Integer type from an attr::IntType.\n+    pub fn from_attr<C: HasDataLayout>(cx: &C, ity: IntegerType) -> Integer {\n+        let dl = cx.data_layout();\n+\n+        match ity {\n+            IntegerType::Pointer(_) => dl.ptr_sized_integer(),\n+            IntegerType::Fixed(x, _) => x,\n+        }\n+    }\n+\n     pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n         let dl = cx.data_layout();\n \n@@ -1172,12 +1304,7 @@ pub enum TagEncoding<V: Idx> {\n     /// For example, `Option<(usize, &T)>`  is represented such that\n     /// `None` has a null pointer for the second tuple field, and\n     /// `Some` is the identity function (with a non-null reference).\n-    Niche {\n-        untagged_variant: V,\n-        #[cfg(feature = \"nightly\")]\n-        niche_variants: RangeInclusive<V>,\n-        niche_start: u128,\n-    },\n+    Niche { untagged_variant: V, niche_variants: RangeInclusive<V>, niche_start: u128 },\n }\n \n #[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n@@ -1568,3 +1695,13 @@ impl<V: Idx> LayoutS<V> {\n         }\n     }\n }\n+\n+#[derive(Copy, Clone, Debug)]\n+pub enum StructKind {\n+    /// A tuple, closure, or univariant which cannot be coerced to unsized.\n+    AlwaysSized,\n+    /// A univariant, the last field of which may be coerced to unsized.\n+    MaybeUnsized,\n+    /// A univariant, but with a prefix of an arbitrary size & alignment (e.g., enum tag).\n+    Prefixed(Size, Align),\n+}"}, {"sha": "a432498abcca4001a5498a150796de84300a4644", "filename": "compiler/rustc_traits/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_traits%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_traits%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_traits%2FCargo.toml?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -12,6 +12,7 @@ rustc_hir = { path = \"../rustc_hir\" }\n rustc_index = { path = \"../rustc_index\" }\n rustc_ast = { path = \"../rustc_ast\" }\n rustc_span = { path = \"../rustc_span\" }\n+rustc_target = { path = \"../rustc_target\" }\n chalk-ir = \"0.87.0\"\n chalk-engine = \"0.87.0\"\n chalk-solve = \"0.87.0\""}, {"sha": "344c8b93c170489dfb726ff7b12eabc1cb47025b", "filename": "compiler/rustc_traits/src/chalk/db.rs", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_traits%2Fsrc%2Fchalk%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_traits%2Fsrc%2Fchalk%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_traits%2Fsrc%2Fchalk%2Fdb.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -9,9 +9,9 @@\n use rustc_middle::traits::ChalkRustInterner as RustInterner;\n use rustc_middle::ty::{self, AssocKind, EarlyBinder, Ty, TyCtxt, TypeFoldable, TypeSuperFoldable};\n use rustc_middle::ty::{InternalSubsts, SubstsRef};\n+use rustc_target::abi::{Integer, IntegerType};\n \n use rustc_ast::ast;\n-use rustc_attr as attr;\n \n use rustc_hir::def_id::DefId;\n \n@@ -218,21 +218,21 @@ impl<'tcx> chalk_solve::RustIrDatabase<RustInterner<'tcx>> for RustIrDatabase<'t\n             c: adt_def.repr().c(),\n             packed: adt_def.repr().packed(),\n             int: adt_def.repr().int.map(|i| match i {\n-                attr::IntType::SignedInt(ty) => match ty {\n-                    ast::IntTy::Isize => int(chalk_ir::IntTy::Isize),\n-                    ast::IntTy::I8 => int(chalk_ir::IntTy::I8),\n-                    ast::IntTy::I16 => int(chalk_ir::IntTy::I16),\n-                    ast::IntTy::I32 => int(chalk_ir::IntTy::I32),\n-                    ast::IntTy::I64 => int(chalk_ir::IntTy::I64),\n-                    ast::IntTy::I128 => int(chalk_ir::IntTy::I128),\n+                IntegerType::Pointer(true) => int(chalk_ir::IntTy::Isize),\n+                IntegerType::Pointer(false) => uint(chalk_ir::UintTy::Usize),\n+                IntegerType::Fixed(i, true) => match i {\n+                    Integer::I8 => int(chalk_ir::IntTy::I8),\n+                    Integer::I16 => int(chalk_ir::IntTy::I16),\n+                    Integer::I32 => int(chalk_ir::IntTy::I32),\n+                    Integer::I64 => int(chalk_ir::IntTy::I64),\n+                    Integer::I128 => int(chalk_ir::IntTy::I128),\n                 },\n-                attr::IntType::UnsignedInt(ty) => match ty {\n-                    ast::UintTy::Usize => uint(chalk_ir::UintTy::Usize),\n-                    ast::UintTy::U8 => uint(chalk_ir::UintTy::U8),\n-                    ast::UintTy::U16 => uint(chalk_ir::UintTy::U16),\n-                    ast::UintTy::U32 => uint(chalk_ir::UintTy::U32),\n-                    ast::UintTy::U64 => uint(chalk_ir::UintTy::U64),\n-                    ast::UintTy::U128 => uint(chalk_ir::UintTy::U128),\n+                IntegerType::Fixed(i, false) => match i {\n+                    Integer::I8 => uint(chalk_ir::UintTy::U8),\n+                    Integer::I16 => uint(chalk_ir::UintTy::U16),\n+                    Integer::I32 => uint(chalk_ir::UintTy::U32),\n+                    Integer::I64 => uint(chalk_ir::UintTy::U64),\n+                    Integer::I128 => uint(chalk_ir::UintTy::U128),\n                 },\n             }),\n         })"}, {"sha": "0af8276b246c1bdddf40cdc36c1c61eecd01c7f4", "filename": "compiler/rustc_ty_utils/src/layout.rs", "status": "modified", "additions": 32, "deletions": 940, "changes": 972, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -13,13 +13,8 @@ use rustc_span::symbol::Symbol;\n use rustc_span::DUMMY_SP;\n use rustc_target::abi::*;\n \n-use std::cmp::{self, Ordering};\n+use std::fmt::Debug;\n use std::iter;\n-use std::num::NonZeroUsize;\n-use std::ops::Bound;\n-\n-use rand::{seq::SliceRandom, SeedableRng};\n-use rand_xoshiro::Xoshiro128StarStar;\n \n use crate::layout_sanity_check::sanity_check_layout;\n \n@@ -66,16 +61,6 @@ fn layout_of<'tcx>(\n     Ok(layout)\n }\n \n-#[derive(Copy, Clone, Debug)]\n-enum StructKind {\n-    /// A tuple, closure, or univariant which cannot be coerced to unsized.\n-    AlwaysSized,\n-    /// A univariant, the last field of which may be coerced to unsized.\n-    MaybeUnsized,\n-    /// A univariant, but with a prefix of an arbitrary size & alignment (e.g., enum tag).\n-    Prefixed(Size, Align),\n-}\n-\n // Invert a bijective mapping, i.e. `invert(map)[y] = x` if `map[x] = y`.\n // This is used to go between `memory_index` (source field order to memory order)\n // and `inverse_memory_index` (memory order to source field order).\n@@ -89,37 +74,6 @@ fn invert_mapping(map: &[u32]) -> Vec<u32> {\n     inverse\n }\n \n-fn scalar_pair<'tcx>(\n-    cx: &LayoutCx<'tcx, TyCtxt<'tcx>>,\n-    a: Scalar,\n-    b: Scalar,\n-) -> LayoutS<VariantIdx> {\n-    let dl = cx.data_layout();\n-    let b_align = b.align(dl);\n-    let align = a.align(dl).max(b_align).max(dl.aggregate_align);\n-    let b_offset = a.size(dl).align_to(b_align.abi);\n-    let size = (b_offset + b.size(dl)).align_to(align.abi);\n-\n-    // HACK(nox): We iter on `b` and then `a` because `max_by_key`\n-    // returns the last maximum.\n-    let largest_niche = Niche::from_scalar(dl, b_offset, b)\n-        .into_iter()\n-        .chain(Niche::from_scalar(dl, Size::ZERO, a))\n-        .max_by_key(|niche| niche.available(dl));\n-\n-    LayoutS {\n-        variants: Variants::Single { index: VariantIdx::new(0) },\n-        fields: FieldsShape::Arbitrary {\n-            offsets: vec![Size::ZERO, b_offset],\n-            memory_index: vec![0, 1],\n-        },\n-        abi: Abi::ScalarPair(a, b),\n-        largest_niche,\n-        align,\n-        size,\n-    }\n-}\n-\n fn univariant_uninterned<'tcx>(\n     cx: &LayoutCx<'tcx, TyCtxt<'tcx>>,\n     ty: Ty<'tcx>,\n@@ -134,226 +88,7 @@ fn univariant_uninterned<'tcx>(\n         return Err(LayoutError::Unknown(ty));\n     }\n \n-    let mut align = if pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n-\n-    let mut inverse_memory_index: Vec<u32> = (0..fields.len() as u32).collect();\n-\n-    let optimize = !repr.inhibit_struct_field_reordering_opt();\n-    if optimize {\n-        let end = if let StructKind::MaybeUnsized = kind { fields.len() - 1 } else { fields.len() };\n-        let optimizing = &mut inverse_memory_index[..end];\n-        let effective_field_align = |f: &TyAndLayout<'_>| {\n-            if let Some(pack) = pack {\n-                // return the packed alignment in bytes\n-                f.align.abi.min(pack).bytes()\n-            } else {\n-                // returns log2(effective-align).\n-                // This is ok since `pack` applies to all fields equally.\n-                // The calculation assumes that size is an integer multiple of align, except for ZSTs.\n-                //\n-                // group [u8; 4] with align-4 or [u8; 6] with align-2 fields\n-                f.align.abi.bytes().max(f.size.bytes()).trailing_zeros() as u64\n-            }\n-        };\n-\n-        // If `-Z randomize-layout` was enabled for the type definition we can shuffle\n-        // the field ordering to try and catch some code making assumptions about layouts\n-        // we don't guarantee\n-        if repr.can_randomize_type_layout() {\n-            // `ReprOptions.layout_seed` is a deterministic seed that we can use to\n-            // randomize field ordering with\n-            let mut rng = Xoshiro128StarStar::seed_from_u64(repr.field_shuffle_seed);\n-\n-            // Shuffle the ordering of the fields\n-            optimizing.shuffle(&mut rng);\n-\n-            // Otherwise we just leave things alone and actually optimize the type's fields\n-        } else {\n-            match kind {\n-                StructKind::AlwaysSized | StructKind::MaybeUnsized => {\n-                    optimizing.sort_by_key(|&x| {\n-                        // Place ZSTs first to avoid \"interesting offsets\",\n-                        // especially with only one or two non-ZST fields.\n-                        // Then place largest alignments first, largest niches within an alignment group last\n-                        let f = &fields[x as usize];\n-                        let niche_size = f.largest_niche.map_or(0, |n| n.available(cx));\n-                        (!f.is_zst(), cmp::Reverse(effective_field_align(f)), niche_size)\n-                    });\n-                }\n-\n-                StructKind::Prefixed(..) => {\n-                    // Sort in ascending alignment so that the layout stays optimal\n-                    // regardless of the prefix.\n-                    // And put the largest niche in an alignment group at the end\n-                    // so it can be used as discriminant in jagged enums\n-                    optimizing.sort_by_key(|&x| {\n-                        let f = &fields[x as usize];\n-                        let niche_size = f.largest_niche.map_or(0, |n| n.available(cx));\n-                        (effective_field_align(f), niche_size)\n-                    });\n-                }\n-            }\n-\n-            // FIXME(Kixiron): We can always shuffle fields within a given alignment class\n-            //                 regardless of the status of `-Z randomize-layout`\n-        }\n-    }\n-\n-    // inverse_memory_index holds field indices by increasing memory offset.\n-    // That is, if field 5 has offset 0, the first element of inverse_memory_index is 5.\n-    // We now write field offsets to the corresponding offset slot;\n-    // field 5 with offset 0 puts 0 in offsets[5].\n-    // At the bottom of this function, we invert `inverse_memory_index` to\n-    // produce `memory_index` (see `invert_mapping`).\n-\n-    let mut sized = true;\n-    let mut offsets = vec![Size::ZERO; fields.len()];\n-    let mut offset = Size::ZERO;\n-    let mut largest_niche = None;\n-    let mut largest_niche_available = 0;\n-\n-    if let StructKind::Prefixed(prefix_size, prefix_align) = kind {\n-        let prefix_align =\n-            if let Some(pack) = pack { prefix_align.min(pack) } else { prefix_align };\n-        align = align.max(AbiAndPrefAlign::new(prefix_align));\n-        offset = prefix_size.align_to(prefix_align);\n-    }\n-\n-    for &i in &inverse_memory_index {\n-        let field = fields[i as usize];\n-        if !sized {\n-            cx.tcx.sess.delay_span_bug(\n-                DUMMY_SP,\n-                &format!(\n-                    \"univariant: field #{} of `{}` comes after unsized field\",\n-                    offsets.len(),\n-                    ty\n-                ),\n-            );\n-        }\n-\n-        if field.is_unsized() {\n-            sized = false;\n-        }\n-\n-        // Invariant: offset < dl.obj_size_bound() <= 1<<61\n-        let field_align = if let Some(pack) = pack {\n-            field.align.min(AbiAndPrefAlign::new(pack))\n-        } else {\n-            field.align\n-        };\n-        offset = offset.align_to(field_align.abi);\n-        align = align.max(field_align);\n-\n-        debug!(\"univariant offset: {:?} field: {:#?}\", offset, field);\n-        offsets[i as usize] = offset;\n-\n-        if let Some(mut niche) = field.largest_niche {\n-            let available = niche.available(dl);\n-            if available > largest_niche_available {\n-                largest_niche_available = available;\n-                niche.offset += offset;\n-                largest_niche = Some(niche);\n-            }\n-        }\n-\n-        offset = offset.checked_add(field.size, dl).ok_or(LayoutError::SizeOverflow(ty))?;\n-    }\n-\n-    if let Some(repr_align) = repr.align {\n-        align = align.max(AbiAndPrefAlign::new(repr_align));\n-    }\n-\n-    debug!(\"univariant min_size: {:?}\", offset);\n-    let min_size = offset;\n-\n-    // As stated above, inverse_memory_index holds field indices by increasing offset.\n-    // This makes it an already-sorted view of the offsets vec.\n-    // To invert it, consider:\n-    // If field 5 has offset 0, offsets[0] is 5, and memory_index[5] should be 0.\n-    // Field 5 would be the first element, so memory_index is i:\n-    // Note: if we didn't optimize, it's already right.\n-\n-    let memory_index =\n-        if optimize { invert_mapping(&inverse_memory_index) } else { inverse_memory_index };\n-\n-    let size = min_size.align_to(align.abi);\n-    let mut abi = Abi::Aggregate { sized };\n-\n-    // Unpack newtype ABIs and find scalar pairs.\n-    if sized && size.bytes() > 0 {\n-        // All other fields must be ZSTs.\n-        let mut non_zst_fields = fields.iter().enumerate().filter(|&(_, f)| !f.is_zst());\n-\n-        match (non_zst_fields.next(), non_zst_fields.next(), non_zst_fields.next()) {\n-            // We have exactly one non-ZST field.\n-            (Some((i, field)), None, None) => {\n-                // Field fills the struct and it has a scalar or scalar pair ABI.\n-                if offsets[i].bytes() == 0 && align.abi == field.align.abi && size == field.size {\n-                    match field.abi {\n-                        // For plain scalars, or vectors of them, we can't unpack\n-                        // newtypes for `#[repr(C)]`, as that affects C ABIs.\n-                        Abi::Scalar(_) | Abi::Vector { .. } if optimize => {\n-                            abi = field.abi;\n-                        }\n-                        // But scalar pairs are Rust-specific and get\n-                        // treated as aggregates by C ABIs anyway.\n-                        Abi::ScalarPair(..) => {\n-                            abi = field.abi;\n-                        }\n-                        _ => {}\n-                    }\n-                }\n-            }\n-\n-            // Two non-ZST fields, and they're both scalars.\n-            (Some((i, a)), Some((j, b)), None) => {\n-                match (a.abi, b.abi) {\n-                    (Abi::Scalar(a), Abi::Scalar(b)) => {\n-                        // Order by the memory placement, not source order.\n-                        let ((i, a), (j, b)) = if offsets[i] < offsets[j] {\n-                            ((i, a), (j, b))\n-                        } else {\n-                            ((j, b), (i, a))\n-                        };\n-                        let pair = scalar_pair(cx, a, b);\n-                        let pair_offsets = match pair.fields {\n-                            FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n-                                assert_eq!(memory_index, &[0, 1]);\n-                                offsets\n-                            }\n-                            _ => bug!(),\n-                        };\n-                        if offsets[i] == pair_offsets[0]\n-                            && offsets[j] == pair_offsets[1]\n-                            && align == pair.align\n-                            && size == pair.size\n-                        {\n-                            // We can use `ScalarPair` only when it matches our\n-                            // already computed layout (including `#[repr(C)]`).\n-                            abi = pair.abi;\n-                        }\n-                    }\n-                    _ => {}\n-                }\n-            }\n-\n-            _ => {}\n-        }\n-    }\n-\n-    if fields.iter().any(|f| f.abi.is_uninhabited()) {\n-        abi = Abi::Uninhabited;\n-    }\n-\n-    Ok(LayoutS {\n-        variants: Variants::Single { index: VariantIdx::new(0) },\n-        fields: FieldsShape::Arbitrary { offsets, memory_index },\n-        abi,\n-        largest_niche,\n-        align,\n-        size,\n-    })\n+    cx.univariant(dl, fields, repr, kind).ok_or(LayoutError::SizeOverflow(ty))\n }\n \n fn layout_of_uncached<'tcx>(\n@@ -404,14 +139,7 @@ fn layout_of_uncached<'tcx>(\n         }\n \n         // The never type.\n-        ty::Never => tcx.intern_layout(LayoutS {\n-            variants: Variants::Single { index: VariantIdx::new(0) },\n-            fields: FieldsShape::Primitive,\n-            abi: Abi::Uninhabited,\n-            largest_niche: None,\n-            align: dl.i8_align,\n-            size: Size::ZERO,\n-        }),\n+        ty::Never => tcx.intern_layout(cx.layout_of_never_type()),\n \n         // Potentially-wide pointers.\n         ty::Ref(_, pointee, _) | ty::RawPtr(ty::TypeAndMut { ty: pointee, .. }) => {\n@@ -440,15 +168,15 @@ fn layout_of_uncached<'tcx>(\n             };\n \n             // Effectively a (ptr, meta) tuple.\n-            tcx.intern_layout(scalar_pair(cx, data_ptr, metadata))\n+            tcx.intern_layout(cx.scalar_pair(data_ptr, metadata))\n         }\n \n         ty::Dynamic(_, _, ty::DynStar) => {\n             let mut data = scalar_unit(Int(dl.ptr_sized_integer(), false));\n             data.valid_range_mut().start = 0;\n             let mut vtable = scalar_unit(Pointer);\n             vtable.valid_range_mut().start = 1;\n-            tcx.intern_layout(scalar_pair(cx, data, vtable))\n+            tcx.intern_layout(cx.scalar_pair(data, vtable))\n         }\n \n         // Arrays and slices.\n@@ -677,677 +405,41 @@ fn layout_of_uncached<'tcx>(\n                     return Err(LayoutError::Unknown(ty));\n                 }\n \n-                let mut align =\n-                    if def.repr().pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n-\n-                if let Some(repr_align) = def.repr().align {\n-                    align = align.max(AbiAndPrefAlign::new(repr_align));\n-                }\n-\n-                let optimize = !def.repr().inhibit_union_abi_opt();\n-                let mut size = Size::ZERO;\n-                let mut abi = Abi::Aggregate { sized: true };\n-                let index = VariantIdx::new(0);\n-                for field in &variants[index] {\n-                    assert!(field.is_sized());\n-                    align = align.max(field.align);\n-\n-                    // If all non-ZST fields have the same ABI, forward this ABI\n-                    if optimize && !field.is_zst() {\n-                        // Discard valid range information and allow undef\n-                        let field_abi = match field.abi {\n-                            Abi::Scalar(x) => Abi::Scalar(x.to_union()),\n-                            Abi::ScalarPair(x, y) => Abi::ScalarPair(x.to_union(), y.to_union()),\n-                            Abi::Vector { element: x, count } => {\n-                                Abi::Vector { element: x.to_union(), count }\n-                            }\n-                            Abi::Uninhabited | Abi::Aggregate { .. } => {\n-                                Abi::Aggregate { sized: true }\n-                            }\n-                        };\n-\n-                        if size == Size::ZERO {\n-                            // first non ZST: initialize 'abi'\n-                            abi = field_abi;\n-                        } else if abi != field_abi {\n-                            // different fields have different ABI: reset to Aggregate\n-                            abi = Abi::Aggregate { sized: true };\n-                        }\n-                    }\n-\n-                    size = cmp::max(size, field.size);\n-                }\n-\n-                if let Some(pack) = def.repr().pack {\n-                    align = align.min(AbiAndPrefAlign::new(pack));\n-                }\n-\n-                return Ok(tcx.intern_layout(LayoutS {\n-                    variants: Variants::Single { index },\n-                    fields: FieldsShape::Union(\n-                        NonZeroUsize::new(variants[index].len()).ok_or(LayoutError::Unknown(ty))?,\n-                    ),\n-                    abi,\n-                    largest_niche: None,\n-                    align,\n-                    size: size.align_to(align.abi),\n-                }));\n-            }\n-\n-            // A variant is absent if it's uninhabited and only has ZST fields.\n-            // Present uninhabited variants only require space for their fields,\n-            // but *not* an encoding of the discriminant (e.g., a tag value).\n-            // See issue #49298 for more details on the need to leave space\n-            // for non-ZST uninhabited data (mostly partial initialization).\n-            let absent = |fields: &[TyAndLayout<'_>]| {\n-                let uninhabited = fields.iter().any(|f| f.abi.is_uninhabited());\n-                let is_zst = fields.iter().all(|f| f.is_zst());\n-                uninhabited && is_zst\n-            };\n-            let (present_first, present_second) = {\n-                let mut present_variants = variants\n-                    .iter_enumerated()\n-                    .filter_map(|(i, v)| if absent(v) { None } else { Some(i) });\n-                (present_variants.next(), present_variants.next())\n-            };\n-            let present_first = match present_first {\n-                Some(present_first) => present_first,\n-                // Uninhabited because it has no variants, or only absent ones.\n-                None if def.is_enum() => {\n-                    return Ok(tcx.layout_of(param_env.and(tcx.types.never))?.layout);\n-                }\n-                // If it's a struct, still compute a layout so that we can still compute the\n-                // field offsets.\n-                None => VariantIdx::new(0),\n-            };\n-\n-            let is_struct = !def.is_enum() ||\n-                    // Only one variant is present.\n-                    (present_second.is_none() &&\n-                        // Representation optimizations are allowed.\n-                        !def.repr().inhibit_enum_layout_opt());\n-            if is_struct {\n-                // Struct, or univariant enum equivalent to a struct.\n-                // (Typechecking will reject discriminant-sizing attrs.)\n-\n-                let v = present_first;\n-                let kind = if def.is_enum() || variants[v].is_empty() {\n-                    StructKind::AlwaysSized\n-                } else {\n-                    let param_env = tcx.param_env(def.did());\n-                    let last_field = def.variant(v).fields.last().unwrap();\n-                    let always_sized = tcx.type_of(last_field.did).is_sized(tcx, param_env);\n-                    if !always_sized { StructKind::MaybeUnsized } else { StructKind::AlwaysSized }\n-                };\n-\n-                let mut st = univariant_uninterned(cx, ty, &variants[v], &def.repr(), kind)?;\n-                st.variants = Variants::Single { index: v };\n-\n-                if def.is_unsafe_cell() {\n-                    let hide_niches = |scalar: &mut _| match scalar {\n-                        Scalar::Initialized { value, valid_range } => {\n-                            *valid_range = WrappingRange::full(value.size(dl))\n-                        }\n-                        // Already doesn't have any niches\n-                        Scalar::Union { .. } => {}\n-                    };\n-                    match &mut st.abi {\n-                        Abi::Uninhabited => {}\n-                        Abi::Scalar(scalar) => hide_niches(scalar),\n-                        Abi::ScalarPair(a, b) => {\n-                            hide_niches(a);\n-                            hide_niches(b);\n-                        }\n-                        Abi::Vector { element, count: _ } => hide_niches(element),\n-                        Abi::Aggregate { sized: _ } => {}\n-                    }\n-                    st.largest_niche = None;\n-                    return Ok(tcx.intern_layout(st));\n-                }\n-\n-                let (start, end) = cx.tcx.layout_scalar_valid_range(def.did());\n-                match st.abi {\n-                    Abi::Scalar(ref mut scalar) | Abi::ScalarPair(ref mut scalar, _) => {\n-                        // the asserts ensure that we are not using the\n-                        // `#[rustc_layout_scalar_valid_range(n)]`\n-                        // attribute to widen the range of anything as that would probably\n-                        // result in UB somewhere\n-                        // FIXME(eddyb) the asserts are probably not needed,\n-                        // as larger validity ranges would result in missed\n-                        // optimizations, *not* wrongly assuming the inner\n-                        // value is valid. e.g. unions enlarge validity ranges,\n-                        // because the values may be uninitialized.\n-                        if let Bound::Included(start) = start {\n-                            // FIXME(eddyb) this might be incorrect - it doesn't\n-                            // account for wrap-around (end < start) ranges.\n-                            let valid_range = scalar.valid_range_mut();\n-                            assert!(valid_range.start <= start);\n-                            valid_range.start = start;\n-                        }\n-                        if let Bound::Included(end) = end {\n-                            // FIXME(eddyb) this might be incorrect - it doesn't\n-                            // account for wrap-around (end < start) ranges.\n-                            let valid_range = scalar.valid_range_mut();\n-                            assert!(valid_range.end >= end);\n-                            valid_range.end = end;\n-                        }\n-\n-                        // Update `largest_niche` if we have introduced a larger niche.\n-                        let niche = Niche::from_scalar(dl, Size::ZERO, *scalar);\n-                        if let Some(niche) = niche {\n-                            match st.largest_niche {\n-                                Some(largest_niche) => {\n-                                    // Replace the existing niche even if they're equal,\n-                                    // because this one is at a lower offset.\n-                                    if largest_niche.available(dl) <= niche.available(dl) {\n-                                        st.largest_niche = Some(niche);\n-                                    }\n-                                }\n-                                None => st.largest_niche = Some(niche),\n-                            }\n-                        }\n-                    }\n-                    _ => assert!(\n-                        start == Bound::Unbounded && end == Bound::Unbounded,\n-                        \"nonscalar layout for layout_scalar_valid_range type {:?}: {:#?}\",\n-                        def,\n-                        st,\n-                    ),\n-                }\n-\n-                return Ok(tcx.intern_layout(st));\n-            }\n-\n-            // At this point, we have handled all unions and\n-            // structs. (We have also handled univariant enums\n-            // that allow representation optimization.)\n-            assert!(def.is_enum());\n-\n-            // Until we've decided whether to use the tagged or\n-            // niche filling LayoutS, we don't want to intern the\n-            // variant layouts, so we can't store them in the\n-            // overall LayoutS. Store the overall LayoutS\n-            // and the variant LayoutSs here until then.\n-            struct TmpLayout {\n-                layout: LayoutS<VariantIdx>,\n-                variants: IndexVec<VariantIdx, LayoutS<VariantIdx>>,\n+                return Ok(tcx.intern_layout(\n+                    cx.layout_of_union(&def.repr(), &variants).ok_or(LayoutError::Unknown(ty))?,\n+                ));\n             }\n \n-            let calculate_niche_filling_layout =\n-                || -> Result<Option<TmpLayout>, LayoutError<'tcx>> {\n-                    // The current code for niche-filling relies on variant indices\n-                    // instead of actual discriminants, so enums with\n-                    // explicit discriminants (RFC #2363) would misbehave.\n-                    if def.repr().inhibit_enum_layout_opt()\n+            tcx.intern_layout(\n+                cx.layout_of_struct_or_enum(\n+                    &def.repr(),\n+                    &variants,\n+                    def.is_enum(),\n+                    def.is_unsafe_cell(),\n+                    tcx.layout_scalar_valid_range(def.did()),\n+                    |min, max| Integer::repr_discr(tcx, ty, &def.repr(), min, max),\n+                    def.is_enum()\n+                        .then(|| def.discriminants(tcx).map(|(v, d)| (v, d.val as i128)))\n+                        .into_iter()\n+                        .flatten(),\n+                    def.repr().inhibit_enum_layout_opt()\n                         || def\n                             .variants()\n                             .iter_enumerated()\n-                            .any(|(i, v)| v.discr != ty::VariantDiscr::Relative(i.as_u32()))\n+                            .any(|(i, v)| v.discr != ty::VariantDiscr::Relative(i.as_u32())),\n                     {\n-                        return Ok(None);\n-                    }\n-\n-                    if variants.len() < 2 {\n-                        return Ok(None);\n-                    }\n-\n-                    let mut align = dl.aggregate_align;\n-                    let mut variant_layouts = variants\n-                        .iter_enumerated()\n-                        .map(|(j, v)| {\n-                            let mut st = univariant_uninterned(\n-                                cx,\n-                                ty,\n-                                v,\n-                                &def.repr(),\n-                                StructKind::AlwaysSized,\n-                            )?;\n-                            st.variants = Variants::Single { index: j };\n-\n-                            align = align.max(st.align);\n-\n-                            Ok(st)\n-                        })\n-                        .collect::<Result<IndexVec<VariantIdx, _>, _>>()?;\n-\n-                    let largest_variant_index = match variant_layouts\n-                        .iter_enumerated()\n-                        .max_by_key(|(_i, layout)| layout.size.bytes())\n-                        .map(|(i, _layout)| i)\n-                    {\n-                        None => return Ok(None),\n-                        Some(i) => i,\n-                    };\n-\n-                    let all_indices = VariantIdx::new(0)..=VariantIdx::new(variants.len() - 1);\n-                    let needs_disc = |index: VariantIdx| {\n-                        index != largest_variant_index && !absent(&variants[index])\n-                    };\n-                    let niche_variants = all_indices.clone().find(|v| needs_disc(*v)).unwrap()\n-                        ..=all_indices.rev().find(|v| needs_disc(*v)).unwrap();\n-\n-                    let count = niche_variants.size_hint().1.unwrap() as u128;\n-\n-                    // Find the field with the largest niche\n-                    let (field_index, niche, (niche_start, niche_scalar)) = match variants\n-                        [largest_variant_index]\n-                        .iter()\n-                        .enumerate()\n-                        .filter_map(|(j, field)| Some((j, field.largest_niche?)))\n-                        .max_by_key(|(_, niche)| niche.available(dl))\n-                        .and_then(|(j, niche)| Some((j, niche, niche.reserve(cx, count)?)))\n-                    {\n-                        None => return Ok(None),\n-                        Some(x) => x,\n-                    };\n-\n-                    let niche_offset = niche.offset\n-                        + variant_layouts[largest_variant_index].fields.offset(field_index);\n-                    let niche_size = niche.value.size(dl);\n-                    let size = variant_layouts[largest_variant_index].size.align_to(align.abi);\n-\n-                    let all_variants_fit =\n-                        variant_layouts.iter_enumerated_mut().all(|(i, layout)| {\n-                            if i == largest_variant_index {\n-                                return true;\n-                            }\n-\n-                            layout.largest_niche = None;\n-\n-                            if layout.size <= niche_offset {\n-                                // This variant will fit before the niche.\n-                                return true;\n-                            }\n-\n-                            // Determine if it'll fit after the niche.\n-                            let this_align = layout.align.abi;\n-                            let this_offset = (niche_offset + niche_size).align_to(this_align);\n-\n-                            if this_offset + layout.size > size {\n-                                return false;\n-                            }\n-\n-                            // It'll fit, but we need to make some adjustments.\n-                            match layout.fields {\n-                                FieldsShape::Arbitrary { ref mut offsets, .. } => {\n-                                    for (j, offset) in offsets.iter_mut().enumerate() {\n-                                        if !variants[i][j].is_zst() {\n-                                            *offset += this_offset;\n-                                        }\n-                                    }\n-                                }\n-                                _ => {\n-                                    panic!(\"Layout of fields should be Arbitrary for variants\")\n+                        let param_env = tcx.param_env(def.did());\n+                        def.is_struct()\n+                            && match def.variants().iter().next().and_then(|x| x.fields.last()) {\n+                                Some(last_field) => {\n+                                    tcx.type_of(last_field.did).is_sized(tcx, param_env)\n                                 }\n+                                None => false,\n                             }\n-\n-                            // It can't be a Scalar or ScalarPair because the offset isn't 0.\n-                            if !layout.abi.is_uninhabited() {\n-                                layout.abi = Abi::Aggregate { sized: true };\n-                            }\n-                            layout.size += this_offset;\n-\n-                            true\n-                        });\n-\n-                    if !all_variants_fit {\n-                        return Ok(None);\n-                    }\n-\n-                    let largest_niche = Niche::from_scalar(dl, niche_offset, niche_scalar);\n-\n-                    let others_zst = variant_layouts\n-                        .iter_enumerated()\n-                        .all(|(i, layout)| i == largest_variant_index || layout.size == Size::ZERO);\n-                    let same_size = size == variant_layouts[largest_variant_index].size;\n-                    let same_align = align == variant_layouts[largest_variant_index].align;\n-\n-                    let abi = if variant_layouts.iter().all(|v| v.abi.is_uninhabited()) {\n-                        Abi::Uninhabited\n-                    } else if same_size && same_align && others_zst {\n-                        match variant_layouts[largest_variant_index].abi {\n-                            // When the total alignment and size match, we can use the\n-                            // same ABI as the scalar variant with the reserved niche.\n-                            Abi::Scalar(_) => Abi::Scalar(niche_scalar),\n-                            Abi::ScalarPair(first, second) => {\n-                                // Only the niche is guaranteed to be initialised,\n-                                // so use union layouts for the other primitive.\n-                                if niche_offset == Size::ZERO {\n-                                    Abi::ScalarPair(niche_scalar, second.to_union())\n-                                } else {\n-                                    Abi::ScalarPair(first.to_union(), niche_scalar)\n-                                }\n-                            }\n-                            _ => Abi::Aggregate { sized: true },\n-                        }\n-                    } else {\n-                        Abi::Aggregate { sized: true }\n-                    };\n-\n-                    let layout = LayoutS {\n-                        variants: Variants::Multiple {\n-                            tag: niche_scalar,\n-                            tag_encoding: TagEncoding::Niche {\n-                                untagged_variant: largest_variant_index,\n-                                niche_variants,\n-                                niche_start,\n-                            },\n-                            tag_field: 0,\n-                            variants: IndexVec::new(),\n-                        },\n-                        fields: FieldsShape::Arbitrary {\n-                            offsets: vec![niche_offset],\n-                            memory_index: vec![0],\n-                        },\n-                        abi,\n-                        largest_niche,\n-                        size,\n-                        align,\n-                    };\n-\n-                    Ok(Some(TmpLayout { layout, variants: variant_layouts }))\n-                };\n-\n-            let niche_filling_layout = calculate_niche_filling_layout()?;\n-\n-            let (mut min, mut max) = (i128::MAX, i128::MIN);\n-            let discr_type = def.repr().discr_type();\n-            let bits = Integer::from_attr(cx, discr_type).size().bits();\n-            for (i, discr) in def.discriminants(tcx) {\n-                if variants[i].iter().any(|f| f.abi.is_uninhabited()) {\n-                    continue;\n-                }\n-                let mut x = discr.val as i128;\n-                if discr_type.is_signed() {\n-                    // sign extend the raw representation to be an i128\n-                    x = (x << (128 - bits)) >> (128 - bits);\n-                }\n-                if x < min {\n-                    min = x;\n-                }\n-                if x > max {\n-                    max = x;\n-                }\n-            }\n-            // We might have no inhabited variants, so pretend there's at least one.\n-            if (min, max) == (i128::MAX, i128::MIN) {\n-                min = 0;\n-                max = 0;\n-            }\n-            assert!(min <= max, \"discriminant range is {}...{}\", min, max);\n-            let (min_ity, signed) = Integer::repr_discr(tcx, ty, &def.repr(), min, max);\n-\n-            let mut align = dl.aggregate_align;\n-            let mut size = Size::ZERO;\n-\n-            // We're interested in the smallest alignment, so start large.\n-            let mut start_align = Align::from_bytes(256).unwrap();\n-            assert_eq!(Integer::for_align(dl, start_align), None);\n-\n-            // repr(C) on an enum tells us to make a (tag, union) layout,\n-            // so we need to grow the prefix alignment to be at least\n-            // the alignment of the union. (This value is used both for\n-            // determining the alignment of the overall enum, and the\n-            // determining the alignment of the payload after the tag.)\n-            let mut prefix_align = min_ity.align(dl).abi;\n-            if def.repr().c() {\n-                for fields in &variants {\n-                    for field in fields {\n-                        prefix_align = prefix_align.max(field.align.abi);\n-                    }\n-                }\n-            }\n-\n-            // Create the set of structs that represent each variant.\n-            let mut layout_variants = variants\n-                .iter_enumerated()\n-                .map(|(i, field_layouts)| {\n-                    let mut st = univariant_uninterned(\n-                        cx,\n-                        ty,\n-                        &field_layouts,\n-                        &def.repr(),\n-                        StructKind::Prefixed(min_ity.size(), prefix_align),\n-                    )?;\n-                    st.variants = Variants::Single { index: i };\n-                    // Find the first field we can't move later\n-                    // to make room for a larger discriminant.\n-                    for field in st.fields.index_by_increasing_offset().map(|j| field_layouts[j]) {\n-                        if !field.is_zst() || field.align.abi.bytes() != 1 {\n-                            start_align = start_align.min(field.align.abi);\n-                            break;\n-                        }\n-                    }\n-                    size = cmp::max(size, st.size);\n-                    align = align.max(st.align);\n-                    Ok(st)\n-                })\n-                .collect::<Result<IndexVec<VariantIdx, _>, _>>()?;\n-\n-            // Align the maximum variant size to the largest alignment.\n-            size = size.align_to(align.abi);\n-\n-            if size.bytes() >= dl.obj_size_bound() {\n-                return Err(LayoutError::SizeOverflow(ty));\n-            }\n-\n-            let typeck_ity = Integer::from_attr(dl, def.repr().discr_type());\n-            if typeck_ity < min_ity {\n-                // It is a bug if Layout decided on a greater discriminant size than typeck for\n-                // some reason at this point (based on values discriminant can take on). Mostly\n-                // because this discriminant will be loaded, and then stored into variable of\n-                // type calculated by typeck. Consider such case (a bug): typeck decided on\n-                // byte-sized discriminant, but layout thinks we need a 16-bit to store all\n-                // discriminant values. That would be a bug, because then, in codegen, in order\n-                // to store this 16-bit discriminant into 8-bit sized temporary some of the\n-                // space necessary to represent would have to be discarded (or layout is wrong\n-                // on thinking it needs 16 bits)\n-                bug!(\n-                    \"layout decided on a larger discriminant type ({:?}) than typeck ({:?})\",\n-                    min_ity,\n-                    typeck_ity\n-                );\n-                // However, it is fine to make discr type however large (as an optimisation)\n-                // after this point \u2013 we\u2019ll just truncate the value we load in codegen.\n-            }\n-\n-            // Check to see if we should use a different type for the\n-            // discriminant. We can safely use a type with the same size\n-            // as the alignment of the first field of each variant.\n-            // We increase the size of the discriminant to avoid LLVM copying\n-            // padding when it doesn't need to. This normally causes unaligned\n-            // load/stores and excessive memcpy/memset operations. By using a\n-            // bigger integer size, LLVM can be sure about its contents and\n-            // won't be so conservative.\n-\n-            // Use the initial field alignment\n-            let mut ity = if def.repr().c() || def.repr().int.is_some() {\n-                min_ity\n-            } else {\n-                Integer::for_align(dl, start_align).unwrap_or(min_ity)\n-            };\n-\n-            // If the alignment is not larger than the chosen discriminant size,\n-            // don't use the alignment as the final size.\n-            if ity <= min_ity {\n-                ity = min_ity;\n-            } else {\n-                // Patch up the variants' first few fields.\n-                let old_ity_size = min_ity.size();\n-                let new_ity_size = ity.size();\n-                for variant in &mut layout_variants {\n-                    match variant.fields {\n-                        FieldsShape::Arbitrary { ref mut offsets, .. } => {\n-                            for i in offsets {\n-                                if *i <= old_ity_size {\n-                                    assert_eq!(*i, old_ity_size);\n-                                    *i = new_ity_size;\n-                                }\n-                            }\n-                            // We might be making the struct larger.\n-                            if variant.size <= old_ity_size {\n-                                variant.size = new_ity_size;\n-                            }\n-                        }\n-                        _ => bug!(),\n-                    }\n-                }\n-            }\n-\n-            let tag_mask = ity.size().unsigned_int_max();\n-            let tag = Scalar::Initialized {\n-                value: Int(ity, signed),\n-                valid_range: WrappingRange {\n-                    start: (min as u128 & tag_mask),\n-                    end: (max as u128 & tag_mask),\n-                },\n-            };\n-            let mut abi = Abi::Aggregate { sized: true };\n-\n-            if layout_variants.iter().all(|v| v.abi.is_uninhabited()) {\n-                abi = Abi::Uninhabited;\n-            } else if tag.size(dl) == size {\n-                // Make sure we only use scalar layout when the enum is entirely its\n-                // own tag (i.e. it has no padding nor any non-ZST variant fields).\n-                abi = Abi::Scalar(tag);\n-            } else {\n-                // Try to use a ScalarPair for all tagged enums.\n-                let mut common_prim = None;\n-                let mut common_prim_initialized_in_all_variants = true;\n-                for (field_layouts, layout_variant) in iter::zip(&variants, &layout_variants) {\n-                    let FieldsShape::Arbitrary { ref offsets, .. } = layout_variant.fields else {\n-                            bug!();\n-                        };\n-                    let mut fields = iter::zip(field_layouts, offsets).filter(|p| !p.0.is_zst());\n-                    let (field, offset) = match (fields.next(), fields.next()) {\n-                        (None, None) => {\n-                            common_prim_initialized_in_all_variants = false;\n-                            continue;\n-                        }\n-                        (Some(pair), None) => pair,\n-                        _ => {\n-                            common_prim = None;\n-                            break;\n-                        }\n-                    };\n-                    let prim = match field.abi {\n-                        Abi::Scalar(scalar) => {\n-                            common_prim_initialized_in_all_variants &=\n-                                matches!(scalar, Scalar::Initialized { .. });\n-                            scalar.primitive()\n-                        }\n-                        _ => {\n-                            common_prim = None;\n-                            break;\n-                        }\n-                    };\n-                    if let Some(pair) = common_prim {\n-                        // This is pretty conservative. We could go fancier\n-                        // by conflating things like i32 and u32, or even\n-                        // realising that (u8, u8) could just cohabit with\n-                        // u16 or even u32.\n-                        if pair != (prim, offset) {\n-                            common_prim = None;\n-                            break;\n-                        }\n-                    } else {\n-                        common_prim = Some((prim, offset));\n-                    }\n-                }\n-                if let Some((prim, offset)) = common_prim {\n-                    let prim_scalar = if common_prim_initialized_in_all_variants {\n-                        scalar_unit(prim)\n-                    } else {\n-                        // Common prim might be uninit.\n-                        Scalar::Union { value: prim }\n-                    };\n-                    let pair = scalar_pair(cx, tag, prim_scalar);\n-                    let pair_offsets = match pair.fields {\n-                        FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n-                            assert_eq!(memory_index, &[0, 1]);\n-                            offsets\n-                        }\n-                        _ => bug!(),\n-                    };\n-                    if pair_offsets[0] == Size::ZERO\n-                        && pair_offsets[1] == *offset\n-                        && align == pair.align\n-                        && size == pair.size\n-                    {\n-                        // We can use `ScalarPair` only when it matches our\n-                        // already computed layout (including `#[repr(C)]`).\n-                        abi = pair.abi;\n-                    }\n-                }\n-            }\n-\n-            // If we pick a \"clever\" (by-value) ABI, we might have to adjust the ABI of the\n-            // variants to ensure they are consistent. This is because a downcast is\n-            // semantically a NOP, and thus should not affect layout.\n-            if matches!(abi, Abi::Scalar(..) | Abi::ScalarPair(..)) {\n-                for variant in &mut layout_variants {\n-                    // We only do this for variants with fields; the others are not accessed anyway.\n-                    // Also do not overwrite any already existing \"clever\" ABIs.\n-                    if variant.fields.count() > 0 && matches!(variant.abi, Abi::Aggregate { .. }) {\n-                        variant.abi = abi;\n-                        // Also need to bump up the size and alignment, so that the entire value fits in here.\n-                        variant.size = cmp::max(variant.size, size);\n-                        variant.align.abi = cmp::max(variant.align.abi, align.abi);\n-                    }\n-                }\n-            }\n-\n-            let largest_niche = Niche::from_scalar(dl, Size::ZERO, tag);\n-\n-            let tagged_layout = LayoutS {\n-                variants: Variants::Multiple {\n-                    tag,\n-                    tag_encoding: TagEncoding::Direct,\n-                    tag_field: 0,\n-                    variants: IndexVec::new(),\n-                },\n-                fields: FieldsShape::Arbitrary { offsets: vec![Size::ZERO], memory_index: vec![0] },\n-                largest_niche,\n-                abi,\n-                align,\n-                size,\n-            };\n-\n-            let tagged_layout = TmpLayout { layout: tagged_layout, variants: layout_variants };\n-\n-            let mut best_layout = match (tagged_layout, niche_filling_layout) {\n-                (tl, Some(nl)) => {\n-                    // Pick the smaller layout; otherwise,\n-                    // pick the layout with the larger niche; otherwise,\n-                    // pick tagged as it has simpler codegen.\n-                    use Ordering::*;\n-                    let niche_size = |tmp_l: &TmpLayout| {\n-                        tmp_l.layout.largest_niche.map_or(0, |n| n.available(dl))\n-                    };\n-                    match (\n-                        tl.layout.size.cmp(&nl.layout.size),\n-                        niche_size(&tl).cmp(&niche_size(&nl)),\n-                    ) {\n-                        (Greater, _) => nl,\n-                        (Equal, Less) => nl,\n-                        _ => tl,\n-                    }\n-                }\n-                (tl, None) => tl,\n-            };\n-\n-            // Now we can intern the variant layouts and store them in the enum layout.\n-            best_layout.layout.variants = match best_layout.layout.variants {\n-                Variants::Multiple { tag, tag_encoding, tag_field, .. } => Variants::Multiple {\n-                    tag,\n-                    tag_encoding,\n-                    tag_field,\n-                    variants: best_layout.variants,\n-                },\n-                _ => bug!(),\n-            };\n-\n-            tcx.intern_layout(best_layout.layout)\n+                    },\n+                )\n+                .ok_or(LayoutError::SizeOverflow(ty))?,\n+            )\n         }\n \n         // Types with no meaningful known layout."}, {"sha": "adbcfd3189b76d0fad22532f0e767fe7e63a7932", "filename": "src/tools/clippy/clippy_lints/src/casts/cast_possible_truncation.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fcasts%2Fcast_possible_truncation.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fcasts%2Fcast_possible_truncation.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fcasts%2Fcast_possible_truncation.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -2,12 +2,11 @@ use clippy_utils::consts::{constant, Constant};\n use clippy_utils::diagnostics::span_lint;\n use clippy_utils::expr_or_init;\n use clippy_utils::ty::{get_discriminant_value, is_isize_or_usize};\n-use rustc_ast::ast;\n-use rustc_attr::IntType;\n use rustc_hir::def::{DefKind, Res};\n use rustc_hir::{BinOpKind, Expr, ExprKind};\n use rustc_lint::LateContext;\n use rustc_middle::ty::{self, FloatTy, Ty};\n+use rustc_target::abi::IntegerType;\n \n use super::{utils, CAST_ENUM_TRUNCATION, CAST_POSSIBLE_TRUNCATION};\n \n@@ -122,7 +121,7 @@ pub(super) fn check(cx: &LateContext<'_>, expr: &Expr<'_>, cast_expr: &Expr<'_>,\n             let cast_from_ptr_size = def.repr().int.map_or(true, |ty| {\n                 matches!(\n                     ty,\n-                    IntType::SignedInt(ast::IntTy::Isize) | IntType::UnsignedInt(ast::UintTy::Usize)\n+                    IntegerType::Pointer(_),\n                 )\n             });\n             let suffix = match (cast_from_ptr_size, is_isize_or_usize(cast_to)) {"}, {"sha": "601990cd6a3169776a4e3f0cd8005d07196d18d0", "filename": "src/tools/clippy/clippy_lints/src/lib.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/27fb904d680996fe48e04aef65d4d655bdab843b/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/27fb904d680996fe48e04aef65d4d655bdab843b/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Flib.rs?ref=27fb904d680996fe48e04aef65d4d655bdab843b", "patch": "@@ -26,7 +26,6 @@\n extern crate rustc_arena;\n extern crate rustc_ast;\n extern crate rustc_ast_pretty;\n-extern crate rustc_attr;\n extern crate rustc_data_structures;\n extern crate rustc_driver;\n extern crate rustc_errors;"}]}