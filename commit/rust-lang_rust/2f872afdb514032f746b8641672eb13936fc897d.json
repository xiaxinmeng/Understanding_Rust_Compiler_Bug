{"sha": "2f872afdb514032f746b8641672eb13936fc897d", "node_id": "C_kwDOAAsO6NoAKDJmODcyYWZkYjUxNDAzMmY3NDZiODY0MTY3MmViMTM5MzZmYzg5N2Q", "commit": {"author": {"name": "Thom Chiovoloni", "email": "chiovolonit@gmail.com", "date": "2022-05-11T00:04:26Z"}, "committer": {"name": "Thom Chiovoloni", "email": "thom@shift.click", "date": "2022-07-01T13:21:18Z"}, "message": "Allow arithmetic and certain bitwise ops on AtomicPtr\n\nThis is mainly to support migrating from AtomicUsize, for the strict\nprovenance experiment.\n\nFixes #95492", "tree": {"sha": "ab97498f8ad12357d101eae1f717e4175c81a6dd", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/ab97498f8ad12357d101eae1f717e4175c81a6dd"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/2f872afdb514032f746b8641672eb13936fc897d", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niHUEABYKAB0WIQQszICQ1r0Zqrp7OLPXcz0dendfCgUCYr70zgAKCRDXcz0dendf\nCkHEAP9KNwcYMMSW2Ey2HWqK07yVYZYvPxJkSvVMRbdtiIorTwEAuNMfKZJNPNUM\nxgh5ZM85R+FY1h3Yxu3a42/TKQIU4g0=\n=2qKU\n-----END PGP SIGNATURE-----", "payload": "tree ab97498f8ad12357d101eae1f717e4175c81a6dd\nparent 7425fb293f510a6f138e82a963a3bc599a5b9e1c\nauthor Thom Chiovoloni <chiovolonit@gmail.com> 1652227466 -0700\ncommitter Thom Chiovoloni <thom@shift.click> 1656681678 -0700\n\nAllow arithmetic and certain bitwise ops on AtomicPtr\n\nThis is mainly to support migrating from AtomicUsize, for the strict\nprovenance experiment.\n\nFixes #95492\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/2f872afdb514032f746b8641672eb13936fc897d", "html_url": "https://github.com/rust-lang/rust/commit/2f872afdb514032f746b8641672eb13936fc897d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/2f872afdb514032f746b8641672eb13936fc897d/comments", "author": {"login": "thomcc", "id": 860665, "node_id": "MDQ6VXNlcjg2MDY2NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/860665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomcc", "html_url": "https://github.com/thomcc", "followers_url": "https://api.github.com/users/thomcc/followers", "following_url": "https://api.github.com/users/thomcc/following{/other_user}", "gists_url": "https://api.github.com/users/thomcc/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomcc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomcc/subscriptions", "organizations_url": "https://api.github.com/users/thomcc/orgs", "repos_url": "https://api.github.com/users/thomcc/repos", "events_url": "https://api.github.com/users/thomcc/events{/privacy}", "received_events_url": "https://api.github.com/users/thomcc/received_events", "type": "User", "site_admin": false}, "committer": {"login": "thomcc", "id": 860665, "node_id": "MDQ6VXNlcjg2MDY2NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/860665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomcc", "html_url": "https://github.com/thomcc", "followers_url": "https://api.github.com/users/thomcc/followers", "following_url": "https://api.github.com/users/thomcc/following{/other_user}", "gists_url": "https://api.github.com/users/thomcc/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomcc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomcc/subscriptions", "organizations_url": "https://api.github.com/users/thomcc/orgs", "repos_url": "https://api.github.com/users/thomcc/repos", "events_url": "https://api.github.com/users/thomcc/events{/privacy}", "received_events_url": "https://api.github.com/users/thomcc/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "7425fb293f510a6f138e82a963a3bc599a5b9e1c", "url": "https://api.github.com/repos/rust-lang/rust/commits/7425fb293f510a6f138e82a963a3bc599a5b9e1c", "html_url": "https://github.com/rust-lang/rust/commit/7425fb293f510a6f138e82a963a3bc599a5b9e1c"}], "stats": {"total": 431, "additions": 428, "deletions": 3}, "files": [{"sha": "645afae30d88729b518c338f656330a0670f84bd", "filename": "compiler/rustc_codegen_ssa/src/mir/intrinsic.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/2f872afdb514032f746b8641672eb13936fc897d/compiler%2Frustc_codegen_ssa%2Fsrc%2Fmir%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2f872afdb514032f746b8641672eb13936fc897d/compiler%2Frustc_codegen_ssa%2Fsrc%2Fmir%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_codegen_ssa%2Fsrc%2Fmir%2Fintrinsic.rs?ref=2f872afdb514032f746b8641672eb13936fc897d", "patch": "@@ -513,9 +513,7 @@ impl<'a, 'tcx, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         };\n \n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, bx.tcx()).is_some()\n-                            || (ty.is_unsafe_ptr() && op == \"xchg\")\n-                        {\n+                        if int_type_width_signed(ty, bx.tcx()).is_some() || ty.is_unsafe_ptr() {\n                             let mut ptr = args[0].immediate();\n                             let mut val = args[1].immediate();\n                             if ty.is_unsafe_ptr() {"}, {"sha": "bb6d82ff13de3890d3e06533573e1d549f1a5525", "filename": "library/core/src/sync/atomic.rs", "status": "modified", "additions": 341, "deletions": 0, "changes": 341, "blob_url": "https://github.com/rust-lang/rust/blob/2f872afdb514032f746b8641672eb13936fc897d/library%2Fcore%2Fsrc%2Fsync%2Fatomic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2f872afdb514032f746b8641672eb13936fc897d/library%2Fcore%2Fsrc%2Fsync%2Fatomic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Fsrc%2Fsync%2Fatomic.rs?ref=2f872afdb514032f746b8641672eb13936fc897d", "patch": "@@ -1451,6 +1451,347 @@ impl<T> AtomicPtr<T> {\n         }\n         Err(prev)\n     }\n+\n+    /// Offsets the pointer's address by adding `val` (in units of `T`),\n+    /// returning the previous pointer.\n+    ///\n+    /// This is equivalent to using [`wrapping_add`] to atomically perform the\n+    /// equivalent of `ptr = ptr.wrapping_add(val);`.\n+    ///\n+    /// This method operates in units of `T`, which means that it cannot be used\n+    /// to offset the pointer by an amount which is not a multiple of\n+    /// `size_of::<T>()`. This can sometimes be inconvenient, as you may want to\n+    /// work with a deliberately misaligned pointer. In such cases, you may use\n+    /// the [`fetch_add_bytes`](Self::fetch_add_bytes) method instead.\n+    ///\n+    /// `fetch_add` takes an [`Ordering`] argument which describes the memory\n+    /// ordering of this operation. All ordering modes are possible. Note that\n+    /// using [`Acquire`] makes the store part of this operation [`Relaxed`],\n+    /// and using [`Release`] makes the load part [`Relaxed`].\n+    ///\n+    /// **Note**: This method is only available on platforms that support atomic\n+    /// operations on [`AtomicPtr`].\n+    ///\n+    /// [`wrapping_add`]: pointer::wrapping_add\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// #![feature(strict_provenance_atomic_ptr, strict_provenance)]\n+    /// use core::sync::atomic::{AtomicPtr, Ordering};\n+    ///\n+    /// let atom = AtomicPtr::<i64>::new(core::ptr::null_mut());\n+    /// assert_eq!(atom.fetch_add(1, Ordering::Relaxed).addr(), 0);\n+    /// // Note: units of `size_of::<i64>()`.\n+    /// assert_eq!(atom.load(Ordering::Relaxed).addr(), 8);\n+    /// ```\n+    #[inline]\n+    #[cfg(target_has_atomic = \"ptr\")]\n+    #[unstable(feature = \"strict_provenance_atomic_ptr\", issue = \"95228\")]\n+    pub fn fetch_add(&self, val: usize, order: Ordering) -> *mut T {\n+        self.fetch_add_bytes(val.wrapping_mul(core::mem::size_of::<T>()), order)\n+    }\n+\n+    /// Offsets the pointer's address by subtracting `val` (in units of `T`),\n+    /// returning the previous pointer.\n+    ///\n+    /// This is equivalent to using [`wrapping_sub`] to atomically perform the\n+    /// equivalent of `ptr = ptr.wrapping_sub(val);`.\n+    ///\n+    /// This method operates in units of `T`, which means that it cannot be used\n+    /// to offset the pointer by an amount which is not a multiple of\n+    /// `size_of::<T>()`. This can sometimes be inconvenient, as you may want to\n+    /// work with a deliberately misaligned pointer. In such cases, you may use\n+    /// the [`fetch_sub_bytes`](Self::fetch_sub_bytes) method instead.\n+    ///\n+    /// `fetch_sub` takes an [`Ordering`] argument which describes the memory\n+    /// ordering of this operation. All ordering modes are possible. Note that\n+    /// using [`Acquire`] makes the store part of this operation [`Relaxed`],\n+    /// and using [`Release`] makes the load part [`Relaxed`].\n+    ///\n+    /// **Note**: This method is only available on platforms that support atomic\n+    /// operations on [`AtomicPtr`].\n+    ///\n+    /// [`wrapping_sub`]: pointer::wrapping_sub\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// #![feature(strict_provenance_atomic_ptr)]\n+    /// use core::sync::atomic::{AtomicPtr, Ordering};\n+    ///\n+    /// let array = [1i32, 2i32];\n+    /// let atom = AtomicPtr::new(array.as_ptr().wrapping_add(1) as *mut _);\n+    ///\n+    /// assert!(core::ptr::eq(\n+    ///     atom.fetch_sub(1, Ordering::Relaxed),\n+    ///     &array[1],\n+    /// ));\n+    /// assert!(core::ptr::eq(atom.load(Ordering::Relaxed), &array[0]));\n+    /// ```\n+    #[inline]\n+    #[cfg(target_has_atomic = \"ptr\")]\n+    #[unstable(feature = \"strict_provenance_atomic_ptr\", issue = \"95228\")]\n+    pub fn fetch_sub(&self, val: usize, order: Ordering) -> *mut T {\n+        self.fetch_sub_bytes(val.wrapping_mul(core::mem::size_of::<T>()), order)\n+    }\n+\n+    /// Offsets the pointer's address by adding `val` *bytes*, returning the\n+    /// previous pointer.\n+    ///\n+    /// This is equivalent to using [`wrapping_add`] and [`cast`] to atomically\n+    /// perform `ptr = ptr.cast::<u8>().wrapping_add(val).cast::<T>()`.\n+    ///\n+    /// `fetch_add_bytes` takes an [`Ordering`] argument which describes the\n+    /// memory ordering of this operation. All ordering modes are possible. Note\n+    /// that using [`Acquire`] makes the store part of this operation\n+    /// [`Relaxed`], and using [`Release`] makes the load part [`Relaxed`].\n+    ///\n+    /// **Note**: This method is only available on platforms that support atomic\n+    /// operations on [`AtomicPtr`].\n+    ///\n+    /// [`wrapping_add`]: pointer::wrapping_add\n+    /// [`cast`]: pointer::cast\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// #![feature(strict_provenance_atomic_ptr, strict_provenance)]\n+    /// use core::sync::atomic::{AtomicPtr, Ordering};\n+    ///\n+    /// let atom = AtomicPtr::<i64>::new(core::ptr::null_mut());\n+    /// assert_eq!(atom.fetch_add_bytes(1, Ordering::Relaxed).addr(), 0);\n+    /// // Note: in units of bytes, not `size_of::<i64>()`.\n+    /// assert_eq!(atom.load(Ordering::Relaxed).addr(), 1);\n+    /// ```\n+    #[inline]\n+    #[cfg(target_has_atomic = \"ptr\")]\n+    #[unstable(feature = \"strict_provenance_atomic_ptr\", issue = \"95228\")]\n+    pub fn fetch_add_bytes(&self, val: usize, order: Ordering) -> *mut T {\n+        #[cfg(not(bootstrap))]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_add(self.p.get(), core::ptr::invalid_mut(val), order).cast()\n+        }\n+        #[cfg(bootstrap)]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_add(self.p.get().cast::<usize>(), val, order) as *mut T\n+        }\n+    }\n+\n+    /// Offsets the pointer's address by subtracting `val` *bytes*, returning the\n+    /// previous pointer.\n+    ///\n+    /// This is equivalent to using [`wrapping_sub`] and [`cast`] to atomically\n+    /// perform `ptr = ptr.cast::<u8>().wrapping_sub(val).cast::<T>()`.\n+    ///\n+    /// `fetch_add_bytes` takes an [`Ordering`] argument which describes the\n+    /// memory ordering of this operation. All ordering modes are possible. Note\n+    /// that using [`Acquire`] makes the store part of this operation\n+    /// [`Relaxed`], and using [`Release`] makes the load part [`Relaxed`].\n+    ///\n+    /// **Note**: This method is only available on platforms that support atomic\n+    /// operations on [`AtomicPtr`].\n+    ///\n+    /// [`wrapping_sub`]: pointer::wrapping_sub\n+    /// [`cast`]: pointer::cast\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// #![feature(strict_provenance_atomic_ptr, strict_provenance)]\n+    /// use core::sync::atomic::{AtomicPtr, Ordering};\n+    ///\n+    /// let atom = AtomicPtr::<i64>::new(core::ptr::invalid_mut(1));\n+    /// assert_eq!(atom.fetch_sub_bytes(1, Ordering::Relaxed).addr(), 1);\n+    /// assert_eq!(atom.load(Ordering::Relaxed).addr(), 0);\n+    /// ```\n+    #[inline]\n+    #[cfg(target_has_atomic = \"ptr\")]\n+    #[unstable(feature = \"strict_provenance_atomic_ptr\", issue = \"95228\")]\n+    pub fn fetch_sub_bytes(&self, val: usize, order: Ordering) -> *mut T {\n+        #[cfg(not(bootstrap))]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_sub(self.p.get(), core::ptr::invalid_mut(val), order).cast()\n+        }\n+        #[cfg(bootstrap)]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_sub(self.p.get().cast::<usize>(), val, order) as *mut T\n+        }\n+    }\n+\n+    /// Performs a bitwise \"or\" operation on the address of the current pointer,\n+    /// and the argument `val`, and stores a pointer with provenance of the\n+    /// current pointer and the resulting address.\n+    ///\n+    /// This is equivalent equivalent to using [`map_addr`] to atomically\n+    /// perform `ptr = ptr.map_addr(|a| a | val)`. This can be used in tagged\n+    /// pointer schemes to atomically set tag bits.\n+    ///\n+    /// **Caveat**: This operation returns the previous value. To compute the\n+    /// stored value without losing provenance, you may use [`map_addr`]. For\n+    /// example: `a.fetch_or(val).map_addr(|a| a | val)`.\n+    ///\n+    /// `fetch_or` takes an [`Ordering`] argument which describes the memory\n+    /// ordering of this operation. All ordering modes are possible. Note that\n+    /// using [`Acquire`] makes the store part of this operation [`Relaxed`],\n+    /// and using [`Release`] makes the load part [`Relaxed`].\n+    ///\n+    /// **Note**: This method is only available on platforms that support atomic\n+    /// operations on [`AtomicPtr`].\n+    ///\n+    /// This API and its claimed semantics are part of the Strict Provenance\n+    /// experiment, see the [module documentation for `ptr`][crate::ptr] for\n+    /// details.\n+    ///\n+    /// [`map_addr`]: pointer::map_addr\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// #![feature(strict_provenance_atomic_ptr, strict_provenance)]\n+    /// use core::sync::atomic::{AtomicPtr, Ordering};\n+    ///\n+    /// let pointer = &mut 3i64 as *mut i64;\n+    ///\n+    /// let atom = AtomicPtr::<i64>::new(pointer);\n+    /// // Tag the bottom bit of the pointer.\n+    /// assert_eq!(atom.fetch_or(1, Ordering::Relaxed).addr() & 1, 0);\n+    /// // Extract and untag.\n+    /// let tagged = atom.load(Ordering::Relaxed);\n+    /// assert_eq!(tagged.addr() & 1, 1);\n+    /// assert_eq!(tagged.map_addr(|p| p & !1), pointer);\n+    /// ```\n+    #[inline]\n+    #[cfg(target_has_atomic = \"ptr\")]\n+    #[unstable(feature = \"strict_provenance_atomic_ptr\", issue = \"95228\")]\n+    pub fn fetch_or(&self, val: usize, order: Ordering) -> *mut T {\n+        #[cfg(not(bootstrap))]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_or(self.p.get(), core::ptr::invalid_mut(val), order).cast()\n+        }\n+        #[cfg(bootstrap)]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_or(self.p.get().cast::<usize>(), val, order) as *mut T\n+        }\n+    }\n+\n+    /// Performs a bitwise \"and\" operation on the address of the current\n+    /// pointer, and the argument `val`, and stores a pointer with provenance of\n+    /// the current pointer and the resulting address.\n+    ///\n+    /// This is equivalent equivalent to using [`map_addr`] to atomically\n+    /// perform `ptr = ptr.map_addr(|a| a & val)`. This can be used in tagged\n+    /// pointer schemes to atomically unset tag bits.\n+    ///\n+    /// **Caveat**: This operation returns the previous value. To compute the\n+    /// stored value without losing provenance, you may use [`map_addr`]. For\n+    /// example: `a.fetch_and(val).map_addr(|a| a & val)`.\n+    ///\n+    /// `fetch_and` takes an [`Ordering`] argument which describes the memory\n+    /// ordering of this operation. All ordering modes are possible. Note that\n+    /// using [`Acquire`] makes the store part of this operation [`Relaxed`],\n+    /// and using [`Release`] makes the load part [`Relaxed`].\n+    ///\n+    /// **Note**: This method is only available on platforms that support atomic\n+    /// operations on [`AtomicPtr`].\n+    ///\n+    /// This API and its claimed semantics are part of the Strict Provenance\n+    /// experiment, see the [module documentation for `ptr`][crate::ptr] for\n+    /// details.\n+    ///\n+    /// [`map_addr`]: pointer::map_addr\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// #![feature(strict_provenance_atomic_ptr, strict_provenance)]\n+    /// use core::sync::atomic::{AtomicPtr, Ordering};\n+    ///\n+    /// let pointer = &mut 3i64 as *mut i64;\n+    /// // A tagged pointer\n+    /// let atom = AtomicPtr::<i64>::new(pointer.map_addr(|a| a | 1));\n+    /// assert_eq!(atom.fetch_or(1, Ordering::Relaxed).addr() & 1, 1);\n+    /// // Untag, and extract the previously tagged pointer.\n+    /// let untagged = atom.fetch_and(!1, Ordering::Relaxed)\n+    ///     .map_addr(|a| a & !1);\n+    /// assert_eq!(untagged, pointer);\n+    /// ```\n+    #[inline]\n+    #[cfg(target_has_atomic = \"ptr\")]\n+    #[unstable(feature = \"strict_provenance_atomic_ptr\", issue = \"95228\")]\n+    pub fn fetch_and(&self, val: usize, order: Ordering) -> *mut T {\n+        #[cfg(not(bootstrap))]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_and(self.p.get(), core::ptr::invalid_mut(val), order).cast()\n+        }\n+        #[cfg(bootstrap)]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_and(self.p.get().cast::<usize>(), val, order) as *mut T\n+        }\n+    }\n+\n+    /// Performs a bitwise \"xor\" operation on the address of the current\n+    /// pointer, and the argument `val`, and stores a pointer with provenance of\n+    /// the current pointer and the resulting address.\n+    ///\n+    /// This is equivalent equivalent to using [`map_addr`] to atomically\n+    /// perform `ptr = ptr.map_addr(|a| a ^ val)`. This can be used in tagged\n+    /// pointer schemes to atomically toggle tag bits.\n+    ///\n+    /// **Caveat**: This operation returns the previous value. To compute the\n+    /// stored value without losing provenance, you may use [`map_addr`]. For\n+    /// example: `a.fetch_xor(val).map_addr(|a| a ^ val)`.\n+    ///\n+    /// `fetch_xor` takes an [`Ordering`] argument which describes the memory\n+    /// ordering of this operation. All ordering modes are possible. Note that\n+    /// using [`Acquire`] makes the store part of this operation [`Relaxed`],\n+    /// and using [`Release`] makes the load part [`Relaxed`].\n+    ///\n+    /// **Note**: This method is only available on platforms that support atomic\n+    /// operations on [`AtomicPtr`].\n+    ///\n+    /// This API and its claimed semantics are part of the Strict Provenance\n+    /// experiment, see the [module documentation for `ptr`][crate::ptr] for\n+    /// details.\n+    ///\n+    /// [`map_addr`]: pointer::map_addr\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// #![feature(strict_provenance_atomic_ptr, strict_provenance)]\n+    /// use core::sync::atomic::{AtomicPtr, Ordering};\n+    ///\n+    /// let pointer = &mut 3i64 as *mut i64;\n+    /// let atom = AtomicPtr::<i64>::new(pointer);\n+    ///\n+    /// // Toggle a tag bit on the pointer.\n+    /// atom.fetch_xor(1, Ordering::Relaxed);\n+    /// assert_eq!(atom.load(Ordering::Relaxed).addr() & 1, 1);\n+    /// ```\n+    #[inline]\n+    #[cfg(target_has_atomic = \"ptr\")]\n+    #[unstable(feature = \"strict_provenance_atomic_ptr\", issue = \"95228\")]\n+    pub fn fetch_xor(&self, val: usize, order: Ordering) -> *mut T {\n+        #[cfg(not(bootstrap))]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_xor(self.p.get(), core::ptr::invalid_mut(val), order).cast()\n+        }\n+        #[cfg(bootstrap)]\n+        // SAFETY: data races are prevented by atomic intrinsics.\n+        unsafe {\n+            atomic_xor(self.p.get().cast::<usize>(), val, order) as *mut T\n+        }\n+    }\n }\n \n #[cfg(target_has_atomic_load_store = \"8\")]"}, {"sha": "2c048435dde9776870b9ebc40fc010471047d08c", "filename": "library/core/tests/atomic.rs", "status": "modified", "additions": 85, "deletions": 0, "changes": 85, "blob_url": "https://github.com/rust-lang/rust/blob/2f872afdb514032f746b8641672eb13936fc897d/library%2Fcore%2Ftests%2Fatomic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2f872afdb514032f746b8641672eb13936fc897d/library%2Fcore%2Ftests%2Fatomic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Ftests%2Fatomic.rs?ref=2f872afdb514032f746b8641672eb13936fc897d", "patch": "@@ -127,6 +127,91 @@ fn int_max() {\n     assert_eq!(x.load(SeqCst), 0xf731);\n }\n \n+#[test]\n+#[cfg(any(not(target_arch = \"arm\"), target_os = \"linux\"))] // Missing intrinsic in compiler-builtins\n+fn ptr_add_null() {\n+    let atom = AtomicPtr::<i64>::new(core::ptr::null_mut());\n+    assert_eq!(atom.fetch_add(1, SeqCst).addr(), 0);\n+    assert_eq!(atom.load(SeqCst).addr(), 8);\n+\n+    assert_eq!(atom.fetch_add_bytes(1, SeqCst).addr(), 8);\n+    assert_eq!(atom.load(SeqCst).addr(), 9);\n+\n+    assert_eq!(atom.fetch_sub(1, SeqCst).addr(), 9);\n+    assert_eq!(atom.load(SeqCst).addr(), 1);\n+\n+    assert_eq!(atom.fetch_sub_bytes(1, SeqCst).addr(), 1);\n+    assert_eq!(atom.load(SeqCst).addr(), 0);\n+}\n+\n+#[test]\n+#[cfg(any(not(target_arch = \"arm\"), target_os = \"linux\"))] // Missing intrinsic in compiler-builtins\n+fn ptr_add_data() {\n+    let num = 0i64;\n+    let n = &num as *const i64 as *mut _;\n+    let atom = AtomicPtr::<i64>::new(n);\n+    assert_eq!(atom.fetch_add(1, SeqCst), n);\n+    assert_eq!(atom.load(SeqCst), n.wrapping_add(1));\n+\n+    assert_eq!(atom.fetch_sub(1, SeqCst), n.wrapping_add(1));\n+    assert_eq!(atom.load(SeqCst), n);\n+    let bytes_from_n = |b| n.cast::<u8>().wrapping_add(b).cast::<i64>();\n+\n+    assert_eq!(atom.fetch_add_bytes(1, SeqCst), n);\n+    assert_eq!(atom.load(SeqCst), bytes_from_n(1));\n+\n+    assert_eq!(atom.fetch_add_bytes(5, SeqCst), bytes_from_n(1));\n+    assert_eq!(atom.load(SeqCst), bytes_from_n(6));\n+\n+    assert_eq!(atom.fetch_sub_bytes(1, SeqCst), bytes_from_n(6));\n+    assert_eq!(atom.load(SeqCst), bytes_from_n(5));\n+\n+    assert_eq!(atom.fetch_sub_bytes(5, SeqCst), bytes_from_n(5));\n+    assert_eq!(atom.load(SeqCst), n);\n+}\n+\n+#[test]\n+#[cfg(any(not(target_arch = \"arm\"), target_os = \"linux\"))] // Missing intrinsic in compiler-builtins\n+fn ptr_bitops() {\n+    let atom = AtomicPtr::<i64>::new(core::ptr::null_mut());\n+    assert_eq!(atom.fetch_or(0b0111, SeqCst).addr(), 0);\n+    assert_eq!(atom.load(SeqCst).addr(), 0b0111);\n+\n+    assert_eq!(atom.fetch_and(0b1101, SeqCst).addr(), 0b0111);\n+    assert_eq!(atom.load(SeqCst).addr(), 0b0101);\n+\n+    assert_eq!(atom.fetch_xor(0b1111, SeqCst).addr(), 0b0101);\n+    assert_eq!(atom.load(SeqCst).addr(), 0b1010);\n+}\n+\n+#[test]\n+#[cfg(any(not(target_arch = \"arm\"), target_os = \"linux\"))] // Missing intrinsic in compiler-builtins\n+fn ptr_bitops_tagging() {\n+    #[repr(align(16))]\n+    struct Tagme(u128);\n+\n+    let tagme = Tagme(1000);\n+    let ptr = &tagme as *const Tagme as *mut Tagme;\n+    let atom: AtomicPtr<Tagme> = AtomicPtr::new(ptr);\n+\n+    const MASK_TAG: usize = 0b1111;\n+    const MASK_PTR: usize = !MASK_TAG;\n+\n+    assert_eq!(ptr.addr() & MASK_TAG, 0);\n+\n+    assert_eq!(atom.fetch_or(0b0111, SeqCst), ptr);\n+    assert_eq!(atom.load(SeqCst), ptr.map_addr(|a| a | 0b111));\n+\n+    assert_eq!(atom.fetch_and(MASK_PTR | 0b0010, SeqCst), ptr.map_addr(|a| a | 0b111));\n+    assert_eq!(atom.load(SeqCst), ptr.map_addr(|a| a | 0b0010));\n+\n+    assert_eq!(atom.fetch_xor(0b1011, SeqCst), ptr.map_addr(|a| a | 0b0010));\n+    assert_eq!(atom.load(SeqCst), ptr.map_addr(|a| a | 0b1001));\n+\n+    assert_eq!(atom.fetch_and(MASK_PTR, SeqCst), ptr.map_addr(|a| a | 0b1001));\n+    assert_eq!(atom.load(SeqCst), ptr);\n+}\n+\n static S_FALSE: AtomicBool = AtomicBool::new(false);\n static S_TRUE: AtomicBool = AtomicBool::new(true);\n static S_INT: AtomicIsize = AtomicIsize::new(0);"}, {"sha": "fe89dd8c88d97614085f27468000ec72feabf503", "filename": "library/core/tests/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/2f872afdb514032f746b8641672eb13936fc897d/library%2Fcore%2Ftests%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2f872afdb514032f746b8641672eb13936fc897d/library%2Fcore%2Ftests%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Ftests%2Flib.rs?ref=2f872afdb514032f746b8641672eb13936fc897d", "patch": "@@ -90,6 +90,7 @@\n #![feature(slice_group_by)]\n #![feature(split_array)]\n #![feature(strict_provenance)]\n+#![feature(strict_provenance_atomic_ptr)]\n #![feature(trusted_random_access)]\n #![feature(unsize)]\n #![feature(unzip_option)]"}]}