{"sha": "f6f96e2a8757717ca8d701d26d37b067e95bb584", "node_id": "MDY6Q29tbWl0NzI0NzEyOmY2Zjk2ZTJhODc1NzcxN2NhOGQ3MDFkMjZkMzdiMDY3ZTk1YmI1ODQ=", "commit": {"author": {"name": "Tyson Nottingham", "email": "tgnottingham@gmail.com", "date": "2020-10-03T02:34:01Z"}, "committer": {"name": "Tyson Nottingham", "email": "tgnottingham@gmail.com", "date": "2020-10-03T17:03:30Z"}, "message": "perf: buffer SipHasher128", "tree": {"sha": "e278bf61a905a16308b5d547a9fb41789c6214c5", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/e278bf61a905a16308b5d547a9fb41789c6214c5"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f6f96e2a8757717ca8d701d26d37b067e95bb584", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f6f96e2a8757717ca8d701d26d37b067e95bb584", "html_url": "https://github.com/rust-lang/rust/commit/f6f96e2a8757717ca8d701d26d37b067e95bb584", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f6f96e2a8757717ca8d701d26d37b067e95bb584/comments", "author": {"login": "tgnottingham", "id": 3668166, "node_id": "MDQ6VXNlcjM2NjgxNjY=", "avatar_url": "https://avatars.githubusercontent.com/u/3668166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgnottingham", "html_url": "https://github.com/tgnottingham", "followers_url": "https://api.github.com/users/tgnottingham/followers", "following_url": "https://api.github.com/users/tgnottingham/following{/other_user}", "gists_url": "https://api.github.com/users/tgnottingham/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgnottingham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgnottingham/subscriptions", "organizations_url": "https://api.github.com/users/tgnottingham/orgs", "repos_url": "https://api.github.com/users/tgnottingham/repos", "events_url": "https://api.github.com/users/tgnottingham/events{/privacy}", "received_events_url": "https://api.github.com/users/tgnottingham/received_events", "type": "User", "site_admin": false}, "committer": {"login": "tgnottingham", "id": 3668166, "node_id": "MDQ6VXNlcjM2NjgxNjY=", "avatar_url": "https://avatars.githubusercontent.com/u/3668166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgnottingham", "html_url": "https://github.com/tgnottingham", "followers_url": "https://api.github.com/users/tgnottingham/followers", "following_url": "https://api.github.com/users/tgnottingham/following{/other_user}", "gists_url": "https://api.github.com/users/tgnottingham/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgnottingham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgnottingham/subscriptions", "organizations_url": "https://api.github.com/users/tgnottingham/orgs", "repos_url": "https://api.github.com/users/tgnottingham/repos", "events_url": "https://api.github.com/users/tgnottingham/events{/privacy}", "received_events_url": "https://api.github.com/users/tgnottingham/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "6ebad43c255e63ac0734646fe817de5780d76b45", "url": "https://api.github.com/repos/rust-lang/rust/commits/6ebad43c255e63ac0734646fe817de5780d76b45", "html_url": "https://github.com/rust-lang/rust/commit/6ebad43c255e63ac0734646fe817de5780d76b45"}], "stats": {"total": 545, "additions": 349, "deletions": 196}, "files": [{"sha": "20c68227bd9675719ae0acb90a56e04602557253", "filename": "compiler/rustc_data_structures/src/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/f6f96e2a8757717ca8d701d26d37b067e95bb584/compiler%2Frustc_data_structures%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f6f96e2a8757717ca8d701d26d37b067e95bb584/compiler%2Frustc_data_structures%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_data_structures%2Fsrc%2Flib.rs?ref=f6f96e2a8757717ca8d701d26d37b067e95bb584", "patch": "@@ -28,6 +28,7 @@\n #![feature(const_panic)]\n #![feature(min_const_generics)]\n #![feature(once_cell)]\n+#![feature(maybe_uninit_uninit_array)]\n #![allow(rustc::default_hash_types)]\n \n #[macro_use]"}, {"sha": "4acb0e69e9951f3444a447f74688ed119c314c6b", "filename": "compiler/rustc_data_structures/src/sip128.rs", "status": "modified", "additions": 303, "deletions": 196, "changes": 499, "blob_url": "https://github.com/rust-lang/rust/blob/f6f96e2a8757717ca8d701d26d37b067e95bb584/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f6f96e2a8757717ca8d701d26d37b067e95bb584/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs?ref=f6f96e2a8757717ca8d701d26d37b067e95bb584", "patch": "@@ -1,21 +1,24 @@\n //! This is a copy of `core::hash::sip` adapted to providing 128 bit hashes.\n \n-use std::cmp;\n use std::hash::Hasher;\n-use std::mem;\n+use std::mem::{self, MaybeUninit};\n use std::ptr;\n \n #[cfg(test)]\n mod tests;\n \n+const BUFFER_SIZE_ELEMS: usize = 8;\n+const BUFFER_SIZE_BYTES: usize = BUFFER_SIZE_ELEMS * mem::size_of::<u64>();\n+const BUFFER_SIZE_ELEMS_SPILL: usize = BUFFER_SIZE_ELEMS + 1;\n+const BUFFER_SIZE_BYTES_SPILL: usize = BUFFER_SIZE_ELEMS_SPILL * mem::size_of::<u64>();\n+const BUFFER_SPILL_INDEX: usize = BUFFER_SIZE_ELEMS_SPILL - 1;\n+\n #[derive(Debug, Clone)]\n pub struct SipHasher128 {\n-    k0: u64,\n-    k1: u64,\n-    length: usize, // how many bytes we've processed\n-    state: State,  // hash State\n-    tail: u64,     // unprocessed bytes le\n-    ntail: usize,  // how many bytes in tail are valid\n+    nbuf: usize,                                      // how many bytes in buf are valid\n+    buf: [MaybeUninit<u64>; BUFFER_SIZE_ELEMS_SPILL], // unprocessed bytes le\n+    state: State,                                     // hash State\n+    processed: usize,                                 // how many bytes we've processed\n }\n \n #[derive(Debug, Clone, Copy)]\n@@ -51,271 +54,375 @@ macro_rules! compress {\n     }};\n }\n \n-/// Loads an integer of the desired type from a byte stream, in LE order. Uses\n-/// `copy_nonoverlapping` to let the compiler generate the most efficient way\n-/// to load it from a possibly unaligned address.\n-///\n-/// Unsafe because: unchecked indexing at i..i+size_of(int_ty)\n-macro_rules! load_int_le {\n-    ($buf:expr, $i:expr, $int_ty:ident) => {{\n-        debug_assert!($i + mem::size_of::<$int_ty>() <= $buf.len());\n-        let mut data = 0 as $int_ty;\n-        ptr::copy_nonoverlapping(\n-            $buf.get_unchecked($i),\n-            &mut data as *mut _ as *mut u8,\n-            mem::size_of::<$int_ty>(),\n-        );\n-        data.to_le()\n-    }};\n-}\n-\n-/// Loads a u64 using up to 7 bytes of a byte slice. It looks clumsy but the\n-/// `copy_nonoverlapping` calls that occur (via `load_int_le!`) all have fixed\n-/// sizes and avoid calling `memcpy`, which is good for speed.\n-///\n-/// Unsafe because: unchecked indexing at start..start+len\n+// Copies up to 8 bytes from source to destination. This may be faster than\n+// calling `ptr::copy_nonoverlapping` with an arbitrary count, since all of\n+// the copies have fixed sizes and thus avoid calling memcpy.\n #[inline]\n-unsafe fn u8to64_le(buf: &[u8], start: usize, len: usize) -> u64 {\n-    debug_assert!(len < 8);\n-    let mut i = 0; // current byte index (from LSB) in the output u64\n-    let mut out = 0;\n-    if i + 3 < len {\n-        out = load_int_le!(buf, start + i, u32) as u64;\n+unsafe fn copy_nonoverlapping_small(src: *const u8, dst: *mut u8, count: usize) {\n+    debug_assert!(count <= 8);\n+\n+    if count == 8 {\n+        ptr::copy_nonoverlapping(src, dst, 8);\n+        return;\n+    }\n+\n+    let mut i = 0;\n+    if i + 3 < count {\n+        ptr::copy_nonoverlapping(src.add(i), dst.add(i), 4);\n         i += 4;\n     }\n-    if i + 1 < len {\n-        out |= (load_int_le!(buf, start + i, u16) as u64) << (i * 8);\n+\n+    if i + 1 < count {\n+        ptr::copy_nonoverlapping(src.add(i), dst.add(i), 2);\n         i += 2\n     }\n-    if i < len {\n-        out |= (*buf.get_unchecked(start + i) as u64) << (i * 8);\n+\n+    if i < count {\n+        *dst.add(i) = *src.add(i);\n         i += 1;\n     }\n-    debug_assert_eq!(i, len);\n-    out\n+\n+    debug_assert_eq!(i, count);\n }\n \n+// Implementation\n+//\n+// This implementation uses buffering to reduce the hashing cost for inputs\n+// consisting of many small integers. Buffering simplifies the integration of\n+// integer input--the integer write function typically just appends to the\n+// buffer with a statically sized write, updates metadata, and returns.\n+//\n+// Buffering also prevents alternating between writes that do and do not trigger\n+// the hashing process. Only when the entire buffer is full do we transition\n+// into hashing. This allows us to keep the hash state in registers for longer,\n+// instead of loading and storing it before and after processing each element.\n+//\n+// When a write fills the buffer, a buffer processing function is invoked to\n+// hash all of the buffered input. The buffer processing functions are marked\n+// #[inline(never)] so that they aren't inlined into the append functions, which\n+// ensures the more frequently called append functions remain inlineable and\n+// don't include register pushing/popping that would only be made necessary by\n+// inclusion of the complex buffer processing path which uses those registers.\n+//\n+// The buffer includes a \"spill\"--an extra element at the end--which simplifies\n+// the integer write buffer processing path. The value that fills the buffer can\n+// be written with a statically sized write that may spill over into the spill.\n+// After the buffer is processed, the part of the value that spilled over can\n+// written from the spill to the beginning of the buffer with another statically\n+// sized write. Due to static sizes, this scheme performs better than copying\n+// the exact number of bytes needed into the end and beginning of the buffer.\n+//\n+// The buffer is uninitialized, which improves performance, but may preclude\n+// efficient implementation of alternative approaches. The improvement is not so\n+// large that an alternative approach should be disregarded because it cannot be\n+// efficiently implemented with an uninitialized buffer. On the other hand, an\n+// uninitialized buffer may become more important should a larger one be used.\n+//\n+// Platform Dependence\n+//\n+// The SipHash algorithm operates on byte sequences. It parses the input stream\n+// as 8-byte little-endian integers. Therefore, given the same byte sequence, it\n+// produces the same result on big- and little-endian hardware.\n+//\n+// However, the Hasher trait has methods which operate on multi-byte integers.\n+// How they are converted into byte sequences can be endian-dependent (by using\n+// native byte order) or independent (by consistently using either LE or BE byte\n+// order). It can also be `isize` and `usize` size dependent (by using the\n+// native size), or independent (by converting to a common size), supposing the\n+// values can be represented in 32 bits.\n+//\n+// In order to make SipHasher128 consistent with SipHasher in libstd, we choose\n+// to do the integer to byte sequence conversion in the platform-dependent way.\n+// Clients can achieve (nearly) platform-independent hashing by widening `isize`\n+// and `usize` integers to 64 bits on 32-bit systems and byte-swapping integers\n+// on big-endian systems before passing them to the writing functions. This\n+// causes the input byte sequence to look identical on big- and little- endian\n+// systems (supposing `isize` and `usize` values can be represented in 32 bits),\n+// which ensures platform-independent results.\n impl SipHasher128 {\n     #[inline]\n     pub fn new_with_keys(key0: u64, key1: u64) -> SipHasher128 {\n-        let mut state = SipHasher128 {\n-            k0: key0,\n-            k1: key1,\n-            length: 0,\n-            state: State { v0: 0, v1: 0, v2: 0, v3: 0 },\n-            tail: 0,\n-            ntail: 0,\n+        let mut hasher = SipHasher128 {\n+            nbuf: 0,\n+            buf: MaybeUninit::uninit_array(),\n+            state: State {\n+                v0: key0 ^ 0x736f6d6570736575,\n+                // The XOR with 0xee is only done on 128-bit algorithm version.\n+                v1: key1 ^ (0x646f72616e646f6d ^ 0xee),\n+                v2: key0 ^ 0x6c7967656e657261,\n+                v3: key1 ^ 0x7465646279746573,\n+            },\n+            processed: 0,\n         };\n-        state.reset();\n-        state\n+\n+        unsafe {\n+            // Initialize spill because we read from it in short_write_process_buffer.\n+            *hasher.buf.get_unchecked_mut(BUFFER_SPILL_INDEX) = MaybeUninit::zeroed();\n+        }\n+\n+        hasher\n     }\n \n+    // A specialized write function for values with size <= 8.\n     #[inline]\n-    fn reset(&mut self) {\n-        self.length = 0;\n-        self.state.v0 = self.k0 ^ 0x736f6d6570736575;\n-        self.state.v1 = self.k1 ^ 0x646f72616e646f6d;\n-        self.state.v2 = self.k0 ^ 0x6c7967656e657261;\n-        self.state.v3 = self.k1 ^ 0x7465646279746573;\n-        self.ntail = 0;\n-\n-        // This is only done in the 128 bit version:\n-        self.state.v1 ^= 0xee;\n+    fn short_write<T>(&mut self, x: T) {\n+        let size = mem::size_of::<T>();\n+        let nbuf = self.nbuf;\n+        debug_assert!(size <= 8);\n+        debug_assert!(nbuf < BUFFER_SIZE_BYTES);\n+        debug_assert!(nbuf + size < BUFFER_SIZE_BYTES_SPILL);\n+\n+        if nbuf + size < BUFFER_SIZE_BYTES {\n+            unsafe {\n+                // The memcpy call is optimized away because the size is known.\n+                let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n+                ptr::copy_nonoverlapping(&x as *const _ as *const u8, dst, size);\n+            }\n+\n+            self.nbuf = nbuf + size;\n+\n+            return;\n+        }\n+\n+        unsafe { self.short_write_process_buffer(x) }\n     }\n \n-    // A specialized write function for values with size <= 8.\n-    //\n-    // The input must be zero-extended to 64-bits by the caller. This extension\n-    // isn't hashed, but the implementation requires it for correctness.\n-    //\n-    // This function, given the same integer size and value, has the same effect\n-    // on both little- and big-endian hardware. It operates on values without\n-    // depending on their sequence in memory, so is independent of endianness.\n-    //\n-    // However, we want SipHasher128 to be platform-dependent, in order to be\n-    // consistent with the platform-dependent SipHasher in libstd. In other\n-    // words, we want:\n-    //\n-    // - little-endian: `write_u32(0xDDCCBBAA)` == `write([0xAA, 0xBB, 0xCC, 0xDD])`\n-    // - big-endian:    `write_u32(0xDDCCBBAA)` == `write([0xDD, 0xCC, 0xBB, 0xAA])`\n-    //\n-    // Therefore, in order to produce endian-dependent results, SipHasher128's\n-    // `write_xxx` Hasher trait methods byte-swap `x` prior to zero-extending.\n+    // A specialized write function for values with size <= 8 that should only\n+    // be called when the write would cause the buffer to fill.\n     //\n-    // If clients of SipHasher128 itself want platform-independent results, they\n-    // *also* must byte-swap integer inputs before invoking the `write_xxx`\n-    // methods on big-endian hardware (that is, two byte-swaps must occur--one\n-    // in the client, and one in SipHasher128). Additionally, they must extend\n-    // `usize` and `isize` types to 64 bits on 32-bit systems.\n-    #[inline]\n-    fn short_write<T>(&mut self, _x: T, x: u64) {\n+    // SAFETY: the write of x into self.buf starting at byte offset self.nbuf\n+    // must cause self.buf to become fully initialized (and not overflow) if it\n+    // wasn't already.\n+    #[inline(never)]\n+    unsafe fn short_write_process_buffer<T>(&mut self, x: T) {\n         let size = mem::size_of::<T>();\n-        self.length += size;\n-\n-        // The original number must be zero-extended, not sign-extended.\n-        debug_assert!(if size < 8 { x >> (8 * size) == 0 } else { true });\n-\n-        // The number of bytes needed to fill `self.tail`.\n-        let needed = 8 - self.ntail;\n-\n-        // SipHash parses the input stream as 8-byte little-endian integers.\n-        // Inputs are put into `self.tail` until 8 bytes of data have been\n-        // collected, and then that word is processed.\n-        //\n-        // For example, imagine that `self.tail` is 0x0000_00EE_DDCC_BBAA,\n-        // `self.ntail` is 5 (because 5 bytes have been put into `self.tail`),\n-        // and `needed` is therefore 3.\n-        //\n-        // - Scenario 1, `self.write_u8(0xFF)`: we have already zero-extended\n-        //   the input to 0x0000_0000_0000_00FF. We now left-shift it five\n-        //   bytes, giving 0x0000_FF00_0000_0000. We then bitwise-OR that value\n-        //   into `self.tail`, resulting in 0x0000_FFEE_DDCC_BBAA.\n-        //   (Zero-extension of the original input is critical in this scenario\n-        //   because we don't want the high two bytes of `self.tail` to be\n-        //   touched by the bitwise-OR.) `self.tail` is not yet full, so we\n-        //   return early, after updating `self.ntail` to 6.\n-        //\n-        // - Scenario 2, `self.write_u32(0xIIHH_GGFF)`: we have already\n-        //   zero-extended the input to 0x0000_0000_IIHH_GGFF. We now\n-        //   left-shift it five bytes, giving 0xHHGG_FF00_0000_0000. We then\n-        //   bitwise-OR that value into `self.tail`, resulting in\n-        //   0xHHGG_FFEE_DDCC_BBAA. `self.tail` is now full, and we can use it\n-        //   to update `self.state`. (As mentioned above, this assumes a\n-        //   little-endian machine; on a big-endian machine we would have\n-        //   byte-swapped 0xIIHH_GGFF in the caller, giving 0xFFGG_HHII, and we\n-        //   would then end up bitwise-ORing 0xGGHH_II00_0000_0000 into\n-        //   `self.tail`).\n-        //\n-        self.tail |= x << (8 * self.ntail);\n-        if size < needed {\n-            self.ntail += size;\n+        let nbuf = self.nbuf;\n+        debug_assert!(size <= 8);\n+        debug_assert!(nbuf < BUFFER_SIZE_BYTES);\n+        debug_assert!(nbuf + size >= BUFFER_SIZE_BYTES);\n+        debug_assert!(nbuf + size < BUFFER_SIZE_BYTES_SPILL);\n+\n+        // Copy first part of input into end of buffer, possibly into spill\n+        // element. The memcpy call is optimized away because the size is known.\n+        let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n+        ptr::copy_nonoverlapping(&x as *const _ as *const u8, dst, size);\n+\n+        // Process buffer.\n+        for i in 0..BUFFER_SIZE_ELEMS {\n+            let elem = self.buf.get_unchecked(i).assume_init().to_le();\n+            self.state.v3 ^= elem;\n+            Sip24Rounds::c_rounds(&mut self.state);\n+            self.state.v0 ^= elem;\n+        }\n+\n+        // Copy remaining input into start of buffer by copying size - 1\n+        // elements from spill (at most size - 1 bytes could have overflowed\n+        // into the spill). The memcpy call is optimized away because the size\n+        // is known. And the whole copy is optimized away for size == 1.\n+        let src = self.buf.get_unchecked(BUFFER_SPILL_INDEX) as *const _ as *const u8;\n+        ptr::copy_nonoverlapping(src, self.buf.as_mut_ptr() as *mut u8, size - 1);\n+\n+        // This function should only be called when the write fills the buffer.\n+        // Therefore, when size == 1, the new self.nbuf must be zero. The size\n+        // is statically known, so the branch is optimized away.\n+        self.nbuf = if size == 1 { 0 } else { nbuf + size - BUFFER_SIZE_BYTES };\n+        self.processed += BUFFER_SIZE_BYTES;\n+    }\n+\n+    // A write function for byte slices.\n+    #[inline]\n+    fn slice_write(&mut self, msg: &[u8]) {\n+        let length = msg.len();\n+        let nbuf = self.nbuf;\n+        debug_assert!(nbuf < BUFFER_SIZE_BYTES);\n+\n+        if nbuf + length < BUFFER_SIZE_BYTES {\n+            unsafe {\n+                let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n+\n+                if length < 8 {\n+                    copy_nonoverlapping_small(msg.as_ptr(), dst, length);\n+                } else {\n+                    // This memcpy is *not* optimized away.\n+                    ptr::copy_nonoverlapping(msg.as_ptr(), dst, length);\n+                }\n+            }\n+\n+            self.nbuf = nbuf + length;\n+\n             return;\n         }\n \n-        // `self.tail` is full, process it.\n-        self.state.v3 ^= self.tail;\n-        Sip24Rounds::c_rounds(&mut self.state);\n-        self.state.v0 ^= self.tail;\n-\n-        // Continuing scenario 2: we have one byte left over from the input. We\n-        // set `self.ntail` to 1 and `self.tail` to `0x0000_0000_IIHH_GGFF >>\n-        // 8*3`, which is 0x0000_0000_0000_00II. (Or on a big-endian machine\n-        // the prior byte-swapping would leave us with 0x0000_0000_0000_00FF.)\n-        //\n-        // The `if` is needed to avoid shifting by 64 bits, which Rust\n-        // complains about.\n-        self.ntail = size - needed;\n-        self.tail = if needed < 8 { x >> (8 * needed) } else { 0 };\n+        unsafe { self.slice_write_process_buffer(msg) }\n+    }\n+\n+    // A write function for byte slices that should only be called when the\n+    // write would cause the buffer to fill.\n+    //\n+    // SAFETY: self.buf must be initialized up to the byte offset self.nbuf, and\n+    // msg must contain enough bytes to initialize the rest of the element\n+    // containing the byte offset self.nbuf.\n+    #[inline(never)]\n+    unsafe fn slice_write_process_buffer(&mut self, msg: &[u8]) {\n+        let length = msg.len();\n+        let nbuf = self.nbuf;\n+        debug_assert!(nbuf < BUFFER_SIZE_BYTES);\n+        debug_assert!(nbuf + length >= BUFFER_SIZE_BYTES);\n+\n+        // Always copy first part of input into current element of buffer.\n+        // This function should only be called when the write fills the buffer,\n+        // so we know that there is enough input to fill the current element.\n+        let valid_in_elem = nbuf & 0x7;\n+        let needed_in_elem = 8 - valid_in_elem;\n+\n+        let src = msg.as_ptr();\n+        let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n+        copy_nonoverlapping_small(src, dst, needed_in_elem);\n+\n+        // Process buffer.\n+\n+        // Using nbuf / 8 + 1 rather than (nbuf + needed_in_elem) / 8 to show\n+        // the compiler that this loop's upper bound is > 0. We know that is\n+        // true, because last step ensured we have a full element in the buffer.\n+        let last = nbuf / 8 + 1;\n+\n+        for i in 0..last {\n+            let elem = self.buf.get_unchecked(i).assume_init().to_le();\n+            self.state.v3 ^= elem;\n+            Sip24Rounds::c_rounds(&mut self.state);\n+            self.state.v0 ^= elem;\n+        }\n+\n+        // Process the remaining u64-sized chunks of input.\n+        let mut processed = needed_in_elem;\n+        let input_left = length - processed;\n+        let u64s_left = input_left / 8;\n+        let u8s_left = input_left & 0x7;\n+\n+        for _ in 0..u64s_left {\n+            let elem = (msg.as_ptr().add(processed) as *const u64).read_unaligned().to_le();\n+            self.state.v3 ^= elem;\n+            Sip24Rounds::c_rounds(&mut self.state);\n+            self.state.v0 ^= elem;\n+            processed += 8;\n+        }\n+\n+        // Copy remaining input into start of buffer.\n+        let src = msg.as_ptr().add(processed);\n+        let dst = self.buf.as_mut_ptr() as *mut u8;\n+        copy_nonoverlapping_small(src, dst, u8s_left);\n+\n+        self.nbuf = u8s_left;\n+        self.processed += nbuf + processed;\n     }\n \n     #[inline]\n     pub fn finish128(mut self) -> (u64, u64) {\n-        let b: u64 = ((self.length as u64 & 0xff) << 56) | self.tail;\n+        debug_assert!(self.nbuf < BUFFER_SIZE_BYTES);\n+\n+        // Process full elements in buffer.\n+        let last = self.nbuf / 8;\n+\n+        // Since we're consuming self, avoid updating members for a potential\n+        // performance gain.\n+        let mut state = self.state;\n \n-        self.state.v3 ^= b;\n-        Sip24Rounds::c_rounds(&mut self.state);\n-        self.state.v0 ^= b;\n+        for i in 0..last {\n+            let elem = unsafe { self.buf.get_unchecked(i).assume_init().to_le() };\n+            state.v3 ^= elem;\n+            Sip24Rounds::c_rounds(&mut state);\n+            state.v0 ^= elem;\n+        }\n+\n+        // Get remaining partial element.\n+        let elem = if self.nbuf % 8 != 0 {\n+            unsafe {\n+                // Ensure element is initialized by writing zero bytes. At most\n+                // seven are required given the above check. It's safe to write\n+                // this many because we have the spill element and we maintain\n+                // self.nbuf such that this write will start before the spill.\n+                let dst = (self.buf.as_mut_ptr() as *mut u8).add(self.nbuf);\n+                ptr::write_bytes(dst, 0, 7);\n+                self.buf.get_unchecked(last).assume_init().to_le()\n+            }\n+        } else {\n+            0\n+        };\n+\n+        // Finalize the hash.\n+        let length = self.processed + self.nbuf;\n+        let b: u64 = ((length as u64 & 0xff) << 56) | elem;\n \n-        self.state.v2 ^= 0xee;\n-        Sip24Rounds::d_rounds(&mut self.state);\n-        let _0 = self.state.v0 ^ self.state.v1 ^ self.state.v2 ^ self.state.v3;\n+        state.v3 ^= b;\n+        Sip24Rounds::c_rounds(&mut state);\n+        state.v0 ^= b;\n+\n+        state.v2 ^= 0xee;\n+        Sip24Rounds::d_rounds(&mut state);\n+        let _0 = state.v0 ^ state.v1 ^ state.v2 ^ state.v3;\n+\n+        state.v1 ^= 0xdd;\n+        Sip24Rounds::d_rounds(&mut state);\n+        let _1 = state.v0 ^ state.v1 ^ state.v2 ^ state.v3;\n \n-        self.state.v1 ^= 0xdd;\n-        Sip24Rounds::d_rounds(&mut self.state);\n-        let _1 = self.state.v0 ^ self.state.v1 ^ self.state.v2 ^ self.state.v3;\n         (_0, _1)\n     }\n }\n \n impl Hasher for SipHasher128 {\n     #[inline]\n     fn write_u8(&mut self, i: u8) {\n-        self.short_write(i, i as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_u16(&mut self, i: u16) {\n-        self.short_write(i, i.to_le() as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_u32(&mut self, i: u32) {\n-        self.short_write(i, i.to_le() as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_u64(&mut self, i: u64) {\n-        self.short_write(i, i.to_le() as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_usize(&mut self, i: usize) {\n-        self.short_write(i, i.to_le() as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_i8(&mut self, i: i8) {\n-        self.short_write(i, i as u8 as u64);\n+        self.short_write(i as u8);\n     }\n \n     #[inline]\n     fn write_i16(&mut self, i: i16) {\n-        self.short_write(i, (i as u16).to_le() as u64);\n+        self.short_write(i as u16);\n     }\n \n     #[inline]\n     fn write_i32(&mut self, i: i32) {\n-        self.short_write(i, (i as u32).to_le() as u64);\n+        self.short_write(i as u32);\n     }\n \n     #[inline]\n     fn write_i64(&mut self, i: i64) {\n-        self.short_write(i, (i as u64).to_le() as u64);\n+        self.short_write(i as u64);\n     }\n \n     #[inline]\n     fn write_isize(&mut self, i: isize) {\n-        self.short_write(i, (i as usize).to_le() as u64);\n+        self.short_write(i as usize);\n     }\n \n     #[inline]\n     fn write(&mut self, msg: &[u8]) {\n-        let length = msg.len();\n-        self.length += length;\n-\n-        let mut needed = 0;\n-\n-        if self.ntail != 0 {\n-            needed = 8 - self.ntail;\n-            self.tail |= unsafe { u8to64_le(msg, 0, cmp::min(length, needed)) } << (8 * self.ntail);\n-            if length < needed {\n-                self.ntail += length;\n-                return;\n-            } else {\n-                self.state.v3 ^= self.tail;\n-                Sip24Rounds::c_rounds(&mut self.state);\n-                self.state.v0 ^= self.tail;\n-                self.ntail = 0;\n-            }\n-        }\n-\n-        // Buffered tail is now flushed, process new input.\n-        let len = length - needed;\n-        let left = len & 0x7;\n-\n-        let mut i = needed;\n-        while i < len - left {\n-            let mi = unsafe { load_int_le!(msg, i, u64) };\n-\n-            self.state.v3 ^= mi;\n-            Sip24Rounds::c_rounds(&mut self.state);\n-            self.state.v0 ^= mi;\n-\n-            i += 8;\n-        }\n-\n-        self.tail = unsafe { u8to64_le(msg, i, left) };\n-        self.ntail = left;\n+        self.slice_write(msg);\n     }\n \n     fn finish(&self) -> u64 {"}, {"sha": "eda7ddc4f6d3b0fa8c0f83e3a7ec35e883d00aa7", "filename": "compiler/rustc_data_structures/src/sip128/tests.rs", "status": "modified", "additions": 45, "deletions": 0, "changes": 45, "blob_url": "https://github.com/rust-lang/rust/blob/f6f96e2a8757717ca8d701d26d37b067e95bb584/compiler%2Frustc_data_structures%2Fsrc%2Fsip128%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f6f96e2a8757717ca8d701d26d37b067e95bb584/compiler%2Frustc_data_structures%2Fsrc%2Fsip128%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_data_structures%2Fsrc%2Fsip128%2Ftests.rs?ref=f6f96e2a8757717ca8d701d26d37b067e95bb584", "patch": "@@ -450,3 +450,48 @@ fn test_short_write_works() {\n \n     assert_eq!(h1_hash, h2_hash);\n }\n+\n+macro_rules! test_fill_buffer {\n+    ($type:ty, $write_method:ident) => {{\n+        // Test filling and overfilling the buffer from all possible offsets\n+        // for a given integer type and its corresponding write method.\n+        const SIZE: usize = std::mem::size_of::<$type>();\n+        let input = [42; BUFFER_SIZE_BYTES];\n+        let x = 0x01234567_89ABCDEF_76543210_FEDCBA98_u128 as $type;\n+        let x_bytes = &x.to_ne_bytes();\n+\n+        for i in 1..=SIZE {\n+            let s = &input[..BUFFER_SIZE_BYTES - i];\n+\n+            let mut h1 = SipHasher128::new_with_keys(7, 13);\n+            h1.write(s);\n+            h1.$write_method(x);\n+\n+            let mut h2 = SipHasher128::new_with_keys(7, 13);\n+            h2.write(s);\n+            h2.write(x_bytes);\n+\n+            let h1_hash = h1.finish128();\n+            let h2_hash = h2.finish128();\n+\n+            assert_eq!(h1_hash, h2_hash);\n+        }\n+    }};\n+}\n+\n+#[test]\n+fn test_fill_buffer() {\n+    test_fill_buffer!(u8, write_u8);\n+    test_fill_buffer!(u16, write_u16);\n+    test_fill_buffer!(u32, write_u32);\n+    test_fill_buffer!(u64, write_u64);\n+    test_fill_buffer!(u128, write_u128);\n+    test_fill_buffer!(usize, write_usize);\n+\n+    test_fill_buffer!(i8, write_i8);\n+    test_fill_buffer!(i16, write_i16);\n+    test_fill_buffer!(i32, write_i32);\n+    test_fill_buffer!(i64, write_i64);\n+    test_fill_buffer!(i128, write_i128);\n+    test_fill_buffer!(isize, write_isize);\n+}"}]}