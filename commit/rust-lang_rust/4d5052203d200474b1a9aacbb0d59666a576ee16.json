{"sha": "4d5052203d200474b1a9aacbb0d59666a576ee16", "node_id": "MDY6Q29tbWl0NzI0NzEyOjRkNTA1MjIwM2QyMDA0NzRiMWE5YWFjYmIwZDU5NjY2YTU3NmVlMTY=", "commit": {"author": {"name": "Igor Aleksanov", "email": "popzxc@yandex.ru", "date": "2019-10-17T06:47:43Z"}, "committer": {"name": "Igor Aleksanov", "email": "popzxc@yandex.ru", "date": "2019-10-17T14:38:44Z"}, "message": "Split libtest into several smaller modules", "tree": {"sha": "1a3689c7190b3132f53de27ada3ba04c372adbaf", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/1a3689c7190b3132f53de27ada3ba04c372adbaf"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/4d5052203d200474b1a9aacbb0d59666a576ee16", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/4d5052203d200474b1a9aacbb0d59666a576ee16", "html_url": "https://github.com/rust-lang/rust/commit/4d5052203d200474b1a9aacbb0d59666a576ee16", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/4d5052203d200474b1a9aacbb0d59666a576ee16/comments", "author": {"login": "popzxc", "id": 12111581, "node_id": "MDQ6VXNlcjEyMTExNTgx", "avatar_url": "https://avatars.githubusercontent.com/u/12111581?v=4", "gravatar_id": "", "url": "https://api.github.com/users/popzxc", "html_url": "https://github.com/popzxc", "followers_url": "https://api.github.com/users/popzxc/followers", "following_url": "https://api.github.com/users/popzxc/following{/other_user}", "gists_url": "https://api.github.com/users/popzxc/gists{/gist_id}", "starred_url": "https://api.github.com/users/popzxc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/popzxc/subscriptions", "organizations_url": "https://api.github.com/users/popzxc/orgs", "repos_url": "https://api.github.com/users/popzxc/repos", "events_url": "https://api.github.com/users/popzxc/events{/privacy}", "received_events_url": "https://api.github.com/users/popzxc/received_events", "type": "User", "site_admin": false}, "committer": {"login": "popzxc", "id": 12111581, "node_id": "MDQ6VXNlcjEyMTExNTgx", "avatar_url": "https://avatars.githubusercontent.com/u/12111581?v=4", "gravatar_id": "", "url": "https://api.github.com/users/popzxc", "html_url": "https://github.com/popzxc", "followers_url": "https://api.github.com/users/popzxc/followers", "following_url": "https://api.github.com/users/popzxc/following{/other_user}", "gists_url": "https://api.github.com/users/popzxc/gists{/gist_id}", "starred_url": "https://api.github.com/users/popzxc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/popzxc/subscriptions", "organizations_url": "https://api.github.com/users/popzxc/orgs", "repos_url": "https://api.github.com/users/popzxc/repos", "events_url": "https://api.github.com/users/popzxc/events{/privacy}", "received_events_url": "https://api.github.com/users/popzxc/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "a16dca337de610986252bb800953e57bf395863f", "url": "https://api.github.com/repos/rust-lang/rust/commits/a16dca337de610986252bb800953e57bf395863f", "html_url": "https://github.com/rust-lang/rust/commit/a16dca337de610986252bb800953e57bf395863f"}], "stats": {"total": 2847, "additions": 1462, "deletions": 1385}, "files": [{"sha": "055a74f691cd4ae758fa46925e4ae11d5b6d1885", "filename": "src/libtest/bench.rs", "status": "added", "additions": 251, "deletions": 0, "changes": 251, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fbench.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fbench.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fbench.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,251 @@\n+//! Benchmarking module.\n+use super::{\n+    BenchMode, MonitorMsg, Sender, Sink, TestDesc, TestResult\n+};\n+\n+use crate::stats;\n+use std::time::{Duration, Instant};\n+use std::cmp;\n+use std::io;\n+use std::panic::{catch_unwind, AssertUnwindSafe};\n+use std::sync::{Arc, Mutex};\n+use std::hint::black_box;\n+\n+/// Manager of the benchmarking runs.\n+///\n+/// This is fed into functions marked with `#[bench]` to allow for\n+/// set-up & tear-down before running a piece of code repeatedly via a\n+/// call to `iter`.\n+#[derive(Clone)]\n+pub struct Bencher {\n+    mode: BenchMode,\n+    summary: Option<stats::Summary>,\n+    pub bytes: u64,\n+}\n+\n+impl Bencher {\n+    /// Callback for benchmark functions to run in their body.\n+    pub fn iter<T, F>(&mut self, mut inner: F)\n+    where\n+        F: FnMut() -> T,\n+    {\n+        if self.mode == BenchMode::Single {\n+            ns_iter_inner(&mut inner, 1);\n+            return;\n+        }\n+\n+        self.summary = Some(iter(&mut inner));\n+    }\n+\n+    pub fn bench<F>(&mut self, mut f: F) -> Option<stats::Summary>\n+    where\n+        F: FnMut(&mut Bencher),\n+    {\n+        f(self);\n+        return self.summary;\n+    }\n+}\n+\n+#[derive(Debug, Clone, PartialEq)]\n+pub struct BenchSamples {\n+    pub ns_iter_summ: stats::Summary,\n+    pub mb_s: usize,\n+}\n+\n+pub fn fmt_bench_samples(bs: &BenchSamples) -> String {\n+    use std::fmt::Write;\n+    let mut output = String::new();\n+\n+    let median = bs.ns_iter_summ.median as usize;\n+    let deviation = (bs.ns_iter_summ.max - bs.ns_iter_summ.min) as usize;\n+\n+    output\n+        .write_fmt(format_args!(\n+            \"{:>11} ns/iter (+/- {})\",\n+            fmt_thousands_sep(median, ','),\n+            fmt_thousands_sep(deviation, ',')\n+        ))\n+        .unwrap();\n+    if bs.mb_s != 0 {\n+        output\n+            .write_fmt(format_args!(\" = {} MB/s\", bs.mb_s))\n+            .unwrap();\n+    }\n+    output\n+}\n+\n+// Format a number with thousands separators\n+fn fmt_thousands_sep(mut n: usize, sep: char) -> String {\n+    use std::fmt::Write;\n+    let mut output = String::new();\n+    let mut trailing = false;\n+    for &pow in &[9, 6, 3, 0] {\n+        let base = 10_usize.pow(pow);\n+        if pow == 0 || trailing || n / base != 0 {\n+            if !trailing {\n+                output.write_fmt(format_args!(\"{}\", n / base)).unwrap();\n+            } else {\n+                output.write_fmt(format_args!(\"{:03}\", n / base)).unwrap();\n+            }\n+            if pow != 0 {\n+                output.push(sep);\n+            }\n+            trailing = true;\n+        }\n+        n %= base;\n+    }\n+\n+    output\n+}\n+\n+fn ns_from_dur(dur: Duration) -> u64 {\n+    dur.as_secs() * 1_000_000_000 + (dur.subsec_nanos() as u64)\n+}\n+\n+fn ns_iter_inner<T, F>(inner: &mut F, k: u64) -> u64\n+where\n+    F: FnMut() -> T,\n+{\n+    let start = Instant::now();\n+    for _ in 0..k {\n+        black_box(inner());\n+    }\n+    return ns_from_dur(start.elapsed());\n+}\n+\n+pub fn iter<T, F>(inner: &mut F) -> stats::Summary\n+where\n+    F: FnMut() -> T,\n+{\n+    // Initial bench run to get ballpark figure.\n+    let ns_single = ns_iter_inner(inner, 1);\n+\n+    // Try to estimate iter count for 1ms falling back to 1m\n+    // iterations if first run took < 1ns.\n+    let ns_target_total = 1_000_000; // 1ms\n+    let mut n = ns_target_total / cmp::max(1, ns_single);\n+\n+    // if the first run took more than 1ms we don't want to just\n+    // be left doing 0 iterations on every loop. The unfortunate\n+    // side effect of not being able to do as many runs is\n+    // automatically handled by the statistical analysis below\n+    // (i.e., larger error bars).\n+    n = cmp::max(1, n);\n+\n+    let mut total_run = Duration::new(0, 0);\n+    let samples: &mut [f64] = &mut [0.0_f64; 50];\n+    loop {\n+        let loop_start = Instant::now();\n+\n+        for p in &mut *samples {\n+            *p = ns_iter_inner(inner, n) as f64 / n as f64;\n+        }\n+\n+        stats::winsorize(samples, 5.0);\n+        let summ = stats::Summary::new(samples);\n+\n+        for p in &mut *samples {\n+            let ns = ns_iter_inner(inner, 5 * n);\n+            *p = ns as f64 / (5 * n) as f64;\n+        }\n+\n+        stats::winsorize(samples, 5.0);\n+        let summ5 = stats::Summary::new(samples);\n+\n+        let loop_run = loop_start.elapsed();\n+\n+        // If we've run for 100ms and seem to have converged to a\n+        // stable median.\n+        if loop_run > Duration::from_millis(100)\n+            && summ.median_abs_dev_pct < 1.0\n+            && summ.median - summ5.median < summ5.median_abs_dev\n+        {\n+            return summ5;\n+        }\n+\n+        total_run = total_run + loop_run;\n+        // Longest we ever run for is 3s.\n+        if total_run > Duration::from_secs(3) {\n+            return summ5;\n+        }\n+\n+        // If we overflow here just return the results so far. We check a\n+        // multiplier of 10 because we're about to multiply by 2 and the\n+        // next iteration of the loop will also multiply by 5 (to calculate\n+        // the summ5 result)\n+        n = match n.checked_mul(10) {\n+            Some(_) => n * 2,\n+            None => {\n+                return summ5;\n+            }\n+        };\n+    }\n+}\n+\n+pub fn benchmark<F>(desc: TestDesc, monitor_ch: Sender<MonitorMsg>, nocapture: bool, f: F)\n+where\n+    F: FnMut(&mut Bencher),\n+{\n+    let mut bs = Bencher {\n+        mode: BenchMode::Auto,\n+        summary: None,\n+        bytes: 0,\n+    };\n+\n+    let data = Arc::new(Mutex::new(Vec::new()));\n+    let oldio = if !nocapture {\n+        Some((\n+            io::set_print(Some(Box::new(Sink(data.clone())))),\n+            io::set_panic(Some(Box::new(Sink(data.clone())))),\n+        ))\n+    } else {\n+        None\n+    };\n+\n+    let result = catch_unwind(AssertUnwindSafe(|| bs.bench(f)));\n+\n+    if let Some((printio, panicio)) = oldio {\n+        io::set_print(printio);\n+        io::set_panic(panicio);\n+    }\n+\n+    let test_result = match result {\n+        //bs.bench(f) {\n+        Ok(Some(ns_iter_summ)) => {\n+            let ns_iter = cmp::max(ns_iter_summ.median as u64, 1);\n+            let mb_s = bs.bytes * 1000 / ns_iter;\n+\n+            let bs = BenchSamples {\n+                ns_iter_summ,\n+                mb_s: mb_s as usize,\n+            };\n+            TestResult::TrBench(bs)\n+        }\n+        Ok(None) => {\n+            // iter not called, so no data.\n+            // FIXME: error in this case?\n+            let samples: &mut [f64] = &mut [0.0_f64; 1];\n+            let bs = BenchSamples {\n+                ns_iter_summ: stats::Summary::new(samples),\n+                mb_s: 0,\n+            };\n+            TestResult::TrBench(bs)\n+        }\n+        Err(_) => TestResult::TrFailed,\n+    };\n+\n+    let stdout = data.lock().unwrap().to_vec();\n+    monitor_ch.send((desc, test_result, None, stdout)).unwrap();\n+}\n+\n+pub fn run_once<F>(f: F)\n+where\n+    F: FnMut(&mut Bencher),\n+{\n+    let mut bs = Bencher {\n+        mode: BenchMode::Single,\n+        summary: None,\n+        bytes: 0,\n+    };\n+    bs.bench(f);\n+}"}, {"sha": "b35193701d6ef0c408bbd35034451100faeddc92", "filename": "src/libtest/cli.rs", "status": "added", "additions": 384, "deletions": 0, "changes": 384, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fcli.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fcli.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fcli.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,384 @@\n+//! Module converting command-line arguments into test configuration.\n+\n+use std::env;\n+use std::path::PathBuf;\n+use getopts;\n+\n+use super::options::{RunIgnored, ColorConfig, OutputFormat, Options};\n+use super::time::TestTimeOptions;\n+use super::helpers::isatty;\n+\n+#[derive(Debug)]\n+pub struct TestOpts {\n+    pub list: bool,\n+    pub filter: Option<String>,\n+    pub filter_exact: bool,\n+    pub exclude_should_panic: bool,\n+    pub run_ignored: RunIgnored,\n+    pub run_tests: bool,\n+    pub bench_benchmarks: bool,\n+    pub logfile: Option<PathBuf>,\n+    pub nocapture: bool,\n+    pub color: ColorConfig,\n+    pub format: OutputFormat,\n+    pub test_threads: Option<usize>,\n+    pub skip: Vec<String>,\n+    pub time_options: Option<TestTimeOptions>,\n+    pub options: Options,\n+}\n+\n+impl TestOpts {\n+    pub fn use_color(&self) -> bool {\n+        match self.color {\n+            ColorConfig::AutoColor => !self.nocapture && isatty::stdout_isatty(),\n+            ColorConfig::AlwaysColor => true,\n+            ColorConfig::NeverColor => false,\n+        }\n+    }\n+}\n+\n+/// Result of parsing the options.\n+pub type OptRes = Result<TestOpts, String>;\n+/// Result of parsing the option part.\n+type OptPartRes<T> = Result<Option<T>, String>;\n+\n+fn optgroups() -> getopts::Options {\n+    let mut opts = getopts::Options::new();\n+    opts.optflag(\"\", \"include-ignored\", \"Run ignored and not ignored tests\")\n+        .optflag(\"\", \"ignored\", \"Run only ignored tests\")\n+        .optflag(\"\", \"exclude-should-panic\", \"Excludes tests marked as should_panic\")\n+        .optflag(\"\", \"test\", \"Run tests and not benchmarks\")\n+        .optflag(\"\", \"bench\", \"Run benchmarks instead of tests\")\n+        .optflag(\"\", \"list\", \"List all tests and benchmarks\")\n+        .optflag(\"h\", \"help\", \"Display this message (longer with --help)\")\n+        .optopt(\n+            \"\",\n+            \"logfile\",\n+            \"Write logs to the specified file instead \\\n+             of stdout\",\n+            \"PATH\",\n+        )\n+        .optflag(\n+            \"\",\n+            \"nocapture\",\n+            \"don't capture stdout/stderr of each \\\n+             task, allow printing directly\",\n+        )\n+        .optopt(\n+            \"\",\n+            \"test-threads\",\n+            \"Number of threads used for running tests \\\n+             in parallel\",\n+            \"n_threads\",\n+        )\n+        .optmulti(\n+            \"\",\n+            \"skip\",\n+            \"Skip tests whose names contain FILTER (this flag can \\\n+             be used multiple times)\",\n+            \"FILTER\",\n+        )\n+        .optflag(\n+            \"q\",\n+            \"quiet\",\n+            \"Display one character per test instead of one line. \\\n+             Alias to --format=terse\",\n+        )\n+        .optflag(\n+            \"\",\n+            \"exact\",\n+            \"Exactly match filters rather than by substring\",\n+        )\n+        .optopt(\n+            \"\",\n+            \"color\",\n+            \"Configure coloring of output:\n+            auto   = colorize if stdout is a tty and tests are run on serially (default);\n+            always = always colorize output;\n+            never  = never colorize output;\",\n+            \"auto|always|never\",\n+        )\n+        .optopt(\n+            \"\",\n+            \"format\",\n+            \"Configure formatting of output:\n+            pretty = Print verbose output;\n+            terse  = Display one character per test;\n+            json   = Output a json document\",\n+            \"pretty|terse|json\",\n+        )\n+        .optflag(\n+            \"\",\n+            \"show-output\",\n+            \"Show captured stdout of successful tests\"\n+        )\n+        .optopt(\n+            \"Z\",\n+            \"\",\n+            \"Enable nightly-only flags:\n+            unstable-options = Allow use of experimental features\",\n+            \"unstable-options\",\n+        )\n+        .optflagopt(\n+            \"\",\n+            \"report-time\",\n+            \"Show execution time of each test. Awailable values:\n+            plain   = do not colorize the execution time (default);\n+            colored = colorize output according to the `color` parameter value;\n+\n+            Threshold values for colorized output can be configured via\n+            `RUST_TEST_TIME_UNIT`, `RUST_TEST_TIME_INTEGRATION` and\n+            `RUST_TEST_TIME_DOCTEST` environment variables.\n+\n+            Expected format of environment variable is `VARIABLE=WARN_TIME,CRITICAL_TIME`.\n+\n+            Not available for --format=terse\",\n+            \"plain|colored\"\n+        )\n+        .optflag(\n+            \"\",\n+            \"ensure-time\",\n+            \"Treat excess of the test execution time limit as error.\n+\n+            Threshold values for this option can be configured via\n+            `RUST_TEST_TIME_UNIT`, `RUST_TEST_TIME_INTEGRATION` and\n+            `RUST_TEST_TIME_DOCTEST` environment variables.\n+\n+            Expected format of environment variable is `VARIABLE=WARN_TIME,CRITICAL_TIME`.\n+\n+            `CRITICAL_TIME` here means the limit that should not be exceeded by test.\n+            \"\n+        );\n+    return opts;\n+}\n+\n+fn usage(binary: &str, options: &getopts::Options) {\n+    let message = format!(\"Usage: {} [OPTIONS] [FILTER]\", binary);\n+    println!(\n+        r#\"{usage}\n+\n+The FILTER string is tested against the name of all tests, and only those\n+tests whose names contain the filter are run.\n+\n+By default, all tests are run in parallel. This can be altered with the\n+--test-threads flag or the RUST_TEST_THREADS environment variable when running\n+tests (set it to 1).\n+\n+All tests have their standard output and standard error captured by default.\n+This can be overridden with the --nocapture flag or setting RUST_TEST_NOCAPTURE\n+environment variable to a value other than \"0\". Logging is not captured by default.\n+\n+Test Attributes:\n+\n+    `#[test]`        - Indicates a function is a test to be run. This function\n+                       takes no arguments.\n+    `#[bench]`       - Indicates a function is a benchmark to be run. This\n+                       function takes one argument (test::Bencher).\n+    `#[should_panic]` - This function (also labeled with `#[test]`) will only pass if\n+                        the code causes a panic (an assertion failure or panic!)\n+                        A message may be provided, which the failure string must\n+                        contain: #[should_panic(expected = \"foo\")].\n+    `#[ignore]`       - When applied to a function which is already attributed as a\n+                        test, then the test runner will ignore these tests during\n+                        normal test runs. Running with --ignored or --include-ignored will run\n+                        these tests.\"#,\n+        usage = options.usage(&message)\n+    );\n+}\n+\n+// FIXME: Copied from libsyntax until linkage errors are resolved. Issue #47566\n+fn is_nightly() -> bool {\n+    // Whether this is a feature-staged build, i.e., on the beta or stable channel\n+    let disable_unstable_features = option_env!(\"CFG_DISABLE_UNSTABLE_FEATURES\").is_some();\n+    // Whether we should enable unstable features for bootstrapping\n+    let bootstrap = env::var(\"RUSTC_BOOTSTRAP\").is_ok();\n+\n+    bootstrap || !disable_unstable_features\n+}\n+\n+// Gets the option value and checks if unstable features are enabled.\n+macro_rules! unstable_optflag {\n+    ($matches:ident, $allow_unstable:ident, $option_name:literal) => {{\n+        let opt = $matches.opt_present($option_name);\n+        if !$allow_unstable && opt {\n+            return Some(Err(format!(\n+                \"The \\\"{}\\\" flag is only accepted on the nightly compiler\",\n+                $option_name\n+            )));\n+        }\n+\n+        opt\n+    }};\n+}\n+\n+// Gets the CLI options assotiated with `report-time` feature.\n+fn get_time_options(\n+    matches: &getopts::Matches,\n+    allow_unstable: bool)\n+-> Option<OptPartRes<TestTimeOptions>> {\n+    let report_time = unstable_optflag!(matches, allow_unstable, \"report-time\");\n+    let colored_opt_str = matches.opt_str(\"report-time\");\n+    let mut report_time_colored = report_time && colored_opt_str == Some(\"colored\".into());\n+    let ensure_test_time = unstable_optflag!(matches, allow_unstable, \"ensure-time\");\n+\n+    // If `ensure-test-time` option is provided, time output is enforced,\n+    // so user won't be confused if any of tests will silently fail.\n+    let options = if report_time || ensure_test_time {\n+        if ensure_test_time && !report_time {\n+            report_time_colored = true;\n+        }\n+        Some(TestTimeOptions::new_from_env(ensure_test_time, report_time_colored))\n+    } else {\n+        None\n+    };\n+\n+    Some(Ok(options))\n+}\n+\n+// Parses command line arguments into test options\n+pub fn parse_opts(args: &[String]) -> Option<OptRes> {\n+    let mut allow_unstable = false;\n+    let opts = optgroups();\n+    let args = args.get(1..).unwrap_or(args);\n+    let matches = match opts.parse(args) {\n+        Ok(m) => m,\n+        Err(f) => return Some(Err(f.to_string())),\n+    };\n+\n+    if let Some(opt) = matches.opt_str(\"Z\") {\n+        if !is_nightly() {\n+            return Some(Err(\n+                \"the option `Z` is only accepted on the nightly compiler\".into(),\n+            ));\n+        }\n+\n+        match &*opt {\n+            \"unstable-options\" => {\n+                allow_unstable = true;\n+            }\n+            _ => {\n+                return Some(Err(\"Unrecognized option to `Z`\".into()));\n+            }\n+        }\n+    };\n+\n+    if matches.opt_present(\"h\") {\n+        usage(&args[0], &opts);\n+        return None;\n+    }\n+\n+    let filter = if !matches.free.is_empty() {\n+        Some(matches.free[0].clone())\n+    } else {\n+        None\n+    };\n+\n+    let exclude_should_panic = unstable_optflag!(matches, allow_unstable, \"exclude-should-panic\");\n+\n+    let include_ignored = unstable_optflag!(matches, allow_unstable, \"include-ignored\");\n+\n+    let run_ignored = match (include_ignored, matches.opt_present(\"ignored\")) {\n+        (true, true) => {\n+            return Some(Err(\n+                \"the options --include-ignored and --ignored are mutually exclusive\".into(),\n+            ));\n+        }\n+        (true, false) => RunIgnored::Yes,\n+        (false, true) => RunIgnored::Only,\n+        (false, false) => RunIgnored::No,\n+    };\n+    let quiet = matches.opt_present(\"quiet\");\n+    let exact = matches.opt_present(\"exact\");\n+    let list = matches.opt_present(\"list\");\n+\n+    let logfile = matches.opt_str(\"logfile\");\n+    let logfile = logfile.map(|s| PathBuf::from(&s));\n+\n+    let bench_benchmarks = matches.opt_present(\"bench\");\n+    let run_tests = !bench_benchmarks || matches.opt_present(\"test\");\n+\n+    let mut nocapture = matches.opt_present(\"nocapture\");\n+    if !nocapture {\n+        nocapture = match env::var(\"RUST_TEST_NOCAPTURE\") {\n+            Ok(val) => &val != \"0\",\n+            Err(_) => false,\n+        };\n+    }\n+\n+    let time_options = match get_time_options(&matches, allow_unstable) {\n+        Some(Ok(val)) => val,\n+        Some(Err(e)) => return Some(Err(e)),\n+        None => panic!(\"Unexpected output from `get_time_options`\"),\n+    };\n+\n+    let test_threads = match matches.opt_str(\"test-threads\") {\n+        Some(n_str) => match n_str.parse::<usize>() {\n+            Ok(0) => return Some(Err(\"argument for --test-threads must not be 0\".to_string())),\n+            Ok(n) => Some(n),\n+            Err(e) => {\n+                return Some(Err(format!(\n+                    \"argument for --test-threads must be a number > 0 \\\n+                     (error: {})\",\n+                    e\n+                )));\n+            }\n+        },\n+        None => None,\n+    };\n+\n+    let color = match matches.opt_str(\"color\").as_ref().map(|s| &**s) {\n+        Some(\"auto\") | None => ColorConfig::AutoColor,\n+        Some(\"always\") => ColorConfig::AlwaysColor,\n+        Some(\"never\") => ColorConfig::NeverColor,\n+\n+        Some(v) => {\n+            return Some(Err(format!(\n+                \"argument for --color must be auto, always, or never (was \\\n+                 {})\",\n+                v\n+            )));\n+        }\n+    };\n+\n+    let format = match matches.opt_str(\"format\").as_ref().map(|s| &**s) {\n+        None if quiet => OutputFormat::Terse,\n+        Some(\"pretty\") | None => OutputFormat::Pretty,\n+        Some(\"terse\") => OutputFormat::Terse,\n+        Some(\"json\") => {\n+            if !allow_unstable {\n+                return Some(Err(\n+                    \"The \\\"json\\\" format is only accepted on the nightly compiler\".into(),\n+                ));\n+            }\n+            OutputFormat::Json\n+        }\n+\n+        Some(v) => {\n+            return Some(Err(format!(\n+                \"argument for --format must be pretty, terse, or json (was \\\n+                 {})\",\n+                v\n+            )));\n+        }\n+    };\n+\n+    let test_opts = TestOpts {\n+        list,\n+        filter,\n+        filter_exact: exact,\n+        exclude_should_panic,\n+        run_ignored,\n+        run_tests,\n+        bench_benchmarks,\n+        logfile,\n+        nocapture,\n+        color,\n+        format,\n+        test_threads,\n+        skip: matches.opt_strs(\"skip\"),\n+        time_options,\n+        options: Options::new().display_output(matches.opt_present(\"show-output\")),\n+    };\n+\n+    Some(Ok(test_opts))\n+}"}, {"sha": "ff756c456dae648b4e63b2432b0a273d016e974b", "filename": "src/libtest/formatters/json.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fformatters%2Fjson.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fformatters%2Fjson.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fformatters%2Fjson.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -27,7 +27,7 @@ impl<T: Write> JsonFormatter<T> {\n         ty: &str,\n         name: &str,\n         evt: &str,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n         stdout: Option<Cow<'_, str>>,\n         extra: Option<&str>,\n     ) -> io::Result<()> {\n@@ -76,7 +76,7 @@ impl<T: Write> OutputFormatter for JsonFormatter<T> {\n         &mut self,\n         desc: &TestDesc,\n         result: &TestResult,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n         stdout: &[u8],\n         state: &ConsoleTestState,\n     ) -> io::Result<()> {"}, {"sha": "72432cd8e3c2bf41c1191080cf9b283ae29f320d", "filename": "src/libtest/formatters/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fformatters%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fformatters%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fformatters%2Fmod.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -16,7 +16,7 @@ pub(crate) trait OutputFormatter {\n         &mut self,\n         desc: &TestDesc,\n         result: &TestResult,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n         stdout: &[u8],\n         state: &ConsoleTestState,\n     ) -> io::Result<()>;"}, {"sha": "84e1a44dab807c143ca9b70a55f458429feb8f21", "filename": "src/libtest/formatters/pretty.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fformatters%2Fpretty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fformatters%2Fpretty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fformatters%2Fpretty.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -3,7 +3,7 @@ use super::*;\n pub(crate) struct PrettyFormatter<T> {\n     out: OutputLocation<T>,\n     use_color: bool,\n-    time_options: Option<TestTimeOptions>,\n+    time_options: Option<time::TestTimeOptions>,\n \n     /// Number of columns to fill when aligning names\n     max_name_len: usize,\n@@ -17,7 +17,7 @@ impl<T: Write> PrettyFormatter<T> {\n         use_color: bool,\n         max_name_len: usize,\n         is_multithreaded: bool,\n-        time_options: Option<TestTimeOptions>,\n+        time_options: Option<time::TestTimeOptions>,\n     ) -> Self {\n         PrettyFormatter {\n             out,\n@@ -93,7 +93,7 @@ impl<T: Write> PrettyFormatter<T> {\n     fn write_time(\n         &mut self,\n         desc: &TestDesc,\n-        exec_time: Option<&TestExecTime>\n+        exec_time: Option<&time::TestExecTime>\n     ) -> io::Result<()> {\n         if let (Some(opts), Some(time)) = (self.time_options, exec_time) {\n             let time_str = format!(\" <{}>\", time);\n@@ -194,7 +194,7 @@ impl<T: Write> OutputFormatter for PrettyFormatter<T> {\n         &mut self,\n         desc: &TestDesc,\n         result: &TestResult,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n         _: &[u8],\n         _: &ConsoleTestState,\n     ) -> io::Result<()> {\n@@ -225,7 +225,7 @@ impl<T: Write> OutputFormatter for PrettyFormatter<T> {\n \n         self.write_plain(&format!(\n             \"test {} has been running for over {} seconds\\n\",\n-            desc.name, TEST_WARN_TIMEOUT_S\n+            desc.name, time::TEST_WARN_TIMEOUT_S\n         ))\n     }\n "}, {"sha": "50407d1130f86a1e74a95b7299d8d03c705242df", "filename": "src/libtest/formatters/terse.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fformatters%2Fterse.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fformatters%2Fterse.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fformatters%2Fterse.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -174,7 +174,7 @@ impl<T: Write> OutputFormatter for TerseFormatter<T> {\n         &mut self,\n         desc: &TestDesc,\n         result: &TestResult,\n-        _: Option<&TestExecTime>,\n+        _: Option<&time::TestExecTime>,\n         _: &[u8],\n         _: &ConsoleTestState,\n     ) -> io::Result<()> {\n@@ -196,7 +196,7 @@ impl<T: Write> OutputFormatter for TerseFormatter<T> {\n     fn write_timeout(&mut self, desc: &TestDesc) -> io::Result<()> {\n         self.write_plain(&format!(\n             \"test {} has been running for over {} seconds\\n\",\n-            desc.name, TEST_WARN_TIMEOUT_S\n+            desc.name, time::TEST_WARN_TIMEOUT_S\n         ))\n     }\n "}, {"sha": "f0292c2d2c792c0ef694cf805ac375e3bf6bbcac", "filename": "src/libtest/helpers/concurrency.rs", "status": "added", "additions": 153, "deletions": 0, "changes": 153, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fhelpers%2Fconcurrency.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fhelpers%2Fconcurrency.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fconcurrency.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,153 @@\n+//! Helper module which helps to determine amount of threads to be used\n+//! during tests execution.\n+use std::env;\n+\n+#[cfg(any(unix, target_os = \"cloudabi\"))]\n+use libc;\n+\n+#[allow(deprecated)]\n+pub fn get_concurrency() -> usize {\n+    return match env::var(\"RUST_TEST_THREADS\") {\n+        Ok(s) => {\n+            let opt_n: Option<usize> = s.parse().ok();\n+            match opt_n {\n+                Some(n) if n > 0 => n,\n+                _ => panic!(\n+                    \"RUST_TEST_THREADS is `{}`, should be a positive integer.\",\n+                    s\n+                ),\n+            }\n+        }\n+        Err(..) => num_cpus(),\n+    };\n+\n+    #[cfg(windows)]\n+    #[allow(nonstandard_style)]\n+    fn num_cpus() -> usize {\n+        #[repr(C)]\n+        struct SYSTEM_INFO {\n+            wProcessorArchitecture: u16,\n+            wReserved: u16,\n+            dwPageSize: u32,\n+            lpMinimumApplicationAddress: *mut u8,\n+            lpMaximumApplicationAddress: *mut u8,\n+            dwActiveProcessorMask: *mut u8,\n+            dwNumberOfProcessors: u32,\n+            dwProcessorType: u32,\n+            dwAllocationGranularity: u32,\n+            wProcessorLevel: u16,\n+            wProcessorRevision: u16,\n+        }\n+        extern \"system\" {\n+            fn GetSystemInfo(info: *mut SYSTEM_INFO) -> i32;\n+        }\n+        unsafe {\n+            let mut sysinfo = std::mem::zeroed();\n+            GetSystemInfo(&mut sysinfo);\n+            sysinfo.dwNumberOfProcessors as usize\n+        }\n+    }\n+\n+    #[cfg(target_os = \"vxworks\")]\n+    fn num_cpus() -> usize {\n+        // FIXME: Implement num_cpus on vxWorks\n+        1\n+    }\n+\n+    #[cfg(target_os = \"redox\")]\n+    fn num_cpus() -> usize {\n+        // FIXME: Implement num_cpus on Redox\n+        1\n+    }\n+\n+    #[cfg(any(\n+        all(target_arch = \"wasm32\", not(target_os = \"emscripten\")),\n+        all(target_vendor = \"fortanix\", target_env = \"sgx\")\n+    ))]\n+    fn num_cpus() -> usize {\n+        1\n+    }\n+\n+    #[cfg(any(\n+        target_os = \"android\",\n+        target_os = \"cloudabi\",\n+        target_os = \"emscripten\",\n+        target_os = \"fuchsia\",\n+        target_os = \"ios\",\n+        target_os = \"linux\",\n+        target_os = \"macos\",\n+        target_os = \"solaris\",\n+    ))]\n+    fn num_cpus() -> usize {\n+        unsafe { libc::sysconf(libc::_SC_NPROCESSORS_ONLN) as usize }\n+    }\n+\n+    #[cfg(any(\n+        target_os = \"freebsd\",\n+        target_os = \"dragonfly\",\n+        target_os = \"netbsd\"\n+    ))]\n+    fn num_cpus() -> usize {\n+        use std::ptr;\n+\n+        let mut cpus: libc::c_uint = 0;\n+        let mut cpus_size = std::mem::size_of_val(&cpus);\n+\n+        unsafe {\n+            cpus = libc::sysconf(libc::_SC_NPROCESSORS_ONLN) as libc::c_uint;\n+        }\n+        if cpus < 1 {\n+            let mut mib = [libc::CTL_HW, libc::HW_NCPU, 0, 0];\n+            unsafe {\n+                libc::sysctl(\n+                    mib.as_mut_ptr(),\n+                    2,\n+                    &mut cpus as *mut _ as *mut _,\n+                    &mut cpus_size as *mut _ as *mut _,\n+                    ptr::null_mut(),\n+                    0,\n+                );\n+            }\n+            if cpus < 1 {\n+                cpus = 1;\n+            }\n+        }\n+        cpus as usize\n+    }\n+\n+    #[cfg(target_os = \"openbsd\")]\n+    fn num_cpus() -> usize {\n+        use std::ptr;\n+\n+        let mut cpus: libc::c_uint = 0;\n+        let mut cpus_size = std::mem::size_of_val(&cpus);\n+        let mut mib = [libc::CTL_HW, libc::HW_NCPU, 0, 0];\n+\n+        unsafe {\n+            libc::sysctl(\n+                mib.as_mut_ptr(),\n+                2,\n+                &mut cpus as *mut _ as *mut _,\n+                &mut cpus_size as *mut _ as *mut _,\n+                ptr::null_mut(),\n+                0,\n+            );\n+        }\n+        if cpus < 1 {\n+            cpus = 1;\n+        }\n+        cpus as usize\n+    }\n+\n+    #[cfg(target_os = \"haiku\")]\n+    fn num_cpus() -> usize {\n+        // FIXME: implement\n+        1\n+    }\n+\n+    #[cfg(target_os = \"l4re\")]\n+    fn num_cpus() -> usize {\n+        // FIXME: implement\n+        1\n+    }\n+}"}, {"sha": "638328aea18cf02b04c6090ec7b56af78beb767f", "filename": "src/libtest/helpers/isatty.rs", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fhelpers%2Fisatty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fhelpers%2Fisatty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fisatty.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,33 @@\n+//! Helper module which provides a function to test\n+//! if stdout is a tty.\n+\n+#[cfg(any(\n+    target_os = \"cloudabi\",\n+    all(target_arch = \"wasm32\", not(target_os = \"emscripten\")),\n+    all(target_vendor = \"fortanix\", target_env = \"sgx\")\n+))]\n+pub fn stdout_isatty() -> bool {\n+    // FIXME: Implement isatty on SGX\n+    false\n+}\n+#[cfg(unix)]\n+pub fn stdout_isatty() -> bool {\n+    unsafe { libc::isatty(libc::STDOUT_FILENO) != 0 }\n+}\n+#[cfg(windows)]\n+pub fn stdout_isatty() -> bool {\n+    type DWORD = u32;\n+    type BOOL = i32;\n+    type HANDLE = *mut u8;\n+    type LPDWORD = *mut u32;\n+    const STD_OUTPUT_HANDLE: DWORD = -11i32 as DWORD;\n+    extern \"system\" {\n+        fn GetStdHandle(which: DWORD) -> HANDLE;\n+        fn GetConsoleMode(hConsoleHandle: HANDLE, lpMode: LPDWORD) -> BOOL;\n+    }\n+    unsafe {\n+        let handle = GetStdHandle(STD_OUTPUT_HANDLE);\n+        let mut out = 0;\n+        GetConsoleMode(handle, &mut out) != 0\n+    }\n+}\n\\ No newline at end of file"}, {"sha": "f77a23e6875b2c1b6a65e1d00a9169c8872fec47", "filename": "src/libtest/helpers/metrics.rs", "status": "added", "additions": 50, "deletions": 0, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fhelpers%2Fmetrics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fhelpers%2Fmetrics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fmetrics.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,50 @@\n+//! Benchmark metrics.\n+use std::collections::BTreeMap;\n+\n+#[derive(Clone, PartialEq, Debug, Copy)]\n+pub struct Metric {\n+    value: f64,\n+    noise: f64,\n+}\n+\n+impl Metric {\n+    pub fn new(value: f64, noise: f64) -> Metric {\n+        Metric { value, noise }\n+    }\n+}\n+\n+#[derive(Clone, PartialEq)]\n+pub struct MetricMap(BTreeMap<String, Metric>);\n+\n+impl MetricMap {\n+    pub fn new() -> MetricMap {\n+        MetricMap(BTreeMap::new())\n+    }\n+\n+    /// Insert a named `value` (+/- `noise`) metric into the map. The value\n+    /// must be non-negative. The `noise` indicates the uncertainty of the\n+    /// metric, which doubles as the \"noise range\" of acceptable\n+    /// pairwise-regressions on this named value, when comparing from one\n+    /// metric to the next using `compare_to_old`.\n+    ///\n+    /// If `noise` is positive, then it means this metric is of a value\n+    /// you want to see grow smaller, so a change larger than `noise` in the\n+    /// positive direction represents a regression.\n+    ///\n+    /// If `noise` is negative, then it means this metric is of a value\n+    /// you want to see grow larger, so a change larger than `noise` in the\n+    /// negative direction represents a regression.\n+    pub fn insert_metric(&mut self, name: &str, value: f64, noise: f64) {\n+        let m = Metric { value, noise };\n+        self.0.insert(name.to_owned(), m);\n+    }\n+\n+    pub fn fmt_metrics(&self) -> String {\n+        let v = self\n+            .0\n+            .iter()\n+            .map(|(k, v)| format!(\"{}: {} (+/- {})\", *k, v.value, v.noise))\n+            .collect::<Vec<_>>();\n+        v.join(\", \")\n+    }\n+}"}, {"sha": "0bbe77b1c50afebe793fb49ffa8d1aa3ad4ba815", "filename": "src/libtest/helpers/mod.rs", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fhelpers%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Fhelpers%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fmod.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,6 @@\n+//! Module with common helpers not directly related to tests\n+//! but used in `libtest`.\n+\n+pub mod concurrency;\n+pub mod isatty;\n+pub mod metrics;"}, {"sha": "f79994671114f1a3a9745f0884277f936f569b74", "filename": "src/libtest/lib.rs", "status": "modified", "additions": 42, "deletions": 1375, "changes": 1417, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Flib.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -30,33 +30,21 @@\n #![feature(termination_trait_lib)]\n #![feature(test)]\n \n-use getopts;\n-#[cfg(any(unix, target_os = \"cloudabi\"))]\n-extern crate libc;\n use term;\n \n pub use self::ColorConfig::*;\n-use self::NamePadding::*;\n use self::OutputLocation::*;\n use self::TestEvent::*;\n-pub use self::TestFn::*;\n-pub use self::TestName::*;\n-pub use self::TestResult::*;\n+pub use self::types::TestName::*;\n \n-use std::any::Any;\n use std::borrow::Cow;\n-use std::cmp;\n-use std::collections::BTreeMap;\n use std::env;\n-use std::fmt;\n use std::fs::File;\n use std::io;\n use std::io::prelude::*;\n use std::panic::{self, catch_unwind, AssertUnwindSafe, PanicInfo};\n-use std::path::PathBuf;\n use std::process;\n use std::process::{ExitStatus, Command, Termination};\n-use std::str::FromStr;\n use std::sync::mpsc::{channel, Sender};\n use std::sync::{Arc, Mutex};\n use std::thread;\n@@ -65,276 +53,48 @@ use std::time::{Duration, Instant};\n #[cfg(test)]\n mod tests;\n \n-const TEST_WARN_TIMEOUT_S: u64 = 60;\n const QUIET_MODE_MAX_COLUMN: usize = 100; // insert a '\\n' after 100 tests in quiet mode\n \n const SECONDARY_TEST_INVOKER_VAR: &'static str = \"__RUST_TEST_INVOKE\";\n \n-// Return codes for secondary process.\n-// Start somewhere other than 0 so we know the return code means what we think\n-// it means.\n-const TR_OK: i32 = 50;\n-const TR_FAILED: i32 = 51;\n-\n-/// This small module contains constants used by `report-time` option.\n-/// Those constants values will be used if corresponding environment variables are not set.\n-///\n-/// To override values for unit-tests, use a constant `RUST_TEST_TIME_UNIT`,\n-/// To override values for integration tests, use a constant `RUST_TEST_TIME_INTEGRATION`,\n-/// To override values for doctests, use a constant `RUST_TEST_TIME_DOCTEST`.\n-///\n-/// Example of the expected format is `RUST_TEST_TIME_xxx=100,200`, where 100 means\n-/// warn time, and 200 means critical time.\n-pub mod time_constants {\n-    use std::time::Duration;\n-    use super::TEST_WARN_TIMEOUT_S;\n-\n-    /// Environment variable for overriding default threshold for unit-tests.\n-    pub const UNIT_ENV_NAME: &str = \"RUST_TEST_TIME_UNIT\";\n-\n-    // Unit tests are supposed to be really quick.\n-    pub const UNIT_WARN: Duration = Duration::from_millis(50);\n-    pub const UNIT_CRITICAL: Duration = Duration::from_millis(100);\n-\n-    /// Environment variable for overriding default threshold for unit-tests.\n-    pub const INTEGRATION_ENV_NAME: &str = \"RUST_TEST_TIME_INTEGRATION\";\n-\n-    // Integration tests may have a lot of work, so they can take longer to execute.\n-    pub const INTEGRATION_WARN: Duration = Duration::from_millis(500);\n-    pub const INTEGRATION_CRITICAL: Duration = Duration::from_millis(1000);\n-\n-    /// Environment variable for overriding default threshold for unit-tests.\n-    pub const DOCTEST_ENV_NAME: &str = \"RUST_TEST_TIME_DOCTEST\";\n-\n-    // Doctests are similar to integration tests, because they can include a lot of\n-    // initialization code.\n-    pub const DOCTEST_WARN: Duration = INTEGRATION_WARN;\n-    pub const DOCTEST_CRITICAL: Duration = INTEGRATION_CRITICAL;\n-\n-    // Do not suppose anything about unknown tests, base limits on the\n-    // `TEST_WARN_TIMEOUT_S` constant.\n-    pub const UNKNOWN_WARN: Duration = Duration::from_secs(TEST_WARN_TIMEOUT_S);\n-    pub const UNKNOWN_CRITICAL: Duration = Duration::from_secs(TEST_WARN_TIMEOUT_S * 2);\n-}\n-\n // to be used by rustc to compile tests in libtest\n pub mod test {\n     pub use crate::{\n-        assert_test_result, filter_tests, parse_opts, run_test, test_main, test_main_static,\n-        Bencher, DynTestFn, DynTestName, Metric, MetricMap, Options, RunIgnored, RunStrategy,\n-        ShouldPanic, StaticBenchFn, StaticTestFn, StaticTestName, TestDesc, TestDescAndFn, TestName,\n-        TestOpts, TestTimeOptions, TestType, TestResult, TrFailed, TrFailedMsg, TrIgnored, TrOk,\n+        bench::Bencher,\n+        cli::{parse_opts, TestOpts},\n+        helpers::metrics::{Metric, MetricMap},\n+        options::{ShouldPanic, Options, RunIgnored, RunStrategy},\n+        test_result::{TestResult, TrFailed, TrFailedMsg, TrIgnored, TrOk},\n+        time::TestTimeOptions,\n+        types::{\n+            DynTestFn, DynTestName, StaticBenchFn, StaticTestFn, StaticTestName, TestDesc, TestDescAndFn,\n+            TestName, TestType,\n+        },\n+        assert_test_result, filter_tests, run_test, test_main, test_main_static,\n     };\n }\n \n-mod formatters;\n-pub mod stats;\n-\n-use crate::formatters::{JsonFormatter, OutputFormatter, PrettyFormatter, TerseFormatter};\n-\n-/// Whether to execute tests concurrently or not\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-pub enum Concurrent {\n-    Yes,\n-    No,\n-}\n-\n-/// Type of the test according to the [rust book](https://doc.rust-lang.org/cargo/guide/tests.html)\n-/// conventions.\n-#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n-pub enum TestType {\n-    /// Unit-tests are expected to be in the `src` folder of the crate.\n-    UnitTest,\n-    /// Integration-style tests are expected to be in the `tests` folder of the crate.\n-    IntegrationTest,\n-    /// Doctests are created by the `librustdoc` manually, so it's a different type of test.\n-    DocTest,\n-    /// Tests for the sources that don't follow the project layout convention\n-    /// (e.g. tests in raw `main.rs` compiled by calling `rustc --test` directly).\n-    Unknown,\n-}\n-\n-// The name of a test. By convention this follows the rules for rust\n-// paths; i.e., it should be a series of identifiers separated by double\n-// colons. This way if some test runner wants to arrange the tests\n-// hierarchically it may.\n-\n-#[derive(Clone, PartialEq, Eq, Hash, Debug)]\n-pub enum TestName {\n-    StaticTestName(&'static str),\n-    DynTestName(String),\n-    AlignedTestName(Cow<'static, str>, NamePadding),\n-}\n-impl TestName {\n-    fn as_slice(&self) -> &str {\n-        match *self {\n-            StaticTestName(s) => s,\n-            DynTestName(ref s) => s,\n-            AlignedTestName(ref s, _) => &*s,\n-        }\n-    }\n-\n-    fn padding(&self) -> NamePadding {\n-        match self {\n-            &AlignedTestName(_, p) => p,\n-            _ => PadNone,\n-        }\n-    }\n-\n-    fn with_padding(&self, padding: NamePadding) -> TestName {\n-        let name = match self {\n-            &TestName::StaticTestName(name) => Cow::Borrowed(name),\n-            &TestName::DynTestName(ref name) => Cow::Owned(name.clone()),\n-            &TestName::AlignedTestName(ref name, _) => name.clone(),\n-        };\n-\n-        TestName::AlignedTestName(name, padding)\n-    }\n-}\n-impl fmt::Display for TestName {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        fmt::Display::fmt(self.as_slice(), f)\n-    }\n-}\n-\n-#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n-pub enum NamePadding {\n-    PadNone,\n-    PadOnRight,\n-}\n-\n-impl TestDesc {\n-    fn padded_name(&self, column_count: usize, align: NamePadding) -> String {\n-        let mut name = String::from(self.name.as_slice());\n-        let fill = column_count.saturating_sub(name.len());\n-        let pad = \" \".repeat(fill);\n-        match align {\n-            PadNone => name,\n-            PadOnRight => {\n-                name.push_str(&pad);\n-                name\n-            }\n-        }\n-    }\n-}\n-\n-/// Represents a benchmark function.\n-pub trait TDynBenchFn: Send {\n-    fn run(&self, harness: &mut Bencher);\n-}\n-\n-// A function that runs a test. If the function returns successfully,\n-// the test succeeds; if the function panics then the test fails. We\n-// may need to come up with a more clever definition of test in order\n-// to support isolation of tests into threads.\n-pub enum TestFn {\n-    StaticTestFn(fn()),\n-    StaticBenchFn(fn(&mut Bencher)),\n-    DynTestFn(Box<dyn FnOnce() + Send>),\n-    DynBenchFn(Box<dyn TDynBenchFn + 'static>),\n-}\n-\n-impl TestFn {\n-    fn padding(&self) -> NamePadding {\n-        match *self {\n-            StaticTestFn(..) => PadNone,\n-            StaticBenchFn(..) => PadOnRight,\n-            DynTestFn(..) => PadNone,\n-            DynBenchFn(..) => PadOnRight,\n-        }\n-    }\n-}\n-\n-impl fmt::Debug for TestFn {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        f.write_str(match *self {\n-            StaticTestFn(..) => \"StaticTestFn(..)\",\n-            StaticBenchFn(..) => \"StaticBenchFn(..)\",\n-            DynTestFn(..) => \"DynTestFn(..)\",\n-            DynBenchFn(..) => \"DynBenchFn(..)\",\n-        })\n-    }\n-}\n-\n-/// Manager of the benchmarking runs.\n-///\n-/// This is fed into functions marked with `#[bench]` to allow for\n-/// set-up & tear-down before running a piece of code repeatedly via a\n-/// call to `iter`.\n-#[derive(Clone)]\n-pub struct Bencher {\n-    mode: BenchMode,\n-    summary: Option<stats::Summary>,\n-    pub bytes: u64,\n-}\n-\n-#[derive(Clone, PartialEq, Eq)]\n-pub enum BenchMode {\n-    Auto,\n-    Single,\n-}\n-\n-#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n-pub enum ShouldPanic {\n-    No,\n-    Yes,\n-    YesWithMessage(&'static str),\n-}\n-\n-// The definition of a single test. A test runner will run a list of\n-// these.\n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n-pub struct TestDesc {\n-    pub name: TestName,\n-    pub ignore: bool,\n-    pub should_panic: ShouldPanic,\n-    pub allow_fail: bool,\n-    pub test_type: TestType,\n-}\n-\n-#[derive(Debug)]\n-pub struct TestDescAndFn {\n-    pub desc: TestDesc,\n-    pub testfn: TestFn,\n-}\n-\n-#[derive(Clone, PartialEq, Debug, Copy)]\n-pub struct Metric {\n-    value: f64,\n-    noise: f64,\n-}\n-\n-impl Metric {\n-    pub fn new(value: f64, noise: f64) -> Metric {\n-        Metric { value, noise }\n-    }\n-}\n+use bench::*;\n+use test_result::*;\n+use types::*;\n+use options::*;\n+use cli::*;\n \n-/// In case we want to add other options as well, just add them in this struct.\n-#[derive(Copy, Clone, Debug)]\n-pub struct Options {\n-    display_output: bool,\n-    panic_abort: bool,\n-}\n+use helpers::concurrency::get_concurrency;\n+use helpers::metrics::MetricMap;\n \n-impl Options {\n-    pub fn new() -> Options {\n-        Options {\n-            display_output: false,\n-            panic_abort: false,\n-        }\n-    }\n+mod formatters;\n+pub mod stats;\n \n-    pub fn display_output(mut self, display_output: bool) -> Options {\n-        self.display_output = display_output;\n-        self\n-    }\n+mod cli;\n+mod helpers;\n+mod time;\n+mod types;\n+mod options;\n+mod bench;\n+mod test_result;\n \n-    pub fn panic_abort(mut self, panic_abort: bool) -> Options {\n-        self.panic_abort = panic_abort;\n-        self\n-    }\n-}\n+use crate::formatters::{JsonFormatter, OutputFormatter, PrettyFormatter, TerseFormatter};\n \n // The default console test runner. It accepts the command line\n // arguments and a vector of test_descs.\n@@ -440,556 +200,6 @@ pub fn assert_test_result<T: Termination>(result: T) {\n     );\n }\n \n-#[derive(Copy, Clone, Debug)]\n-pub enum ColorConfig {\n-    AutoColor,\n-    AlwaysColor,\n-    NeverColor,\n-}\n-\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-pub enum OutputFormat {\n-    Pretty,\n-    Terse,\n-    Json,\n-}\n-\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-pub enum RunIgnored {\n-    Yes,\n-    No,\n-    Only,\n-}\n-\n-/// Structure denoting time limits for test execution.\n-#[derive(Copy, Clone, Debug, Default, PartialEq, Eq)]\n-pub struct TimeThreshold {\n-    pub warn: Duration,\n-    pub critical: Duration,\n-}\n-\n-impl TimeThreshold {\n-    /// Creates a new `TimeThreshold` instance with provided durations.\n-    pub fn new(warn: Duration, critical: Duration) -> Self {\n-        Self {\n-            warn,\n-            critical,\n-        }\n-    }\n-\n-    /// Attempts to create a `TimeThreshold` instance with values obtained\n-    /// from the environment variable, and returns `None` if the variable\n-    /// is not set.\n-    /// Environment variable format is expected to match `\\d+,\\d+`.\n-    ///\n-    /// # Panics\n-    ///\n-    /// Panics if variable with provided name is set but contains inappropriate\n-    /// value.\n-    pub fn from_env_var(env_var_name: &str) -> Option<Self> {\n-        let durations_str = env::var(env_var_name).ok()?;\n-\n-        // Split string into 2 substrings by comma and try to parse numbers.\n-        let mut durations = durations_str\n-            .splitn(2, ',')\n-            .map(|v| {\n-                u64::from_str(v).unwrap_or_else(|_| {\n-                    panic!(\n-                        \"Duration value in variable {} is expected to be a number, but got {}\",\n-                        env_var_name, v\n-                    )\n-                })\n-            });\n-\n-        // Callback to be called if the environment variable has unexpected structure.\n-        let panic_on_incorrect_value = || {\n-            panic!(\n-                \"Duration variable {} expected to have 2 numbers separated by comma, but got {}\",\n-                env_var_name, durations_str\n-            );\n-        };\n-\n-        let (warn, critical) = (\n-            durations.next().unwrap_or_else(panic_on_incorrect_value),\n-            durations.next().unwrap_or_else(panic_on_incorrect_value)\n-        );\n-\n-        if warn > critical {\n-            panic!(\"Test execution warn time should be less or equal to the critical time\");\n-        }\n-\n-        Some(Self::new(Duration::from_millis(warn), Duration::from_millis(critical)))\n-    }\n-}\n-\n-/// Structure with parameters for calculating test execution time.\n-#[derive(Copy, Clone, Debug, Default, PartialEq, Eq)]\n-pub struct TestTimeOptions {\n-    /// Denotes if the test critical execution time limit excess should be considered\n-    /// a test failure.\n-    pub error_on_excess: bool,\n-    pub colored: bool,\n-    pub unit_threshold: TimeThreshold,\n-    pub integration_threshold: TimeThreshold,\n-    pub doctest_threshold: TimeThreshold,\n-}\n-\n-impl TestTimeOptions {\n-    pub fn new_from_env(error_on_excess: bool, colored: bool) -> Self {\n-        let unit_threshold =\n-            TimeThreshold::from_env_var(time_constants::UNIT_ENV_NAME)\n-                .unwrap_or_else(Self::default_unit);\n-\n-        let integration_threshold =\n-            TimeThreshold::from_env_var(time_constants::INTEGRATION_ENV_NAME)\n-                .unwrap_or_else(Self::default_integration);\n-\n-        let doctest_threshold =\n-            TimeThreshold::from_env_var(time_constants::DOCTEST_ENV_NAME)\n-                .unwrap_or_else(Self::default_doctest);\n-\n-        Self {\n-            error_on_excess,\n-            colored,\n-            unit_threshold,\n-            integration_threshold,\n-            doctest_threshold,\n-        }\n-    }\n-\n-    pub fn is_warn(&self, test: &TestDesc, exec_time: &TestExecTime) -> bool {\n-        exec_time.0 >= self.warn_time(test)\n-    }\n-\n-    pub fn is_critical(&self, test: &TestDesc, exec_time: &TestExecTime) -> bool {\n-        exec_time.0 >= self.critical_time(test)\n-    }\n-\n-    fn warn_time(&self, test: &TestDesc) -> Duration {\n-        match test.test_type {\n-            TestType::UnitTest => self.unit_threshold.warn,\n-            TestType::IntegrationTest => self.integration_threshold.warn,\n-            TestType::DocTest => self.doctest_threshold.warn,\n-            TestType::Unknown => time_constants::UNKNOWN_WARN,\n-        }\n-    }\n-\n-    fn critical_time(&self, test: &TestDesc) -> Duration {\n-        match test.test_type {\n-            TestType::UnitTest => self.unit_threshold.critical,\n-            TestType::IntegrationTest => self.integration_threshold.critical,\n-            TestType::DocTest => self.doctest_threshold.critical,\n-            TestType::Unknown => time_constants::UNKNOWN_CRITICAL,\n-        }\n-    }\n-\n-    fn default_unit() -> TimeThreshold {\n-        TimeThreshold::new(time_constants::UNIT_WARN, time_constants::UNIT_CRITICAL)\n-    }\n-\n-    fn default_integration() -> TimeThreshold {\n-        TimeThreshold::new(time_constants::INTEGRATION_WARN, time_constants::INTEGRATION_CRITICAL)\n-    }\n-\n-    fn default_doctest() -> TimeThreshold {\n-        TimeThreshold::new(time_constants::DOCTEST_WARN, time_constants::DOCTEST_CRITICAL)\n-    }\n-}\n-\n-#[derive(Debug)]\n-pub struct TestOpts {\n-    pub list: bool,\n-    pub filter: Option<String>,\n-    pub filter_exact: bool,\n-    pub exclude_should_panic: bool,\n-    pub run_ignored: RunIgnored,\n-    pub run_tests: bool,\n-    pub bench_benchmarks: bool,\n-    pub logfile: Option<PathBuf>,\n-    pub nocapture: bool,\n-    pub color: ColorConfig,\n-    pub format: OutputFormat,\n-    pub test_threads: Option<usize>,\n-    pub skip: Vec<String>,\n-    pub time_options: Option<TestTimeOptions>,\n-    pub options: Options,\n-}\n-\n-/// Result of parsing the options.\n-pub type OptRes = Result<TestOpts, String>;\n-/// Result of parsing the option part.\n-type OptPartRes<T> = Result<Option<T>, String>;\n-\n-fn optgroups() -> getopts::Options {\n-    let mut opts = getopts::Options::new();\n-    opts.optflag(\"\", \"include-ignored\", \"Run ignored and not ignored tests\")\n-        .optflag(\"\", \"ignored\", \"Run only ignored tests\")\n-        .optflag(\"\", \"exclude-should-panic\", \"Excludes tests marked as should_panic\")\n-        .optflag(\"\", \"test\", \"Run tests and not benchmarks\")\n-        .optflag(\"\", \"bench\", \"Run benchmarks instead of tests\")\n-        .optflag(\"\", \"list\", \"List all tests and benchmarks\")\n-        .optflag(\"h\", \"help\", \"Display this message (longer with --help)\")\n-        .optopt(\n-            \"\",\n-            \"logfile\",\n-            \"Write logs to the specified file instead \\\n-             of stdout\",\n-            \"PATH\",\n-        )\n-        .optflag(\n-            \"\",\n-            \"nocapture\",\n-            \"don't capture stdout/stderr of each \\\n-             task, allow printing directly\",\n-        )\n-        .optopt(\n-            \"\",\n-            \"test-threads\",\n-            \"Number of threads used for running tests \\\n-             in parallel\",\n-            \"n_threads\",\n-        )\n-        .optmulti(\n-            \"\",\n-            \"skip\",\n-            \"Skip tests whose names contain FILTER (this flag can \\\n-             be used multiple times)\",\n-            \"FILTER\",\n-        )\n-        .optflag(\n-            \"q\",\n-            \"quiet\",\n-            \"Display one character per test instead of one line. \\\n-             Alias to --format=terse\",\n-        )\n-        .optflag(\n-            \"\",\n-            \"exact\",\n-            \"Exactly match filters rather than by substring\",\n-        )\n-        .optopt(\n-            \"\",\n-            \"color\",\n-            \"Configure coloring of output:\n-            auto   = colorize if stdout is a tty and tests are run on serially (default);\n-            always = always colorize output;\n-            never  = never colorize output;\",\n-            \"auto|always|never\",\n-        )\n-        .optopt(\n-            \"\",\n-            \"format\",\n-            \"Configure formatting of output:\n-            pretty = Print verbose output;\n-            terse  = Display one character per test;\n-            json   = Output a json document\",\n-            \"pretty|terse|json\",\n-        )\n-        .optflag(\n-            \"\",\n-            \"show-output\",\n-            \"Show captured stdout of successful tests\"\n-        )\n-        .optopt(\n-            \"Z\",\n-            \"\",\n-            \"Enable nightly-only flags:\n-            unstable-options = Allow use of experimental features\",\n-            \"unstable-options\",\n-        )\n-        .optflagopt(\n-            \"\",\n-            \"report-time\",\n-            \"Show execution time of each test. Awailable values:\n-            plain   = do not colorize the execution time (default);\n-            colored = colorize output according to the `color` parameter value;\n-\n-            Threshold values for colorized output can be configured via\n-            `RUST_TEST_TIME_UNIT`, `RUST_TEST_TIME_INTEGRATION` and\n-            `RUST_TEST_TIME_DOCTEST` environment variables.\n-\n-            Expected format of environment variable is `VARIABLE=WARN_TIME,CRITICAL_TIME`.\n-\n-            Not available for --format=terse\",\n-            \"plain|colored\"\n-        )\n-        .optflag(\n-            \"\",\n-            \"ensure-time\",\n-            \"Treat excess of the test execution time limit as error.\n-\n-            Threshold values for this option can be configured via\n-            `RUST_TEST_TIME_UNIT`, `RUST_TEST_TIME_INTEGRATION` and\n-            `RUST_TEST_TIME_DOCTEST` environment variables.\n-\n-            Expected format of environment variable is `VARIABLE=WARN_TIME,CRITICAL_TIME`.\n-\n-            `CRITICAL_TIME` here means the limit that should not be exceeded by test.\n-            \"\n-        );\n-    return opts;\n-}\n-\n-fn usage(binary: &str, options: &getopts::Options) {\n-    let message = format!(\"Usage: {} [OPTIONS] [FILTER]\", binary);\n-    println!(\n-        r#\"{usage}\n-\n-The FILTER string is tested against the name of all tests, and only those\n-tests whose names contain the filter are run.\n-\n-By default, all tests are run in parallel. This can be altered with the\n---test-threads flag or the RUST_TEST_THREADS environment variable when running\n-tests (set it to 1).\n-\n-All tests have their standard output and standard error captured by default.\n-This can be overridden with the --nocapture flag or setting RUST_TEST_NOCAPTURE\n-environment variable to a value other than \"0\". Logging is not captured by default.\n-\n-Test Attributes:\n-\n-    `#[test]`        - Indicates a function is a test to be run. This function\n-                       takes no arguments.\n-    `#[bench]`       - Indicates a function is a benchmark to be run. This\n-                       function takes one argument (test::Bencher).\n-    `#[should_panic]` - This function (also labeled with `#[test]`) will only pass if\n-                        the code causes a panic (an assertion failure or panic!)\n-                        A message may be provided, which the failure string must\n-                        contain: #[should_panic(expected = \"foo\")].\n-    `#[ignore]`       - When applied to a function which is already attributed as a\n-                        test, then the test runner will ignore these tests during\n-                        normal test runs. Running with --ignored or --include-ignored will run\n-                        these tests.\"#,\n-        usage = options.usage(&message)\n-    );\n-}\n-\n-// FIXME: Copied from libsyntax until linkage errors are resolved. Issue #47566\n-fn is_nightly() -> bool {\n-    // Whether this is a feature-staged build, i.e., on the beta or stable channel\n-    let disable_unstable_features = option_env!(\"CFG_DISABLE_UNSTABLE_FEATURES\").is_some();\n-    // Whether we should enable unstable features for bootstrapping\n-    let bootstrap = env::var(\"RUSTC_BOOTSTRAP\").is_ok();\n-\n-    bootstrap || !disable_unstable_features\n-}\n-\n-// Gets the option value and checks if unstable features are enabled.\n-macro_rules! unstable_optflag {\n-    ($matches:ident, $allow_unstable:ident, $option_name:literal) => {{\n-        let opt = $matches.opt_present($option_name);\n-        if !$allow_unstable && opt {\n-            return Some(Err(format!(\n-                \"The \\\"{}\\\" flag is only accepted on the nightly compiler\",\n-                $option_name\n-            )));\n-        }\n-\n-        opt\n-    }};\n-}\n-\n-// Gets the CLI options assotiated with `report-time` feature.\n-fn get_time_options(\n-    matches: &getopts::Matches,\n-    allow_unstable: bool)\n--> Option<OptPartRes<TestTimeOptions>> {\n-    let report_time = unstable_optflag!(matches, allow_unstable, \"report-time\");\n-    let colored_opt_str = matches.opt_str(\"report-time\");\n-    let mut report_time_colored = report_time && colored_opt_str == Some(\"colored\".into());\n-    let ensure_test_time = unstable_optflag!(matches, allow_unstable, \"ensure-time\");\n-\n-    // If `ensure-test-time` option is provided, time output is enforced,\n-    // so user won't be confused if any of tests will silently fail.\n-    let options = if report_time || ensure_test_time {\n-        if ensure_test_time && !report_time {\n-            report_time_colored = true;\n-        }\n-        Some(TestTimeOptions::new_from_env(ensure_test_time, report_time_colored))\n-    } else {\n-        None\n-    };\n-\n-    Some(Ok(options))\n-}\n-\n-// Parses command line arguments into test options\n-pub fn parse_opts(args: &[String]) -> Option<OptRes> {\n-    let mut allow_unstable = false;\n-    let opts = optgroups();\n-    let args = args.get(1..).unwrap_or(args);\n-    let matches = match opts.parse(args) {\n-        Ok(m) => m,\n-        Err(f) => return Some(Err(f.to_string())),\n-    };\n-\n-    if let Some(opt) = matches.opt_str(\"Z\") {\n-        if !is_nightly() {\n-            return Some(Err(\n-                \"the option `Z` is only accepted on the nightly compiler\".into(),\n-            ));\n-        }\n-\n-        match &*opt {\n-            \"unstable-options\" => {\n-                allow_unstable = true;\n-            }\n-            _ => {\n-                return Some(Err(\"Unrecognized option to `Z`\".into()));\n-            }\n-        }\n-    };\n-\n-    if matches.opt_present(\"h\") {\n-        usage(&args[0], &opts);\n-        return None;\n-    }\n-\n-    let filter = if !matches.free.is_empty() {\n-        Some(matches.free[0].clone())\n-    } else {\n-        None\n-    };\n-\n-    let exclude_should_panic = unstable_optflag!(matches, allow_unstable, \"exclude-should-panic\");\n-\n-    let include_ignored = unstable_optflag!(matches, allow_unstable, \"include-ignored\");\n-\n-    let run_ignored = match (include_ignored, matches.opt_present(\"ignored\")) {\n-        (true, true) => {\n-            return Some(Err(\n-                \"the options --include-ignored and --ignored are mutually exclusive\".into(),\n-            ));\n-        }\n-        (true, false) => RunIgnored::Yes,\n-        (false, true) => RunIgnored::Only,\n-        (false, false) => RunIgnored::No,\n-    };\n-    let quiet = matches.opt_present(\"quiet\");\n-    let exact = matches.opt_present(\"exact\");\n-    let list = matches.opt_present(\"list\");\n-\n-    let logfile = matches.opt_str(\"logfile\");\n-    let logfile = logfile.map(|s| PathBuf::from(&s));\n-\n-    let bench_benchmarks = matches.opt_present(\"bench\");\n-    let run_tests = !bench_benchmarks || matches.opt_present(\"test\");\n-\n-    let mut nocapture = matches.opt_present(\"nocapture\");\n-    if !nocapture {\n-        nocapture = match env::var(\"RUST_TEST_NOCAPTURE\") {\n-            Ok(val) => &val != \"0\",\n-            Err(_) => false,\n-        };\n-    }\n-\n-    let time_options = match get_time_options(&matches, allow_unstable) {\n-        Some(Ok(val)) => val,\n-        Some(Err(e)) => return Some(Err(e)),\n-        None => panic!(\"Unexpected output from `get_time_options`\"),\n-    };\n-\n-    let test_threads = match matches.opt_str(\"test-threads\") {\n-        Some(n_str) => match n_str.parse::<usize>() {\n-            Ok(0) => return Some(Err(\"argument for --test-threads must not be 0\".to_string())),\n-            Ok(n) => Some(n),\n-            Err(e) => {\n-                return Some(Err(format!(\n-                    \"argument for --test-threads must be a number > 0 \\\n-                     (error: {})\",\n-                    e\n-                )));\n-            }\n-        },\n-        None => None,\n-    };\n-\n-    let color = match matches.opt_str(\"color\").as_ref().map(|s| &**s) {\n-        Some(\"auto\") | None => AutoColor,\n-        Some(\"always\") => AlwaysColor,\n-        Some(\"never\") => NeverColor,\n-\n-        Some(v) => {\n-            return Some(Err(format!(\n-                \"argument for --color must be auto, always, or never (was \\\n-                 {})\",\n-                v\n-            )));\n-        }\n-    };\n-\n-    let format = match matches.opt_str(\"format\").as_ref().map(|s| &**s) {\n-        None if quiet => OutputFormat::Terse,\n-        Some(\"pretty\") | None => OutputFormat::Pretty,\n-        Some(\"terse\") => OutputFormat::Terse,\n-        Some(\"json\") => {\n-            if !allow_unstable {\n-                return Some(Err(\n-                    \"The \\\"json\\\" format is only accepted on the nightly compiler\".into(),\n-                ));\n-            }\n-            OutputFormat::Json\n-        }\n-\n-        Some(v) => {\n-            return Some(Err(format!(\n-                \"argument for --format must be pretty, terse, or json (was \\\n-                 {})\",\n-                v\n-            )));\n-        }\n-    };\n-\n-    let test_opts = TestOpts {\n-        list,\n-        filter,\n-        filter_exact: exact,\n-        exclude_should_panic,\n-        run_ignored,\n-        run_tests,\n-        bench_benchmarks,\n-        logfile,\n-        nocapture,\n-        color,\n-        format,\n-        test_threads,\n-        skip: matches.opt_strs(\"skip\"),\n-        time_options,\n-        options: Options::new().display_output(matches.opt_present(\"show-output\")),\n-    };\n-\n-    Some(Ok(test_opts))\n-}\n-\n-#[derive(Debug, Clone, PartialEq)]\n-pub struct BenchSamples {\n-    ns_iter_summ: stats::Summary,\n-    mb_s: usize,\n-}\n-\n-#[derive(Debug, Clone, PartialEq)]\n-pub enum TestResult {\n-    TrOk,\n-    TrFailed,\n-    TrFailedMsg(String),\n-    TrIgnored,\n-    TrAllowedFail,\n-    TrBench(BenchSamples),\n-    TrTimedFail,\n-}\n-\n-unsafe impl Send for TestResult {}\n-\n-/// The meassured execution time of a unit test.\n-#[derive(Clone, PartialEq)]\n-pub struct TestExecTime(Duration);\n-\n-impl fmt::Display for TestExecTime {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        write!(f, \"{:.3}s\", self.0.as_secs_f64())\n-    }\n-}\n-\n enum OutputLocation<T> {\n     Pretty(Box<term::StdoutTerminal>),\n     Raw(T),\n@@ -1071,7 +281,7 @@ impl ConsoleTestState {\n \n     pub fn write_log_result(&mut self,test: &TestDesc,\n         result: &TestResult,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n     ) -> io::Result<()> {\n         self.write_log(|| format!(\n             \"{} {}\",\n@@ -1097,52 +307,6 @@ impl ConsoleTestState {\n     }\n }\n \n-// Format a number with thousands separators\n-fn fmt_thousands_sep(mut n: usize, sep: char) -> String {\n-    use std::fmt::Write;\n-    let mut output = String::new();\n-    let mut trailing = false;\n-    for &pow in &[9, 6, 3, 0] {\n-        let base = 10_usize.pow(pow);\n-        if pow == 0 || trailing || n / base != 0 {\n-            if !trailing {\n-                output.write_fmt(format_args!(\"{}\", n / base)).unwrap();\n-            } else {\n-                output.write_fmt(format_args!(\"{:03}\", n / base)).unwrap();\n-            }\n-            if pow != 0 {\n-                output.push(sep);\n-            }\n-            trailing = true;\n-        }\n-        n %= base;\n-    }\n-\n-    output\n-}\n-\n-pub fn fmt_bench_samples(bs: &BenchSamples) -> String {\n-    use std::fmt::Write;\n-    let mut output = String::new();\n-\n-    let median = bs.ns_iter_summ.median as usize;\n-    let deviation = (bs.ns_iter_summ.max - bs.ns_iter_summ.min) as usize;\n-\n-    output\n-        .write_fmt(format_args!(\n-            \"{:>11} ns/iter (+/- {})\",\n-            fmt_thousands_sep(median, ','),\n-            fmt_thousands_sep(deviation, ',')\n-        ))\n-        .unwrap();\n-    if bs.mb_s != 0 {\n-        output\n-            .write_fmt(format_args!(\" = {} MB/s\", bs.mb_s))\n-            .unwrap();\n-    }\n-    output\n-}\n-\n // List the tests to console, and optionally to logfile. Filters are honored.\n pub fn list_tests_console(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> io::Result<()> {\n     let mut output = match term::stdout() {\n@@ -1271,14 +435,14 @@ pub fn run_tests_console(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> io::Resu\n     let mut out: Box<dyn OutputFormatter> = match opts.format {\n         OutputFormat::Pretty => Box::new(PrettyFormatter::new(\n             output,\n-            use_color(opts),\n+            opts.use_color(),\n             max_name_len,\n             is_multithreaded,\n             opts.time_options,\n         )),\n         OutputFormat::Terse => Box::new(TerseFormatter::new(\n             output,\n-            use_color(opts),\n+            opts.use_color(),\n             max_name_len,\n             is_multithreaded,\n         )),\n@@ -1299,55 +463,16 @@ pub fn run_tests_console(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> io::Resu\n     return out.write_run_finish(&st);\n }\n \n-fn use_color(opts: &TestOpts) -> bool {\n-    match opts.color {\n-        AutoColor => !opts.nocapture && stdout_isatty(),\n-        AlwaysColor => true,\n-        NeverColor => false,\n-    }\n-}\n-\n-#[cfg(any(\n-    target_os = \"cloudabi\",\n-    all(target_arch = \"wasm32\", not(target_os = \"emscripten\")),\n-    all(target_vendor = \"fortanix\", target_env = \"sgx\")\n-))]\n-fn stdout_isatty() -> bool {\n-    // FIXME: Implement isatty on SGX\n-    false\n-}\n-#[cfg(unix)]\n-fn stdout_isatty() -> bool {\n-    unsafe { libc::isatty(libc::STDOUT_FILENO) != 0 }\n-}\n-#[cfg(windows)]\n-fn stdout_isatty() -> bool {\n-    type DWORD = u32;\n-    type BOOL = i32;\n-    type HANDLE = *mut u8;\n-    type LPDWORD = *mut u32;\n-    const STD_OUTPUT_HANDLE: DWORD = -11i32 as DWORD;\n-    extern \"system\" {\n-        fn GetStdHandle(which: DWORD) -> HANDLE;\n-        fn GetConsoleMode(hConsoleHandle: HANDLE, lpMode: LPDWORD) -> BOOL;\n-    }\n-    unsafe {\n-        let handle = GetStdHandle(STD_OUTPUT_HANDLE);\n-        let mut out = 0;\n-        GetConsoleMode(handle, &mut out) != 0\n-    }\n-}\n-\n #[derive(Clone)]\n pub enum TestEvent {\n     TeFiltered(Vec<TestDesc>),\n     TeWait(TestDesc),\n-    TeResult(TestDesc, TestResult, Option<TestExecTime>, Vec<u8>),\n+    TeResult(TestDesc, TestResult, Option<time::TestExecTime>, Vec<u8>),\n     TeTimeout(TestDesc),\n     TeFilteredOut(usize),\n }\n \n-pub type MonitorMsg = (TestDesc, TestResult, Option<TestExecTime>, Vec<u8>);\n+pub type MonitorMsg = (TestDesc, TestResult, Option<time::TestExecTime>, Vec<u8>);\n \n struct Sink(Arc<Mutex<Vec<u8>>>);\n impl Write for Sink {\n@@ -1359,18 +484,6 @@ impl Write for Sink {\n     }\n }\n \n-#[derive(Clone, Copy)]\n-pub enum RunStrategy {\n-    /// Runs the test in the current process, and sends the result back over the\n-    /// supplied channel.\n-    InProcess,\n-\n-    /// Spawns a subprocess to run the test, and sends the result back over the\n-    /// supplied channel. Requires `argv[0]` to exist and point to the binary\n-    /// that's currently running.\n-    SpawnPrimary,\n-}\n-\n pub fn run_tests<F>(opts: &TestOpts, tests: Vec<TestDescAndFn>, mut callback: F) -> io::Result<()>\n where\n     F: FnMut(TestEvent) -> io::Result<()>,\n@@ -1467,7 +580,7 @@ where\n         while pending > 0 || !remaining.is_empty() {\n             while pending < concurrency && !remaining.is_empty() {\n                 let test = remaining.pop().unwrap();\n-                let timeout = Instant::now() + Duration::from_secs(TEST_WARN_TIMEOUT_S);\n+                let timeout = time::get_default_test_timeout();\n                 running_tests.insert(test.desc.clone(), timeout);\n                 callback(TeWait(test.desc.clone()))?; //here no pad\n                 run_test(opts, !opts.run_tests, test, run_strategy, tx.clone(), Concurrent::Yes);\n@@ -1510,153 +623,6 @@ where\n     Ok(())\n }\n \n-#[allow(deprecated)]\n-fn get_concurrency() -> usize {\n-    return match env::var(\"RUST_TEST_THREADS\") {\n-        Ok(s) => {\n-            let opt_n: Option<usize> = s.parse().ok();\n-            match opt_n {\n-                Some(n) if n > 0 => n,\n-                _ => panic!(\n-                    \"RUST_TEST_THREADS is `{}`, should be a positive integer.\",\n-                    s\n-                ),\n-            }\n-        }\n-        Err(..) => num_cpus(),\n-    };\n-\n-    #[cfg(windows)]\n-    #[allow(nonstandard_style)]\n-    fn num_cpus() -> usize {\n-        #[repr(C)]\n-        struct SYSTEM_INFO {\n-            wProcessorArchitecture: u16,\n-            wReserved: u16,\n-            dwPageSize: u32,\n-            lpMinimumApplicationAddress: *mut u8,\n-            lpMaximumApplicationAddress: *mut u8,\n-            dwActiveProcessorMask: *mut u8,\n-            dwNumberOfProcessors: u32,\n-            dwProcessorType: u32,\n-            dwAllocationGranularity: u32,\n-            wProcessorLevel: u16,\n-            wProcessorRevision: u16,\n-        }\n-        extern \"system\" {\n-            fn GetSystemInfo(info: *mut SYSTEM_INFO) -> i32;\n-        }\n-        unsafe {\n-            let mut sysinfo = std::mem::zeroed();\n-            GetSystemInfo(&mut sysinfo);\n-            sysinfo.dwNumberOfProcessors as usize\n-        }\n-    }\n-\n-    #[cfg(target_os = \"vxworks\")]\n-    fn num_cpus() -> usize {\n-        // FIXME: Implement num_cpus on vxWorks\n-        1\n-    }\n-\n-    #[cfg(target_os = \"redox\")]\n-    fn num_cpus() -> usize {\n-        // FIXME: Implement num_cpus on Redox\n-        1\n-    }\n-\n-    #[cfg(any(\n-        all(target_arch = \"wasm32\", not(target_os = \"emscripten\")),\n-        all(target_vendor = \"fortanix\", target_env = \"sgx\")\n-    ))]\n-    fn num_cpus() -> usize {\n-        1\n-    }\n-\n-    #[cfg(any(\n-        target_os = \"android\",\n-        target_os = \"cloudabi\",\n-        target_os = \"emscripten\",\n-        target_os = \"fuchsia\",\n-        target_os = \"ios\",\n-        target_os = \"linux\",\n-        target_os = \"macos\",\n-        target_os = \"solaris\",\n-    ))]\n-    fn num_cpus() -> usize {\n-        unsafe { libc::sysconf(libc::_SC_NPROCESSORS_ONLN) as usize }\n-    }\n-\n-    #[cfg(any(\n-        target_os = \"freebsd\",\n-        target_os = \"dragonfly\",\n-        target_os = \"netbsd\"\n-    ))]\n-    fn num_cpus() -> usize {\n-        use std::ptr;\n-\n-        let mut cpus: libc::c_uint = 0;\n-        let mut cpus_size = std::mem::size_of_val(&cpus);\n-\n-        unsafe {\n-            cpus = libc::sysconf(libc::_SC_NPROCESSORS_ONLN) as libc::c_uint;\n-        }\n-        if cpus < 1 {\n-            let mut mib = [libc::CTL_HW, libc::HW_NCPU, 0, 0];\n-            unsafe {\n-                libc::sysctl(\n-                    mib.as_mut_ptr(),\n-                    2,\n-                    &mut cpus as *mut _ as *mut _,\n-                    &mut cpus_size as *mut _ as *mut _,\n-                    ptr::null_mut(),\n-                    0,\n-                );\n-            }\n-            if cpus < 1 {\n-                cpus = 1;\n-            }\n-        }\n-        cpus as usize\n-    }\n-\n-    #[cfg(target_os = \"openbsd\")]\n-    fn num_cpus() -> usize {\n-        use std::ptr;\n-\n-        let mut cpus: libc::c_uint = 0;\n-        let mut cpus_size = std::mem::size_of_val(&cpus);\n-        let mut mib = [libc::CTL_HW, libc::HW_NCPU, 0, 0];\n-\n-        unsafe {\n-            libc::sysctl(\n-                mib.as_mut_ptr(),\n-                2,\n-                &mut cpus as *mut _ as *mut _,\n-                &mut cpus_size as *mut _ as *mut _,\n-                ptr::null_mut(),\n-                0,\n-            );\n-        }\n-        if cpus < 1 {\n-            cpus = 1;\n-        }\n-        cpus as usize\n-    }\n-\n-    #[cfg(target_os = \"haiku\")]\n-    fn num_cpus() -> usize {\n-        // FIXME: implement\n-        1\n-    }\n-\n-    #[cfg(target_os = \"l4re\")]\n-    fn num_cpus() -> usize {\n-        // FIXME: implement\n-        1\n-    }\n-}\n-\n pub fn filter_tests(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> Vec<TestDescAndFn> {\n     let mut filtered = tests;\n     let matches_filter = |test: &TestDescAndFn, filter: &str| {\n@@ -1748,7 +714,7 @@ pub fn run_test(\n         pub strategy: RunStrategy,\n         pub nocapture: bool,\n         pub concurrency: Concurrent,\n-        pub time: Option<TestTimeOptions>,\n+        pub time: Option<time::TestTimeOptions>,\n     }\n \n     fn run_test_inner(\n@@ -1835,86 +801,13 @@ fn __rust_begin_short_backtrace<F: FnOnce()>(f: F) {\n     f()\n }\n \n-fn calc_result<'a>(\n-    desc: &TestDesc,\n-    task_result: Result<(), &'a (dyn Any + 'static + Send)>,\n-    time_opts: &Option<TestTimeOptions>,\n-    exec_time: &Option<TestExecTime>\n-) -> TestResult {\n-    let result = match (&desc.should_panic, task_result) {\n-        (&ShouldPanic::No, Ok(())) | (&ShouldPanic::Yes, Err(_)) => TrOk,\n-        (&ShouldPanic::YesWithMessage(msg), Err(ref err)) => {\n-            if err\n-                .downcast_ref::<String>()\n-                .map(|e| &**e)\n-                .or_else(|| err.downcast_ref::<&'static str>().map(|e| *e))\n-                .map(|e| e.contains(msg))\n-                .unwrap_or(false)\n-            {\n-                TrOk\n-            } else {\n-                if desc.allow_fail {\n-                    TrAllowedFail\n-                } else {\n-                    TrFailedMsg(format!(\"panic did not include expected string '{}'\", msg))\n-                }\n-            }\n-        }\n-        (&ShouldPanic::Yes, Ok(())) => TrFailedMsg(\"test did not panic as expected\".to_string()),\n-        _ if desc.allow_fail => TrAllowedFail,\n-        _ => TrFailed,\n-    };\n-\n-    // If test is already failed (or allowed to fail), do not change the result.\n-    if result != TrOk {\n-        return result;\n-    }\n-\n-    // Check if test is failed due to timeout.\n-    if let (Some(opts), Some(time)) = (time_opts, exec_time) {\n-        if opts.error_on_excess && opts.is_critical(desc, time) {\n-            return TrTimedFail;\n-        }\n-    }\n-\n-    result\n-}\n-\n-fn get_result_from_exit_code(\n-    desc: &TestDesc,\n-    code: i32,\n-    time_opts: &Option<TestTimeOptions>,\n-    exec_time: &Option<TestExecTime>,\n-) -> TestResult {\n-    let result = match (desc.allow_fail, code) {\n-        (_, TR_OK) => TrOk,\n-        (true, TR_FAILED) => TrAllowedFail,\n-        (false, TR_FAILED) => TrFailed,\n-        (_, _) => TrFailedMsg(format!(\"got unexpected return code {}\", code)),\n-    };\n-\n-    // If test is already failed (or allowed to fail), do not change the result.\n-    if result != TrOk {\n-        return result;\n-    }\n-\n-    // Check if test is failed due to timeout.\n-    if let (Some(opts), Some(time)) = (time_opts, exec_time) {\n-        if opts.error_on_excess && opts.is_critical(desc, time) {\n-            return TrTimedFail;\n-        }\n-    }\n-\n-    result\n-}\n-\n fn run_test_in_process(\n     desc: TestDesc,\n     nocapture: bool,\n     report_time: bool,\n     testfn: Box<dyn FnOnce() + Send>,\n     monitor_ch: Sender<MonitorMsg>,\n-    time_opts: Option<TestTimeOptions>,\n+    time_opts: Option<time::TestTimeOptions>,\n ) {\n     // Buffer for capturing standard I/O\n     let data = Arc::new(Mutex::new(Vec::new()));\n@@ -1936,7 +829,7 @@ fn run_test_in_process(\n     let result = catch_unwind(AssertUnwindSafe(testfn));\n     let exec_time = start.map(|start| {\n         let duration = start.elapsed();\n-        TestExecTime(duration)\n+        time::TestExecTime(duration)\n     });\n \n     if let Some((printio, panicio)) = oldio {\n@@ -1956,7 +849,7 @@ fn spawn_test_subprocess(\n     desc: TestDesc,\n     report_time: bool,\n     monitor_ch: Sender<MonitorMsg>,\n-    time_opts: Option<TestTimeOptions>,\n+    time_opts: Option<time::TestTimeOptions>,\n ) {\n     let (result, test_output, exec_time) = (|| {\n         let args = env::args().collect::<Vec<_>>();\n@@ -1978,7 +871,7 @@ fn spawn_test_subprocess(\n             };\n         let exec_time = start.map(|start| {\n             let duration = start.elapsed();\n-            TestExecTime(duration)\n+            time::TestExecTime(duration)\n         });\n \n         let std::process::Output { stdout, stderr, status } = output;\n@@ -2025,9 +918,9 @@ fn run_test_in_spawned_subprocess(\n         }\n \n         if let TrOk = test_result {\n-            process::exit(TR_OK);\n+            process::exit(test_result::TR_OK);\n         } else {\n-            process::exit(TR_FAILED);\n+            process::exit(test_result::TR_FAILED);\n         }\n     });\n     let record_result2 = record_result.clone();\n@@ -2053,229 +946,3 @@ fn get_exit_code(status: ExitStatus) -> Result<i32, String> {\n         }\n     }\n }\n-\n-#[derive(Clone, PartialEq)]\n-pub struct MetricMap(BTreeMap<String, Metric>);\n-\n-impl MetricMap {\n-    pub fn new() -> MetricMap {\n-        MetricMap(BTreeMap::new())\n-    }\n-\n-    /// Insert a named `value` (+/- `noise`) metric into the map. The value\n-    /// must be non-negative. The `noise` indicates the uncertainty of the\n-    /// metric, which doubles as the \"noise range\" of acceptable\n-    /// pairwise-regressions on this named value, when comparing from one\n-    /// metric to the next using `compare_to_old`.\n-    ///\n-    /// If `noise` is positive, then it means this metric is of a value\n-    /// you want to see grow smaller, so a change larger than `noise` in the\n-    /// positive direction represents a regression.\n-    ///\n-    /// If `noise` is negative, then it means this metric is of a value\n-    /// you want to see grow larger, so a change larger than `noise` in the\n-    /// negative direction represents a regression.\n-    pub fn insert_metric(&mut self, name: &str, value: f64, noise: f64) {\n-        let m = Metric { value, noise };\n-        self.0.insert(name.to_owned(), m);\n-    }\n-\n-    pub fn fmt_metrics(&self) -> String {\n-        let v = self\n-            .0\n-            .iter()\n-            .map(|(k, v)| format!(\"{}: {} (+/- {})\", *k, v.value, v.noise))\n-            .collect::<Vec<_>>();\n-        v.join(\", \")\n-    }\n-}\n-\n-// Benchmarking\n-\n-pub use std::hint::black_box;\n-\n-impl Bencher {\n-    /// Callback for benchmark functions to run in their body.\n-    pub fn iter<T, F>(&mut self, mut inner: F)\n-    where\n-        F: FnMut() -> T,\n-    {\n-        if self.mode == BenchMode::Single {\n-            ns_iter_inner(&mut inner, 1);\n-            return;\n-        }\n-\n-        self.summary = Some(iter(&mut inner));\n-    }\n-\n-    pub fn bench<F>(&mut self, mut f: F) -> Option<stats::Summary>\n-    where\n-        F: FnMut(&mut Bencher),\n-    {\n-        f(self);\n-        return self.summary;\n-    }\n-}\n-\n-fn ns_from_dur(dur: Duration) -> u64 {\n-    dur.as_secs() * 1_000_000_000 + (dur.subsec_nanos() as u64)\n-}\n-\n-fn ns_iter_inner<T, F>(inner: &mut F, k: u64) -> u64\n-where\n-    F: FnMut() -> T,\n-{\n-    let start = Instant::now();\n-    for _ in 0..k {\n-        black_box(inner());\n-    }\n-    return ns_from_dur(start.elapsed());\n-}\n-\n-pub fn iter<T, F>(inner: &mut F) -> stats::Summary\n-where\n-    F: FnMut() -> T,\n-{\n-    // Initial bench run to get ballpark figure.\n-    let ns_single = ns_iter_inner(inner, 1);\n-\n-    // Try to estimate iter count for 1ms falling back to 1m\n-    // iterations if first run took < 1ns.\n-    let ns_target_total = 1_000_000; // 1ms\n-    let mut n = ns_target_total / cmp::max(1, ns_single);\n-\n-    // if the first run took more than 1ms we don't want to just\n-    // be left doing 0 iterations on every loop. The unfortunate\n-    // side effect of not being able to do as many runs is\n-    // automatically handled by the statistical analysis below\n-    // (i.e., larger error bars).\n-    n = cmp::max(1, n);\n-\n-    let mut total_run = Duration::new(0, 0);\n-    let samples: &mut [f64] = &mut [0.0_f64; 50];\n-    loop {\n-        let loop_start = Instant::now();\n-\n-        for p in &mut *samples {\n-            *p = ns_iter_inner(inner, n) as f64 / n as f64;\n-        }\n-\n-        stats::winsorize(samples, 5.0);\n-        let summ = stats::Summary::new(samples);\n-\n-        for p in &mut *samples {\n-            let ns = ns_iter_inner(inner, 5 * n);\n-            *p = ns as f64 / (5 * n) as f64;\n-        }\n-\n-        stats::winsorize(samples, 5.0);\n-        let summ5 = stats::Summary::new(samples);\n-\n-        let loop_run = loop_start.elapsed();\n-\n-        // If we've run for 100ms and seem to have converged to a\n-        // stable median.\n-        if loop_run > Duration::from_millis(100)\n-            && summ.median_abs_dev_pct < 1.0\n-            && summ.median - summ5.median < summ5.median_abs_dev\n-        {\n-            return summ5;\n-        }\n-\n-        total_run = total_run + loop_run;\n-        // Longest we ever run for is 3s.\n-        if total_run > Duration::from_secs(3) {\n-            return summ5;\n-        }\n-\n-        // If we overflow here just return the results so far. We check a\n-        // multiplier of 10 because we're about to multiply by 2 and the\n-        // next iteration of the loop will also multiply by 5 (to calculate\n-        // the summ5 result)\n-        n = match n.checked_mul(10) {\n-            Some(_) => n * 2,\n-            None => {\n-                return summ5;\n-            }\n-        };\n-    }\n-}\n-\n-pub mod bench {\n-    use super::{\n-        BenchMode, BenchSamples, Bencher, MonitorMsg, Sender, Sink, TestDesc, TestResult\n-    };\n-    use crate::stats;\n-    use std::cmp;\n-    use std::io;\n-    use std::panic::{catch_unwind, AssertUnwindSafe};\n-    use std::sync::{Arc, Mutex};\n-\n-    pub fn benchmark<F>(desc: TestDesc, monitor_ch: Sender<MonitorMsg>, nocapture: bool, f: F)\n-    where\n-        F: FnMut(&mut Bencher),\n-    {\n-        let mut bs = Bencher {\n-            mode: BenchMode::Auto,\n-            summary: None,\n-            bytes: 0,\n-        };\n-\n-        let data = Arc::new(Mutex::new(Vec::new()));\n-        let oldio = if !nocapture {\n-            Some((\n-                io::set_print(Some(Box::new(Sink(data.clone())))),\n-                io::set_panic(Some(Box::new(Sink(data.clone())))),\n-            ))\n-        } else {\n-            None\n-        };\n-\n-        let result = catch_unwind(AssertUnwindSafe(|| bs.bench(f)));\n-\n-        if let Some((printio, panicio)) = oldio {\n-            io::set_print(printio);\n-            io::set_panic(panicio);\n-        }\n-\n-        let test_result = match result {\n-            //bs.bench(f) {\n-            Ok(Some(ns_iter_summ)) => {\n-                let ns_iter = cmp::max(ns_iter_summ.median as u64, 1);\n-                let mb_s = bs.bytes * 1000 / ns_iter;\n-\n-                let bs = BenchSamples {\n-                    ns_iter_summ,\n-                    mb_s: mb_s as usize,\n-                };\n-                TestResult::TrBench(bs)\n-            }\n-            Ok(None) => {\n-                // iter not called, so no data.\n-                // FIXME: error in this case?\n-                let samples: &mut [f64] = &mut [0.0_f64; 1];\n-                let bs = BenchSamples {\n-                    ns_iter_summ: stats::Summary::new(samples),\n-                    mb_s: 0,\n-                };\n-                TestResult::TrBench(bs)\n-            }\n-            Err(_) => TestResult::TrFailed,\n-        };\n-\n-        let stdout = data.lock().unwrap().to_vec();\n-        monitor_ch.send((desc, test_result, None, stdout)).unwrap();\n-    }\n-\n-    pub fn run_once<F>(f: F)\n-    where\n-        F: FnMut(&mut Bencher),\n-    {\n-        let mut bs = Bencher {\n-            mode: BenchMode::Single,\n-            summary: None,\n-            bytes: 0,\n-        };\n-        bs.bench(f);\n-    }\n-}"}, {"sha": "0a604cae0ca3352086fa7ddc5f1940511639b256", "filename": "src/libtest/options.rs", "status": "added", "additions": 80, "deletions": 0, "changes": 80, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Foptions.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Foptions.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Foptions.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,80 @@\n+//! Enums denoting options for test execution.\n+\n+/// Whether to execute tests concurrently or not\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum Concurrent {\n+    Yes,\n+    No,\n+}\n+\n+#[derive(Clone, PartialEq, Eq)]\n+pub enum BenchMode {\n+    Auto,\n+    Single,\n+}\n+\n+#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n+pub enum ShouldPanic {\n+    No,\n+    Yes,\n+    YesWithMessage(&'static str),\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub enum ColorConfig {\n+    AutoColor,\n+    AlwaysColor,\n+    NeverColor,\n+}\n+\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum OutputFormat {\n+    Pretty,\n+    Terse,\n+    Json,\n+}\n+\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum RunIgnored {\n+    Yes,\n+    No,\n+    Only,\n+}\n+\n+#[derive(Clone, Copy)]\n+pub enum RunStrategy {\n+    /// Runs the test in the current process, and sends the result back over the\n+    /// supplied channel.\n+    InProcess,\n+\n+    /// Spawns a subprocess to run the test, and sends the result back over the\n+    /// supplied channel. Requires `argv[0]` to exist and point to the binary\n+    /// that's currently running.\n+    SpawnPrimary,\n+}\n+\n+/// In case we want to add other options as well, just add them in this struct.\n+#[derive(Copy, Clone, Debug)]\n+pub struct Options {\n+    pub display_output: bool,\n+    pub panic_abort: bool,\n+}\n+\n+impl Options {\n+    pub fn new() -> Options {\n+        Options {\n+            display_output: false,\n+            panic_abort: false,\n+        }\n+    }\n+\n+    pub fn display_output(mut self, display_output: bool) -> Options {\n+        self.display_output = display_output;\n+        self\n+    }\n+\n+    pub fn panic_abort(mut self, panic_abort: bool) -> Options {\n+        self.panic_abort = panic_abort;\n+        self\n+    }\n+}"}, {"sha": "4eb3f93e2a42bb0b1d0695d26fbdabd107758763", "filename": "src/libtest/test_result.rs", "status": "added", "additions": 102, "deletions": 0, "changes": 102, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Ftest_result.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Ftest_result.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Ftest_result.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,102 @@\n+\n+use std::any::Any;\n+\n+use super::bench::BenchSamples;\n+use super::time;\n+use super::types::TestDesc;\n+use super::options::ShouldPanic;\n+\n+pub use self::TestResult::*;\n+\n+// Return codes for secondary process.\n+// Start somewhere other than 0 so we know the return code means what we think\n+// it means.\n+pub const TR_OK: i32 = 50;\n+pub const TR_FAILED: i32 = 51;\n+\n+#[derive(Debug, Clone, PartialEq)]\n+pub enum TestResult {\n+    TrOk,\n+    TrFailed,\n+    TrFailedMsg(String),\n+    TrIgnored,\n+    TrAllowedFail,\n+    TrBench(BenchSamples),\n+    TrTimedFail,\n+}\n+\n+unsafe impl Send for TestResult {}\n+\n+\n+pub fn calc_result<'a>(\n+    desc: &TestDesc,\n+    task_result: Result<(), &'a (dyn Any + 'static + Send)>,\n+    time_opts: &Option<time::TestTimeOptions>,\n+    exec_time: &Option<time::TestExecTime>\n+) -> TestResult {\n+    let result = match (&desc.should_panic, task_result) {\n+        (&ShouldPanic::No, Ok(())) | (&ShouldPanic::Yes, Err(_)) => TestResult::TrOk,\n+        (&ShouldPanic::YesWithMessage(msg), Err(ref err)) => {\n+            if err\n+                .downcast_ref::<String>()\n+                .map(|e| &**e)\n+                .or_else(|| err.downcast_ref::<&'static str>().map(|e| *e))\n+                .map(|e| e.contains(msg))\n+                .unwrap_or(false)\n+            {\n+                TestResult::TrOk\n+            } else {\n+                if desc.allow_fail {\n+                    TestResult::TrAllowedFail\n+                } else {\n+                    TestResult::TrFailedMsg(format!(\"panic did not include expected string '{}'\", msg))\n+                }\n+            }\n+        }\n+        (&ShouldPanic::Yes, Ok(())) => TestResult::TrFailedMsg(\"test did not panic as expected\".to_string()),\n+        _ if desc.allow_fail => TestResult::TrAllowedFail,\n+        _ => TestResult::TrFailed,\n+    };\n+\n+    // If test is already failed (or allowed to fail), do not change the result.\n+    if result != TestResult::TrOk {\n+        return result;\n+    }\n+\n+    // Check if test is failed due to timeout.\n+    if let (Some(opts), Some(time)) = (time_opts, exec_time) {\n+        if opts.error_on_excess && opts.is_critical(desc, time) {\n+            return TestResult::TrTimedFail;\n+        }\n+    }\n+\n+    result\n+}\n+\n+pub fn get_result_from_exit_code(\n+    desc: &TestDesc,\n+    code: i32,\n+    time_opts: &Option<time::TestTimeOptions>,\n+    exec_time: &Option<time::TestExecTime>,\n+) -> TestResult {\n+    let result = match (desc.allow_fail, code) {\n+        (_, TR_OK) => TestResult::TrOk,\n+        (true, TR_FAILED) => TestResult::TrAllowedFail,\n+        (false, TR_FAILED) => TestResult::TrFailed,\n+        (_, _) => TestResult::TrFailedMsg(format!(\"got unexpected return code {}\", code)),\n+    };\n+\n+    // If test is already failed (or allowed to fail), do not change the result.\n+    if result != TestResult::TrOk {\n+        return result;\n+    }\n+\n+    // Check if test is failed due to timeout.\n+    if let (Some(opts), Some(time)) = (time_opts, exec_time) {\n+        if opts.error_on_excess && opts.is_critical(desc, time) {\n+            return TestResult::TrTimedFail;\n+        }\n+    }\n+\n+    result\n+}"}, {"sha": "b7ce764505bfefe573184016e9f6342438db567f", "filename": "src/libtest/time.rs", "status": "added", "additions": 206, "deletions": 0, "changes": 206, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Ftime.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Ftime.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Ftime.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,206 @@\n+//! Module `time` contains everything related to the time measurement of unit tests\n+//! execution.\n+//! Two main purposes of this module:\n+//! - Check whether test is timed out.\n+//! - Provide helpers for `report-time` and `measure-time` options. \n+\n+use std::time::{Duration, Instant};\n+use std::str::FromStr;\n+use std::fmt;\n+use std::env;\n+\n+use super::types::{TestDesc, TestType};\n+\n+pub const TEST_WARN_TIMEOUT_S: u64 = 60;\n+\n+/// This small module contains constants used by `report-time` option.\n+/// Those constants values will be used if corresponding environment variables are not set.\n+///\n+/// To override values for unit-tests, use a constant `RUST_TEST_TIME_UNIT`,\n+/// To override values for integration tests, use a constant `RUST_TEST_TIME_INTEGRATION`,\n+/// To override values for doctests, use a constant `RUST_TEST_TIME_DOCTEST`.\n+///\n+/// Example of the expected format is `RUST_TEST_TIME_xxx=100,200`, where 100 means\n+/// warn time, and 200 means critical time.\n+pub mod time_constants {\n+    use std::time::Duration;\n+    use super::TEST_WARN_TIMEOUT_S;\n+\n+    /// Environment variable for overriding default threshold for unit-tests.\n+    pub const UNIT_ENV_NAME: &str = \"RUST_TEST_TIME_UNIT\";\n+\n+    // Unit tests are supposed to be really quick.\n+    pub const UNIT_WARN: Duration = Duration::from_millis(50);\n+    pub const UNIT_CRITICAL: Duration = Duration::from_millis(100);\n+\n+    /// Environment variable for overriding default threshold for unit-tests.\n+    pub const INTEGRATION_ENV_NAME: &str = \"RUST_TEST_TIME_INTEGRATION\";\n+\n+    // Integration tests may have a lot of work, so they can take longer to execute.\n+    pub const INTEGRATION_WARN: Duration = Duration::from_millis(500);\n+    pub const INTEGRATION_CRITICAL: Duration = Duration::from_millis(1000);\n+\n+    /// Environment variable for overriding default threshold for unit-tests.\n+    pub const DOCTEST_ENV_NAME: &str = \"RUST_TEST_TIME_DOCTEST\";\n+\n+    // Doctests are similar to integration tests, because they can include a lot of\n+    // initialization code.\n+    pub const DOCTEST_WARN: Duration = INTEGRATION_WARN;\n+    pub const DOCTEST_CRITICAL: Duration = INTEGRATION_CRITICAL;\n+\n+    // Do not suppose anything about unknown tests, base limits on the\n+    // `TEST_WARN_TIMEOUT_S` constant.\n+    pub const UNKNOWN_WARN: Duration = Duration::from_secs(TEST_WARN_TIMEOUT_S);\n+    pub const UNKNOWN_CRITICAL: Duration = Duration::from_secs(TEST_WARN_TIMEOUT_S * 2);\n+}\n+\n+/// Returns an `Instance` object denoting when the test should be considered\n+/// timed out. \n+pub fn get_default_test_timeout() -> Instant {\n+    Instant::now() + Duration::from_secs(TEST_WARN_TIMEOUT_S)\n+}\n+\n+/// The meassured execution time of a unit test.\n+#[derive(Clone, PartialEq)]\n+pub struct TestExecTime(pub Duration);\n+\n+impl fmt::Display for TestExecTime {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        write!(f, \"{:.3}s\", self.0.as_secs_f64())\n+    }\n+}\n+\n+/// Structure denoting time limits for test execution.\n+#[derive(Copy, Clone, Debug, Default, PartialEq, Eq)]\n+pub struct TimeThreshold {\n+    pub warn: Duration,\n+    pub critical: Duration,\n+}\n+\n+impl TimeThreshold {\n+    /// Creates a new `TimeThreshold` instance with provided durations.\n+    pub fn new(warn: Duration, critical: Duration) -> Self {\n+        Self {\n+            warn,\n+            critical,\n+        }\n+    }\n+\n+    /// Attempts to create a `TimeThreshold` instance with values obtained\n+    /// from the environment variable, and returns `None` if the variable\n+    /// is not set.\n+    /// Environment variable format is expected to match `\\d+,\\d+`.\n+    ///\n+    /// # Panics\n+    ///\n+    /// Panics if variable with provided name is set but contains inappropriate\n+    /// value.\n+    pub fn from_env_var(env_var_name: &str) -> Option<Self> {\n+        let durations_str = env::var(env_var_name).ok()?;\n+\n+        // Split string into 2 substrings by comma and try to parse numbers.\n+        let mut durations = durations_str\n+            .splitn(2, ',')\n+            .map(|v| {\n+                u64::from_str(v).unwrap_or_else(|_| {\n+                    panic!(\n+                        \"Duration value in variable {} is expected to be a number, but got {}\",\n+                        env_var_name, v\n+                    )\n+                })\n+            });\n+\n+        // Callback to be called if the environment variable has unexpected structure.\n+        let panic_on_incorrect_value = || {\n+            panic!(\n+                \"Duration variable {} expected to have 2 numbers separated by comma, but got {}\",\n+                env_var_name, durations_str\n+            );\n+        };\n+\n+        let (warn, critical) = (\n+            durations.next().unwrap_or_else(panic_on_incorrect_value),\n+            durations.next().unwrap_or_else(panic_on_incorrect_value)\n+        );\n+\n+        if warn > critical {\n+            panic!(\"Test execution warn time should be less or equal to the critical time\");\n+        }\n+\n+        Some(Self::new(Duration::from_millis(warn), Duration::from_millis(critical)))\n+    }\n+}\n+\n+/// Structure with parameters for calculating test execution time.\n+#[derive(Copy, Clone, Debug, Default, PartialEq, Eq)]\n+pub struct TestTimeOptions {\n+    /// Denotes if the test critical execution time limit excess should be considered\n+    /// a test failure.\n+    pub error_on_excess: bool,\n+    pub colored: bool,\n+    pub unit_threshold: TimeThreshold,\n+    pub integration_threshold: TimeThreshold,\n+    pub doctest_threshold: TimeThreshold,\n+}\n+\n+impl TestTimeOptions {\n+    pub fn new_from_env(error_on_excess: bool, colored: bool) -> Self {\n+        let unit_threshold =\n+            TimeThreshold::from_env_var(time_constants::UNIT_ENV_NAME)\n+                .unwrap_or_else(Self::default_unit);\n+\n+        let integration_threshold =\n+            TimeThreshold::from_env_var(time_constants::INTEGRATION_ENV_NAME)\n+                .unwrap_or_else(Self::default_integration);\n+\n+        let doctest_threshold =\n+            TimeThreshold::from_env_var(time_constants::DOCTEST_ENV_NAME)\n+                .unwrap_or_else(Self::default_doctest);\n+\n+        Self {\n+            error_on_excess,\n+            colored,\n+            unit_threshold,\n+            integration_threshold,\n+            doctest_threshold,\n+        }\n+    }\n+\n+    pub fn is_warn(&self, test: &TestDesc, exec_time: &TestExecTime) -> bool {\n+        exec_time.0 >= self.warn_time(test)\n+    }\n+\n+    pub fn is_critical(&self, test: &TestDesc, exec_time: &TestExecTime) -> bool {\n+        exec_time.0 >= self.critical_time(test)\n+    }\n+\n+    fn warn_time(&self, test: &TestDesc) -> Duration {\n+        match test.test_type {\n+            TestType::UnitTest => self.unit_threshold.warn,\n+            TestType::IntegrationTest => self.integration_threshold.warn,\n+            TestType::DocTest => self.doctest_threshold.warn,\n+            TestType::Unknown => time_constants::UNKNOWN_WARN,\n+        }\n+    }\n+\n+    fn critical_time(&self, test: &TestDesc) -> Duration {\n+        match test.test_type {\n+            TestType::UnitTest => self.unit_threshold.critical,\n+            TestType::IntegrationTest => self.integration_threshold.critical,\n+            TestType::DocTest => self.doctest_threshold.critical,\n+            TestType::Unknown => time_constants::UNKNOWN_CRITICAL,\n+        }\n+    }\n+\n+    fn default_unit() -> TimeThreshold {\n+        TimeThreshold::new(time_constants::UNIT_WARN, time_constants::UNIT_CRITICAL)\n+    }\n+\n+    fn default_integration() -> TimeThreshold {\n+        TimeThreshold::new(time_constants::INTEGRATION_WARN, time_constants::INTEGRATION_CRITICAL)\n+    }\n+\n+    fn default_doctest() -> TimeThreshold {\n+        TimeThreshold::new(time_constants::DOCTEST_WARN, time_constants::DOCTEST_CRITICAL)\n+    }\n+}"}, {"sha": "89bcf2cf2853be1ee49aaa16c28f0ba30f18b02e", "filename": "src/libtest/types.rs", "status": "added", "additions": 145, "deletions": 0, "changes": 145, "blob_url": "https://github.com/rust-lang/rust/blob/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Ftypes.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d5052203d200474b1a9aacbb0d59666a576ee16/src%2Flibtest%2Ftypes.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Ftypes.rs?ref=4d5052203d200474b1a9aacbb0d59666a576ee16", "patch": "@@ -0,0 +1,145 @@\n+//! Common types used by `libtest`.\n+\n+use std::fmt;\n+use std::borrow::Cow;\n+\n+use super::options;\n+use super::bench::Bencher;\n+\n+pub use NamePadding::*;\n+pub use TestName::*;\n+pub use TestFn::*;\n+\n+/// Type of the test according to the [rust book](https://doc.rust-lang.org/cargo/guide/tests.html)\n+/// conventions.\n+#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n+pub enum TestType {\n+    /// Unit-tests are expected to be in the `src` folder of the crate.\n+    UnitTest,\n+    /// Integration-style tests are expected to be in the `tests` folder of the crate.\n+    IntegrationTest,\n+    /// Doctests are created by the `librustdoc` manually, so it's a different type of test.\n+    DocTest,\n+    /// Tests for the sources that don't follow the project layout convention\n+    /// (e.g. tests in raw `main.rs` compiled by calling `rustc --test` directly).\n+    Unknown,\n+}\n+\n+#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n+pub enum NamePadding {\n+    PadNone,\n+    PadOnRight,\n+}\n+\n+// The name of a test. By convention this follows the rules for rust\n+// paths; i.e., it should be a series of identifiers separated by double\n+// colons. This way if some test runner wants to arrange the tests\n+// hierarchically it may.\n+#[derive(Clone, PartialEq, Eq, Hash, Debug)]\n+pub enum TestName {\n+    StaticTestName(&'static str),\n+    DynTestName(String),\n+    AlignedTestName(Cow<'static, str>, NamePadding),\n+}\n+\n+impl TestName {\n+    pub fn as_slice(&self) -> &str {\n+        match *self {\n+            StaticTestName(s) => s,\n+            DynTestName(ref s) => s,\n+            AlignedTestName(ref s, _) => &*s,\n+        }\n+    }\n+\n+    pub fn padding(&self) -> NamePadding {\n+        match self {\n+            &AlignedTestName(_, p) => p,\n+            _ => PadNone,\n+        }\n+    }\n+\n+    pub fn with_padding(&self, padding: NamePadding) -> TestName {\n+        let name = match self {\n+            &TestName::StaticTestName(name) => Cow::Borrowed(name),\n+            &TestName::DynTestName(ref name) => Cow::Owned(name.clone()),\n+            &TestName::AlignedTestName(ref name, _) => name.clone(),\n+        };\n+\n+        TestName::AlignedTestName(name, padding)\n+    }\n+}\n+impl fmt::Display for TestName {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        fmt::Display::fmt(self.as_slice(), f)\n+    }\n+}\n+\n+/// Represents a benchmark function.\n+pub trait TDynBenchFn: Send {\n+    fn run(&self, harness: &mut Bencher);\n+}\n+\n+// A function that runs a test. If the function returns successfully,\n+// the test succeeds; if the function panics then the test fails. We\n+// may need to come up with a more clever definition of test in order\n+// to support isolation of tests into threads.\n+pub enum TestFn {\n+    StaticTestFn(fn()),\n+    StaticBenchFn(fn(&mut Bencher)),\n+    DynTestFn(Box<dyn FnOnce() + Send>),\n+    DynBenchFn(Box<dyn TDynBenchFn + 'static>),\n+}\n+\n+impl TestFn {\n+    pub fn padding(&self) -> NamePadding {\n+        match *self {\n+            StaticTestFn(..) => PadNone,\n+            StaticBenchFn(..) => PadOnRight,\n+            DynTestFn(..) => PadNone,\n+            DynBenchFn(..) => PadOnRight,\n+        }\n+    }\n+}\n+\n+impl fmt::Debug for TestFn {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        f.write_str(match *self {\n+            StaticTestFn(..) => \"StaticTestFn(..)\",\n+            StaticBenchFn(..) => \"StaticBenchFn(..)\",\n+            DynTestFn(..) => \"DynTestFn(..)\",\n+            DynBenchFn(..) => \"DynBenchFn(..)\",\n+        })\n+    }\n+}\n+\n+// The definition of a single test. A test runner will run a list of\n+// these.\n+#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+pub struct TestDesc {\n+    pub name: TestName,\n+    pub ignore: bool,\n+    pub should_panic: options::ShouldPanic,\n+    pub allow_fail: bool,\n+    pub test_type: TestType,\n+}\n+\n+impl TestDesc {\n+    pub fn padded_name(&self, column_count: usize, align: NamePadding) -> String {\n+        let mut name = String::from(self.name.as_slice());\n+        let fill = column_count.saturating_sub(name.len());\n+        let pad = \" \".repeat(fill);\n+        match align {\n+            PadNone => name,\n+            PadOnRight => {\n+                name.push_str(&pad);\n+                name\n+            }\n+        }\n+    }\n+}\n+\n+#[derive(Debug)]\n+pub struct TestDescAndFn {\n+    pub desc: TestDesc,\n+    pub testfn: TestFn,\n+}"}]}