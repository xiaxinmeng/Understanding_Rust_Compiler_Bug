{"sha": "a2bc4c2a7453f2e72df6f2e6c3273d6b3f0114a9", "node_id": "MDY6Q29tbWl0NzI0NzEyOmEyYmM0YzJhNzQ1M2YyZTcyZGY2ZjJlNmMzMjczZDZiM2YwMTE0YTk=", "commit": {"author": {"name": "Veetaha", "email": "gerzoh1@gmail.com", "date": "2020-01-26T20:32:23Z"}, "committer": {"name": "Veetaha", "email": "gerzoh1@gmail.com", "date": "2020-02-03T22:00:55Z"}, "message": "ra_syntax: fixed doc comment", "tree": {"sha": "9bcfc05c681b6decb685ecc9c712c79a7a2eb328", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/9bcfc05c681b6decb685ecc9c712c79a7a2eb328"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a2bc4c2a7453f2e72df6f2e6c3273d6b3f0114a9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a2bc4c2a7453f2e72df6f2e6c3273d6b3f0114a9", "html_url": "https://github.com/rust-lang/rust/commit/a2bc4c2a7453f2e72df6f2e6c3273d6b3f0114a9", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a2bc4c2a7453f2e72df6f2e6c3273d6b3f0114a9/comments", "author": null, "committer": null, "parents": [{"sha": "ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "url": "https://api.github.com/repos/rust-lang/rust/commits/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "html_url": "https://github.com/rust-lang/rust/commit/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d"}], "stats": {"total": 2, "additions": 1, "deletions": 1}, "files": [{"sha": "9f321cd06814bcfafbb09c45512d0cbd597df2f8", "filename": "crates/ra_syntax/src/parsing/lexer.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/a2bc4c2a7453f2e72df6f2e6c3273d6b3f0114a9/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a2bc4c2a7453f2e72df6f2e6c3273d6b3f0114a9/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs?ref=a2bc4c2a7453f2e72df6f2e6c3273d6b3f0114a9", "patch": "@@ -53,7 +53,7 @@ pub fn tokenize(text: &str) -> ParsedTokens {\n }\n \n /// Break a string up into its component tokens.\n-/// Returns `ParsedTokens` which are basically a pair `(Vec<Token>, Vec<SyntaxError>)`.\n+/// Writes to `ParsedTokens` which are basically a pair `(Vec<Token>, Vec<SyntaxError>)`.\n /// Beware that it checks for shebang first and its length contributes to resulting\n /// tokens offsets.\n pub fn tokenize_append(text: &str, parsed: &mut ParsedTokens) {"}]}