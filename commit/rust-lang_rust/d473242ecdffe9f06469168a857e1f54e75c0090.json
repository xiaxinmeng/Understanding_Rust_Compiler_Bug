{"sha": "d473242ecdffe9f06469168a857e1f54e75c0090", "node_id": "MDY6Q29tbWl0NzI0NzEyOmQ0NzMyNDJlY2RmZmU5ZjA2NDY5MTY4YTg1N2UxZjU0ZTc1YzAwOTA=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-11-29T18:44:28Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-11-29T18:44:28Z"}, "message": "Auto merge of #1617 - JCTyblaidd:data_race_detector, r=RalfJung\n\nAdd simple data-race detector\n\nPartially fixes data-race detection, see #1372, based on Dynamic Race Detection for C++11\n\n- This does not explore weak memory behaviour, only exploring one sequentially consistent ordering.\n- Data-race detection is only enabled after the first thread is created, so should have minimal overhead for non-concurrent execution.\n- ~~Does not attempt to re-use thread id's so creating and joining threads lots of time in an execution will result in the vector clocks growing in size and slowing down program execution~~ It does now", "tree": {"sha": "88c1265a106ed4141675961adf594bee723d2320", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/88c1265a106ed4141675961adf594bee723d2320"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/d473242ecdffe9f06469168a857e1f54e75c0090", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/d473242ecdffe9f06469168a857e1f54e75c0090", "html_url": "https://github.com/rust-lang/rust/commit/d473242ecdffe9f06469168a857e1f54e75c0090", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/d473242ecdffe9f06469168a857e1f54e75c0090/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ce031b4e8e26e48f674287462e741ead4d5bea01", "url": "https://api.github.com/repos/rust-lang/rust/commits/ce031b4e8e26e48f674287462e741ead4d5bea01", "html_url": "https://github.com/rust-lang/rust/commit/ce031b4e8e26e48f674287462e741ead4d5bea01"}, {"sha": "cbb695f782dceca959661e9c57d7aeb120cbc1d8", "url": "https://api.github.com/repos/rust-lang/rust/commits/cbb695f782dceca959661e9c57d7aeb120cbc1d8", "html_url": "https://github.com/rust-lang/rust/commit/cbb695f782dceca959661e9c57d7aeb120cbc1d8"}], "stats": {"total": 3414, "additions": 3224, "deletions": 190}, "files": [{"sha": "78838acb2a5fc6b2ded43fcc6c33b0bcb973386f", "filename": "Cargo.lock", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -282,6 +282,7 @@ dependencies = [\n  \"rustc-workspace-hack\",\n  \"rustc_version\",\n  \"shell-escape\",\n+ \"smallvec\",\n ]\n \n [[package]]\n@@ -496,6 +497,12 @@ version = \"0.1.5\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"45bb67a18fa91266cc7807181f62f9178a6873bfad7dc788c42e6430db40184f\"\n \n+[[package]]\n+name = \"smallvec\"\n+version = \"1.4.2\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"fbee7696b84bbf3d89a1c2eccff0850e3047ed46bfcd2e92c29a2d074d57e252\"\n+\n [[package]]\n name = \"socket2\"\n version = \"0.3.15\""}, {"sha": "4413dab321e72e20557d2ffee6bfdf2316176c58", "filename": "Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/Cargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/Cargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.toml?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -30,6 +30,7 @@ log = \"0.4\"\n shell-escape = \"0.1.4\"\n hex = \"0.4.0\"\n rand = \"0.7\"\n+smallvec = \"1.4.2\"\n \n # A noop dependency that changes in the Rust repository, it's a bit of a hack.\n # See the `src/tools/rustc-workspace-hack/README.md` file in `rust-lang/rust`"}, {"sha": "57e28607105565b064349a42c4f8616e5517f532", "filename": "bench-cargo-miri/mse/src/main.rs", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/bench-cargo-miri%2Fmse%2Fsrc%2Fmain.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/bench-cargo-miri%2Fmse%2Fsrc%2Fmain.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/bench-cargo-miri%2Fmse%2Fsrc%2Fmain.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -2,6 +2,9 @@ static EXPECTED: &[u8] = &[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n static PCM: &[i16] = &[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -2, 0, -2, 0, -2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 0, 0, -2, 0, -2, 0, -2, 0, -2, -2, -2, -3, -3, -3, -3, -4, -2, -5, -2, -5, -2, -4, 0, -4, 0, -4, 0, -4, 1, -4, 1, -4, 2, -4, 2, -4, 2, -4, 2, -4, 2, -3, 1, -4, 0, -4, 0, -5, 0, -5, 0, -5, 0, -4, 2, -4, 3, -4, 4, -3, 5, -2, 5, -3, 6, -3, 6, -3, 5, -3, 5, -2, 4, -2, 3, -5, 0, -6, 0, -3, -2, -4, -4, -9, -5, -9, -4, -4, -2, -4, -2, -4, 0, -2, 1, 1, 1, 4, 2, 8, 2, 12, 1, 13, 0, 12, 0, 11, 0, 8, -2, 7, 0, 7, -3, 11, -8, 15, -9, 17, -6, 17, -5, 13, -3, 7, 0, 3, 0, -2, 0, -4, 0, -4, -2, -6, 0, -14, -2, -17, -4, -8, 0, -7, 5, -17, 7, -18, 10, -7, 18, -2, 25, -3, 27, 0, 31, 4, 34, 4, 34, 8, 36, 8, 37, 2, 36, 4, 34, 8, 28, 3, 15, 0, 11, 0, 12, -5, 8, -4, 10, 0, 23, -4, 31, -8, 30, -2, 30, 0, 26, -6, 22, -6, 20, -12, 15, -19, 10, -10, 13, -14, 6, -43, -13, -43, -16, -9, -12, -10, -29, -42, -40, -37, -28, -5, -21, 1, -24, -8, -20, 4, -18, 26, -24, 44, -26, 66, -30, 86, -37, 88, -41, 72, -46, 50, -31, 28, 23, 14, 64, 16, 51, 26, 32, 34, 39, 42, 48, 35, 58, 0, 72, -36, 69, -59, 58, -98, 54, -124, 36, -103, 12, -110, 5, -173, -19, -146, -59, -4, -42, 51, 1, -23, -6, -30, -6, 45, 46, 47, 70, 6, 55, 19, 60, 38, 62, 42, 47, 61, 46, 40, 42, -19, 22, -34, 6, -35, -50, -61, -141, -37, -171, 17, -163, 26, -180, 46, -154, 80, -63, 48, -4, 18, 20, 50, 47, 58, 53, 44, 61, 57, 85, 37, 80, 0, 86, -8, 106, -95, 49, -213, -8, -131, 47, 49, 63, 40, -39, -69, -74, -37, -20, 63, -12, 58, -14, -12, 25, -31, 41, 11, 45, 76, 47, 167, 5, 261, -37, 277, -83, 183, -172, 35, -122, -79, 138, -70, 266, 69, 124, 228, 0, 391, -29, 594, -84, 702, -78, 627, -8, 551, -13, 509, 13, 372, 120, 352, 125, 622, 127, 691, 223, 362, 126, 386, -33, 915, 198, 958, 457, 456, 298, 500, 233, 1027, 469, 1096, 426, 918, 160, 1067, 141, 1220, 189, 1245, 164, 1375, 297, 1378, 503, 1299, 702, 1550, 929, 1799, 855, 1752, 547, 1830, 602, 1928, 832, 1736, 796, 1735, 933, 1961, 1385, 1935, 1562, 2105, 1485, 2716, 1449, 2948, 1305, 2768, 1205, 2716, 1346, 2531, 1450, 2470, 1653, 3117, 2111, 3370, 2176, 2696, 1947, 2925, 2305, 3846, 2658, 2425, 2184, -877, 1981, -2261, 2623, -1645, 2908, -1876, 2732, -2704, 2953, -2484, 3116, -2120, 2954, -2442, 3216, -2466, 3499, -2192, 3234, -2392, 3361, -2497, 3869, -2078, 3772, -1858, 3915, -2066, 4438, -2285, 2934, -2294, -280, -2066, -1762, -1992, -1412, -2298, -1535, -2399, -1789, -2223, -1419, -2244, -1334, -2092, -1476, -1777, -1396, -2014, -1571, -2199, -1574, -1843, -1167, -1910, -1446, -2007, -1818];\n \n fn main() {\n+    #[cfg(increase_thread_usage)]\n+    let thread = std::thread::spawn(|| 4);\n+    \n     for _ in 0..2 {\n         mse(PCM.len(), PCM, EXPECTED);\n     }"}, {"sha": "1117b69116a5d71ecd9fc7091d59a9affb01ee7f", "filename": "src/bin/miri.rs", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fbin%2Fmiri.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fbin%2Fmiri.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fbin%2Fmiri.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -195,6 +195,9 @@ fn main() {\n                 \"-Zmiri-disable-stacked-borrows\" => {\n                     miri_config.stacked_borrows = false;\n                 }\n+                \"-Zmiri-disable-data-race-detector\" => {\n+                    miri_config.data_race_detector = false;\n+                }\n                 \"-Zmiri-disable-alignment-check\" => {\n                     miri_config.check_alignment = miri::AlignmentCheck::None;\n                 }"}, {"sha": "aca735e6f2220f2c9c088cafaf403447c63d3c30", "filename": "src/data_race.rs", "status": "added", "additions": 1381, "deletions": 0, "changes": 1381, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdata_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,1381 @@\n+//! Implementation of a data-race detector using Lamport Timestamps / Vector-clocks\n+//! based on the Dyamic Race Detection for C++:\n+//! https://www.doc.ic.ac.uk/~afd/homepages/papers/pdfs/2017/POPL.pdf\n+//! which does not report false-positives when fences are used, and gives better\n+//! accuracy in presence of read-modify-write operations.\n+//!\n+//! This does not explore weak memory orders and so can still miss data-races\n+//! but should not report false-positives\n+//!\n+//! Data-race definiton from(https://en.cppreference.com/w/cpp/language/memory_model#Threads_and_data_races):\n+//! a data race occurs between two memory accesses if they are on different threads, at least one operation\n+//! is non-atomic, at least one operation is a write and neither access happens-before the other. Read the link\n+//! for full definition.\n+//!\n+//! This re-uses vector indexes for threads that are known to be unable to report data-races, this is valid\n+//! because it only re-uses vector indexes once all currently-active (not-terminated) threads have an internal\n+//! vector clock that happens-after the join operation of the candidate thread. Threads that have not been joined\n+//! on are not considered. Since the thread's vector clock will only increase and a data-race implies that\n+//! there is some index x where clock[x] > thread_clock, when this is true clock[candidate-idx] > thread_clock\n+//! can never hold and hence a data-race can never be reported in that vector index again.\n+//! This means that the thread-index can be safely re-used, starting on the next timestamp for the newly created\n+//! thread.\n+//!\n+//! The sequentially consistant ordering corresponds to the ordering that the threads\n+//! are currently scheduled, this means that the data-race detector has no additional\n+//! logic for sequentially consistent accesses at the moment since they are indistinguishable\n+//! from acquire/release operations. If weak memory orderings are explored then this\n+//! may need to change or be updated accordingly.\n+//!\n+//! Per the C++ spec for the memory model a sequentially consistent operation:\n+//!   \"A load operation with this memory order performs an acquire operation,\n+//!    a store performs a release operation, and read-modify-write performs\n+//!    both an acquire operation and a release operation, plus a single total\n+//!    order exists in which all threads observe all modifications in the same\n+//!    order (see Sequentially-consistent ordering below) \"\n+//! So in the absence of weak memory effects a seq-cst load & a seq-cst store is identical\n+//! to a acquire load and a release store given the global sequentially consistent order\n+//! of the schedule.\n+//!\n+//! The timestamps used in the data-race detector assign each sequence of non-atomic operations\n+//! followed by a single atomic or concurrent operation a single timestamp.\n+//! Write, Read, Write, ThreadJoin will be represented by a single timestamp value on a thread.\n+//! This is because extra increment operations between the operations in the sequence are not\n+//! required for accurate reporting of data-race values.\n+//!\n+//! As per the paper a threads timestamp is only incremented after a release operation is performed\n+//! so some atomic operations that only perform acquires do not increment the timestamp. Due to shared\n+//! code some atomic operations may increment the timestamp when not necessary but this has no effect\n+//! on the data-race detection code.\n+//!\n+//! FIXME:\n+//! currently we have our own local copy of the currently active thread index and names, this is due\n+//! in part to the inability to access the current location of threads.active_thread inside the AllocExtra\n+//! read, write and deallocate functions and should be cleaned up in the future.\n+\n+use std::{\n+    cell::{Cell, Ref, RefCell, RefMut},\n+    fmt::Debug,\n+    mem,\n+    rc::Rc,\n+};\n+\n+use rustc_data_structures::fx::{FxHashMap, FxHashSet};\n+use rustc_index::vec::{Idx, IndexVec};\n+use rustc_middle::{mir, ty::layout::TyAndLayout};\n+use rustc_target::abi::Size;\n+\n+use crate::{\n+    ImmTy, Immediate, InterpResult, MPlaceTy, MemPlaceMeta, MiriEvalContext, MiriEvalContextExt,\n+    OpTy, Pointer, RangeMap, ScalarMaybeUninit, Tag, ThreadId, VClock, VSmallClockMap, VTimestamp,\n+    VectorIdx,\n+};\n+\n+pub type AllocExtra = VClockAlloc;\n+pub type MemoryExtra = Rc<GlobalState>;\n+\n+/// Valid atomic read-write operations, alias of atomic::Ordering (not non-exhaustive).\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum AtomicRwOp {\n+    Relaxed,\n+    Acquire,\n+    Release,\n+    AcqRel,\n+    SeqCst,\n+}\n+\n+/// Valid atomic read operations, subset of atomic::Ordering.\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum AtomicReadOp {\n+    Relaxed,\n+    Acquire,\n+    SeqCst,\n+}\n+\n+/// Valid atomic write operations, subset of atomic::Ordering.\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum AtomicWriteOp {\n+    Relaxed,\n+    Release,\n+    SeqCst,\n+}\n+\n+/// Valid atomic fence operations, subset of atomic::Ordering.\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum AtomicFenceOp {\n+    Acquire,\n+    Release,\n+    AcqRel,\n+    SeqCst,\n+}\n+\n+/// The current set of vector clocks describing the state\n+/// of a thread, contains the happens-before clock and\n+/// additional metadata to model atomic fence operations.\n+#[derive(Clone, Default, Debug)]\n+struct ThreadClockSet {\n+    /// The increasing clock representing timestamps\n+    /// that happen-before this thread.\n+    clock: VClock,\n+\n+    /// The set of timestamps that will happen-before this\n+    /// thread once it performs an acquire fence.\n+    fence_acquire: VClock,\n+\n+    /// The last timesamp of happens-before relations that\n+    /// have been released by this thread by a fence.\n+    fence_release: VClock,\n+}\n+\n+impl ThreadClockSet {\n+    /// Apply the effects of a release fence to this\n+    /// set of thread vector clocks.\n+    #[inline]\n+    fn apply_release_fence(&mut self) {\n+        self.fence_release.clone_from(&self.clock);\n+    }\n+\n+    /// Apply the effects of a acquire fence to this\n+    /// set of thread vector clocks.\n+    #[inline]\n+    fn apply_acquire_fence(&mut self) {\n+        self.clock.join(&self.fence_acquire);\n+    }\n+\n+    /// Increment the happens-before clock at a\n+    /// known index.\n+    #[inline]\n+    fn increment_clock(&mut self, index: VectorIdx) {\n+        self.clock.increment_index(index);\n+    }\n+\n+    /// Join the happens-before clock with that of\n+    /// another thread, used to model thread join\n+    /// operations.\n+    fn join_with(&mut self, other: &ThreadClockSet) {\n+        self.clock.join(&other.clock);\n+    }\n+}\n+\n+/// Error returned by finding a data race\n+/// should be elaborated upon.\n+#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug)]\n+pub struct DataRace;\n+\n+/// Externally stored memory cell clocks\n+/// explicitly to reduce memory usage for the\n+/// common case where no atomic operations\n+/// exists on the memory cell.\n+#[derive(Clone, PartialEq, Eq, Default, Debug)]\n+struct AtomicMemoryCellClocks {\n+    /// The clock-vector of the timestamp of the last atomic\n+    /// read operation performed by each thread.\n+    /// This detects potential data-races between atomic read\n+    /// and non-atomic write operations.\n+    read_vector: VClock,\n+\n+    /// The clock-vector of the timestamp of the last atomic\n+    /// write operation performed by each thread.\n+    /// This detects potential data-races between atomic write\n+    /// and non-atomic read or write operations.\n+    write_vector: VClock,\n+\n+    /// Synchronization vector for acquire-release semantics\n+    /// contains the vector of timestamps that will\n+    /// happen-before a thread if an acquire-load is\n+    /// performed on the data.\n+    sync_vector: VClock,\n+\n+    /// The Hash-Map of all threads for which a release\n+    /// sequence exists in the memory cell, required\n+    /// since read-modify-write operations do not\n+    /// invalidate existing release sequences.\n+    /// See page 6 of linked paper.\n+    release_sequences: VSmallClockMap,\n+}\n+\n+/// Memory Cell vector clock metadata\n+/// for data-race detection.\n+#[derive(Clone, PartialEq, Eq, Debug)]\n+struct MemoryCellClocks {\n+    /// The vector-clock timestamp of the last write\n+    /// corresponding to the writing threads timestamp.\n+    write: VTimestamp,\n+\n+    /// The identifier of the vector index, corresponding to a thread\n+    /// that performed the last write operation.\n+    write_index: VectorIdx,\n+\n+    /// The vector-clock of the timestamp of the last read operation\n+    /// performed by a thread since the last write operation occured.\n+    /// It is reset to zero on each write operation.\n+    read: VClock,\n+\n+    /// Atomic acquire & release sequence tracking clocks.\n+    /// For non-atomic memory in the common case this\n+    /// value is set to None.\n+    atomic_ops: Option<Box<AtomicMemoryCellClocks>>,\n+}\n+\n+/// Create a default memory cell clocks instance\n+/// for uninitialized memory.\n+impl Default for MemoryCellClocks {\n+    fn default() -> Self {\n+        MemoryCellClocks {\n+            read: VClock::default(),\n+            write: 0,\n+            write_index: VectorIdx::MAX_INDEX,\n+            atomic_ops: None,\n+        }\n+    }\n+}\n+\n+impl MemoryCellClocks {\n+    /// Load the internal atomic memory cells if they exist.\n+    #[inline]\n+    fn atomic(&self) -> Option<&AtomicMemoryCellClocks> {\n+        match &self.atomic_ops {\n+            Some(op) => Some(&*op),\n+            None => None,\n+        }\n+    }\n+\n+    /// Load or create the internal atomic memory metadata\n+    /// if it does not exist.\n+    #[inline]\n+    fn atomic_mut(&mut self) -> &mut AtomicMemoryCellClocks {\n+        self.atomic_ops.get_or_insert_with(Default::default)\n+    }\n+\n+    /// Update memory cell data-race tracking for atomic\n+    /// load acquire semantics, is a no-op if this memory was\n+    /// not used previously as atomic memory.\n+    fn load_acquire(\n+        &mut self,\n+        clocks: &mut ThreadClockSet,\n+        index: VectorIdx,\n+    ) -> Result<(), DataRace> {\n+        self.atomic_read_detect(clocks, index)?;\n+        if let Some(atomic) = self.atomic() {\n+            clocks.clock.join(&atomic.sync_vector);\n+        }\n+        Ok(())\n+    }\n+\n+    /// Update memory cell data-race tracking for atomic\n+    /// load relaxed semantics, is a no-op if this memory was\n+    /// not used previously as atomic memory.\n+    fn load_relaxed(\n+        &mut self,\n+        clocks: &mut ThreadClockSet,\n+        index: VectorIdx,\n+    ) -> Result<(), DataRace> {\n+        self.atomic_read_detect(clocks, index)?;\n+        if let Some(atomic) = self.atomic() {\n+            clocks.fence_acquire.join(&atomic.sync_vector);\n+        }\n+        Ok(())\n+    }\n+\n+    /// Update the memory cell data-race tracking for atomic\n+    /// store release semantics.\n+    fn store_release(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, index)?;\n+        let atomic = self.atomic_mut();\n+        atomic.sync_vector.clone_from(&clocks.clock);\n+        atomic.release_sequences.clear();\n+        atomic.release_sequences.insert(index, &clocks.clock);\n+        Ok(())\n+    }\n+\n+    /// Update the memory cell data-race tracking for atomic\n+    /// store relaxed semantics.\n+    fn store_relaxed(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, index)?;\n+        let atomic = self.atomic_mut();\n+        atomic.sync_vector.clone_from(&clocks.fence_release);\n+        if let Some(release) = atomic.release_sequences.get(index) {\n+            atomic.sync_vector.join(release);\n+        }\n+        atomic.release_sequences.retain_index(index);\n+        Ok(())\n+    }\n+\n+    /// Update the memory cell data-race tracking for atomic\n+    /// store release semantics for RMW operations.\n+    fn rmw_release(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, index)?;\n+        let atomic = self.atomic_mut();\n+        atomic.sync_vector.join(&clocks.clock);\n+        atomic.release_sequences.insert(index, &clocks.clock);\n+        Ok(())\n+    }\n+\n+    /// Update the memory cell data-race tracking for atomic\n+    /// store relaxed semantics for RMW operations.\n+    fn rmw_relaxed(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, index)?;\n+        let atomic = self.atomic_mut();\n+        atomic.sync_vector.join(&clocks.fence_release);\n+        Ok(())\n+    }\n+\n+    /// Detect data-races with an atomic read, caused by a non-atomic write that does\n+    /// not happen-before the atomic-read.\n+    fn atomic_read_detect(\n+        &mut self,\n+        clocks: &ThreadClockSet,\n+        index: VectorIdx,\n+    ) -> Result<(), DataRace> {\n+        log::trace!(\"Atomic read with vectors: {:#?} :: {:#?}\", self, clocks);\n+        if self.write <= clocks.clock[self.write_index] {\n+            let atomic = self.atomic_mut();\n+            atomic.read_vector.set_at_index(&clocks.clock, index);\n+            Ok(())\n+        } else {\n+            Err(DataRace)\n+        }\n+    }\n+\n+    /// Detect data-races with an atomic write, either with a non-atomic read or with\n+    /// a non-atomic write.\n+    fn atomic_write_detect(\n+        &mut self,\n+        clocks: &ThreadClockSet,\n+        index: VectorIdx,\n+    ) -> Result<(), DataRace> {\n+        log::trace!(\"Atomic write with vectors: {:#?} :: {:#?}\", self, clocks);\n+        if self.write <= clocks.clock[self.write_index] && self.read <= clocks.clock {\n+            let atomic = self.atomic_mut();\n+            atomic.write_vector.set_at_index(&clocks.clock, index);\n+            Ok(())\n+        } else {\n+            Err(DataRace)\n+        }\n+    }\n+\n+    /// Detect races for non-atomic read operations at the current memory cell\n+    /// returns true if a data-race is detected.\n+    fn read_race_detect(\n+        &mut self,\n+        clocks: &ThreadClockSet,\n+        index: VectorIdx,\n+    ) -> Result<(), DataRace> {\n+        log::trace!(\"Unsynchronized read with vectors: {:#?} :: {:#?}\", self, clocks);\n+        if self.write <= clocks.clock[self.write_index] {\n+            let race_free = if let Some(atomic) = self.atomic() {\n+                atomic.write_vector <= clocks.clock\n+            } else {\n+                true\n+            };\n+            if race_free {\n+                self.read.set_at_index(&clocks.clock, index);\n+                Ok(())\n+            } else {\n+                Err(DataRace)\n+            }\n+        } else {\n+            Err(DataRace)\n+        }\n+    }\n+\n+    /// Detect races for non-atomic write operations at the current memory cell\n+    /// returns true if a data-race is detected.\n+    fn write_race_detect(\n+        &mut self,\n+        clocks: &ThreadClockSet,\n+        index: VectorIdx,\n+    ) -> Result<(), DataRace> {\n+        log::trace!(\"Unsynchronized write with vectors: {:#?} :: {:#?}\", self, clocks);\n+        if self.write <= clocks.clock[self.write_index] && self.read <= clocks.clock {\n+            let race_free = if let Some(atomic) = self.atomic() {\n+                atomic.write_vector <= clocks.clock && atomic.read_vector <= clocks.clock\n+            } else {\n+                true\n+            };\n+            if race_free {\n+                self.write = clocks.clock[index];\n+                self.write_index = index;\n+                self.read.set_zero_vector();\n+                Ok(())\n+            } else {\n+                Err(DataRace)\n+            }\n+        } else {\n+            Err(DataRace)\n+        }\n+    }\n+}\n+\n+/// Evaluation context extensions.\n+impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n+pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n+    /// Atomic variant of read_scalar_at_offset.\n+    fn read_scalar_at_offset_atomic(\n+        &self,\n+        op: OpTy<'tcx, Tag>,\n+        offset: u64,\n+        layout: TyAndLayout<'tcx>,\n+        atomic: AtomicReadOp,\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_ref();\n+        let op_place = this.deref_operand(op)?;\n+        let offset = Size::from_bytes(offset);\n+\n+        // Ensure that the following read at an offset is within bounds.\n+        assert!(op_place.layout.size >= offset + layout.size);\n+        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n+        this.read_scalar_atomic(value_place, atomic)\n+    }\n+\n+    /// Atomic variant of write_scalar_at_offset.\n+    fn write_scalar_at_offset_atomic(\n+        &mut self,\n+        op: OpTy<'tcx, Tag>,\n+        offset: u64,\n+        value: impl Into<ScalarMaybeUninit<Tag>>,\n+        layout: TyAndLayout<'tcx>,\n+        atomic: AtomicWriteOp,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let op_place = this.deref_operand(op)?;\n+        let offset = Size::from_bytes(offset);\n+\n+        // Ensure that the following read at an offset is within bounds.\n+        assert!(op_place.layout.size >= offset + layout.size);\n+        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n+        this.write_scalar_atomic(value.into(), value_place, atomic)\n+    }\n+\n+    /// Perform an atomic read operation at the memory location.\n+    fn read_scalar_atomic(\n+        &self,\n+        place: MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicReadOp,\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_ref();\n+        let scalar = this.allow_data_races_ref(move |this| this.read_scalar(place.into()))?;\n+        self.validate_atomic_load(place, atomic)?;\n+        Ok(scalar)\n+    }\n+\n+    /// Perform an atomic write operation at the memory location.\n+    fn write_scalar_atomic(\n+        &mut self,\n+        val: ScalarMaybeUninit<Tag>,\n+        dest: MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicWriteOp,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        this.allow_data_races_mut(move |this| this.write_scalar(val, dest.into()))?;\n+        self.validate_atomic_store(dest, atomic)\n+    }\n+\n+    /// Perform a atomic operation on a memory location.\n+    fn atomic_op_immediate(\n+        &mut self,\n+        place: MPlaceTy<'tcx, Tag>,\n+        rhs: ImmTy<'tcx, Tag>,\n+        op: mir::BinOp,\n+        neg: bool,\n+        atomic: AtomicRwOp,\n+    ) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n+        let this = self.eval_context_mut();\n+\n+        let old = this.allow_data_races_mut(|this| this.read_immediate(place.into()))?;\n+\n+        // Atomics wrap around on overflow.\n+        let val = this.binary_op(op, old, rhs)?;\n+        let val = if neg { this.unary_op(mir::UnOp::Not, val)? } else { val };\n+        this.allow_data_races_mut(|this| this.write_immediate(*val, place.into()))?;\n+\n+        this.validate_atomic_rmw(place, atomic)?;\n+        Ok(old)\n+    }\n+\n+    /// Perform an atomic exchange with a memory place and a new\n+    /// scalar value, the old value is returned.\n+    fn atomic_exchange_scalar(\n+        &mut self,\n+        place: MPlaceTy<'tcx, Tag>,\n+        new: ScalarMaybeUninit<Tag>,\n+        atomic: AtomicRwOp,\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_mut();\n+\n+        let old = this.allow_data_races_mut(|this| this.read_scalar(place.into()))?;\n+        this.allow_data_races_mut(|this| this.write_scalar(new, place.into()))?;\n+        this.validate_atomic_rmw(place, atomic)?;\n+        Ok(old)\n+    }\n+\n+    /// Perform an atomic compare and exchange at a given memory location.\n+    /// On success an atomic RMW operation is performed and on failure\n+    /// only an atomic read occurs.\n+    fn atomic_compare_exchange_scalar(\n+        &mut self,\n+        place: MPlaceTy<'tcx, Tag>,\n+        expect_old: ImmTy<'tcx, Tag>,\n+        new: ScalarMaybeUninit<Tag>,\n+        success: AtomicRwOp,\n+        fail: AtomicReadOp,\n+    ) -> InterpResult<'tcx, Immediate<Tag>> {\n+        let this = self.eval_context_mut();\n+\n+        // Failure ordering cannot be stronger than success ordering, therefore first attempt\n+        // to read with the failure ordering and if successfull then try again with the success\n+        // read ordering and write in the success case.\n+        // Read as immediate for the sake of `binary_op()`\n+        let old = this.allow_data_races_mut(|this| this.read_immediate(place.into()))?;\n+\n+        // `binary_op` will bail if either of them is not a scalar.\n+        let eq = this.overflowing_binary_op(mir::BinOp::Eq, old, expect_old)?.0;\n+        let res = Immediate::ScalarPair(old.to_scalar_or_uninit(), eq.into());\n+\n+        // Update ptr depending on comparison.\n+        // if successful, perform a full rw-atomic validation\n+        // otherwise treat this as an atomic load with the fail ordering.\n+        if eq.to_bool()? {\n+            this.allow_data_races_mut(|this| this.write_scalar(new, place.into()))?;\n+            this.validate_atomic_rmw(place, success)?;\n+        } else {\n+            this.validate_atomic_load(place, fail)?;\n+        }\n+\n+        // Return the old value.\n+        Ok(res)\n+    }\n+\n+    /// Update the data-race detector for an atomic read occuring at the\n+    /// associated memory-place and on the current thread.\n+    fn validate_atomic_load(\n+        &self,\n+        place: MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicReadOp,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_ref();\n+        this.validate_atomic_op(\n+            place,\n+            atomic,\n+            \"Atomic Load\",\n+            move |memory, clocks, index, atomic| {\n+                if atomic == AtomicReadOp::Relaxed {\n+                    memory.load_relaxed(&mut *clocks, index)\n+                } else {\n+                    memory.load_acquire(&mut *clocks, index)\n+                }\n+            },\n+        )\n+    }\n+\n+    /// Update the data-race detector for an atomic write occuring at the\n+    /// associated memory-place and on the current thread.\n+    fn validate_atomic_store(\n+        &mut self,\n+        place: MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicWriteOp,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_ref();\n+        this.validate_atomic_op(\n+            place,\n+            atomic,\n+            \"Atomic Store\",\n+            move |memory, clocks, index, atomic| {\n+                if atomic == AtomicWriteOp::Relaxed {\n+                    memory.store_relaxed(clocks, index)\n+                } else {\n+                    memory.store_release(clocks, index)\n+                }\n+            },\n+        )\n+    }\n+\n+    /// Update the data-race detector for an atomic read-modify-write occuring\n+    /// at the associated memory place and on the current thread.\n+    fn validate_atomic_rmw(\n+        &mut self,\n+        place: MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicRwOp,\n+    ) -> InterpResult<'tcx> {\n+        use AtomicRwOp::*;\n+        let acquire = matches!(atomic, Acquire | AcqRel | SeqCst);\n+        let release = matches!(atomic, Release | AcqRel | SeqCst);\n+        let this = self.eval_context_ref();\n+        this.validate_atomic_op(place, atomic, \"Atomic RMW\", move |memory, clocks, index, _| {\n+            if acquire {\n+                memory.load_acquire(clocks, index)?;\n+            } else {\n+                memory.load_relaxed(clocks, index)?;\n+            }\n+            if release {\n+                memory.rmw_release(clocks, index)\n+            } else {\n+                memory.rmw_relaxed(clocks, index)\n+            }\n+        })\n+    }\n+\n+    /// Update the data-race detector for an atomic fence on the current thread.\n+    fn validate_atomic_fence(&mut self, atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.maybe_perform_sync_operation(move |index, mut clocks| {\n+                log::trace!(\"Atomic fence on {:?} with ordering {:?}\", index, atomic);\n+\n+                // Apply data-race detection for the current fences\n+                // this treats AcqRel and SeqCst as the same as a acquire\n+                // and release fence applied in the same timestamp.\n+                if atomic != AtomicFenceOp::Release {\n+                    // Either Acquire | AcqRel | SeqCst\n+                    clocks.apply_acquire_fence();\n+                }\n+                if atomic != AtomicFenceOp::Acquire {\n+                    // Either Release | AcqRel | SeqCst\n+                    clocks.apply_release_fence();\n+                }\n+                \n+                // Increment timestamp in case of release semantics.\n+                Ok(atomic != AtomicFenceOp::Acquire)\n+            })\n+        } else {\n+            Ok(())\n+        }\n+    }\n+}\n+\n+/// Vector clock metadata for a logical memory allocation.\n+#[derive(Debug, Clone)]\n+pub struct VClockAlloc {\n+    /// Assigning each byte a MemoryCellClocks.\n+    alloc_ranges: RefCell<RangeMap<MemoryCellClocks>>,\n+\n+    // Pointer to global state.\n+    global: MemoryExtra,\n+}\n+\n+impl VClockAlloc {\n+    /// Create a new data-race allocation detector.\n+    pub fn new_allocation(global: &MemoryExtra, len: Size) -> VClockAlloc {\n+        VClockAlloc {\n+            global: Rc::clone(global),\n+            alloc_ranges: RefCell::new(RangeMap::new(len, MemoryCellClocks::default())),\n+        }\n+    }\n+\n+    // Find an index, if one exists where the value\n+    // in `l` is greater than the value in `r`.\n+    fn find_gt_index(l: &VClock, r: &VClock) -> Option<VectorIdx> {\n+        let l_slice = l.as_slice();\n+        let r_slice = r.as_slice();\n+        l_slice\n+            .iter()\n+            .zip(r_slice.iter())\n+            .enumerate()\n+            .find_map(|(idx, (&l, &r))| if l > r { Some(idx) } else { None })\n+            .or_else(|| {\n+                if l_slice.len() > r_slice.len() {\n+                    // By invariant, if l_slice is longer\n+                    // then one element must be larger.\n+                    // This just validates that this is true\n+                    // and reports earlier elements first.\n+                    let l_remainder_slice = &l_slice[r_slice.len()..];\n+                    let idx = l_remainder_slice\n+                        .iter()\n+                        .enumerate()\n+                        .find_map(|(idx, &r)| if r == 0 { None } else { Some(idx) })\n+                        .expect(\"Invalid VClock Invariant\");\n+                    Some(idx)\n+                } else {\n+                    None\n+                }\n+            })\n+            .map(|idx| VectorIdx::new(idx))\n+    }\n+\n+    /// Report a data-race found in the program.\n+    /// This finds the two racing threads and the type\n+    /// of data-race that occured. This will also\n+    /// return info about the memory location the data-race\n+    /// occured in.\n+    #[cold]\n+    #[inline(never)]\n+    fn report_data_race<'tcx>(\n+        global: &MemoryExtra,\n+        range: &MemoryCellClocks,\n+        action: &str,\n+        is_atomic: bool,\n+        pointer: Pointer<Tag>,\n+        len: Size,\n+    ) -> InterpResult<'tcx> {\n+        let (current_index, current_clocks) = global.current_thread_state();\n+        let write_clock;\n+        let (other_action, other_thread, other_clock) = if range.write\n+            > current_clocks.clock[range.write_index]\n+        {\n+            // Convert the write action into the vector clock it\n+            // represents for diagnostic purposes.\n+            write_clock = VClock::new_with_index(range.write_index, range.write);\n+            (\"WRITE\", range.write_index, &write_clock)\n+        } else if let Some(idx) = Self::find_gt_index(&range.read, &current_clocks.clock) {\n+            (\"READ\", idx, &range.read)\n+        } else if !is_atomic {\n+            if let Some(atomic) = range.atomic() {\n+                if let Some(idx) = Self::find_gt_index(&atomic.write_vector, &current_clocks.clock)\n+                {\n+                    (\"ATOMIC_STORE\", idx, &atomic.write_vector)\n+                } else if let Some(idx) =\n+                    Self::find_gt_index(&atomic.read_vector, &current_clocks.clock)\n+                {\n+                    (\"ATOMIC_LOAD\", idx, &atomic.read_vector)\n+                } else {\n+                    unreachable!(\n+                        \"Failed to report data-race for non-atomic operation: no race found\"\n+                    )\n+                }\n+            } else {\n+                unreachable!(\n+                    \"Failed to report data-race for non-atomic operation: no atomic component\"\n+                )\n+            }\n+        } else {\n+            unreachable!(\"Failed to report data-race for atomic operation\")\n+        };\n+\n+        // Load elaborated thread information about the racing thread actions.\n+        let current_thread_info = global.print_thread_metadata(current_index);\n+        let other_thread_info = global.print_thread_metadata(other_thread);\n+\n+        // Throw the data-race detection.\n+        throw_ub_format!(\n+            \"Data race detected between {} on {} and {} on {}, memory({:?},offset={},size={})\\\n+            \\n\\t\\t -current vector clock = {:?}\\\n+            \\n\\t\\t -conflicting timestamp = {:?}\",\n+            action,\n+            current_thread_info,\n+            other_action,\n+            other_thread_info,\n+            pointer.alloc_id,\n+            pointer.offset.bytes(),\n+            len.bytes(),\n+            current_clocks.clock,\n+            other_clock\n+        )\n+    }\n+\n+    /// Detect data-races for an unsychronized read operation, will not perform\n+    /// data-race detection if `multi-threaded` is false, either due to no threads\n+    /// being created or if it is temporarily disabled during a racy read or write\n+    /// operation for which data-race detection is handled separately, for example\n+    /// atomic read operations.\n+    pub fn read<'tcx>(&self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n+        if self.global.multi_threaded.get() {\n+            let (index, clocks) = self.global.current_thread_state();\n+            let mut alloc_ranges = self.alloc_ranges.borrow_mut();\n+            for (_, range) in alloc_ranges.iter_mut(pointer.offset, len) {\n+                if let Err(DataRace) = range.read_race_detect(&*clocks, index) {\n+                    // Report data-race.\n+                    return Self::report_data_race(\n+                        &self.global,\n+                        range,\n+                        \"READ\",\n+                        false,\n+                        pointer,\n+                        len,\n+                    );\n+                }\n+            }\n+            Ok(())\n+        } else {\n+            Ok(())\n+        }\n+    }\n+\n+    // Shared code for detecting data-races on unique access to a section of memory\n+    fn unique_access<'tcx>(\n+        &mut self,\n+        pointer: Pointer<Tag>,\n+        len: Size,\n+        action: &str,\n+    ) -> InterpResult<'tcx> {\n+        if self.global.multi_threaded.get() {\n+            let (index, clocks) = self.global.current_thread_state();\n+            for (_, range) in self.alloc_ranges.get_mut().iter_mut(pointer.offset, len) {\n+                if let Err(DataRace) = range.write_race_detect(&*clocks, index) {\n+                    // Report data-race\n+                    return Self::report_data_race(\n+                        &self.global,\n+                        range,\n+                        action,\n+                        false,\n+                        pointer,\n+                        len,\n+                    );\n+                }\n+            }\n+            Ok(())\n+        } else {\n+            Ok(())\n+        }\n+    }\n+\n+    /// Detect data-races for an unsychronized write operation, will not perform\n+    /// data-race threads if `multi-threaded` is false, either due to no threads\n+    /// being created or if it is temporarily disabled during a racy read or write\n+    /// operation\n+    pub fn write<'tcx>(&mut self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n+        self.unique_access(pointer, len, \"Write\")\n+    }\n+\n+    /// Detect data-races for an unsychronized deallocate operation, will not perform\n+    /// data-race threads if `multi-threaded` is false, either due to no threads\n+    /// being created or if it is temporarily disabled during a racy read or write\n+    /// operation\n+    pub fn deallocate<'tcx>(&mut self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n+        self.unique_access(pointer, len, \"Deallocate\")\n+    }\n+}\n+\n+impl<'mir, 'tcx: 'mir> EvalContextPrivExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n+trait EvalContextPrivExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n+    // Temporarily allow data-races to occur, this should only be\n+    // used if either one of the appropiate `validate_atomic` functions\n+    // will be called to treat a memory access as atomic or if the memory\n+    // being accessed should be treated as internal state, that cannot be\n+    // accessed by the interpreted program.\n+    #[inline]\n+    fn allow_data_races_ref<R>(&self, op: impl FnOnce(&MiriEvalContext<'mir, 'tcx>) -> R) -> R {\n+        let this = self.eval_context_ref();\n+        let old = if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.multi_threaded.replace(false)\n+        } else {\n+            false\n+        };\n+        let result = op(this);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.multi_threaded.set(old);\n+        }\n+        result\n+    }\n+\n+    /// Same as `allow_data_races_ref`, this temporarily disables any data-race detection and\n+    /// so should only be used for atomic operations or internal state that the program cannot\n+    /// access.\n+    #[inline]\n+    fn allow_data_races_mut<R>(\n+        &mut self,\n+        op: impl FnOnce(&mut MiriEvalContext<'mir, 'tcx>) -> R,\n+    ) -> R {\n+        let this = self.eval_context_mut();\n+        let old = if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.multi_threaded.replace(false)\n+        } else {\n+            false\n+        };\n+        let result = op(this);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.multi_threaded.set(old);\n+        }\n+        result\n+    }\n+\n+    /// Generic atomic operation implementation,\n+    /// this accesses memory via get_raw instead of\n+    /// get_raw_mut, due to issues calling get_raw_mut\n+    /// for atomic loads from read-only memory.\n+    /// FIXME: is this valid, or should get_raw_mut be used for\n+    /// atomic-stores/atomic-rmw?\n+    fn validate_atomic_op<A: Debug + Copy>(\n+        &self,\n+        place: MPlaceTy<'tcx, Tag>,\n+        atomic: A,\n+        description: &str,\n+        mut op: impl FnMut(\n+            &mut MemoryCellClocks,\n+            &mut ThreadClockSet,\n+            VectorIdx,\n+            A,\n+        ) -> Result<(), DataRace>,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_ref();\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            if data_race.multi_threaded.get() {\n+                // Load and log the atomic operation.\n+                let place_ptr = place.ptr.assert_ptr();\n+                let size = place.layout.size;\n+                let alloc_meta =\n+                    &this.memory.get_raw(place_ptr.alloc_id)?.extra.data_race.as_ref().unwrap();\n+                log::trace!(\n+                    \"Atomic op({}) with ordering {:?} on memory({:?}, offset={}, size={})\",\n+                    description,\n+                    &atomic,\n+                    place_ptr.alloc_id,\n+                    place_ptr.offset.bytes(),\n+                    size.bytes()\n+                );\n+\n+                // Perform the atomic operation.\n+                let data_race = &alloc_meta.global;\n+                data_race.maybe_perform_sync_operation(|index, mut clocks| {\n+                    for (_, range) in\n+                        alloc_meta.alloc_ranges.borrow_mut().iter_mut(place_ptr.offset, size)\n+                    {\n+                        if let Err(DataRace) = op(range, &mut *clocks, index, atomic) {\n+                            mem::drop(clocks);\n+                            return VClockAlloc::report_data_race(\n+                                &alloc_meta.global,\n+                                range,\n+                                description,\n+                                true,\n+                                place_ptr,\n+                                size,\n+                            ).map(|_| true);\n+                        }\n+                    }\n+\n+                    // This conservatively assumes all operations have release semantics\n+                    Ok(true)\n+                })?;\n+\n+                // Log changes to atomic memory.\n+                if log::log_enabled!(log::Level::Trace) {\n+                    for (_, range) in alloc_meta.alloc_ranges.borrow().iter(place_ptr.offset, size)\n+                    {\n+                        log::trace!(\n+                            \"Updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n+                            place.ptr.assert_ptr().alloc_id,\n+                            place_ptr.offset.bytes(),\n+                            size.bytes(),\n+                            range.atomic_ops\n+                        );\n+                    }\n+                }\n+            }\n+        }\n+        Ok(())\n+    }\n+}\n+\n+/// Extra metadata associated with a thread.\n+#[derive(Debug, Clone, Default)]\n+struct ThreadExtraState {\n+    /// The current vector index in use by the\n+    /// thread currently, this is set to None\n+    /// after the vector index has been re-used\n+    /// and hence the value will never need to be\n+    /// read during data-race reporting.\n+    vector_index: Option<VectorIdx>,\n+\n+    /// The name of the thread, updated for better\n+    /// diagnostics when reporting detected data\n+    /// races.\n+    thread_name: Option<Box<str>>,\n+\n+    /// Thread termination vector clock, this\n+    /// is set on thread termination and is used\n+    /// for joining on threads since the vector_index\n+    /// may be re-used when the join operation occurs.\n+    termination_vector_clock: Option<VClock>,\n+}\n+\n+/// Global data-race detection state, contains the currently\n+/// executing thread as well as the vector-clocks associated\n+/// with each of the threads.\n+#[derive(Debug, Clone)]\n+pub struct GlobalState {\n+    /// Set to true once the first additional\n+    /// thread has launched, due to the dependency\n+    /// between before and after a thread launch.\n+    /// Any data-races must be recorded after this\n+    /// so concurrent execution can ignore recording\n+    /// any data-races.\n+    multi_threaded: Cell<bool>,\n+\n+    /// Mapping of a vector index to a known set of thread\n+    /// clocks, this is not directly mapping from a thread id\n+    /// since it may refer to multiple threads.\n+    vector_clocks: RefCell<IndexVec<VectorIdx, ThreadClockSet>>,\n+\n+    /// Mapping of a given vector index to the current thread\n+    /// that the execution is representing, this may change\n+    /// if a vector index is re-assigned to a new thread.\n+    vector_info: RefCell<IndexVec<VectorIdx, ThreadId>>,\n+\n+    /// The mapping of a given thread to assocaited thread metadata.\n+    thread_info: RefCell<IndexVec<ThreadId, ThreadExtraState>>,\n+\n+    /// The current vector index being executed.\n+    current_index: Cell<VectorIdx>,\n+\n+    /// Potential vector indices that could be re-used on thread creation\n+    /// values are inserted here on after the thread has terminated and\n+    /// been joined with, and hence may potentially become free\n+    /// for use as the index for a new thread.\n+    /// Elements in this set may still require the vector index to\n+    /// report data-races, and can only be re-used after all\n+    /// active vector-clocks catch up with the threads timestamp.\n+    reuse_candidates: RefCell<FxHashSet<VectorIdx>>,\n+\n+    /// Counts the number of threads that are currently active\n+    /// if the number of active threads reduces to 1 and then\n+    /// a join operation occures with the remaining main thread\n+    /// then multi-threaded execution may be disabled.\n+    active_thread_count: Cell<usize>,\n+\n+    /// This contains threads that have terminated, but not yet joined\n+    /// and so cannot become re-use candidates until a join operation\n+    /// occurs.\n+    /// The associated vector index will be moved into re-use candidates\n+    /// after the join operation occurs.\n+    terminated_threads: RefCell<FxHashMap<ThreadId, VectorIdx>>,\n+}\n+\n+impl GlobalState {\n+    /// Create a new global state, setup with just thread-id=0\n+    /// advanced to timestamp = 1.\n+    pub fn new() -> Self {\n+        let global_state = GlobalState {\n+            multi_threaded: Cell::new(false),\n+            vector_clocks: RefCell::new(IndexVec::new()),\n+            vector_info: RefCell::new(IndexVec::new()),\n+            thread_info: RefCell::new(IndexVec::new()),\n+            current_index: Cell::new(VectorIdx::new(0)),\n+            active_thread_count: Cell::new(1),\n+            reuse_candidates: RefCell::new(FxHashSet::default()),\n+            terminated_threads: RefCell::new(FxHashMap::default()),\n+        };\n+\n+        // Setup the main-thread since it is not explicitly created:\n+        // uses vector index and thread-id 0, also the rust runtime gives\n+        // the main-thread a name of \"main\".\n+        let index = global_state.vector_clocks.borrow_mut().push(ThreadClockSet::default());\n+        global_state.vector_info.borrow_mut().push(ThreadId::new(0));\n+        global_state.thread_info.borrow_mut().push(ThreadExtraState {\n+            vector_index: Some(index),\n+            thread_name: Some(\"main\".to_string().into_boxed_str()),\n+            termination_vector_clock: None,\n+        });\n+\n+        global_state\n+    }\n+\n+    // Try to find vector index values that can potentially be re-used\n+    // by a new thread instead of a new vector index being created.\n+    fn find_vector_index_reuse_candidate(&self) -> Option<VectorIdx> {\n+        let mut reuse = self.reuse_candidates.borrow_mut();\n+        let vector_clocks = self.vector_clocks.borrow();\n+        let vector_info = self.vector_info.borrow();\n+        let terminated_threads = self.terminated_threads.borrow();\n+        for &candidate in reuse.iter() {\n+            let target_timestamp = vector_clocks[candidate].clock[candidate];\n+            if vector_clocks.iter_enumerated().all(|(clock_idx, clock)| {\n+                // The thread happens before the clock, and hence cannot report\n+                // a data-race with this the candidate index.\n+                let no_data_race = clock.clock[candidate] >= target_timestamp;\n+\n+                // The vector represents a thread that has terminated and hence cannot\n+                // report a data-race with the candidate index.\n+                let thread_id = vector_info[clock_idx];\n+                let vector_terminated =\n+                    reuse.contains(&clock_idx) || terminated_threads.contains_key(&thread_id);\n+\n+                // The vector index cannot report a race with the candidate index\n+                // and hence allows the candidate index to be re-used.\n+                no_data_race || vector_terminated\n+            }) {\n+                // All vector clocks for each vector index are equal to\n+                // the target timestamp, and the thread is known to have\n+                // terminated, therefore this vector clock index cannot\n+                // report any more data-races.\n+                assert!(reuse.remove(&candidate));\n+                return Some(candidate);\n+            }\n+        }\n+        None\n+    }\n+\n+    // Hook for thread creation, enabled multi-threaded execution and marks\n+    // the current thread timestamp as happening-before the current thread.\n+    #[inline]\n+    pub fn thread_created(&self, thread: ThreadId) {\n+        let current_index = self.current_index();\n+\n+        // Increment the number of active threads.\n+        let active_threads = self.active_thread_count.get();\n+        self.active_thread_count.set(active_threads + 1);\n+\n+        // Enable multi-threaded execution, there are now two threads\n+        // so data-races are now possible.\n+        self.multi_threaded.set(true);\n+\n+        // Load and setup the associated thread metadata\n+        let mut thread_info = self.thread_info.borrow_mut();\n+        thread_info.ensure_contains_elem(thread, Default::default);\n+\n+        // Assign a vector index for the thread, attempting to re-use an old\n+        // vector index that can no longer report any data-races if possible.\n+        let created_index = if let Some(reuse_index) = self.find_vector_index_reuse_candidate() {\n+            // Now re-configure the re-use candidate, increment the clock\n+            // for the new sync use of the vector.\n+            let mut vector_clocks = self.vector_clocks.borrow_mut();\n+            vector_clocks[reuse_index].increment_clock(reuse_index);\n+\n+            // Locate the old thread the vector was associated with and update\n+            // it to represent the new thread instead.\n+            let mut vector_info = self.vector_info.borrow_mut();\n+            let old_thread = vector_info[reuse_index];\n+            vector_info[reuse_index] = thread;\n+\n+            // Mark the thread the vector index was associated with as no longer\n+            // representing a thread index.\n+            thread_info[old_thread].vector_index = None;\n+\n+            reuse_index\n+        } else {\n+            // No vector re-use candidates available, instead create\n+            // a new vector index.\n+            let mut vector_info = self.vector_info.borrow_mut();\n+            vector_info.push(thread)\n+        };\n+\n+        // Mark the chosen vector index as in use by the thread.\n+        thread_info[thread].vector_index = Some(created_index);\n+\n+        // Create a thread clock set if applicable.\n+        let mut vector_clocks = self.vector_clocks.borrow_mut();\n+        if created_index == vector_clocks.next_index() {\n+            vector_clocks.push(ThreadClockSet::default());\n+        }\n+\n+        // Now load the two clocks and configure the initial state.\n+        let (current, created) = vector_clocks.pick2_mut(current_index, created_index);\n+\n+        // Join the created with current, since the current threads\n+        // previous actions happen-before the created thread.\n+        created.join_with(current);\n+\n+        // Advance both threads after the synchronized operation.\n+        // Both operations are considered to have release semantics.\n+        current.increment_clock(current_index);\n+        created.increment_clock(created_index);\n+    }\n+\n+    /// Hook on a thread join to update the implicit happens-before relation\n+    /// between the joined thead and the current thread.\n+    #[inline]\n+    pub fn thread_joined(&self, current_thread: ThreadId, join_thread: ThreadId) {\n+        let mut clocks_vec = self.vector_clocks.borrow_mut();\n+        let thread_info = self.thread_info.borrow();\n+\n+        // Load the vector clock of the current thread.\n+        let current_index = thread_info[current_thread]\n+            .vector_index\n+            .expect(\"Performed thread join on thread with no assigned vector\");\n+        let current = &mut clocks_vec[current_index];\n+\n+        // Load the associated vector clock for the terminated thread.\n+        let join_clock = thread_info[join_thread]\n+            .termination_vector_clock\n+            .as_ref()\n+            .expect(\"Joined with thread but thread has not terminated\");\n+\n+\n+        // The join thread happens-before the current thread\n+        // so update the current vector clock.\n+        // Is not a release operation so the clock is not incremented.\n+        current.clock.join(join_clock);\n+\n+        // Check the number of active threads, if the value is 1\n+        // then test for potentially disabling multi-threaded execution.\n+        let active_threads = self.active_thread_count.get();\n+        if active_threads == 1 {\n+            // May potentially be able to disable multi-threaded execution.\n+            let current_clock = &clocks_vec[current_index];\n+            if clocks_vec\n+                .iter_enumerated()\n+                .all(|(idx, clocks)| clocks.clock[idx] <= current_clock.clock[idx])\n+            {\n+                // The all thread termations happen-before the current clock\n+                // therefore no data-races can be reported until a new thread\n+                // is created, so disable multi-threaded execution.\n+                self.multi_threaded.set(false);\n+            }\n+        }\n+\n+        // If the thread is marked as terminated but not joined\n+        // then move the thread to the re-use set.\n+        let mut termination = self.terminated_threads.borrow_mut();\n+        if let Some(index) = termination.remove(&join_thread) {\n+            let mut reuse = self.reuse_candidates.borrow_mut();\n+            reuse.insert(index);\n+        }\n+    }\n+\n+    /// On thread termination, the vector-clock may re-used\n+    /// in the future once all remaining thread-clocks catch\n+    /// up with the time index of the terminated thread.\n+    /// This assiges thread termination with a unique index\n+    /// which will be used to join the thread\n+    /// This should be called strictly before any calls to\n+    /// `thread_joined`.\n+    #[inline]\n+    pub fn thread_terminated(&self) {\n+        let current_index = self.current_index();\n+\n+        // Increment the clock to a unique termination timestamp.\n+        let mut vector_clocks = self.vector_clocks.borrow_mut();\n+        let current_clocks = &mut vector_clocks[current_index];\n+        current_clocks.increment_clock(current_index);\n+\n+        // Load the current thread id for the executing vector.\n+        let vector_info = self.vector_info.borrow();\n+        let current_thread = vector_info[current_index];\n+\n+        // Load the current thread metadata, and move to a terminated\n+        // vector state. Setting up the vector clock all join operations\n+        // will use.\n+        let mut thread_info = self.thread_info.borrow_mut();\n+        let current = &mut thread_info[current_thread];\n+        current.termination_vector_clock = Some(current_clocks.clock.clone());\n+\n+        // Add this thread as a candidate for re-use after a thread join\n+        // occurs.\n+        let mut termination = self.terminated_threads.borrow_mut();\n+        termination.insert(current_thread, current_index);\n+\n+        // Reduce the number of active threads, now that a thread has\n+        // terminated.\n+        let mut active_threads = self.active_thread_count.get();\n+        active_threads -= 1;\n+        self.active_thread_count.set(active_threads);\n+    }\n+\n+    /// Hook for updating the local tracker of the currently\n+    /// enabled thread, should always be updated whenever\n+    /// `active_thread` in thread.rs is updated.\n+    #[inline]\n+    pub fn thread_set_active(&self, thread: ThreadId) {\n+        let thread_info = self.thread_info.borrow();\n+        let vector_idx = thread_info[thread]\n+            .vector_index\n+            .expect(\"Setting thread active with no assigned vector\");\n+        self.current_index.set(vector_idx);\n+    }\n+\n+    /// Hook for updating the local tracker of the threads name\n+    /// this should always mirror the local value in thread.rs\n+    /// the thread name is used for improved diagnostics\n+    /// during a data-race.\n+    #[inline]\n+    pub fn thread_set_name(&self, thread: ThreadId, name: String) {\n+        let name = name.into_boxed_str();\n+        let mut thread_info = self.thread_info.borrow_mut();\n+        thread_info[thread].thread_name = Some(name);\n+    }\n+\n+    /// Attempt to perform a synchronized operation, this\n+    /// will perform no operation if multi-threading is\n+    /// not currently enabled.\n+    /// Otherwise it will increment the clock for the current\n+    /// vector before and after the operation for data-race\n+    /// detection between any happens-before edges the\n+    /// operation may create.\n+    fn maybe_perform_sync_operation<'tcx>(\n+        &self,\n+        op: impl FnOnce(VectorIdx, RefMut<'_, ThreadClockSet>) -> InterpResult<'tcx, bool>,\n+    ) -> InterpResult<'tcx> {\n+        if self.multi_threaded.get() {\n+            let (index, clocks) = self.current_thread_state_mut();\n+            if op(index, clocks)? {\n+                let (_, mut clocks) = self.current_thread_state_mut();\n+                clocks.increment_clock(index);\n+            }\n+        }\n+        Ok(())\n+    }\n+\n+    /// Internal utility to identify a thread stored internally\n+    /// returns the id and the name for better diagnostics.\n+    fn print_thread_metadata(&self, vector: VectorIdx) -> String {\n+        let thread = self.vector_info.borrow()[vector];\n+        let thread_name = &self.thread_info.borrow()[thread].thread_name;\n+        if let Some(name) = thread_name {\n+            let name: &str = name;\n+            format!(\"Thread(id = {:?}, name = {:?})\", thread.to_u32(), &*name)\n+        } else {\n+            format!(\"Thread(id = {:?})\", thread.to_u32())\n+        }\n+    }\n+\n+    /// Acquire a lock, express that the previous call of\n+    /// `validate_lock_release` must happen before this.\n+    /// As this is an acquire operation, the thread timestamp is not\n+    /// incremented.\n+    pub fn validate_lock_acquire(&self, lock: &VClock, thread: ThreadId) {\n+        let (_, mut clocks) = self.load_thread_state_mut(thread);\n+        clocks.clock.join(&lock);\n+    }\n+\n+    /// Release a lock handle, express that this happens-before\n+    /// any subsequent calls to `validate_lock_acquire`.\n+    /// For normal locks this should be equivalent to `validate_lock_release_shared`\n+    /// since an acquire operation should have occured before, however\n+    /// for futex & cond-var operations this is not the case and this\n+    /// operation must be used.\n+    pub fn validate_lock_release(&self, lock: &mut VClock, thread: ThreadId) {\n+        let (index, mut clocks) = self.load_thread_state_mut(thread);\n+        lock.clone_from(&clocks.clock);\n+        clocks.increment_clock(index);\n+    }\n+\n+    /// Release a lock handle, express that this happens-before\n+    /// any subsequent calls to `validate_lock_acquire` as well\n+    /// as any previous calls to this function after any\n+    /// `validate_lock_release` calls.\n+    /// For normal locks this should be equivalent to `validate_lock_release`.\n+    /// This function only exists for joining over the set of concurrent readers\n+    /// in a read-write lock and should not be used for anything else.\n+    pub fn validate_lock_release_shared(&self, lock: &mut VClock, thread: ThreadId) {\n+        let (index, mut clocks) = self.load_thread_state_mut(thread);\n+        lock.join(&clocks.clock);\n+        clocks.increment_clock(index);\n+    }\n+\n+    /// Load the vector index used by the given thread as well as the set of vector clocks\n+    /// used by the thread.\n+    #[inline]\n+    fn load_thread_state_mut(&self, thread: ThreadId) -> (VectorIdx, RefMut<'_, ThreadClockSet>) {\n+        let index = self.thread_info.borrow()[thread]\n+            .vector_index\n+            .expect(\"Loading thread state for thread with no assigned vector\");\n+        let ref_vector = self.vector_clocks.borrow_mut();\n+        let clocks = RefMut::map(ref_vector, |vec| &mut vec[index]);\n+        (index, clocks)\n+    }\n+\n+    /// Load the current vector clock in use and the current set of thread clocks\n+    /// in use for the vector.\n+    #[inline]\n+    fn current_thread_state(&self) -> (VectorIdx, Ref<'_, ThreadClockSet>) {\n+        let index = self.current_index();\n+        let ref_vector = self.vector_clocks.borrow();\n+        let clocks = Ref::map(ref_vector, |vec| &vec[index]);\n+        (index, clocks)\n+    }\n+\n+    /// Load the current vector clock in use and the current set of thread clocks\n+    /// in use for the vector mutably for modification.\n+    #[inline]\n+    fn current_thread_state_mut(&self) -> (VectorIdx, RefMut<'_, ThreadClockSet>) {\n+        let index = self.current_index();\n+        let ref_vector = self.vector_clocks.borrow_mut();\n+        let clocks = RefMut::map(ref_vector, |vec| &mut vec[index]);\n+        (index, clocks)\n+    }\n+\n+    /// Return the current thread, should be the same\n+    /// as the data-race active thread.\n+    #[inline]\n+    fn current_index(&self) -> VectorIdx {\n+        self.current_index.get()\n+    }\n+}"}, {"sha": "0a62f14dd3a153f1332909f1c6ce125a351f63ec", "filename": "src/eval.rs", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Feval.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Feval.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Feval.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -48,6 +48,8 @@ pub struct MiriConfig {\n     pub tracked_alloc_id: Option<AllocId>,\n     /// Whether to track raw pointers in stacked borrows.\n     pub track_raw: bool,\n+    /// Determine if data race detection should be enabled\n+    pub data_race_detector: bool,\n }\n \n impl Default for MiriConfig {\n@@ -65,6 +67,7 @@ impl Default for MiriConfig {\n             tracked_call_id: None,\n             tracked_alloc_id: None,\n             track_raw: false,\n+            data_race_detector: true,\n         }\n     }\n }"}, {"sha": "87effe9c6885215269e5cbd93ff0b28c6916224b", "filename": "src/lib.rs", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -22,6 +22,7 @@ extern crate rustc_mir;\n extern crate rustc_span;\n extern crate rustc_target;\n \n+mod data_race;\n mod diagnostics;\n mod eval;\n mod helpers;\n@@ -34,6 +35,7 @@ mod shims;\n mod stacked_borrows;\n mod sync;\n mod thread;\n+mod vector_clock;\n \n // Establish a \"crate-wide prelude\": we often import `crate::*`.\n \n@@ -52,6 +54,10 @@ pub use crate::shims::panic::{CatchUnwindData, EvalContextExt as _};\n pub use crate::shims::tls::{EvalContextExt as _, TlsData};\n pub use crate::shims::EvalContextExt as _;\n \n+pub use crate::data_race::{\n+    AtomicReadOp, AtomicWriteOp, AtomicRwOp, AtomicFenceOp,\n+    EvalContextExt as DataRaceEvalContextExt\n+};\n pub use crate::diagnostics::{\n     register_diagnostic, report_error, EvalContextExt as DiagnosticsEvalContextExt,\n     TerminationInfo, NonHaltingDiagnostic,\n@@ -74,6 +80,9 @@ pub use crate::thread::{\n pub use crate::sync::{\n     EvalContextExt as SyncEvalContextExt, CondvarId, MutexId, RwLockId\n };\n+pub use crate::vector_clock::{\n+    VClock, VSmallClockMap, VectorIdx, VTimestamp\n+};\n \n /// Insert rustc arguments at the beginning of the argument list that Miri wants to be\n /// set per default, for maximal validation power."}, {"sha": "70f4bd722df7533fc97874bee82bef1b23b126a7", "filename": "src/machine.rs", "status": "modified", "additions": 25, "deletions": 1, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fmachine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fmachine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fmachine.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -109,12 +109,16 @@ impl fmt::Display for MiriMemoryKind {\n pub struct AllocExtra {\n     /// Stacked Borrows state is only added if it is enabled.\n     pub stacked_borrows: Option<stacked_borrows::AllocExtra>,\n+    /// Data race detection via the use of a vector-clock,\n+    ///  this is only added if it is enabled.\n+    pub data_race: Option<data_race::AllocExtra>,\n }\n \n /// Extra global memory data\n #[derive(Clone, Debug)]\n pub struct MemoryExtra {\n     pub stacked_borrows: Option<stacked_borrows::MemoryExtra>,\n+    pub data_race: Option<data_race::MemoryExtra>,\n     pub intptrcast: intptrcast::MemoryExtra,\n \n     /// Mapping extern static names to their canonical allocation.\n@@ -144,8 +148,14 @@ impl MemoryExtra {\n         } else {\n             None\n         };\n+        let data_race = if config.data_race_detector {\n+            Some(Rc::new(data_race::GlobalState::new()))\n+        } else {\n+            None\n+        };\n         MemoryExtra {\n             stacked_borrows,\n+            data_race,\n             intptrcast: Default::default(),\n             extern_statics: FxHashMap::default(),\n             rng: RefCell::new(rng),\n@@ -467,6 +477,11 @@ impl<'mir, 'tcx> Machine<'mir, 'tcx> for Evaluator<'mir, 'tcx> {\n                 // No stacks, no tag.\n                 (None, Tag::Untagged)\n             };\n+        let race_alloc = if let Some(data_race) = &memory_extra.data_race {\n+            Some(data_race::AllocExtra::new_allocation(&data_race, alloc.size))\n+        } else {\n+            None\n+        };\n         let mut stacked_borrows = memory_extra.stacked_borrows.as_ref().map(|sb| sb.borrow_mut());\n         let alloc: Allocation<Tag, Self::AllocExtra> = alloc.with_tags_and_extra(\n             |alloc| {\n@@ -478,7 +493,7 @@ impl<'mir, 'tcx> Machine<'mir, 'tcx> for Evaluator<'mir, 'tcx> {\n                     Tag::Untagged\n                 }\n             },\n-            AllocExtra { stacked_borrows: stacks },\n+            AllocExtra { stacked_borrows: stacks, data_race: race_alloc },\n         );\n         (Cow::Owned(alloc), base_tag)\n     }\n@@ -584,6 +599,9 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n+        if let Some(data_race) = &alloc.extra.data_race {\n+            data_race.read(ptr, size)?;\n+        }\n         if let Some(stacked_borrows) = &alloc.extra.stacked_borrows {\n             stacked_borrows.memory_read(ptr, size)\n         } else {\n@@ -597,6 +615,9 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n+        if let Some(data_race) = &mut alloc.extra.data_race {\n+            data_race.write(ptr, size)?;\n+        }\n         if let Some(stacked_borrows) = &mut alloc.extra.stacked_borrows {\n             stacked_borrows.memory_written(ptr, size)\n         } else {\n@@ -610,6 +631,9 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n+        if let Some(data_race) = &mut alloc.extra.data_race {\n+            data_race.deallocate(ptr, size)?;\n+        }\n         if let Some(stacked_borrows) = &mut alloc.extra.stacked_borrows {\n             stacked_borrows.memory_deallocated(ptr, size)\n         } else {"}, {"sha": "8f7ae6bebb52ed9d4c525e0019ac56427fc4d219", "filename": "src/shims/intrinsics.rs", "status": "modified", "additions": 247, "deletions": 151, "changes": 398, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fshims%2Fintrinsics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fshims%2Fintrinsics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fintrinsics.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -4,7 +4,7 @@ use log::trace;\n \n use rustc_attr as attr;\n use rustc_ast::ast::FloatTy;\n-use rustc_middle::{mir, ty};\n+use rustc_middle::{mir, mir::BinOp, ty};\n use rustc_middle::ty::layout::IntegerExt;\n use rustc_apfloat::{Float, Round};\n use rustc_target::abi::{Align, Integer, LayoutOf};\n@@ -306,157 +306,117 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             }\n \n             // Atomic operations\n-            #[rustfmt::skip]\n-            | \"atomic_load\"\n-            | \"atomic_load_relaxed\"\n-            | \"atomic_load_acq\"\n-            => {\n-                let &[place] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                let val = this.read_scalar(place.into())?; // make sure it fits into a scalar; otherwise it cannot be atomic\n-\n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                this.write_scalar(val, dest)?;\n-            }\n-\n-            #[rustfmt::skip]\n-            | \"atomic_store\"\n-            | \"atomic_store_relaxed\"\n-            | \"atomic_store_rel\"\n-            => {\n-                let &[place, val] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                let val = this.read_scalar(val)?; // make sure it fits into a scalar; otherwise it cannot be atomic\n+            \"atomic_load\" => this.atomic_load(args, dest, AtomicReadOp::SeqCst)?,\n+            \"atomic_load_relaxed\" => this.atomic_load(args, dest, AtomicReadOp::Relaxed)?,\n+            \"atomic_load_acq\" => this.atomic_load(args, dest, AtomicReadOp::Acquire)?,\n+\n+            \"atomic_store\" => this.atomic_store(args, AtomicWriteOp::SeqCst)?,\n+            \"atomic_store_relaxed\" => this.atomic_store(args, AtomicWriteOp::Relaxed)?,\n+            \"atomic_store_rel\" => this.atomic_store(args, AtomicWriteOp::Release)?,\n+\n+            \"atomic_fence_acq\" => this.atomic_fence(args, AtomicFenceOp::Acquire)?,\n+            \"atomic_fence_rel\" => this.atomic_fence(args, AtomicFenceOp::Release)?,\n+            \"atomic_fence_acqrel\" => this.atomic_fence(args, AtomicFenceOp::AcqRel)?,\n+            \"atomic_fence\" => this.atomic_fence(args, AtomicFenceOp::SeqCst)?,\n+\n+            \"atomic_singlethreadfence_acq\" => this.compiler_fence(args, AtomicFenceOp::Acquire)?,\n+            \"atomic_singlethreadfence_rel\" => this.compiler_fence(args, AtomicFenceOp::Release)?,\n+            \"atomic_singlethreadfence_acqrel\" => this.compiler_fence(args, AtomicFenceOp::AcqRel)?,\n+            \"atomic_singlethreadfence\" => this.compiler_fence(args, AtomicFenceOp::SeqCst)?,\n+\n+            \"atomic_xchg\" => this.atomic_exchange(args, dest, AtomicRwOp::SeqCst)?,\n+            \"atomic_xchg_acq\" => this.atomic_exchange(args, dest, AtomicRwOp::Acquire)?,\n+            \"atomic_xchg_rel\" => this.atomic_exchange(args, dest, AtomicRwOp::Release)?,\n+            \"atomic_xchg_acqrel\" => this.atomic_exchange(args, dest, AtomicRwOp::AcqRel)?,\n+            \"atomic_xchg_relaxed\" => this.atomic_exchange(args, dest, AtomicRwOp::Relaxed)?,\n+\n+            \"atomic_cxchg\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::SeqCst\n+            )?,\n+            \"atomic_cxchg_acq\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRwOp::Acquire, AtomicReadOp::Acquire\n+            )?,\n+            \"atomic_cxchg_rel\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRwOp::Release, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_acqrel\" => this.atomic_compare_exchange\n+            (args, dest, AtomicRwOp::AcqRel, AtomicReadOp::Acquire\n+            )?,\n+            \"atomic_cxchg_relaxed\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRwOp::Relaxed, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_acq_failrelaxed\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRwOp::Acquire, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_acqrel_failrelaxed\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRwOp::AcqRel, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_failrelaxed\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_failacq\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::Acquire\n+            )?,\n+\n+            \"atomic_cxchgweak\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::SeqCst\n+            )?,\n+            \"atomic_cxchgweak_acq\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::Acquire, AtomicReadOp::Acquire\n+            )?,\n+            \"atomic_cxchgweak_rel\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::Release, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_acqrel\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::AcqRel, AtomicReadOp::Acquire\n+            )?,\n+            \"atomic_cxchgweak_relaxed\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::Relaxed, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_acq_failrelaxed\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::Acquire, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_acqrel_failrelaxed\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::AcqRel, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_failrelaxed\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_failacq\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::Acquire\n+            )?,\n+\n+            \"atomic_or\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_or_acq\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::Acquire)?,\n+            \"atomic_or_rel\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::Release)?,\n+            \"atomic_or_acqrel\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_or_relaxed\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::Relaxed)?,\n+            \"atomic_xor\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_xor_acq\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::Acquire)?,\n+            \"atomic_xor_rel\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::Release)?,\n+            \"atomic_xor_acqrel\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_xor_relaxed\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::Relaxed)?,\n+            \"atomic_and\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_and_acq\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::Acquire)?,\n+            \"atomic_and_rel\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::Release)?,\n+            \"atomic_and_acqrel\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_and_relaxed\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::Relaxed)?,\n+            \"atomic_nand\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::SeqCst)?,\n+            \"atomic_nand_acq\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::Acquire)?,\n+            \"atomic_nand_rel\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::Release)?,\n+            \"atomic_nand_acqrel\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::AcqRel)?,\n+            \"atomic_nand_relaxed\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::Relaxed)?,\n+            \"atomic_xadd\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_xadd_acq\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::Acquire)?,\n+            \"atomic_xadd_rel\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::Release)?,\n+            \"atomic_xadd_acqrel\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_xadd_relaxed\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::Relaxed)?,\n+            \"atomic_xsub\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_xsub_acq\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::Acquire)?,\n+            \"atomic_xsub_rel\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::Release)?,\n+            \"atomic_xsub_acqrel\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_xsub_relaxed\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::Relaxed)?,\n \n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                this.write_scalar(val, place.into())?;\n-            }\n-\n-            #[rustfmt::skip]\n-            | \"atomic_fence_acq\"\n-            | \"atomic_fence_rel\"\n-            | \"atomic_fence_acqrel\"\n-            | \"atomic_fence\"\n-            | \"atomic_singlethreadfence_acq\"\n-            | \"atomic_singlethreadfence_rel\"\n-            | \"atomic_singlethreadfence_acqrel\"\n-            | \"atomic_singlethreadfence\"\n-            => {\n-                let &[] = check_arg_count(args)?;\n-                // FIXME: this will become relevant once we try to detect data races.\n-            }\n-\n-            _ if intrinsic_name.starts_with(\"atomic_xchg\") => {\n-                let &[place, new] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                let new = this.read_scalar(new)?;\n-                let old = this.read_scalar(place.into())?;\n-\n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                this.write_scalar(old, dest)?; // old value is returned\n-                this.write_scalar(new, place.into())?;\n-            }\n-\n-            _ if intrinsic_name.starts_with(\"atomic_cxchg\") => {\n-                let &[place, expect_old, new] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                let expect_old = this.read_immediate(expect_old)?; // read as immediate for the sake of `binary_op()`\n-                let new = this.read_scalar(new)?;\n-                let old = this.read_immediate(place.into())?; // read as immediate for the sake of `binary_op()`\n-\n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                // `binary_op` will bail if either of them is not a scalar.\n-                let eq = this.overflowing_binary_op(mir::BinOp::Eq, old, expect_old)?.0;\n-                let res = Immediate::ScalarPair(old.to_scalar_or_uninit(), eq.into());\n-                // Return old value.\n-                this.write_immediate(res, dest)?;\n-                // Update ptr depending on comparison.\n-                if eq.to_bool()? {\n-                    this.write_scalar(new, place.into())?;\n-                }\n-            }\n-\n-            #[rustfmt::skip]\n-            | \"atomic_or\"\n-            | \"atomic_or_acq\"\n-            | \"atomic_or_rel\"\n-            | \"atomic_or_acqrel\"\n-            | \"atomic_or_relaxed\"\n-            | \"atomic_xor\"\n-            | \"atomic_xor_acq\"\n-            | \"atomic_xor_rel\"\n-            | \"atomic_xor_acqrel\"\n-            | \"atomic_xor_relaxed\"\n-            | \"atomic_and\"\n-            | \"atomic_and_acq\"\n-            | \"atomic_and_rel\"\n-            | \"atomic_and_acqrel\"\n-            | \"atomic_and_relaxed\"\n-            | \"atomic_nand\"\n-            | \"atomic_nand_acq\"\n-            | \"atomic_nand_rel\"\n-            | \"atomic_nand_acqrel\"\n-            | \"atomic_nand_relaxed\"\n-            | \"atomic_xadd\"\n-            | \"atomic_xadd_acq\"\n-            | \"atomic_xadd_rel\"\n-            | \"atomic_xadd_acqrel\"\n-            | \"atomic_xadd_relaxed\"\n-            | \"atomic_xsub\"\n-            | \"atomic_xsub_acq\"\n-            | \"atomic_xsub_rel\"\n-            | \"atomic_xsub_acqrel\"\n-            | \"atomic_xsub_relaxed\"\n-            => {\n-                let &[place, rhs] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                if !place.layout.ty.is_integral() {\n-                    bug!(\"Atomic arithmetic operations only work on integer types\");\n-                }\n-                let rhs = this.read_immediate(rhs)?;\n-                let old = this.read_immediate(place.into())?;\n-\n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                this.write_immediate(*old, dest)?; // old value is returned\n-                let (op, neg) = match intrinsic_name.split('_').nth(1).unwrap() {\n-                    \"or\" => (mir::BinOp::BitOr, false),\n-                    \"xor\" => (mir::BinOp::BitXor, false),\n-                    \"and\" => (mir::BinOp::BitAnd, false),\n-                    \"xadd\" => (mir::BinOp::Add, false),\n-                    \"xsub\" => (mir::BinOp::Sub, false),\n-                    \"nand\" => (mir::BinOp::BitAnd, true),\n-                    _ => bug!(),\n-                };\n-                // Atomics wrap around on overflow.\n-                let val = this.binary_op(op, old, rhs)?;\n-                let val = if neg { this.unary_op(mir::UnOp::Not, val)? } else { val };\n-                this.write_immediate(*val, place.into())?;\n-            }\n \n             // Query type information\n             \"assert_inhabited\" |\n@@ -498,6 +458,142 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         Ok(())\n     }\n \n+    fn atomic_load(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n+        atomic: AtomicReadOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+\n+        let &[place] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+\n+        // make sure it fits into a scalar; otherwise it cannot be atomic\n+        let val = this.read_scalar_atomic(place, atomic)?;\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+        this.write_scalar(val, dest)?;\n+        Ok(())\n+    }\n+\n+    fn atomic_store(&mut self, args: &[OpTy<'tcx, Tag>], atomic: AtomicWriteOp) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        let &[place, val] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+        let val = this.read_scalar(val)?; // make sure it fits into a scalar; otherwise it cannot be atomic\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+\n+        // Perform atomic store\n+        this.write_scalar_atomic(val, place, atomic)?;\n+        Ok(())\n+    }\n+\n+    fn compiler_fence(&mut self, args: &[OpTy<'tcx, Tag>], atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n+        let &[] = check_arg_count(args)?;\n+        let _ = atomic;\n+        //FIXME: compiler fences are currently ignored\n+        Ok(())\n+    }\n+\n+    fn atomic_fence(&mut self, args: &[OpTy<'tcx, Tag>], atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let &[] = check_arg_count(args)?;\n+        this.validate_atomic_fence(atomic)?;\n+        Ok(())\n+    }\n+\n+    fn atomic_op(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n+        op: mir::BinOp, neg: bool, atomic: AtomicRwOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        let &[place, rhs] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+        if !place.layout.ty.is_integral() {\n+            bug!(\"Atomic arithmetic operations only work on integer types\");\n+        }\n+        let rhs = this.read_immediate(rhs)?;\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+        \n+        let old = this.atomic_op_immediate(place, rhs, op, neg, atomic)?;\n+        this.write_immediate(*old, dest)?; // old value is returned\n+        Ok(())\n+    }\n+    \n+    fn atomic_exchange(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>, atomic: AtomicRwOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        let &[place, new] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+        let new = this.read_scalar(new)?;\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+\n+        let old = this.atomic_exchange_scalar(place, new, atomic)?;\n+        this.write_scalar(old, dest)?; // old value is returned\n+        Ok(())\n+    }\n+\n+    fn atomic_compare_exchange(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n+        success: AtomicRwOp, fail: AtomicReadOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        let &[place, expect_old, new] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+        let expect_old = this.read_immediate(expect_old)?; // read as immediate for the sake of `binary_op()`\n+        let new = this.read_scalar(new)?;\n+\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+\n+        \n+        let old = this.atomic_compare_exchange_scalar(\n+            place, expect_old, new, success, fail\n+        )?;\n+\n+        // Return old value.\n+        this.write_immediate(old, dest)?;\n+        Ok(())\n+    }\n+\n+    fn atomic_compare_exchange_weak(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n+        success: AtomicRwOp, fail: AtomicReadOp\n+    ) -> InterpResult<'tcx> {\n+\n+        // FIXME: the weak part of this is currently not modelled,\n+        //  it is assumed to always succeed unconditionally.\n+        self.atomic_compare_exchange(args, dest, success, fail)\n+    }\n+\n     fn float_to_int_unchecked<F>(\n         &self,\n         f: F,"}, {"sha": "5243431194eba51dfc7d89cfae2ff0e97207d879", "filename": "src/shims/posix/linux/sync.rs", "status": "modified", "additions": 12, "deletions": 1, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fshims%2Fposix%2Flinux%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fshims%2Fposix%2Flinux%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Flinux%2Fsync.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -78,7 +78,18 @@ pub fn futex<'tcx>(\n             // Read an `i32` through the pointer, regardless of any wrapper types.\n             // It's not uncommon for `addr` to be passed as another type than `*mut i32`, such as `*const AtomicI32`.\n             // FIXME: this fails if `addr` is not a pointer type.\n-            let futex_val = this.read_scalar_at_offset(addr.into(), 0, this.machine.layouts.i32)?.to_i32()?;\n+            // The atomic ordering for futex(https://man7.org/linux/man-pages/man2/futex.2.html):\n+            //  \"The load of the value of the futex word is an\n+            //   atomic memory access (i.e., using atomic machine instructions\n+            //   of the respective architecture).  This load, the comparison\n+            //   with the expected value, and starting to sleep are performed\n+            //   atomically and totally ordered with respect to other futex\n+            //   operations on the same futex word.\"\n+            // SeqCst is total order over all operations.\n+            // FIXME: check if this should be changed when weak memory orders are added.\n+            let futex_val = this.read_scalar_at_offset_atomic(\n+                addr.into(), 0, this.machine.layouts.i32, AtomicReadOp::SeqCst\n+            )?.to_i32()?;\n             if val == futex_val {\n                 // The value still matches, so we block the trait make it wait for FUTEX_WAKE.\n                 this.block_thread(thread);"}, {"sha": "868c72289a1a0ce401cb21963cc5ae6771bec9f4", "filename": "src/shims/posix/sync.rs", "status": "modified", "additions": 32, "deletions": 8, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fshims%2Fposix%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fshims%2Fposix%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Fsync.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -62,7 +62,10 @@ fn mutex_get_kind<'mir, 'tcx: 'mir>(\n     mutex_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n     let offset = if ecx.pointer_size().bytes() == 8 { 16 } else { 12 };\n-    ecx.read_scalar_at_offset(mutex_op, offset, ecx.machine.layouts.i32)\n+    ecx.read_scalar_at_offset_atomic(\n+        mutex_op, offset, ecx.machine.layouts.i32,\n+        AtomicReadOp::Relaxed\n+    )\n }\n \n fn mutex_set_kind<'mir, 'tcx: 'mir>(\n@@ -71,22 +74,31 @@ fn mutex_set_kind<'mir, 'tcx: 'mir>(\n     kind: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n     let offset = if ecx.pointer_size().bytes() == 8 { 16 } else { 12 };\n-    ecx.write_scalar_at_offset(mutex_op, offset, kind, ecx.machine.layouts.i32)\n+    ecx.write_scalar_at_offset_atomic(\n+        mutex_op, offset, kind, ecx.machine.layouts.i32,\n+        AtomicWriteOp::Relaxed\n+    )\n }\n \n fn mutex_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset(mutex_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_atomic(\n+        mutex_op, 4, ecx.machine.layouts.u32, \n+        AtomicReadOp::Relaxed\n+    )\n }\n \n fn mutex_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset(mutex_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_atomic(\n+        mutex_op, 4, id, ecx.machine.layouts.u32,\n+        AtomicWriteOp::Relaxed\n+    )\n }\n \n fn mutex_get_or_create_id<'mir, 'tcx: 'mir>(\n@@ -116,15 +128,21 @@ fn rwlock_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset(rwlock_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_atomic(\n+        rwlock_op, 4, ecx.machine.layouts.u32,\n+        AtomicReadOp::Relaxed\n+    )\n }\n \n fn rwlock_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset(rwlock_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_atomic(\n+        rwlock_op, 4, id, ecx.machine.layouts.u32,\n+        AtomicWriteOp::Relaxed\n+    )\n }\n \n fn rwlock_get_or_create_id<'mir, 'tcx: 'mir>(\n@@ -177,15 +195,21 @@ fn cond_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     cond_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset(cond_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_atomic(\n+        cond_op, 4, ecx.machine.layouts.u32,\n+        AtomicReadOp::Relaxed\n+    )\n }\n \n fn cond_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     cond_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset(cond_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_atomic(\n+        cond_op, 4, id, ecx.machine.layouts.u32,\n+        AtomicWriteOp::Relaxed\n+    )\n }\n \n fn cond_get_or_create_id<'mir, 'tcx: 'mir>("}, {"sha": "0ea20cdff6cb3c1de55bfa14cdcb6ee0891e2cef", "filename": "src/shims/posix/thread.rs", "status": "modified", "additions": 15, "deletions": 6, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fshims%2Fposix%2Fthread.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fshims%2Fposix%2Fthread.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Fthread.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -15,25 +15,33 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n \n         this.tcx.sess.warn(\n-            \"thread support is experimental. \\\n-             For example, Miri does not detect data races yet.\",\n+            \"thread support is experimental and incomplete: weak memory effects are not emulated.\"\n         );\n \n+        // Create the new thread\n         let new_thread_id = this.create_thread();\n-        // Also switch to new thread so that we can push the first stackframe.\n-        let old_thread_id = this.set_active_thread(new_thread_id);\n \n+        // Write the current thread-id, switch to the next thread later\n+        // to treat this write operation as occuring on the current thread.\n         let thread_info_place = this.deref_operand(thread)?;\n         this.write_scalar(\n             Scalar::from_uint(new_thread_id.to_u32(), thread_info_place.layout.size),\n             thread_info_place.into(),\n         )?;\n \n+        // Read the function argument that will be sent to the new thread\n+        // before the thread starts executing since reading after the \n+        // context switch will incorrectly report a data-race.\n         let fn_ptr = this.read_scalar(start_routine)?.check_init()?;\n-        let instance = this.memory.get_fn(fn_ptr)?.as_instance()?;\n-\n         let func_arg = this.read_immediate(arg)?;\n \n+        // Finally switch to new thread so that we can push the first stackframe.\n+        // After this all accesses will be treated as occuring in the new thread.\n+        let old_thread_id = this.set_active_thread(new_thread_id);\n+\n+        // Perform the function pointer load in the new thread frame.\n+        let instance = this.memory.get_fn(fn_ptr)?.as_instance()?;\n+\n         // Note: the returned value is currently ignored (see the FIXME in\n         // pthread_join below) because the Rust standard library does not use\n         // it.\n@@ -47,6 +55,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             StackPopCleanup::None { cleanup: true },\n         )?;\n \n+        // Restore the old active thread frame.\n         this.set_active_thread(old_thread_id);\n \n         Ok(0)"}, {"sha": "4d488565faf3ea3744d949a26e51dc1b85690e36", "filename": "src/sync.rs", "status": "modified", "additions": 104, "deletions": 9, "changes": 113, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fsync.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -61,6 +61,12 @@ struct Mutex {\n     lock_count: usize,\n     /// The queue of threads waiting for this mutex.\n     queue: VecDeque<ThreadId>,\n+    /// Data race handle, this tracks the happens-before\n+    /// relationship between each mutex access. It is\n+    /// released to during unlock and acquired from during\n+    /// locking, and therefore stores the clock of the last\n+    /// thread to release this mutex.\n+    data_race: VClock\n }\n \n declare_id!(RwLockId);\n@@ -77,6 +83,25 @@ struct RwLock {\n     writer_queue: VecDeque<ThreadId>,\n     /// The queue of reader threads waiting for this lock.\n     reader_queue: VecDeque<ThreadId>,\n+    /// Data race handle for writers, tracks the happens-before\n+    /// ordering between each write access to a rwlock and is updated\n+    /// after a sequence of concurrent readers to track the happens-\n+    /// before ordering between the set of previous readers and\n+    /// the current writer.\n+    /// Contains the clock of the last thread to release a writer\n+    /// lock or the joined clock of the set of last threads to release\n+    /// shared reader locks.\n+    data_race: VClock,\n+    /// Data race handle for readers, this is temporary storage\n+    /// for the combined happens-before ordering for between all\n+    /// concurrent readers and the next writer, and the value\n+    /// is stored to the main data_race variable once all\n+    /// readers are finished.\n+    /// Has to be stored separately since reader lock acquires\n+    /// must load the clock of the last write and must not \n+    /// add happens-before orderings between shared reader\n+    /// locks.\n+    data_race_reader: VClock,\n }\n \n declare_id!(CondvarId);\n@@ -94,12 +119,24 @@ struct CondvarWaiter {\n #[derive(Default, Debug)]\n struct Condvar {\n     waiters: VecDeque<CondvarWaiter>,\n+    /// Tracks the happens-before relationship\n+    /// between a cond-var signal and a cond-var\n+    /// wait during a non-suprious signal event.\n+    /// Contains the clock of the last thread to\n+    /// perform a futex-signal.\n+    data_race: VClock,\n }\n \n /// The futex state.\n #[derive(Default, Debug)]\n struct Futex {\n     waiters: VecDeque<FutexWaiter>,\n+    /// Tracks the happens-before relationship\n+    /// between a futex-wake and a futex-wait\n+    /// during a non-spurious wake event.\n+    /// Contains the clock of the last thread to\n+    /// perform a futex-wake.\n+    data_race: VClock,\n }\n \n /// A thread waiting on a futex.\n@@ -205,6 +242,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             mutex.owner = Some(thread);\n         }\n         mutex.lock_count = mutex.lock_count.checked_add(1).unwrap();\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.validate_lock_acquire(&mutex.data_race, thread);\n+        }\n     }\n \n     /// Try unlocking by decreasing the lock count and returning the old lock\n@@ -232,6 +272,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 mutex.owner = None;\n                 // The mutex is completely unlocked. Try transfering ownership\n                 // to another thread.\n+                if let Some(data_race) = &this.memory.extra.data_race {\n+                    data_race.validate_lock_release(&mut mutex.data_race, current_owner);\n+                }\n                 this.mutex_dequeue_and_lock(id);\n             }\n             Some(old_lock_count)\n@@ -284,15 +327,20 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n         assert!(!this.rwlock_is_write_locked(id), \"the lock is write locked\");\n         trace!(\"rwlock_reader_lock: {:?} now also held (one more time) by {:?}\", id, reader);\n-        let count = this.machine.threads.sync.rwlocks[id].readers.entry(reader).or_insert(0);\n+        let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+        let count = rwlock.readers.entry(reader).or_insert(0);\n         *count = count.checked_add(1).expect(\"the reader counter overflowed\");\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.validate_lock_acquire(&rwlock.data_race, reader);\n+        }\n     }\n \n     /// Try read-unlock the lock for `reader` and potentially give the lock to a new owner.\n     /// Returns `true` if succeeded, `false` if this `reader` did not hold the lock.\n     fn rwlock_reader_unlock(&mut self, id: RwLockId, reader: ThreadId) -> bool {\n         let this = self.eval_context_mut();\n-        match this.machine.threads.sync.rwlocks[id].readers.entry(reader) {\n+        let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+        match rwlock.readers.entry(reader) {\n             Entry::Occupied(mut entry) => {\n                 let count = entry.get_mut();\n                 assert!(*count > 0, \"rwlock locked with count == 0\");\n@@ -306,8 +354,18 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             }\n             Entry::Vacant(_) => return false, // we did not even own this lock\n         }\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.validate_lock_release_shared(&mut rwlock.data_race_reader, reader);\n+        }\n+\n         // The thread was a reader. If the lock is not held any more, give it to a writer.\n         if this.rwlock_is_locked(id).not() {\n+\n+            // All the readers are finished, so set the writer data-race handle to the value\n+            //  of the union of all reader data race handles, since the set of readers\n+            //  happen-before the writers\n+            let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+            rwlock.data_race.clone_from(&rwlock.data_race_reader);\n             this.rwlock_dequeue_and_lock_writer(id);\n         }\n         true\n@@ -332,7 +390,11 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n         assert!(!this.rwlock_is_locked(id), \"the rwlock is already locked\");\n         trace!(\"rwlock_writer_lock: {:?} now held by {:?}\", id, writer);\n-        this.machine.threads.sync.rwlocks[id].writer = Some(writer);\n+        let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+        rwlock.writer = Some(writer);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.validate_lock_acquire(&rwlock.data_race, writer);\n+        }\n     }\n \n     #[inline]\n@@ -347,6 +409,13 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             }\n             rwlock.writer = None;\n             trace!(\"rwlock_writer_unlock: {:?} unlocked by {:?}\", id, expected_writer);\n+            // Release memory to both reader and writer vector clocks\n+            //  since this writer happens-before both the union of readers once they are finished\n+            //  and the next writer\n+            if let Some(data_race) = &this.memory.extra.data_race {\n+                data_race.validate_lock_release(&mut rwlock.data_race, current_writer);\n+                data_race.validate_lock_release(&mut rwlock.data_race_reader, current_writer);\n+            }\n             // The thread was a writer.\n             //\n             // We are prioritizing writers here against the readers. As a\n@@ -405,10 +474,22 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     /// variable.\n     fn condvar_signal(&mut self, id: CondvarId) -> Option<(ThreadId, MutexId)> {\n         let this = self.eval_context_mut();\n-        this.machine.threads.sync.condvars[id]\n-            .waiters\n+        let current_thread = this.get_active_thread();\n+        let condvar = &mut this.machine.threads.sync.condvars[id];\n+        let data_race = &this.memory.extra.data_race;\n+\n+        // Each condvar signal happens-before the end of the condvar wake\n+        if let Some(data_race) = data_race {\n+            data_race.validate_lock_release(&mut condvar.data_race, current_thread);\n+        }\n+        condvar.waiters\n             .pop_front()\n-            .map(|waiter| (waiter.thread, waiter.mutex))\n+            .map(|waiter| {\n+                if let Some(data_race) = data_race {\n+                    data_race.validate_lock_acquire(&mut condvar.data_race, waiter.thread);\n+                }\n+                (waiter.thread, waiter.mutex)\n+            })\n     }\n \n     #[inline]\n@@ -420,15 +501,29 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n     fn futex_wait(&mut self, addr: Pointer<stacked_borrows::Tag>, thread: ThreadId) {\n         let this = self.eval_context_mut();\n-        let waiters = &mut this.machine.threads.sync.futexes.entry(addr.erase_tag()).or_default().waiters;\n+        let futex = &mut this.machine.threads.sync.futexes.entry(addr.erase_tag()).or_default();\n+        let waiters = &mut futex.waiters;\n         assert!(waiters.iter().all(|waiter| waiter.thread != thread), \"thread is already waiting\");\n         waiters.push_back(FutexWaiter { thread });\n     }\n \n     fn futex_wake(&mut self, addr: Pointer<stacked_borrows::Tag>) -> Option<ThreadId> {\n         let this = self.eval_context_mut();\n-        let waiters = &mut this.machine.threads.sync.futexes.get_mut(&addr.erase_tag())?.waiters;\n-        waiters.pop_front().map(|waiter| waiter.thread)\n+        let current_thread = this.get_active_thread();\n+        let futex = &mut this.machine.threads.sync.futexes.get_mut(&addr.erase_tag())?;\n+        let data_race =  &this.memory.extra.data_race;\n+\n+        // Each futex-wake happens-before the end of the futex wait\n+        if let Some(data_race) = data_race {\n+            data_race.validate_lock_release(&mut futex.data_race, current_thread);\n+        }\n+        let res = futex.waiters.pop_front().map(|waiter| {\n+            if let Some(data_race) = data_race {\n+                data_race.validate_lock_acquire(&futex.data_race, waiter.thread);  \n+            }\n+            waiter.thread\n+        });\n+        res\n     }\n \n     fn futex_remove_waiter(&mut self, addr: Pointer<stacked_borrows::Tag>, thread: ThreadId) {"}, {"sha": "5d783430417bef7030c1fa732ff0a6ccbbe781a8", "filename": "src/thread.rs", "status": "modified", "additions": 42, "deletions": 7, "changes": 49, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fthread.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fthread.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fthread.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -3,6 +3,7 @@\n use std::cell::RefCell;\n use std::collections::hash_map::Entry;\n use std::convert::TryFrom;\n+use std::rc::Rc;\n use std::num::TryFromIntError;\n use std::time::{Duration, Instant, SystemTime};\n \n@@ -327,7 +328,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n     }\n \n     /// Mark that the active thread tries to join the thread with `joined_thread_id`.\n-    fn join_thread(&mut self, joined_thread_id: ThreadId) -> InterpResult<'tcx> {\n+    fn join_thread(&mut self, joined_thread_id: ThreadId, data_race: &Option<Rc<data_race::GlobalState>>) -> InterpResult<'tcx> {\n         if self.threads[joined_thread_id].join_status != ThreadJoinStatus::Joinable {\n             throw_ub_format!(\"trying to join a detached or already joined thread\");\n         }\n@@ -351,6 +352,11 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n                 self.active_thread,\n                 joined_thread_id\n             );\n+        } else {\n+            // The thread has already terminated - mark join happens-before\n+            if let Some(data_race) = data_race {\n+                data_race.thread_joined(self.active_thread, joined_thread_id);\n+            }\n         }\n         Ok(())\n     }\n@@ -425,7 +431,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n \n     /// Wakes up threads joining on the active one and deallocates thread-local statics.\n     /// The `AllocId` that can now be freed is returned.\n-    fn thread_terminated(&mut self) -> Vec<AllocId> {\n+    fn thread_terminated(&mut self, data_race: &Option<Rc<data_race::GlobalState>>) -> Vec<AllocId> {\n         let mut free_tls_statics = Vec::new();\n         {\n             let mut thread_local_statics = self.thread_local_alloc_ids.borrow_mut();\n@@ -440,9 +446,17 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n                 return false;\n             });\n         }\n+        // Set the thread into a terminated state in the data-race detector\n+        if let Some(data_race) = data_race {\n+            data_race.thread_terminated();\n+        }\n         // Check if we need to unblock any threads.\n         for (i, thread) in self.threads.iter_enumerated_mut() {\n             if thread.state == ThreadState::BlockedOnJoin(self.active_thread) {\n+                // The thread has terminated, mark happens-before edge to joining thread\n+                if let Some(data_race) = data_race {\n+                    data_race.thread_joined(i, self.active_thread);\n+                }\n                 trace!(\"unblocking {:?} because {:?} terminated\", i, self.active_thread);\n                 thread.state = ThreadState::Enabled;\n             }\n@@ -456,7 +470,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n     /// used in stateless model checkers such as Loom: run the active thread as\n     /// long as we can and switch only when we have to (the active thread was\n     /// blocked, terminated, or has explicitly asked to be preempted).\n-    fn schedule(&mut self) -> InterpResult<'tcx, SchedulingAction> {\n+    fn schedule(&mut self, data_race: &Option<Rc<data_race::GlobalState>>) -> InterpResult<'tcx, SchedulingAction> {\n         // Check whether the thread has **just** terminated (`check_terminated`\n         // checks whether the thread has popped all its stack and if yes, sets\n         // the thread state to terminated).\n@@ -501,6 +515,9 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n             if thread.state == ThreadState::Enabled {\n                 if !self.yield_active_thread || id != self.active_thread {\n                     self.active_thread = id;\n+                    if let Some(data_race) = data_race {\n+                        data_race.thread_set_active(self.active_thread);\n+                    }\n                     break;\n                 }\n             }\n@@ -554,7 +571,11 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn create_thread(&mut self) -> ThreadId {\n         let this = self.eval_context_mut();\n-        this.machine.threads.create_thread()\n+        let id = this.machine.threads.create_thread();\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.thread_created(id);\n+        }\n+        id\n     }\n \n     #[inline]\n@@ -566,12 +587,17 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn join_thread(&mut self, joined_thread_id: ThreadId) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        this.machine.threads.join_thread(joined_thread_id)\n+        let data_race = &this.memory.extra.data_race;\n+        this.machine.threads.join_thread(joined_thread_id, data_race)?;\n+        Ok(())\n     }\n \n     #[inline]\n     fn set_active_thread(&mut self, thread_id: ThreadId) -> ThreadId {\n         let this = self.eval_context_mut();\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.thread_set_active(thread_id);\n+        }\n         this.machine.threads.set_active_thread_id(thread_id)\n     }\n \n@@ -626,6 +652,13 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn set_active_thread_name(&mut self, new_thread_name: Vec<u8>) {\n         let this = self.eval_context_mut();\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            if let Ok(string) = String::from_utf8(new_thread_name.clone()) {\n+                data_race.thread_set_name(\n+                    this.machine.threads.active_thread, string\n+                );\n+            }\n+        }\n         this.machine.threads.set_thread_name(new_thread_name);\n     }\n \n@@ -695,7 +728,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn schedule(&mut self) -> InterpResult<'tcx, SchedulingAction> {\n         let this = self.eval_context_mut();\n-        this.machine.threads.schedule()\n+        let data_race = &this.memory.extra.data_race;\n+        this.machine.threads.schedule(data_race)\n     }\n \n     /// Handles thread termination of the active thread: wakes up threads joining on this one,\n@@ -705,7 +739,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn thread_terminated(&mut self) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        for alloc_id in this.machine.threads.thread_terminated() {\n+        let data_race = &this.memory.extra.data_race;\n+        for alloc_id in this.machine.threads.thread_terminated(data_race) {\n             let ptr = this.memory.global_base_pointer(alloc_id.into())?;\n             this.memory.deallocate(ptr, None, MiriMemoryKind::Tls.into())?;\n         }"}, {"sha": "6840d7e6cb9906ed3409111774dcad525da28367", "filename": "src/vector_clock.rs", "status": "added", "additions": 660, "deletions": 0, "changes": 660, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fvector_clock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/src%2Fvector_clock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fvector_clock.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,660 @@\n+use rustc_data_structures::fx::FxHashMap;\n+use rustc_index::vec::Idx;\n+use smallvec::SmallVec;\n+use std::{\n+    cmp::Ordering,\n+    convert::TryFrom,\n+    fmt::{self, Debug},\n+    mem,\n+    ops::Index,\n+};\n+\n+/// A vector clock index, this is associated with a thread id\n+/// but in some cases one vector index may be shared with\n+/// multiple thread ids if it safe to do so.\n+#[derive(Clone, Copy, Debug, PartialOrd, Ord, PartialEq, Eq, Hash)]\n+pub struct VectorIdx(u32);\n+\n+impl VectorIdx {\n+    #[inline(always)]\n+    pub fn to_u32(self) -> u32 {\n+        self.0\n+    }\n+\n+    pub const MAX_INDEX: VectorIdx = VectorIdx(u32::MAX);\n+}\n+\n+impl Idx for VectorIdx {\n+    #[inline]\n+    fn new(idx: usize) -> Self {\n+        VectorIdx(u32::try_from(idx).unwrap())\n+    }\n+\n+    #[inline]\n+    fn index(self) -> usize {\n+        usize::try_from(self.0).unwrap()\n+    }\n+}\n+\n+impl From<u32> for VectorIdx {\n+    #[inline]\n+    fn from(id: u32) -> Self {\n+        Self(id)\n+    }\n+}\n+\n+/// A sparse mapping of vector index values to vector clocks, this\n+/// is optimized for the common case with only one element stored\n+/// inside the map.\n+/// This is used to store the set of currently active release\n+/// sequences at a given memory location, since RMW operations\n+/// allow for multiple release sequences to be active at once\n+/// and to be collapsed back to one active release sequence\n+/// once a non RMW atomic store operation occurs.\n+/// An all zero vector is considered to be equal to no\n+/// element stored internally since it will never be\n+/// stored and has no meaning as a release sequence\n+/// vector clock.\n+#[derive(Clone)]\n+pub struct VSmallClockMap(VSmallClockMapInner);\n+\n+#[derive(Clone)]\n+enum VSmallClockMapInner {\n+    /// Zero or 1 vector elements, common\n+    /// case for the sparse set.\n+    /// The all zero vector clock is treated\n+    /// as equal to the empty element.\n+    Small(VectorIdx, VClock),\n+\n+    /// Hash-map of vector clocks.\n+    Large(FxHashMap<VectorIdx, VClock>),\n+}\n+\n+impl VSmallClockMap {\n+    /// Remove all clock vectors from the map, setting them\n+    /// to the zero vector.\n+    pub fn clear(&mut self) {\n+        match &mut self.0 {\n+            VSmallClockMapInner::Small(_, clock) => clock.set_zero_vector(),\n+            VSmallClockMapInner::Large(hash_map) => {\n+                hash_map.clear();\n+            }\n+        }\n+    }\n+\n+    /// Remove all clock vectors except for the clock vector\n+    /// stored at the given index, which is retained.\n+    pub fn retain_index(&mut self, index: VectorIdx) {\n+        match &mut self.0 {\n+            VSmallClockMapInner::Small(small_idx, clock) => {\n+                if index != *small_idx {\n+                    // The zero-vector is considered to equal\n+                    // the empty element.\n+                    clock.set_zero_vector()\n+                }\n+            }\n+            VSmallClockMapInner::Large(hash_map) => {\n+                let value = hash_map.remove(&index).unwrap_or_default();\n+                self.0 = VSmallClockMapInner::Small(index, value);\n+            }\n+        }\n+    }\n+\n+    /// Insert the vector clock into the associated vector\n+    /// index.\n+    pub fn insert(&mut self, index: VectorIdx, clock: &VClock) {\n+        match &mut self.0 {\n+            VSmallClockMapInner::Small(small_idx, small_clock) => {\n+                if small_clock.is_zero_vector() {\n+                    *small_idx = index;\n+                    small_clock.clone_from(clock);\n+                } else if !clock.is_zero_vector() {\n+                    // Convert to using the hash-map representation.\n+                    let mut hash_map = FxHashMap::default();\n+                    hash_map.insert(*small_idx, mem::take(small_clock));\n+                    hash_map.insert(index, clock.clone());\n+                    self.0 = VSmallClockMapInner::Large(hash_map);\n+                }\n+            }\n+            VSmallClockMapInner::Large(hash_map) =>\n+                if !clock.is_zero_vector() {\n+                    hash_map.insert(index, clock.clone());\n+                },\n+        }\n+    }\n+\n+    /// Try to load the vector clock associated with the current\n+    ///  vector index.\n+    pub fn get(&self, index: VectorIdx) -> Option<&VClock> {\n+        match &self.0 {\n+            VSmallClockMapInner::Small(small_idx, small_clock) => {\n+                if *small_idx == index && !small_clock.is_zero_vector() {\n+                    Some(small_clock)\n+                } else {\n+                    None\n+                }\n+            }\n+            VSmallClockMapInner::Large(hash_map) => hash_map.get(&index),\n+        }\n+    }\n+}\n+\n+impl Default for VSmallClockMap {\n+    #[inline]\n+    fn default() -> Self {\n+        VSmallClockMap(VSmallClockMapInner::Small(VectorIdx::new(0), VClock::default()))\n+    }\n+}\n+\n+impl Debug for VSmallClockMap {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        // Print the contents of the small vector clock set as the map\n+        // of vector index to vector clock that they represent.\n+        let mut map = f.debug_map();\n+        match &self.0 {\n+            VSmallClockMapInner::Small(small_idx, small_clock) =>\n+                if !small_clock.is_zero_vector() {\n+                    map.entry(&small_idx, &small_clock);\n+                },\n+            VSmallClockMapInner::Large(hash_map) =>\n+                for (idx, elem) in hash_map.iter() {\n+                    map.entry(idx, elem);\n+                },\n+        }\n+        map.finish()\n+    }\n+}\n+\n+impl PartialEq for VSmallClockMap {\n+    fn eq(&self, other: &Self) -> bool {\n+        use VSmallClockMapInner::*;\n+        match (&self.0, &other.0) {\n+            (Small(i1, c1), Small(i2, c2)) => {\n+                if c1.is_zero_vector() {\n+                    // Either they are both zero or they are non-equal\n+                    c2.is_zero_vector()\n+                } else {\n+                    // At least one is non-zero, so the full comparison is correct\n+                    i1 == i2 && c1 == c2\n+                }\n+            }\n+            (Small(idx, clock), Large(hash_map)) | (Large(hash_map), Small(idx, clock)) => {\n+                if hash_map.len() == 0 {\n+                    // Equal to the empty hash-map\n+                    clock.is_zero_vector()\n+                } else if hash_map.len() == 1 {\n+                    // Equal to the hash-map with one element\n+                    let (hash_idx, hash_clock) = hash_map.iter().next().unwrap();\n+                    hash_idx == idx && hash_clock == clock\n+                } else {\n+                    false\n+                }\n+            }\n+            (Large(map1), Large(map2)) => map1 == map2,\n+        }\n+    }\n+}\n+\n+impl Eq for VSmallClockMap {}\n+\n+/// The size of the vector-clock to store inline\n+/// clock vectors larger than this will be stored on the heap\n+const SMALL_VECTOR: usize = 4;\n+\n+/// The type of the time-stamps recorded in the data-race detector\n+/// set to a type of unsigned integer\n+pub type VTimestamp = u32;\n+\n+/// A vector clock for detecting data-races, this is conceptually\n+/// a map from a vector index (and thus a thread id) to a timestamp.\n+/// The compare operations require that the invariant that the last\n+/// element in the internal timestamp slice must not be a 0, hence\n+/// all zero vector clocks are always represented by the empty slice;\n+/// and allows for the implementation of compare operations to short\n+/// circuit the calculation and return the correct result faster,\n+/// also this means that there is only one unique valid length\n+/// for each set of vector clock values and hence the PartialEq\n+//  and Eq derivations are correct.\n+#[derive(PartialEq, Eq, Default, Debug)]\n+pub struct VClock(SmallVec<[VTimestamp; SMALL_VECTOR]>);\n+\n+impl VClock {\n+    /// Create a new vector-clock containing all zeros except\n+    /// for a value at the given index\n+    pub fn new_with_index(index: VectorIdx, timestamp: VTimestamp) -> VClock {\n+        let len = index.index() + 1;\n+        let mut vec = smallvec::smallvec![0; len];\n+        vec[index.index()] = timestamp;\n+        VClock(vec)\n+    }\n+\n+    /// Load the internal timestamp slice in the vector clock\n+    #[inline]\n+    pub fn as_slice(&self) -> &[VTimestamp] {\n+        self.0.as_slice()\n+    }\n+\n+    /// Get a mutable slice to the internal vector with minimum `min_len`\n+    /// elements, to preserve invariants this vector must modify\n+    /// the `min_len`-1 nth element to a non-zero value\n+    #[inline]\n+    fn get_mut_with_min_len(&mut self, min_len: usize) -> &mut [VTimestamp] {\n+        if self.0.len() < min_len {\n+            self.0.resize(min_len, 0);\n+        }\n+        assert!(self.0.len() >= min_len);\n+        self.0.as_mut_slice()\n+    }\n+\n+    /// Increment the vector clock at a known index\n+    /// this will panic if the vector index overflows\n+    #[inline]\n+    pub fn increment_index(&mut self, idx: VectorIdx) {\n+        let idx = idx.index();\n+        let mut_slice = self.get_mut_with_min_len(idx + 1);\n+        let idx_ref = &mut mut_slice[idx];\n+        *idx_ref = idx_ref.checked_add(1).expect(\"Vector clock overflow\")\n+    }\n+\n+    // Join the two vector-clocks together, this\n+    // sets each vector-element to the maximum value\n+    // of that element in either of the two source elements.\n+    pub fn join(&mut self, other: &Self) {\n+        let rhs_slice = other.as_slice();\n+        let lhs_slice = self.get_mut_with_min_len(rhs_slice.len());\n+        for (l, &r) in lhs_slice.iter_mut().zip(rhs_slice.iter()) {\n+            *l = r.max(*l);\n+        }\n+    }\n+\n+    /// Set the element at the current index of the vector\n+    pub fn set_at_index(&mut self, other: &Self, idx: VectorIdx) {\n+        let idx = idx.index();\n+        let mut_slice = self.get_mut_with_min_len(idx + 1);\n+        let slice = other.as_slice();\n+        mut_slice[idx] = slice[idx];\n+    }\n+\n+    /// Set the vector to the all-zero vector\n+    #[inline]\n+    pub fn set_zero_vector(&mut self) {\n+        self.0.clear();\n+    }\n+\n+    /// Return if this vector is the all-zero vector\n+    pub fn is_zero_vector(&self) -> bool {\n+        self.0.is_empty()\n+    }\n+}\n+\n+impl Clone for VClock {\n+    fn clone(&self) -> Self {\n+        VClock(self.0.clone())\n+    }\n+\n+    // Optimized clone-from, can be removed\n+    // and replaced with a derive once a similar\n+    // optimization is inserted into SmallVec's\n+    // clone implementation.\n+    fn clone_from(&mut self, source: &Self) {\n+        let source_slice = source.as_slice();\n+        self.0.clear();\n+        self.0.extend_from_slice(source_slice);\n+    }\n+}\n+\n+impl PartialOrd for VClock {\n+    fn partial_cmp(&self, other: &VClock) -> Option<Ordering> {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // Iterate through the combined vector slice continuously updating\n+        // the value of `order` to the current comparison of the vector from\n+        // index 0 to the currently checked index.\n+        // An Equal ordering can be converted into Less or Greater ordering\n+        // on finding an element that is less than or greater than the other\n+        // but if one Greater and one Less element-wise comparison is found\n+        // then no ordering is possible and so directly return an ordering\n+        // of None.\n+        let mut iter = lhs_slice.iter().zip(rhs_slice.iter());\n+        let mut order = match iter.next() {\n+            Some((lhs, rhs)) => lhs.cmp(rhs),\n+            None => Ordering::Equal,\n+        };\n+        for (l, r) in iter {\n+            match order {\n+                Ordering::Equal => order = l.cmp(r),\n+                Ordering::Less =>\n+                    if l > r {\n+                        return None;\n+                    },\n+                Ordering::Greater =>\n+                    if l < r {\n+                        return None;\n+                    },\n+            }\n+        }\n+\n+        // Now test if either left or right have trailing elements,\n+        // by the invariant the trailing elements have at least 1\n+        // non zero value, so no additional calculation is required\n+        // to determine the result of the PartialOrder.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        match l_len.cmp(&r_len) {\n+            // Equal means no additional elements: return current order\n+            Ordering::Equal => Some(order),\n+            // Right has at least 1 element > than the implicit 0,\n+            // so the only valid values are Ordering::Less or None.\n+            Ordering::Less => match order {\n+                Ordering::Less | Ordering::Equal => Some(Ordering::Less),\n+                Ordering::Greater => None,\n+            },\n+            // Left has at least 1 element > than the implicit 0,\n+            // so the only valid values are Ordering::Greater or None.\n+            Ordering::Greater => match order {\n+                Ordering::Greater | Ordering::Equal => Some(Ordering::Greater),\n+                Ordering::Less => None,\n+            },\n+        }\n+    }\n+\n+    fn lt(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If l_len > r_len then at least one element\n+        // in l_len is > than r_len, therefore the result\n+        // is either Some(Greater) or None, so return false\n+        // early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len <= r_len {\n+            // If any elements on the left are greater than the right\n+            // then the result is None or Some(Greater), both of which\n+            // return false, the earlier test asserts that no elements in the\n+            // extended tail violate this assumption. Otherwise l <= r, finally\n+            // the case where the values are potentially equal needs to be considered\n+            // and false returned as well\n+            let mut equal = l_len == r_len;\n+            for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n+                if l > r {\n+                    return false;\n+                } else if l < r {\n+                    equal = false;\n+                }\n+            }\n+            !equal\n+        } else {\n+            false\n+        }\n+    }\n+\n+    fn le(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If l_len > r_len then at least one element\n+        // in l_len is > than r_len, therefore the result\n+        // is either Some(Greater) or None, so return false\n+        // early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len <= r_len {\n+            // If any elements on the left are greater than the right\n+            // then the result is None or Some(Greater), both of which\n+            // return false, the earlier test asserts that no elements in the\n+            // extended tail violate this assumption. Otherwise l <= r\n+            !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l > r)\n+        } else {\n+            false\n+        }\n+    }\n+\n+    fn gt(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If r_len > l_len then at least one element\n+        // in r_len is > than l_len, therefore the result\n+        // is either Some(Less) or None, so return false\n+        // early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len >= r_len {\n+            // If any elements on the left are less than the right\n+            // then the result is None or Some(Less), both of which\n+            // return false, the earlier test asserts that no elements in the\n+            // extended tail violate this assumption. Otherwise l >=, finally\n+            // the case where the values are potentially equal needs to be considered\n+            // and false returned as well\n+            let mut equal = l_len == r_len;\n+            for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n+                if l < r {\n+                    return false;\n+                } else if l > r {\n+                    equal = false;\n+                }\n+            }\n+            !equal\n+        } else {\n+            false\n+        }\n+    }\n+\n+    fn ge(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If r_len > l_len then at least one element\n+        // in r_len is > than l_len, therefore the result\n+        // is either Some(Less) or None, so return false\n+        // early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len >= r_len {\n+            // If any elements on the left are less than the right\n+            // then the result is None or Some(Less), both of which\n+            // return false, the earlier test asserts that no elements in the\n+            // extended tail violate this assumption. Otherwise l >= r\n+            !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l < r)\n+        } else {\n+            false\n+        }\n+    }\n+}\n+\n+impl Index<VectorIdx> for VClock {\n+    type Output = VTimestamp;\n+\n+    #[inline]\n+    fn index(&self, index: VectorIdx) -> &VTimestamp {\n+        self.as_slice().get(index.to_u32() as usize).unwrap_or(&0)\n+    }\n+}\n+\n+/// Test vector clock ordering operations\n+///  data-race detection is tested in the external\n+///  test suite\n+#[cfg(test)]\n+mod tests {\n+\n+    use super::{VClock, VSmallClockMap, VTimestamp, VectorIdx};\n+    use std::cmp::Ordering;\n+\n+    #[test]\n+    fn test_equal() {\n+        let mut c1 = VClock::default();\n+        let mut c2 = VClock::default();\n+        assert_eq!(c1, c2);\n+        c1.increment_index(VectorIdx(5));\n+        assert_ne!(c1, c2);\n+        c2.increment_index(VectorIdx(53));\n+        assert_ne!(c1, c2);\n+        c1.increment_index(VectorIdx(53));\n+        assert_ne!(c1, c2);\n+        c2.increment_index(VectorIdx(5));\n+        assert_eq!(c1, c2);\n+    }\n+\n+    #[test]\n+    fn test_partial_order() {\n+        // Small test\n+        assert_order(&[1], &[1], Some(Ordering::Equal));\n+        assert_order(&[1], &[2], Some(Ordering::Less));\n+        assert_order(&[2], &[1], Some(Ordering::Greater));\n+        assert_order(&[1], &[1, 2], Some(Ordering::Less));\n+        assert_order(&[2], &[1, 2], None);\n+\n+        // Misc tests\n+        assert_order(&[400], &[0, 1], None);\n+\n+        // Large test\n+        assert_order(\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 0, 0],\n+            Some(Ordering::Equal),\n+        );\n+        assert_order(\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 0],\n+            Some(Ordering::Less),\n+        );\n+        assert_order(\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11],\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 0, 0],\n+            Some(Ordering::Greater),\n+        );\n+        assert_order(\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11],\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 0],\n+            None,\n+        );\n+        assert_order(\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9],\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 0, 0],\n+            Some(Ordering::Less),\n+        );\n+        assert_order(\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9],\n+            &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 0],\n+            Some(Ordering::Less),\n+        );\n+    }\n+\n+    fn from_slice(mut slice: &[VTimestamp]) -> VClock {\n+        while let Some(0) = slice.last() {\n+            slice = &slice[..slice.len() - 1]\n+        }\n+        VClock(smallvec::SmallVec::from_slice(slice))\n+    }\n+\n+    fn assert_order(l: &[VTimestamp], r: &[VTimestamp], o: Option<Ordering>) {\n+        let l = from_slice(l);\n+        let r = from_slice(r);\n+\n+        //Test partial_cmp\n+        let compare = l.partial_cmp(&r);\n+        assert_eq!(compare, o, \"Invalid comparison\\n l: {:?}\\n r: {:?}\", l, r);\n+        let alt_compare = r.partial_cmp(&l);\n+        assert_eq!(\n+            alt_compare,\n+            o.map(Ordering::reverse),\n+            \"Invalid alt comparison\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+\n+        //Test operators with faster implementations\n+        assert_eq!(\n+            matches!(compare, Some(Ordering::Less)),\n+            l < r,\n+            \"Invalid (<):\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+        assert_eq!(\n+            matches!(compare, Some(Ordering::Less) | Some(Ordering::Equal)),\n+            l <= r,\n+            \"Invalid (<=):\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+        assert_eq!(\n+            matches!(compare, Some(Ordering::Greater)),\n+            l > r,\n+            \"Invalid (>):\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+        assert_eq!(\n+            matches!(compare, Some(Ordering::Greater) | Some(Ordering::Equal)),\n+            l >= r,\n+            \"Invalid (>=):\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare, Some(Ordering::Less)),\n+            r < l,\n+            \"Invalid alt (<):\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare, Some(Ordering::Less) | Some(Ordering::Equal)),\n+            r <= l,\n+            \"Invalid alt (<=):\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare, Some(Ordering::Greater)),\n+            r > l,\n+            \"Invalid alt (>):\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare, Some(Ordering::Greater) | Some(Ordering::Equal)),\n+            r >= l,\n+            \"Invalid alt (>=):\\n l: {:?}\\n r: {:?}\",\n+            l,\n+            r\n+        );\n+    }\n+\n+    #[test]\n+    pub fn test_vclock_set() {\n+        let mut map = VSmallClockMap::default();\n+        let v1 = from_slice(&[3, 0, 1]);\n+        let v2 = from_slice(&[4, 2, 3]);\n+        let v3 = from_slice(&[4, 8, 3]);\n+        map.insert(VectorIdx(0), &v1);\n+        assert_eq!(map.get(VectorIdx(0)), Some(&v1));\n+        map.insert(VectorIdx(5), &v2);\n+        assert_eq!(map.get(VectorIdx(0)), Some(&v1));\n+        assert_eq!(map.get(VectorIdx(5)), Some(&v2));\n+        map.insert(VectorIdx(53), &v3);\n+        assert_eq!(map.get(VectorIdx(0)), Some(&v1));\n+        assert_eq!(map.get(VectorIdx(5)), Some(&v2));\n+        assert_eq!(map.get(VectorIdx(53)), Some(&v3));\n+        map.retain_index(VectorIdx(53));\n+        assert_eq!(map.get(VectorIdx(0)), None);\n+        assert_eq!(map.get(VectorIdx(5)), None);\n+        assert_eq!(map.get(VectorIdx(53)), Some(&v3));\n+        map.clear();\n+        assert_eq!(map.get(VectorIdx(0)), None);\n+        assert_eq!(map.get(VectorIdx(5)), None);\n+        assert_eq!(map.get(VectorIdx(53)), None);\n+        map.insert(VectorIdx(53), &v3);\n+        assert_eq!(map.get(VectorIdx(0)), None);\n+        assert_eq!(map.get(VectorIdx(5)), None);\n+        assert_eq!(map.get(VectorIdx(53)), Some(&v3));\n+    }\n+}"}, {"sha": "0b9610edc64b3e58d90278e68e27b93b91aca7dc", "filename": "tests/compile-fail/data_race/atomic_read_na_write_race1.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_na_write_race1.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_na_write_race1.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_na_write_race1.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+#![feature(core_intrinsics)]\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::intrinsics::atomic_load;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *(c.0 as *mut usize) = 32;\n+        });\n+\n+        let j2 = spawn(move || {\n+            //Equivalent to: (&*c.0).load(Ordering::SeqCst)\n+            atomic_load(c.0 as *mut usize) //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "779babefd8e602339dffa3e675179e65cf9da531", "filename": "tests/compile-fail/data_race/atomic_read_na_write_race2.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_na_write_race2.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_na_write_race2.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_na_write_race2.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::sync::atomic::Ordering;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            atomic_ref.load(Ordering::SeqCst)\n+        });\n+\n+        let j2 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            *atomic_ref.get_mut() = 32; //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "3211a5ae53442c168b6e2f37db64af930da9885b", "filename": "tests/compile-fail/data_race/atomic_write_na_read_race1.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_read_race1.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_read_race1.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_read_race1.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::sync::atomic::Ordering;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            atomic_ref.store(32, Ordering::SeqCst)\n+        });\n+\n+        let j2 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            *atomic_ref.get_mut() //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "131d4e07b829fb1831f0fbb4bb567ca8de7a527d", "filename": "tests/compile-fail/data_race/atomic_write_na_read_race2.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_read_race2.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_read_race2.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_read_race2.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+#![feature(core_intrinsics)]\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::intrinsics::atomic_store;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *(c.0 as *mut usize)\n+        });\n+\n+        let j2 = spawn(move || {\n+            //Equivalent to: (&*c.0).store(32, Ordering::SeqCst)\n+            atomic_store(c.0 as *mut usize, 32); //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "74adf7ae4b8d28cb461c4122681099541042e07e", "filename": "tests/compile-fail/data_race/atomic_write_na_write_race1.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_write_race1.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_write_race1.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_write_race1.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+#![feature(core_intrinsics)]\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::intrinsics::atomic_store;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *(c.0 as *mut usize) = 32;\n+        });\n+\n+        let j2 = spawn(move || {\n+            //Equivalent to: (&*c.0).store(64, Ordering::SeqCst)\n+            atomic_store(c.0 as *mut usize, 64); //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "75ad755fbd2c3c728048b3b5e4a0c3c673ef601a", "filename": "tests/compile-fail/data_race/atomic_write_na_write_race2.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_write_race2.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_write_race2.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_na_write_race2.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::sync::atomic::Ordering;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            atomic_ref.store(64, Ordering::SeqCst);\n+        });\n+\n+        let j2 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            *atomic_ref.get_mut() = 32; //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "d8b5d82f830482b33903304bb825596628a79066", "filename": "tests/compile-fail/data_race/dangling_thread_async_race.rs", "status": "added", "additions": 44, "deletions": 0, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fdangling_thread_async_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fdangling_thread_async_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fdangling_thread_async_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,44 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+// compile-flags: -Zmiri-disable-isolation\n+\n+use std::thread::{spawn, sleep};\n+use std::time::Duration;\n+use std::mem;\n+\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+\n+fn main() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+\n+    let join = unsafe {\n+        spawn(move || {\n+            *c.0 = 32;\n+        })\n+    };\n+\n+    // Detatch the thread and sleep until it terminates\n+    mem::drop(join);\n+    sleep(Duration::from_millis(100));\n+\n+    // Spawn and immediately join a thread\n+    // to execute the join code-path\n+    // and ensure that data-race detection\n+    // remains enabled nevertheless.\n+    spawn(|| ()).join().unwrap();\n+\n+    let join2 = unsafe {\n+        spawn(move || {\n+            *c.0 = 64; //~ ERROR Data race      \n+        })\n+    };\n+\n+    join2.join().unwrap();\n+}"}, {"sha": "172b05bd4f0bb6c0b01f9b17d92ea584888b3687", "filename": "tests/compile-fail/data_race/dangling_thread_race.rs", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fdangling_thread_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fdangling_thread_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fdangling_thread_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,41 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+// compile-flags: -Zmiri-disable-isolation\n+\n+use std::thread::{spawn, sleep};\n+use std::time::Duration;\n+use std::mem;\n+\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+\n+fn main() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+\n+    let join = unsafe {\n+        spawn(move || {\n+            *c.0 = 32;\n+        })\n+    };\n+\n+    // Detatch the thread and sleep until it terminates\n+    mem::drop(join);\n+    sleep(Duration::from_millis(100));\n+\n+    // Spawn and immediately join a thread\n+    // to execute the join code-path\n+    // and ensure that data-race detection\n+    // remains enabled nevertheless.\n+    spawn(|| ()).join().unwrap();\n+\n+\n+    unsafe {\n+        *c.0 = 64; //~ ERROR Data race\n+    }\n+}"}, {"sha": "c2943177713798f42b47fedfb73df726afb7db59", "filename": "tests/compile-fail/data_race/enable_after_join_to_main.rs", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fenable_after_join_to_main.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fenable_after_join_to_main.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fenable_after_join_to_main.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,38 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    // Enable and then join with multiple threads.\n+    let t1 = spawn(|| ());\n+    let t2 = spawn(|| ());\n+    let t3 = spawn(|| ());\n+    let t4 = spawn(|| ());\n+    t1.join().unwrap();\n+    t2.join().unwrap();\n+    t3.join().unwrap();\n+    t4.join().unwrap();\n+\n+    // Perform write-write data race detection.\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0 = 32;\n+        });\n+\n+        let j2 = spawn(move || {\n+            *c.0 = 64; //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "42fd7a51ffbdafc3dc2813149800608481f22ffc", "filename": "tests/compile-fail/data_race/read_write_race.rs", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fread_write_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fread_write_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fread_write_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,27 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0\n+        });\n+\n+        let j2 = spawn(move || {\n+            *c.0 = 64; //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "2ae0aacbcf776199ba0bc282a6c9f64294c1a9f5", "filename": "tests/compile-fail/data_race/relax_acquire_race.rs", "status": "added", "additions": 50, "deletions": 0, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Frelax_acquire_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Frelax_acquire_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Frelax_acquire_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,50 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+use std::sync::atomic::{AtomicUsize, Ordering};\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+static SYNC: AtomicUsize = AtomicUsize::new(0);\n+\n+pub fn main() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+\n+    // Note: this is scheduler-dependent\n+    // the operations need to occur in\n+    // order:\n+    //  1. store release : 1\n+    //  2. load acquire : 1\n+    //  3. store relaxed : 2\n+    //  4. load acquire : 2\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0 = 1;\n+            SYNC.store(1, Ordering::Release);\n+        });\n+\n+        let j2 = spawn(move || {\n+            if SYNC.load(Ordering::Acquire) == 1 {\n+                SYNC.store(2, Ordering::Relaxed);\n+            }\n+        });\n+\n+        let j3 = spawn(move || {\n+            if SYNC.load(Ordering::Acquire) == 2 {\n+                *c.0 //~ ERROR Data race\n+            } else {\n+                0\n+            }\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+        j3.join().unwrap();\n+    }\n+}"}, {"sha": "59263cb7120427f1f5b1a0d65757688fef4ec468", "filename": "tests/compile-fail/data_race/release_seq_race.rs", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Frelease_seq_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Frelease_seq_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Frelease_seq_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,55 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+// compile-flags: -Zmiri-disable-isolation\n+\n+use std::thread::{spawn, sleep};\n+use std::sync::atomic::{AtomicUsize, Ordering};\n+use std::time::Duration;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+static SYNC: AtomicUsize = AtomicUsize::new(0);\n+\n+pub fn main() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+\n+    // Note: this is scheduler-dependent\n+    // the operations need to occur in\n+    // order, the sleep operations currently\n+    // force the desired ordering:\n+    //  1. store release : 1\n+    //  2. store relaxed : 2\n+    //  3. store relaxed : 3\n+    //  4. load acquire : 3\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0 = 1;\n+            SYNC.store(1, Ordering::Release);\n+            sleep(Duration::from_millis(100));\n+            SYNC.store(3, Ordering::Relaxed);\n+        });\n+\n+        let j2 = spawn(move || {\n+            // Blocks the acquire-release sequence\n+            SYNC.store(2, Ordering::Relaxed);\n+        });\n+\n+        let j3 = spawn(move || {\n+            sleep(Duration::from_millis(1000));\n+            if SYNC.load(Ordering::Acquire) == 3 {\n+                *c.0 //~ ERROR Data race\n+            } else {\n+                0\n+            }\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+        j3.join().unwrap();\n+    }\n+}"}, {"sha": "e523f8b374cc244f31ef55f67b5467f7988da440", "filename": "tests/compile-fail/data_race/rmw_race.rs", "status": "added", "additions": 51, "deletions": 0, "changes": 51, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Frmw_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Frmw_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Frmw_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,51 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+use std::sync::atomic::{AtomicUsize, Ordering};\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+static SYNC: AtomicUsize = AtomicUsize::new(0);\n+\n+pub fn main() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+\n+    // Note: this is scheduler-dependent\n+    // the operations need to occur in\n+    // order:\n+    //  1. store release : 1\n+    //  2. RMW relaxed : 1 -> 2\n+    //  3. store relaxed : 3\n+    //  4. load acquire : 3\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0 = 1;\n+            SYNC.store(1, Ordering::Release);\n+        });\n+\n+        let j2 = spawn(move || {\n+            if SYNC.swap(2, Ordering::Relaxed) == 1 {\n+                // Blocks the acquire-release sequence\n+                SYNC.store(3, Ordering::Relaxed);\n+            }\n+        });\n+\n+        let j3 = spawn(move || {\n+            if SYNC.load(Ordering::Acquire) == 3 {\n+                *c.0 //~ ERROR Data race\n+            } else {\n+                0\n+            }\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+        j3.join().unwrap();\n+    }\n+}"}, {"sha": "aca19a46c13d7e3720db17a9e4eccc2b6ece66de", "filename": "tests/compile-fail/data_race/write_write_race.rs", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fwrite_write_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Fcompile-fail%2Fdata_race%2Fwrite_write_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fwrite_write_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,27 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0 = 32;\n+        });\n+\n+        let j2 = spawn(move || {\n+            *c.0 = 64; //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "64e90024ed49bd8c2d937efe2f7a271d0d1b8862", "filename": "tests/run-pass/concurrency/data_race.rs", "status": "added", "additions": 122, "deletions": 0, "changes": 122, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fdata_race.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,122 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+\n+use std::sync::atomic::{AtomicUsize, fence, Ordering};\n+use std::thread::spawn;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+static SYNC: AtomicUsize = AtomicUsize::new(0);\n+\n+fn test_fence_sync() {\n+    let mut var = 0u32;\n+    let ptr = &mut var as *mut u32;\n+    let evil_ptr = EvilSend(ptr);\n+    \n+    \n+    let j1 = spawn(move || {\n+        unsafe { *evil_ptr.0 = 1; }\n+        fence(Ordering::Release);\n+        SYNC.store(1, Ordering::Relaxed)   \n+    });\n+\n+    let j2 = spawn(move || {\n+        if SYNC.load(Ordering::Relaxed) == 1 {\n+            fence(Ordering::Acquire);\n+            unsafe { *evil_ptr.0 }\n+        } else {\n+            0\n+        }\n+    });\n+\n+    j1.join().unwrap();\n+    j2.join().unwrap();\n+}\n+\n+\n+fn test_multiple_reads() {\n+    let mut var = 42u32;\n+    let ptr = &mut var as *mut u32;\n+    let evil_ptr = EvilSend(ptr);\n+\n+    let j1 = spawn(move || unsafe {*evil_ptr.0});\n+    let j2 = spawn(move || unsafe {*evil_ptr.0});\n+    let j3 = spawn(move || unsafe {*evil_ptr.0});\n+    let j4 = spawn(move || unsafe {*evil_ptr.0});\n+\n+    assert_eq!(j1.join().unwrap(), 42);\n+    assert_eq!(j2.join().unwrap(), 42);\n+    assert_eq!(j3.join().unwrap(), 42);\n+    assert_eq!(j4.join().unwrap(), 42);\n+\n+    var = 10;\n+    assert_eq!(var, 10);\n+}\n+\n+pub fn test_rmw_no_block() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0 = 1;\n+            SYNC.store(1, Ordering::Release);\n+        });\n+\n+        let j2 = spawn(move || {\n+            if SYNC.swap(2, Ordering::Relaxed) == 1 {\n+                //No op, blocking store removed\n+            }\n+        });\n+\n+        let j3 = spawn(move || {\n+            if SYNC.load(Ordering::Acquire) == 2 {\n+                *c.0\n+            } else {\n+                0\n+            }\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+        let v = j3.join().unwrap();\n+        assert!(v == 1 || v == 2);\n+    }\n+}\n+\n+pub fn test_release_no_block() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0 = 1;\n+            SYNC.store(1, Ordering::Release);\n+            SYNC.store(3, Ordering::Relaxed);\n+        });\n+\n+        let j2 = spawn(move || {\n+            if SYNC.load(Ordering::Acquire) == 3 {\n+                *c.0\n+            } else {\n+                0\n+            }\n+        });\n+\n+        j1.join().unwrap();\n+        assert_eq!(j2.join().unwrap(),1);\n+    }\n+}\n+\n+pub fn main() {\n+    test_fence_sync();\n+    test_multiple_reads();\n+    test_rmw_no_block();\n+    test_release_no_block();\n+}"}, {"sha": "03676519d4f1c052581dee9227a650c2a4b8d782", "filename": "tests/run-pass/concurrency/data_race.stderr", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fdata_race.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fdata_race.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fdata_race.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,2 @@\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n+"}, {"sha": "8b2d180f11d4a91e5a9889fabf9097374fba353e", "filename": "tests/run-pass/concurrency/disable_data_race_detector.rs", "status": "added", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fdisable_data_race_detector.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fdisable_data_race_detector.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fdisable_data_race_detector.rs?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,28 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+// compile-flags: -Zmiri-disable-data-race-detector\n+\n+use std::thread::spawn;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = 0u32;\n+    let b = &mut a as *mut u32;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *c.0 = 32;\n+        });\n+\n+        let j2 = spawn(move || {\n+            *c.0 = 64; //~ ERROR Data race (but not detected as the detector is disabled)\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "03676519d4f1c052581dee9227a650c2a4b8d782", "filename": "tests/run-pass/concurrency/disable_data_race_detector.stderr", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fdisable_data_race_detector.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fdisable_data_race_detector.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fdisable_data_race_detector.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -0,0 +1,2 @@\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n+"}, {"sha": "03676519d4f1c052581dee9227a650c2a4b8d782", "filename": "tests/run-pass/concurrency/linux-futex.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Flinux-futex.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Flinux-futex.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Flinux-futex.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental. For example, Miri does not detect data races yet.\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n "}, {"sha": "f46b1442d749fc09452e99a44be8c07338b5a157", "filename": "tests/run-pass/concurrency/simple.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fsimple.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fsimple.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fsimple.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -1,4 +1,4 @@\n-warning: thread support is experimental. For example, Miri does not detect data races yet.\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n \n thread '<unnamed>' panicked at 'Hello!', $DIR/simple.rs:54:9\n note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace"}, {"sha": "03676519d4f1c052581dee9227a650c2a4b8d782", "filename": "tests/run-pass/concurrency/sync.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fsync.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fsync.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fsync.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental. For example, Miri does not detect data races yet.\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n "}, {"sha": "03676519d4f1c052581dee9227a650c2a4b8d782", "filename": "tests/run-pass/concurrency/thread_locals.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fthread_locals.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Fthread_locals.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fthread_locals.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental. For example, Miri does not detect data races yet.\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n "}, {"sha": "03676519d4f1c052581dee9227a650c2a4b8d782", "filename": "tests/run-pass/concurrency/tls_lib_drop.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Ftls_lib_drop.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fconcurrency%2Ftls_lib_drop.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Ftls_lib_drop.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental. For example, Miri does not detect data races yet.\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n "}, {"sha": "03676519d4f1c052581dee9227a650c2a4b8d782", "filename": "tests/run-pass/libc.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Flibc.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Flibc.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Flibc.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental. For example, Miri does not detect data races yet.\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n "}, {"sha": "1ee688c1d32cc42c6ef02286f80a8a4a7d7a1162", "filename": "tests/run-pass/panic/concurrent-panic.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fpanic%2Fconcurrent-panic.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/d473242ecdffe9f06469168a857e1f54e75c0090/tests%2Frun-pass%2Fpanic%2Fconcurrent-panic.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fpanic%2Fconcurrent-panic.stderr?ref=d473242ecdffe9f06469168a857e1f54e75c0090", "patch": "@@ -1,4 +1,4 @@\n-warning: thread support is experimental. For example, Miri does not detect data races yet.\n+warning: thread support is experimental and incomplete: weak memory effects are not emulated.\n \n Thread 1 starting, will block on mutex\n Thread 1 reported it has started"}]}