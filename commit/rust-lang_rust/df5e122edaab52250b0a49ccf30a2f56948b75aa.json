{"sha": "df5e122edaab52250b0a49ccf30a2f56948b75aa", "node_id": "MDY6Q29tbWl0NzI0NzEyOmRmNWUxMjJlZGFhYjUyMjUwYjBhNDljY2YzMGEyZjU2OTQ4Yjc1YWE=", "commit": {"author": {"name": "Oliver Schneider", "email": "git-spam-no-reply9815368754983@oli-obk.de", "date": "2017-09-29T10:49:37Z"}, "committer": {"name": "Oliver Schneider", "email": "git-spam-no-reply9815368754983@oli-obk.de", "date": "2017-09-29T10:49:37Z"}, "message": "Merge remote-tracking branch 'miri/upstream' into miri", "tree": {"sha": "7c66dad78b8cb10b69cd39a5ece2d0fac80f3a78", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/7c66dad78b8cb10b69cd39a5ece2d0fac80f3a78"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/df5e122edaab52250b0a49ccf30a2f56948b75aa", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCAAdFiEEYFTdM4NKd7XQft77pp+NIls619kFAlnOJUQACgkQpp+NIls6\n19l7gw/8C1euS4BygjsYD3L3TEPquDhYmnHdIg24A3aAKooqrnfHJ5PMtkg2dKt0\nUWDxvQvJNWkDiClSb/c37oDy96NfViVRdNr7NmhVq1MExWm0vEvdn91K9seXBtTd\n+FtuS/n0QZRNoXV03Tq60KUo51MLUCAyV/3cjrngjm444mOI7Nn54oPr6SfeE7Lo\n6Hb68UigZFtFV6JRywXGcWx5l77rbDp107+qoKG/ufzLqCGO4vhaBtVZsm1GDHGv\nvb9RYT05hz5zh4DnNGqxT9MNobSOMb1rOB3EwfL1AtRnDiCllvqlosArTeQv595K\nka4E10roRjBq1P5s3rKWwVX+JSs0sc/JqUejUiq8grV3v90nPdKKBhn4cZ3Bttyh\nGXMegf/1WpntmjgrZBDTqTda3oRexIeplhWKSHS6tQNMz2AILqZNMvwTDxxPKNLi\n3K9vqmqouPazAJ2h2sVSs8MDS6MPzSd2RZJ4+TUhTMe9l0p9YJfzf2NzN/3BfTBr\n0XUnK0fFubfuoZr2omNryd+brmCvOWVy7rY/BXUk1op+Lv3eUdjdn3h0LMth5Ea8\n4ysLRdC/q4kHQf4i4kcXAsmhP0B+MKK3NptEBn2AomjPix3TCwlHyFhlVqJvy7wO\npXBSP24iqPyJqW72d1JXUb0uhjPkmTT8ZloHYkpg3FztWubTunI=\n=FUCm\n-----END PGP SIGNATURE-----", "payload": "tree 7c66dad78b8cb10b69cd39a5ece2d0fac80f3a78\nparent 0253d98382fa351d53880605a346d0ea2e941a07\nparent f835974f2054939de9e3661fb38ff33b6047e61e\nauthor Oliver Schneider <git-spam-no-reply9815368754983@oli-obk.de> 1506682177 +0200\ncommitter Oliver Schneider <git-spam-no-reply9815368754983@oli-obk.de> 1506682177 +0200\n\nMerge remote-tracking branch 'miri/upstream' into miri\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/df5e122edaab52250b0a49ccf30a2f56948b75aa", "html_url": "https://github.com/rust-lang/rust/commit/df5e122edaab52250b0a49ccf30a2f56948b75aa", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/df5e122edaab52250b0a49ccf30a2f56948b75aa/comments", "author": {"login": "oli-obk", "id": 332036, "node_id": "MDQ6VXNlcjMzMjAzNg==", "avatar_url": "https://avatars.githubusercontent.com/u/332036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oli-obk", "html_url": "https://github.com/oli-obk", "followers_url": "https://api.github.com/users/oli-obk/followers", "following_url": "https://api.github.com/users/oli-obk/following{/other_user}", "gists_url": "https://api.github.com/users/oli-obk/gists{/gist_id}", "starred_url": "https://api.github.com/users/oli-obk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oli-obk/subscriptions", "organizations_url": "https://api.github.com/users/oli-obk/orgs", "repos_url": "https://api.github.com/users/oli-obk/repos", "events_url": "https://api.github.com/users/oli-obk/events{/privacy}", "received_events_url": "https://api.github.com/users/oli-obk/received_events", "type": "User", "site_admin": false}, "committer": {"login": "oli-obk", "id": 332036, "node_id": "MDQ6VXNlcjMzMjAzNg==", "avatar_url": "https://avatars.githubusercontent.com/u/332036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oli-obk", "html_url": "https://github.com/oli-obk", "followers_url": "https://api.github.com/users/oli-obk/followers", "following_url": "https://api.github.com/users/oli-obk/following{/other_user}", "gists_url": "https://api.github.com/users/oli-obk/gists{/gist_id}", "starred_url": "https://api.github.com/users/oli-obk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oli-obk/subscriptions", "organizations_url": "https://api.github.com/users/oli-obk/orgs", "repos_url": "https://api.github.com/users/oli-obk/repos", "events_url": "https://api.github.com/users/oli-obk/events{/privacy}", "received_events_url": "https://api.github.com/users/oli-obk/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "0253d98382fa351d53880605a346d0ea2e941a07", "url": "https://api.github.com/repos/rust-lang/rust/commits/0253d98382fa351d53880605a346d0ea2e941a07", "html_url": "https://github.com/rust-lang/rust/commit/0253d98382fa351d53880605a346d0ea2e941a07"}, {"sha": "f835974f2054939de9e3661fb38ff33b6047e61e", "url": "https://api.github.com/repos/rust-lang/rust/commits/f835974f2054939de9e3661fb38ff33b6047e61e", "html_url": "https://github.com/rust-lang/rust/commit/f835974f2054939de9e3661fb38ff33b6047e61e"}], "stats": {"total": 8266, "additions": 8266, "deletions": 0}, "files": [{"sha": "3c1f41bdcca6c6fd7ef721d7821433059962ee4b", "filename": ".editorconfig", "status": "added", "additions": 25, "deletions": 0, "changes": 25, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/.editorconfig", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/.editorconfig", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/.editorconfig?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,25 @@\n+# EditorConfig helps developers define and maintain consistent\n+# coding styles between different editors and IDEs\n+# editorconfig.org\n+\n+root = true\n+\n+\n+[*]\n+end_of_line = lf\n+charset = utf-8\n+trim_trailing_whitespace = true\n+insert_final_newline = true\n+indent_style = space\n+indent_size = 4\n+\n+[*.rs]\n+indent_style = space\n+indent_size = 4\n+\n+[*.toml]\n+indent_style = space\n+indent_size = 4\n+\n+[*.md]\n+trim_trailing_whitespace = false"}, {"sha": "5ae7c9da31c09b939878bc8815f5a924df46e9f6", "filename": "src/librustc/mir/interpret/cast.rs", "status": "added", "additions": 122, "deletions": 0, "changes": 122, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fcast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fcast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fcast.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,122 @@\n+use rustc::ty::{self, Ty};\n+use syntax::ast::{FloatTy, IntTy, UintTy};\n+\n+use super::{PrimVal, EvalContext, EvalResult, MemoryPointer, PointerArithmetic, Machine};\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    pub(super) fn cast_primval(\n+        &self,\n+        val: PrimVal,\n+        src_ty: Ty<'tcx>,\n+        dest_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx, PrimVal> {\n+        trace!(\"Casting {:?}: {:?} to {:?}\", val, src_ty, dest_ty);\n+        let src_kind = self.ty_to_primval_kind(src_ty)?;\n+\n+        match val {\n+            PrimVal::Undef => Ok(PrimVal::Undef),\n+            PrimVal::Ptr(ptr) => self.cast_from_ptr(ptr, dest_ty),\n+            val @ PrimVal::Bytes(_) => {\n+                use super::PrimValKind::*;\n+                match src_kind {\n+                    F32 => self.cast_from_float(val.to_f32()? as f64, dest_ty),\n+                    F64 => self.cast_from_float(val.to_f64()?, dest_ty),\n+\n+                    I8 | I16 | I32 | I64 | I128 => {\n+                        self.cast_from_signed_int(val.to_i128()?, dest_ty)\n+                    }\n+\n+                    Bool | Char | U8 | U16 | U32 | U64 | U128 | FnPtr | Ptr => {\n+                        self.cast_from_int(val.to_u128()?, dest_ty, false)\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    fn cast_from_signed_int(&self, val: i128, ty: ty::Ty<'tcx>) -> EvalResult<'tcx, PrimVal> {\n+        self.cast_from_int(val as u128, ty, val < 0)\n+    }\n+\n+    fn int_to_int(&self, v: i128, ty: IntTy) -> u128 {\n+        match ty {\n+            IntTy::I8 => v as i8 as u128,\n+            IntTy::I16 => v as i16 as u128,\n+            IntTy::I32 => v as i32 as u128,\n+            IntTy::I64 => v as i64 as u128,\n+            IntTy::I128 => v as u128,\n+            IntTy::Is => {\n+                let ty = self.tcx.sess.target.isize_ty;\n+                self.int_to_int(v, ty)\n+            }\n+        }\n+    }\n+    fn int_to_uint(&self, v: u128, ty: UintTy) -> u128 {\n+        match ty {\n+            UintTy::U8 => v as u8 as u128,\n+            UintTy::U16 => v as u16 as u128,\n+            UintTy::U32 => v as u32 as u128,\n+            UintTy::U64 => v as u64 as u128,\n+            UintTy::U128 => v,\n+            UintTy::Us => {\n+                let ty = self.tcx.sess.target.usize_ty;\n+                self.int_to_uint(v, ty)\n+            }\n+        }\n+    }\n+\n+    fn cast_from_int(\n+        &self,\n+        v: u128,\n+        ty: ty::Ty<'tcx>,\n+        negative: bool,\n+    ) -> EvalResult<'tcx, PrimVal> {\n+        trace!(\"cast_from_int: {}, {}, {}\", v, ty, negative);\n+        use rustc::ty::TypeVariants::*;\n+        match ty.sty {\n+            // Casts to bool are not permitted by rustc, no need to handle them here.\n+            TyInt(ty) => Ok(PrimVal::Bytes(self.int_to_int(v as i128, ty))),\n+            TyUint(ty) => Ok(PrimVal::Bytes(self.int_to_uint(v, ty))),\n+\n+            TyFloat(FloatTy::F64) if negative => Ok(PrimVal::from_f64(v as i128 as f64)),\n+            TyFloat(FloatTy::F64) => Ok(PrimVal::from_f64(v as f64)),\n+            TyFloat(FloatTy::F32) if negative => Ok(PrimVal::from_f32(v as i128 as f32)),\n+            TyFloat(FloatTy::F32) => Ok(PrimVal::from_f32(v as f32)),\n+\n+            TyChar if v as u8 as u128 == v => Ok(PrimVal::Bytes(v)),\n+            TyChar => err!(InvalidChar(v)),\n+\n+            // No alignment check needed for raw pointers.  But we have to truncate to target ptr size.\n+            TyRawPtr(_) => Ok(PrimVal::Bytes(self.memory.truncate_to_ptr(v).0 as u128)),\n+\n+            _ => err!(Unimplemented(format!(\"int to {:?} cast\", ty))),\n+        }\n+    }\n+\n+    fn cast_from_float(&self, val: f64, ty: Ty<'tcx>) -> EvalResult<'tcx, PrimVal> {\n+        use rustc::ty::TypeVariants::*;\n+        match ty.sty {\n+            // Casting negative floats to unsigned integers yields zero.\n+            TyUint(_) if val < 0.0 => self.cast_from_int(0, ty, false),\n+            TyInt(_) if val < 0.0 => self.cast_from_int(val as i128 as u128, ty, true),\n+\n+            TyInt(_) | ty::TyUint(_) => self.cast_from_int(val as u128, ty, false),\n+\n+            TyFloat(FloatTy::F64) => Ok(PrimVal::from_f64(val)),\n+            TyFloat(FloatTy::F32) => Ok(PrimVal::from_f32(val as f32)),\n+            _ => err!(Unimplemented(format!(\"float to {:?} cast\", ty))),\n+        }\n+    }\n+\n+    fn cast_from_ptr(&self, ptr: MemoryPointer, ty: Ty<'tcx>) -> EvalResult<'tcx, PrimVal> {\n+        use rustc::ty::TypeVariants::*;\n+        match ty.sty {\n+            // Casting to a reference or fn pointer is not permitted by rustc, no need to support it here.\n+            TyRawPtr(_) |\n+            TyInt(IntTy::Is) |\n+            TyUint(UintTy::Us) => Ok(PrimVal::Ptr(ptr)),\n+            TyInt(_) | TyUint(_) => err!(ReadPointerAsBytes),\n+            _ => err!(Unimplemented(format!(\"ptr to {:?} cast\", ty))),\n+        }\n+    }\n+}"}, {"sha": "075880fc5bfd14662499cf876dfd9018c28ede1e", "filename": "src/librustc/mir/interpret/const_eval.rs", "status": "added", "additions": 259, "deletions": 0, "changes": 259, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fconst_eval.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fconst_eval.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fconst_eval.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,259 @@\n+use rustc::traits::Reveal;\n+use rustc::ty::{self, TyCtxt, Ty, Instance, layout};\n+use rustc::mir;\n+\n+use syntax::ast::Mutability;\n+use syntax::codemap::Span;\n+\n+use super::{EvalResult, EvalError, EvalErrorKind, GlobalId, Lvalue, Value, PrimVal, EvalContext,\n+            StackPopCleanup, PtrAndAlign, MemoryKind, ValTy};\n+\n+use rustc_const_math::ConstInt;\n+\n+use std::fmt;\n+use std::error::Error;\n+\n+pub fn eval_body_as_primval<'a, 'tcx>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    instance: Instance<'tcx>,\n+) -> EvalResult<'tcx, (PrimVal, Ty<'tcx>)> {\n+    let limits = super::ResourceLimits::default();\n+    let mut ecx = EvalContext::<CompileTimeFunctionEvaluator>::new(tcx, limits, (), ());\n+    let cid = GlobalId {\n+        instance,\n+        promoted: None,\n+    };\n+    if ecx.tcx.has_attr(instance.def_id(), \"linkage\") {\n+        return Err(ConstEvalError::NotConst(\"extern global\".to_string()).into());\n+    }\n+\n+    let mir = ecx.load_mir(instance.def)?;\n+    if !ecx.globals.contains_key(&cid) {\n+        let size = ecx.type_size_with_substs(mir.return_ty, instance.substs)?\n+            .expect(\"unsized global\");\n+        let align = ecx.type_align_with_substs(mir.return_ty, instance.substs)?;\n+        let ptr = ecx.memory.allocate(\n+            size,\n+            align,\n+            MemoryKind::UninitializedStatic,\n+        )?;\n+        let aligned = !ecx.is_packed(mir.return_ty)?;\n+        ecx.globals.insert(\n+            cid,\n+            PtrAndAlign {\n+                ptr: ptr.into(),\n+                aligned,\n+            },\n+        );\n+        let mutable = !mir.return_ty.is_freeze(\n+            ecx.tcx,\n+            ty::ParamEnv::empty(Reveal::All),\n+            mir.span,\n+        );\n+        let mutability = if mutable {\n+            Mutability::Mutable\n+        } else {\n+            Mutability::Immutable\n+        };\n+        let cleanup = StackPopCleanup::MarkStatic(mutability);\n+        let name = ty::tls::with(|tcx| tcx.item_path_str(instance.def_id()));\n+        trace!(\"const_eval: pushing stack frame for global: {}\", name);\n+        ecx.push_stack_frame(\n+            instance,\n+            mir.span,\n+            mir,\n+            Lvalue::from_ptr(ptr),\n+            cleanup,\n+        )?;\n+\n+        while ecx.step()? {}\n+    }\n+    let value = Value::ByRef(*ecx.globals.get(&cid).expect(\"global not cached\"));\n+    let valty = ValTy {\n+        value,\n+        ty: mir.return_ty,\n+    };\n+    Ok((ecx.value_to_primval(valty)?, mir.return_ty))\n+}\n+\n+pub fn eval_body_as_integer<'a, 'tcx>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    instance: Instance<'tcx>,\n+) -> EvalResult<'tcx, ConstInt> {\n+    let (prim, ty) = eval_body_as_primval(tcx, instance)?;\n+    let prim = prim.to_bytes()?;\n+    use syntax::ast::{IntTy, UintTy};\n+    use rustc::ty::TypeVariants::*;\n+    use rustc_const_math::{ConstIsize, ConstUsize};\n+    Ok(match ty.sty {\n+        TyInt(IntTy::I8) => ConstInt::I8(prim as i128 as i8),\n+        TyInt(IntTy::I16) => ConstInt::I16(prim as i128 as i16),\n+        TyInt(IntTy::I32) => ConstInt::I32(prim as i128 as i32),\n+        TyInt(IntTy::I64) => ConstInt::I64(prim as i128 as i64),\n+        TyInt(IntTy::I128) => ConstInt::I128(prim as i128),\n+        TyInt(IntTy::Is) => ConstInt::Isize(\n+            ConstIsize::new(prim as i128 as i64, tcx.sess.target.isize_ty)\n+                .expect(\"miri should already have errored\"),\n+        ),\n+        TyUint(UintTy::U8) => ConstInt::U8(prim as u8),\n+        TyUint(UintTy::U16) => ConstInt::U16(prim as u16),\n+        TyUint(UintTy::U32) => ConstInt::U32(prim as u32),\n+        TyUint(UintTy::U64) => ConstInt::U64(prim as u64),\n+        TyUint(UintTy::U128) => ConstInt::U128(prim),\n+        TyUint(UintTy::Us) => ConstInt::Usize(\n+            ConstUsize::new(prim as u64, tcx.sess.target.usize_ty)\n+                .expect(\"miri should already have errored\"),\n+        ),\n+        _ => {\n+            return Err(\n+                ConstEvalError::NeedsRfc(\n+                    \"evaluating anything other than isize/usize during typeck\".to_string(),\n+                ).into(),\n+            )\n+        }\n+    })\n+}\n+\n+struct CompileTimeFunctionEvaluator;\n+\n+impl<'tcx> Into<EvalError<'tcx>> for ConstEvalError {\n+    fn into(self) -> EvalError<'tcx> {\n+        EvalErrorKind::MachineError(Box::new(self)).into()\n+    }\n+}\n+\n+#[derive(Clone, Debug)]\n+enum ConstEvalError {\n+    NeedsRfc(String),\n+    NotConst(String),\n+}\n+\n+impl fmt::Display for ConstEvalError {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        use self::ConstEvalError::*;\n+        match *self {\n+            NeedsRfc(ref msg) => {\n+                write!(\n+                    f,\n+                    \"\\\"{}\\\" needs an rfc before being allowed inside constants\",\n+                    msg\n+                )\n+            }\n+            NotConst(ref msg) => write!(f, \"Cannot evaluate within constants: \\\"{}\\\"\", msg),\n+        }\n+    }\n+}\n+\n+impl Error for ConstEvalError {\n+    fn description(&self) -> &str {\n+        use self::ConstEvalError::*;\n+        match *self {\n+            NeedsRfc(_) => \"this feature needs an rfc before being allowed inside constants\",\n+            NotConst(_) => \"this feature is not compatible with constant evaluation\",\n+        }\n+    }\n+\n+    fn cause(&self) -> Option<&Error> {\n+        None\n+    }\n+}\n+\n+impl<'tcx> super::Machine<'tcx> for CompileTimeFunctionEvaluator {\n+    type Data = ();\n+    type MemoryData = ();\n+    type MemoryKinds = !;\n+    fn eval_fn_call<'a>(\n+        ecx: &mut EvalContext<'a, 'tcx, Self>,\n+        instance: ty::Instance<'tcx>,\n+        destination: Option<(Lvalue, mir::BasicBlock)>,\n+        _args: &[ValTy<'tcx>],\n+        span: Span,\n+        _sig: ty::FnSig<'tcx>,\n+    ) -> EvalResult<'tcx, bool> {\n+        if !ecx.tcx.is_const_fn(instance.def_id()) {\n+            return Err(\n+                ConstEvalError::NotConst(format!(\"calling non-const fn `{}`\", instance)).into(),\n+            );\n+        }\n+        let mir = match ecx.load_mir(instance.def) {\n+            Ok(mir) => mir,\n+            Err(EvalError { kind: EvalErrorKind::NoMirFor(path), .. }) => {\n+                // some simple things like `malloc` might get accepted in the future\n+                return Err(\n+                    ConstEvalError::NeedsRfc(format!(\"calling extern function `{}`\", path))\n+                        .into(),\n+                );\n+            }\n+            Err(other) => return Err(other),\n+        };\n+        let (return_lvalue, return_to_block) = match destination {\n+            Some((lvalue, block)) => (lvalue, StackPopCleanup::Goto(block)),\n+            None => (Lvalue::undef(), StackPopCleanup::None),\n+        };\n+\n+        ecx.push_stack_frame(\n+            instance,\n+            span,\n+            mir,\n+            return_lvalue,\n+            return_to_block,\n+        )?;\n+\n+        Ok(false)\n+    }\n+\n+    fn call_intrinsic<'a>(\n+        _ecx: &mut EvalContext<'a, 'tcx, Self>,\n+        _instance: ty::Instance<'tcx>,\n+        _args: &[ValTy<'tcx>],\n+        _dest: Lvalue,\n+        _dest_ty: Ty<'tcx>,\n+        _dest_layout: &'tcx layout::Layout,\n+        _target: mir::BasicBlock,\n+    ) -> EvalResult<'tcx> {\n+        Err(\n+            ConstEvalError::NeedsRfc(\"calling intrinsics\".to_string()).into(),\n+        )\n+    }\n+\n+    fn try_ptr_op<'a>(\n+        _ecx: &EvalContext<'a, 'tcx, Self>,\n+        _bin_op: mir::BinOp,\n+        left: PrimVal,\n+        _left_ty: Ty<'tcx>,\n+        right: PrimVal,\n+        _right_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx, Option<(PrimVal, bool)>> {\n+        if left.is_bytes() && right.is_bytes() {\n+            Ok(None)\n+        } else {\n+            Err(\n+                ConstEvalError::NeedsRfc(\"Pointer arithmetic or comparison\".to_string()).into(),\n+            )\n+        }\n+    }\n+\n+    fn mark_static_initialized(m: !) -> EvalResult<'tcx> {\n+        m\n+    }\n+\n+    fn box_alloc<'a>(\n+        _ecx: &mut EvalContext<'a, 'tcx, Self>,\n+        _ty: ty::Ty<'tcx>,\n+        _dest: Lvalue,\n+    ) -> EvalResult<'tcx> {\n+        Err(\n+            ConstEvalError::NeedsRfc(\"Heap allocations via `box` keyword\".to_string()).into(),\n+        )\n+    }\n+\n+    fn global_item_with_linkage<'a>(\n+        _ecx: &mut EvalContext<'a, 'tcx, Self>,\n+        _instance: ty::Instance<'tcx>,\n+        _mutability: Mutability,\n+    ) -> EvalResult<'tcx> {\n+        Err(\n+            ConstEvalError::NotConst(\"statics with `linkage` attribute\".to_string()).into(),\n+        )\n+    }\n+}"}, {"sha": "96911c10cca80c687bfd0eed3ed25db3ff30ffbb", "filename": "src/librustc/mir/interpret/error.rs", "status": "added", "additions": 313, "deletions": 0, "changes": 313, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Ferror.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Ferror.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Ferror.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,313 @@\n+use std::error::Error;\n+use std::{fmt, env};\n+\n+use rustc::mir;\n+use rustc::ty::{FnSig, Ty, layout};\n+\n+use super::{\n+    MemoryPointer, Lock, AccessKind\n+};\n+\n+use rustc_const_math::ConstMathErr;\n+use syntax::codemap::Span;\n+use backtrace::Backtrace;\n+\n+#[derive(Debug)]\n+pub struct EvalError<'tcx> {\n+    pub kind: EvalErrorKind<'tcx>,\n+    pub backtrace: Option<Backtrace>,\n+}\n+\n+impl<'tcx> From<EvalErrorKind<'tcx>> for EvalError<'tcx> {\n+    fn from(kind: EvalErrorKind<'tcx>) -> Self {\n+        let backtrace = match env::var(\"RUST_BACKTRACE\") {\n+            Ok(ref val) if !val.is_empty() => Some(Backtrace::new_unresolved()),\n+            _ => None\n+        };\n+        EvalError {\n+            kind,\n+            backtrace,\n+        }\n+    }\n+}\n+\n+#[derive(Debug)]\n+pub enum EvalErrorKind<'tcx> {\n+    /// This variant is used by machines to signal their own errors that do not\n+    /// match an existing variant\n+    MachineError(Box<Error>),\n+    FunctionPointerTyMismatch(FnSig<'tcx>, FnSig<'tcx>),\n+    NoMirFor(String),\n+    UnterminatedCString(MemoryPointer),\n+    DanglingPointerDeref,\n+    DoubleFree,\n+    InvalidMemoryAccess,\n+    InvalidFunctionPointer,\n+    InvalidBool,\n+    InvalidDiscriminant,\n+    PointerOutOfBounds {\n+        ptr: MemoryPointer,\n+        access: bool,\n+        allocation_size: u64,\n+    },\n+    InvalidNullPointerUsage,\n+    ReadPointerAsBytes,\n+    ReadBytesAsPointer,\n+    InvalidPointerMath,\n+    ReadUndefBytes,\n+    DeadLocal,\n+    InvalidBoolOp(mir::BinOp),\n+    Unimplemented(String),\n+    DerefFunctionPointer,\n+    ExecuteMemory,\n+    ArrayIndexOutOfBounds(Span, u64, u64),\n+    Math(Span, ConstMathErr),\n+    Intrinsic(String),\n+    OverflowingMath,\n+    InvalidChar(u128),\n+    OutOfMemory {\n+        allocation_size: u64,\n+        memory_size: u64,\n+        memory_usage: u64,\n+    },\n+    ExecutionTimeLimitReached,\n+    StackFrameLimitReached,\n+    OutOfTls,\n+    TlsOutOfBounds,\n+    AbiViolation(String),\n+    AlignmentCheckFailed {\n+        required: u64,\n+        has: u64,\n+    },\n+    MemoryLockViolation {\n+        ptr: MemoryPointer,\n+        len: u64,\n+        frame: usize,\n+        access: AccessKind,\n+        lock: Lock,\n+    },\n+    MemoryAcquireConflict {\n+        ptr: MemoryPointer,\n+        len: u64,\n+        kind: AccessKind,\n+        lock: Lock,\n+    },\n+    InvalidMemoryLockRelease {\n+        ptr: MemoryPointer,\n+        len: u64,\n+        frame: usize,\n+        lock: Lock,\n+    },\n+    DeallocatedLockedMemory {\n+        ptr: MemoryPointer,\n+        lock: Lock,\n+    },\n+    ValidationFailure(String),\n+    CalledClosureAsFunction,\n+    VtableForArgumentlessMethod,\n+    ModifiedConstantMemory,\n+    AssumptionNotHeld,\n+    InlineAsm,\n+    TypeNotPrimitive(Ty<'tcx>),\n+    ReallocatedWrongMemoryKind(String, String),\n+    DeallocatedWrongMemoryKind(String, String),\n+    ReallocateNonBasePtr,\n+    DeallocateNonBasePtr,\n+    IncorrectAllocationInformation,\n+    Layout(layout::LayoutError<'tcx>),\n+    HeapAllocZeroBytes,\n+    HeapAllocNonPowerOfTwoAlignment(u64),\n+    Unreachable,\n+    Panic,\n+    ReadFromReturnPointer,\n+    PathNotFound(Vec<String>),\n+}\n+\n+pub type EvalResult<'tcx, T = ()> = Result<T, EvalError<'tcx>>;\n+\n+impl<'tcx> Error for EvalError<'tcx> {\n+    fn description(&self) -> &str {\n+        use self::EvalErrorKind::*;\n+        match self.kind {\n+            MachineError(ref inner) => inner.description(),\n+            FunctionPointerTyMismatch(..) =>\n+                \"tried to call a function through a function pointer of a different type\",\n+            InvalidMemoryAccess =>\n+                \"tried to access memory through an invalid pointer\",\n+            DanglingPointerDeref =>\n+                \"dangling pointer was dereferenced\",\n+            DoubleFree =>\n+                \"tried to deallocate dangling pointer\",\n+            InvalidFunctionPointer =>\n+                \"tried to use a function pointer after offsetting it\",\n+            InvalidBool =>\n+                \"invalid boolean value read\",\n+            InvalidDiscriminant =>\n+                \"invalid enum discriminant value read\",\n+            PointerOutOfBounds { .. } =>\n+                \"pointer offset outside bounds of allocation\",\n+            InvalidNullPointerUsage =>\n+                \"invalid use of NULL pointer\",\n+            MemoryLockViolation { .. } =>\n+                \"memory access conflicts with lock\",\n+            MemoryAcquireConflict { .. } =>\n+                \"new memory lock conflicts with existing lock\",\n+            ValidationFailure(..) =>\n+                \"type validation failed\",\n+            InvalidMemoryLockRelease { .. } =>\n+                \"invalid attempt to release write lock\",\n+            DeallocatedLockedMemory { .. } =>\n+                \"tried to deallocate memory in conflict with a lock\",\n+            ReadPointerAsBytes =>\n+                \"a raw memory access tried to access part of a pointer value as raw bytes\",\n+            ReadBytesAsPointer =>\n+                \"a memory access tried to interpret some bytes as a pointer\",\n+            InvalidPointerMath =>\n+                \"attempted to do invalid arithmetic on pointers that would leak base addresses, e.g. comparing pointers into different allocations\",\n+            ReadUndefBytes =>\n+                \"attempted to read undefined bytes\",\n+            DeadLocal =>\n+                \"tried to access a dead local variable\",\n+            InvalidBoolOp(_) =>\n+                \"invalid boolean operation\",\n+            Unimplemented(ref msg) => msg,\n+            DerefFunctionPointer =>\n+                \"tried to dereference a function pointer\",\n+            ExecuteMemory =>\n+                \"tried to treat a memory pointer as a function pointer\",\n+            ArrayIndexOutOfBounds(..) =>\n+                \"array index out of bounds\",\n+            Math(..) =>\n+                \"mathematical operation failed\",\n+            Intrinsic(..) =>\n+                \"intrinsic failed\",\n+            OverflowingMath =>\n+                \"attempted to do overflowing math\",\n+            NoMirFor(..) =>\n+                \"mir not found\",\n+            InvalidChar(..) =>\n+                \"tried to interpret an invalid 32-bit value as a char\",\n+            OutOfMemory{..} =>\n+                \"could not allocate more memory\",\n+            ExecutionTimeLimitReached =>\n+                \"reached the configured maximum execution time\",\n+            StackFrameLimitReached =>\n+                \"reached the configured maximum number of stack frames\",\n+            OutOfTls =>\n+                \"reached the maximum number of representable TLS keys\",\n+            TlsOutOfBounds =>\n+                \"accessed an invalid (unallocated) TLS key\",\n+            AbiViolation(ref msg) => msg,\n+            AlignmentCheckFailed{..} =>\n+                \"tried to execute a misaligned read or write\",\n+            CalledClosureAsFunction =>\n+                \"tried to call a closure through a function pointer\",\n+            VtableForArgumentlessMethod =>\n+                \"tried to call a vtable function without arguments\",\n+            ModifiedConstantMemory =>\n+                \"tried to modify constant memory\",\n+            AssumptionNotHeld =>\n+                \"`assume` argument was false\",\n+            InlineAsm =>\n+                \"miri does not support inline assembly\",\n+            TypeNotPrimitive(_) =>\n+                \"expected primitive type, got nonprimitive\",\n+            ReallocatedWrongMemoryKind(_, _) =>\n+                \"tried to reallocate memory from one kind to another\",\n+            DeallocatedWrongMemoryKind(_, _) =>\n+                \"tried to deallocate memory of the wrong kind\",\n+            ReallocateNonBasePtr =>\n+                \"tried to reallocate with a pointer not to the beginning of an existing object\",\n+            DeallocateNonBasePtr =>\n+                \"tried to deallocate with a pointer not to the beginning of an existing object\",\n+            IncorrectAllocationInformation =>\n+                \"tried to deallocate or reallocate using incorrect alignment or size\",\n+            Layout(_) =>\n+                \"rustc layout computation failed\",\n+            UnterminatedCString(_) =>\n+                \"attempted to get length of a null terminated string, but no null found before end of allocation\",\n+            HeapAllocZeroBytes =>\n+                \"tried to re-, de- or allocate zero bytes on the heap\",\n+            HeapAllocNonPowerOfTwoAlignment(_) =>\n+                \"tried to re-, de-, or allocate heap memory with alignment that is not a power of two\",\n+            Unreachable =>\n+                \"entered unreachable code\",\n+            Panic =>\n+                \"the evaluated program panicked\",\n+            ReadFromReturnPointer =>\n+                \"tried to read from the return pointer\",\n+            EvalErrorKind::PathNotFound(_) =>\n+                \"a path could not be resolved, maybe the crate is not loaded\",\n+        }\n+    }\n+\n+    fn cause(&self) -> Option<&Error> {\n+        use self::EvalErrorKind::*;\n+        match self.kind {\n+            MachineError(ref inner) => Some(&**inner),\n+            _ => None,\n+        }\n+    }\n+}\n+\n+impl<'tcx> fmt::Display for EvalError<'tcx> {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        use self::EvalErrorKind::*;\n+        match self.kind {\n+            PointerOutOfBounds { ptr, access, allocation_size } => {\n+                write!(f, \"{} at offset {}, outside bounds of allocation {} which has size {}\",\n+                       if access { \"memory access\" } else { \"pointer computed\" },\n+                       ptr.offset, ptr.alloc_id, allocation_size)\n+            },\n+            MemoryLockViolation { ptr, len, frame, access, ref lock } => {\n+                write!(f, \"{:?} access by frame {} at {:?}, size {}, is in conflict with lock {:?}\",\n+                       access, frame, ptr, len, lock)\n+            }\n+            MemoryAcquireConflict { ptr, len, kind, ref lock } => {\n+                write!(f, \"new {:?} lock at {:?}, size {}, is in conflict with lock {:?}\",\n+                       kind, ptr, len, lock)\n+            }\n+            InvalidMemoryLockRelease { ptr, len, frame, ref lock } => {\n+                write!(f, \"frame {} tried to release memory write lock at {:?}, size {}, but cannot release lock {:?}\",\n+                       frame, ptr, len, lock)\n+            }\n+            DeallocatedLockedMemory { ptr, ref lock } => {\n+                write!(f, \"tried to deallocate memory at {:?} in conflict with lock {:?}\",\n+                       ptr, lock)\n+            }\n+            ValidationFailure(ref err) => {\n+                write!(f, \"type validation failed: {}\", err)\n+            }\n+            NoMirFor(ref func) => write!(f, \"no mir for `{}`\", func),\n+            FunctionPointerTyMismatch(sig, got) =>\n+                write!(f, \"tried to call a function with sig {} through a function pointer of type {}\", sig, got),\n+            ArrayIndexOutOfBounds(span, len, index) =>\n+                write!(f, \"index out of bounds: the len is {} but the index is {} at {:?}\", len, index, span),\n+            ReallocatedWrongMemoryKind(ref old, ref new) =>\n+                write!(f, \"tried to reallocate memory from {} to {}\", old, new),\n+            DeallocatedWrongMemoryKind(ref old, ref new) =>\n+                write!(f, \"tried to deallocate {} memory but gave {} as the kind\", old, new),\n+            Math(span, ref err) =>\n+                write!(f, \"{:?} at {:?}\", err, span),\n+            Intrinsic(ref err) =>\n+                write!(f, \"{}\", err),\n+            InvalidChar(c) =>\n+                write!(f, \"tried to interpret an invalid 32-bit value as a char: {}\", c),\n+            OutOfMemory { allocation_size, memory_size, memory_usage } =>\n+                write!(f, \"tried to allocate {} more bytes, but only {} bytes are free of the {} byte memory\",\n+                       allocation_size, memory_size - memory_usage, memory_size),\n+            AlignmentCheckFailed { required, has } =>\n+               write!(f, \"tried to access memory with alignment {}, but alignment {} is required\",\n+                      has, required),\n+            TypeNotPrimitive(ty) =>\n+                write!(f, \"expected primitive type, got {}\", ty),\n+            Layout(ref err) =>\n+                write!(f, \"rustc layout computation failed: {:?}\", err),\n+            PathNotFound(ref path) =>\n+                write!(f, \"Cannot find path {:?}\", path),\n+            MachineError(ref inner) =>\n+                write!(f, \"machine error: {}\", inner),\n+            _ => write!(f, \"{}\", self.description()),\n+        }\n+    }\n+}"}, {"sha": "3388031a30cabe62acf0044350c65dbc948bfe0b", "filename": "src/librustc/mir/interpret/eval_context.rs", "status": "added", "additions": 2534, "deletions": 0, "changes": 2534, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Feval_context.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Feval_context.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Feval_context.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,2534 @@\n+use std::collections::{HashMap, HashSet};\n+use std::fmt::Write;\n+\n+use rustc::hir::def_id::DefId;\n+use rustc::hir::map::definitions::DefPathData;\n+use rustc::middle::const_val::ConstVal;\n+use rustc::middle::region;\n+use rustc::mir;\n+use rustc::traits::Reveal;\n+use rustc::ty::layout::{self, Layout, Size, Align, HasDataLayout};\n+use rustc::ty::subst::{Subst, Substs, Kind};\n+use rustc::ty::{self, Ty, TyCtxt, TypeFoldable};\n+use rustc_data_structures::indexed_vec::Idx;\n+use syntax::codemap::{self, DUMMY_SP};\n+use syntax::ast::Mutability;\n+use syntax::abi::Abi;\n+\n+use super::{EvalError, EvalResult, EvalErrorKind, GlobalId, Lvalue, LvalueExtra, Memory,\n+            MemoryPointer, HasMemory, MemoryKind, operator, PrimVal, PrimValKind, Value, Pointer,\n+            ValidationQuery, Machine};\n+\n+pub struct EvalContext<'a, 'tcx: 'a, M: Machine<'tcx>> {\n+    /// Stores data required by the `Machine`\n+    pub machine_data: M::Data,\n+\n+    /// The results of the type checker, from rustc.\n+    pub tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+\n+    /// The virtual memory system.\n+    pub memory: Memory<'a, 'tcx, M>,\n+\n+    /// Lvalues that were suspended by the validation subsystem, and will be recovered later\n+    pub(crate) suspended: HashMap<DynamicLifetime, Vec<ValidationQuery<'tcx>>>,\n+\n+    /// Precomputed statics, constants and promoteds.\n+    pub globals: HashMap<GlobalId<'tcx>, PtrAndAlign>,\n+\n+    /// The virtual call stack.\n+    pub(crate) stack: Vec<Frame<'tcx>>,\n+\n+    /// The maximum number of stack frames allowed\n+    pub(crate) stack_limit: usize,\n+\n+    /// The maximum number of operations that may be executed.\n+    /// This prevents infinite loops and huge computations from freezing up const eval.\n+    /// Remove once halting problem is solved.\n+    pub(crate) steps_remaining: u64,\n+}\n+\n+/// A stack frame.\n+pub struct Frame<'tcx> {\n+    ////////////////////////////////////////////////////////////////////////////////\n+    // Function and callsite information\n+    ////////////////////////////////////////////////////////////////////////////////\n+    /// The MIR for the function called on this frame.\n+    pub mir: &'tcx mir::Mir<'tcx>,\n+\n+    /// The def_id and substs of the current function\n+    pub instance: ty::Instance<'tcx>,\n+\n+    /// The span of the call site.\n+    pub span: codemap::Span,\n+\n+    ////////////////////////////////////////////////////////////////////////////////\n+    // Return lvalue and locals\n+    ////////////////////////////////////////////////////////////////////////////////\n+    /// The block to return to when returning from the current stack frame\n+    pub return_to_block: StackPopCleanup,\n+\n+    /// The location where the result of the current stack frame should be written to.\n+    pub return_lvalue: Lvalue,\n+\n+    /// The list of locals for this stack frame, stored in order as\n+    /// `[arguments..., variables..., temporaries...]`. The locals are stored as `Option<Value>`s.\n+    /// `None` represents a local that is currently dead, while a live local\n+    /// can either directly contain `PrimVal` or refer to some part of an `Allocation`.\n+    ///\n+    /// Before being initialized, arguments are `Value::ByVal(PrimVal::Undef)` and other locals are `None`.\n+    pub locals: Vec<Option<Value>>,\n+\n+    ////////////////////////////////////////////////////////////////////////////////\n+    // Current position within the function\n+    ////////////////////////////////////////////////////////////////////////////////\n+    /// The block that is currently executed (or will be executed after the above call stacks\n+    /// return).\n+    pub block: mir::BasicBlock,\n+\n+    /// The index of the currently evaluated statment.\n+    pub stmt: usize,\n+}\n+\n+#[derive(Clone, Debug, Eq, PartialEq, Hash)]\n+pub enum StackPopCleanup {\n+    /// The stackframe existed to compute the initial value of a static/constant, make sure it\n+    /// isn't modifyable afterwards in case of constants.\n+    /// In case of `static mut`, mark the memory to ensure it's never marked as immutable through\n+    /// references or deallocated\n+    MarkStatic(Mutability),\n+    /// A regular stackframe added due to a function call will need to get forwarded to the next\n+    /// block\n+    Goto(mir::BasicBlock),\n+    /// The main function and diverging functions have nowhere to return to\n+    None,\n+}\n+\n+#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n+pub struct DynamicLifetime {\n+    pub frame: usize,\n+    pub region: Option<region::Scope>, // \"None\" indicates \"until the function ends\"\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub struct ResourceLimits {\n+    pub memory_size: u64,\n+    pub step_limit: u64,\n+    pub stack_limit: usize,\n+}\n+\n+impl Default for ResourceLimits {\n+    fn default() -> Self {\n+        ResourceLimits {\n+            memory_size: 100 * 1024 * 1024, // 100 MB\n+            step_limit: 1_000_000,\n+            stack_limit: 100,\n+        }\n+    }\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub struct TyAndPacked<'tcx> {\n+    pub ty: Ty<'tcx>,\n+    pub packed: bool,\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub struct ValTy<'tcx> {\n+    pub value: Value,\n+    pub ty: Ty<'tcx>,\n+}\n+\n+impl<'tcx> ::std::ops::Deref for ValTy<'tcx> {\n+    type Target = Value;\n+    fn deref(&self) -> &Value {\n+        &self.value\n+    }\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub struct PtrAndAlign {\n+    pub ptr: Pointer,\n+    /// Remember whether this lvalue is *supposed* to be aligned.\n+    pub aligned: bool,\n+}\n+\n+impl PtrAndAlign {\n+    pub fn to_ptr<'tcx>(self) -> EvalResult<'tcx, MemoryPointer> {\n+        self.ptr.to_ptr()\n+    }\n+    pub fn offset<'tcx, C: HasDataLayout>(self, i: u64, cx: C) -> EvalResult<'tcx, Self> {\n+        Ok(PtrAndAlign {\n+            ptr: self.ptr.offset(i, cx)?,\n+            aligned: self.aligned,\n+        })\n+    }\n+}\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    pub fn new(\n+        tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+        limits: ResourceLimits,\n+        machine_data: M::Data,\n+        memory_data: M::MemoryData,\n+    ) -> Self {\n+        EvalContext {\n+            machine_data,\n+            tcx,\n+            memory: Memory::new(&tcx.data_layout, limits.memory_size, memory_data),\n+            suspended: HashMap::new(),\n+            globals: HashMap::new(),\n+            stack: Vec::new(),\n+            stack_limit: limits.stack_limit,\n+            steps_remaining: limits.step_limit,\n+        }\n+    }\n+\n+    pub fn alloc_ptr(&mut self, ty: Ty<'tcx>) -> EvalResult<'tcx, MemoryPointer> {\n+        let substs = self.substs();\n+        self.alloc_ptr_with_substs(ty, substs)\n+    }\n+\n+    pub fn alloc_ptr_with_substs(\n+        &mut self,\n+        ty: Ty<'tcx>,\n+        substs: &'tcx Substs<'tcx>,\n+    ) -> EvalResult<'tcx, MemoryPointer> {\n+        let size = self.type_size_with_substs(ty, substs)?.expect(\n+            \"cannot alloc memory for unsized type\",\n+        );\n+        let align = self.type_align_with_substs(ty, substs)?;\n+        self.memory.allocate(size, align, MemoryKind::Stack)\n+    }\n+\n+    pub fn memory(&self) -> &Memory<'a, 'tcx, M> {\n+        &self.memory\n+    }\n+\n+    pub fn memory_mut(&mut self) -> &mut Memory<'a, 'tcx, M> {\n+        &mut self.memory\n+    }\n+\n+    pub fn stack(&self) -> &[Frame<'tcx>] {\n+        &self.stack\n+    }\n+\n+    #[inline]\n+    pub fn cur_frame(&self) -> usize {\n+        assert!(self.stack.len() > 0);\n+        self.stack.len() - 1\n+    }\n+\n+    pub fn str_to_value(&mut self, s: &str) -> EvalResult<'tcx, Value> {\n+        let ptr = self.memory.allocate_cached(s.as_bytes())?;\n+        Ok(Value::ByValPair(\n+            PrimVal::Ptr(ptr),\n+            PrimVal::from_u128(s.len() as u128),\n+        ))\n+    }\n+\n+    pub(super) fn const_to_value(&mut self, const_val: &ConstVal<'tcx>) -> EvalResult<'tcx, Value> {\n+        use rustc::middle::const_val::ConstVal::*;\n+\n+        let primval = match *const_val {\n+            Integral(const_int) => PrimVal::Bytes(const_int.to_u128_unchecked()),\n+\n+            Float(val) => PrimVal::Bytes(val.bits),\n+\n+            Bool(b) => PrimVal::from_bool(b),\n+            Char(c) => PrimVal::from_char(c),\n+\n+            Str(ref s) => return self.str_to_value(s),\n+\n+            ByteStr(ref bs) => {\n+                let ptr = self.memory.allocate_cached(bs.data)?;\n+                PrimVal::Ptr(ptr)\n+            }\n+\n+            Unevaluated(def_id, substs) => {\n+                let instance = self.resolve_associated_const(def_id, substs);\n+                let cid = GlobalId {\n+                    instance,\n+                    promoted: None,\n+                };\n+                return Ok(Value::ByRef(*self.globals.get(&cid).expect(\"static/const not cached\")));\n+            }\n+\n+            Aggregate(..) |\n+            Variant(_) => bug!(\"should not have aggregate or variant constants in MIR\"),\n+            // function items are zero sized and thus have no readable value\n+            Function(..) => PrimVal::Undef,\n+        };\n+\n+        Ok(Value::ByVal(primval))\n+    }\n+\n+    pub(super) fn type_is_sized(&self, ty: Ty<'tcx>) -> bool {\n+        // generics are weird, don't run this function on a generic\n+        assert!(!ty.needs_subst());\n+        ty.is_sized(self.tcx, ty::ParamEnv::empty(Reveal::All), DUMMY_SP)\n+    }\n+\n+    pub fn load_mir(\n+        &self,\n+        instance: ty::InstanceDef<'tcx>,\n+    ) -> EvalResult<'tcx, &'tcx mir::Mir<'tcx>> {\n+        trace!(\"load mir {:?}\", instance);\n+        match instance {\n+            ty::InstanceDef::Item(def_id) => {\n+                self.tcx.maybe_optimized_mir(def_id).ok_or_else(|| {\n+                    EvalErrorKind::NoMirFor(self.tcx.item_path_str(def_id)).into()\n+                })\n+            }\n+            _ => Ok(self.tcx.instance_mir(instance)),\n+        }\n+    }\n+\n+    pub fn monomorphize(&self, ty: Ty<'tcx>, substs: &'tcx Substs<'tcx>) -> Ty<'tcx> {\n+        // miri doesn't care about lifetimes, and will choke on some crazy ones\n+        // let's simply get rid of them\n+        let without_lifetimes = self.tcx.erase_regions(&ty);\n+        let substituted = without_lifetimes.subst(self.tcx, substs);\n+        let substituted = self.tcx.normalize_associated_type(&substituted);\n+        substituted\n+    }\n+\n+    /// Return the size and aligment of the value at the given type.\n+    /// Note that the value does not matter if the type is sized. For unsized types,\n+    /// the value has to be a fat pointer, and we only care about the \"extra\" data in it.\n+    pub fn size_and_align_of_dst(\n+        &mut self,\n+        ty: ty::Ty<'tcx>,\n+        value: Value,\n+    ) -> EvalResult<'tcx, (u64, u64)> {\n+        if let Some(size) = self.type_size(ty)? {\n+            Ok((size as u64, self.type_align(ty)? as u64))\n+        } else {\n+            match ty.sty {\n+                ty::TyAdt(..) | ty::TyTuple(..) => {\n+                    // First get the size of all statically known fields.\n+                    // Don't use type_of::sizing_type_of because that expects t to be sized,\n+                    // and it also rounds up to alignment, which we want to avoid,\n+                    // as the unsized field's alignment could be smaller.\n+                    assert!(!ty.is_simd());\n+                    let layout = self.type_layout(ty)?;\n+                    debug!(\"DST {} layout: {:?}\", ty, layout);\n+\n+                    let (sized_size, sized_align) = match *layout {\n+                        ty::layout::Layout::Univariant { ref variant, .. } => {\n+                            (\n+                                variant.offsets.last().map_or(0, |o| o.bytes()),\n+                                variant.align,\n+                            )\n+                        }\n+                        _ => {\n+                            bug!(\n+                                \"size_and_align_of_dst: expcted Univariant for `{}`, found {:#?}\",\n+                                ty,\n+                                layout\n+                            );\n+                        }\n+                    };\n+                    debug!(\n+                        \"DST {} statically sized prefix size: {} align: {:?}\",\n+                        ty,\n+                        sized_size,\n+                        sized_align\n+                    );\n+\n+                    // Recurse to get the size of the dynamically sized field (must be\n+                    // the last field).\n+                    let (unsized_size, unsized_align) = match ty.sty {\n+                        ty::TyAdt(def, substs) => {\n+                            let last_field = def.struct_variant().fields.last().unwrap();\n+                            let field_ty = self.field_ty(substs, last_field);\n+                            self.size_and_align_of_dst(field_ty, value)?\n+                        }\n+                        ty::TyTuple(ref types, _) => {\n+                            let field_ty = types.last().unwrap();\n+                            let field_ty = self.tcx.normalize_associated_type(field_ty);\n+                            self.size_and_align_of_dst(field_ty, value)?\n+                        }\n+                        _ => bug!(\"We already checked that we know this type\"),\n+                    };\n+\n+                    // FIXME (#26403, #27023): We should be adding padding\n+                    // to `sized_size` (to accommodate the `unsized_align`\n+                    // required of the unsized field that follows) before\n+                    // summing it with `sized_size`. (Note that since #26403\n+                    // is unfixed, we do not yet add the necessary padding\n+                    // here. But this is where the add would go.)\n+\n+                    // Return the sum of sizes and max of aligns.\n+                    let size = sized_size + unsized_size;\n+\n+                    // Choose max of two known alignments (combined value must\n+                    // be aligned according to more restrictive of the two).\n+                    let align =\n+                        sized_align.max(Align::from_bytes(unsized_align, unsized_align).unwrap());\n+\n+                    // Issue #27023: must add any necessary padding to `size`\n+                    // (to make it a multiple of `align`) before returning it.\n+                    //\n+                    // Namely, the returned size should be, in C notation:\n+                    //\n+                    //   `size + ((size & (align-1)) ? align : 0)`\n+                    //\n+                    // emulated via the semi-standard fast bit trick:\n+                    //\n+                    //   `(size + (align-1)) & -align`\n+\n+                    let size = Size::from_bytes(size).abi_align(align).bytes();\n+                    Ok((size, align.abi()))\n+                }\n+                ty::TyDynamic(..) => {\n+                    let (_, vtable) = value.into_ptr_vtable_pair(&mut self.memory)?;\n+                    // the second entry in the vtable is the dynamic size of the object.\n+                    self.read_size_and_align_from_vtable(vtable)\n+                }\n+\n+                ty::TySlice(_) | ty::TyStr => {\n+                    let elem_ty = ty.sequence_element_type(self.tcx);\n+                    let elem_size = self.type_size(elem_ty)?.expect(\n+                        \"slice element must be sized\",\n+                    ) as u64;\n+                    let (_, len) = value.into_slice(&mut self.memory)?;\n+                    let align = self.type_align(elem_ty)?;\n+                    Ok((len * elem_size, align as u64))\n+                }\n+\n+                _ => bug!(\"size_of_val::<{:?}>\", ty),\n+            }\n+        }\n+    }\n+\n+    /// Returns the normalized type of a struct field\n+    fn field_ty(&self, param_substs: &Substs<'tcx>, f: &ty::FieldDef) -> ty::Ty<'tcx> {\n+        self.tcx.normalize_associated_type(\n+            &f.ty(self.tcx, param_substs),\n+        )\n+    }\n+\n+    pub fn type_size(&self, ty: Ty<'tcx>) -> EvalResult<'tcx, Option<u64>> {\n+        self.type_size_with_substs(ty, self.substs())\n+    }\n+\n+    pub fn type_align(&self, ty: Ty<'tcx>) -> EvalResult<'tcx, u64> {\n+        self.type_align_with_substs(ty, self.substs())\n+    }\n+\n+    pub fn type_size_with_substs(\n+        &self,\n+        ty: Ty<'tcx>,\n+        substs: &'tcx Substs<'tcx>,\n+    ) -> EvalResult<'tcx, Option<u64>> {\n+        let layout = self.type_layout_with_substs(ty, substs)?;\n+        if layout.is_unsized() {\n+            Ok(None)\n+        } else {\n+            Ok(Some(layout.size(&self.tcx.data_layout).bytes()))\n+        }\n+    }\n+\n+    pub fn type_align_with_substs(\n+        &self,\n+        ty: Ty<'tcx>,\n+        substs: &'tcx Substs<'tcx>,\n+    ) -> EvalResult<'tcx, u64> {\n+        self.type_layout_with_substs(ty, substs).map(|layout| {\n+            layout.align(&self.tcx.data_layout).abi()\n+        })\n+    }\n+\n+    pub fn type_layout(&self, ty: Ty<'tcx>) -> EvalResult<'tcx, &'tcx Layout> {\n+        self.type_layout_with_substs(ty, self.substs())\n+    }\n+\n+    fn type_layout_with_substs(\n+        &self,\n+        ty: Ty<'tcx>,\n+        substs: &'tcx Substs<'tcx>,\n+    ) -> EvalResult<'tcx, &'tcx Layout> {\n+        // TODO(solson): Is this inefficient? Needs investigation.\n+        let ty = self.monomorphize(ty, substs);\n+\n+        ty.layout(self.tcx, ty::ParamEnv::empty(Reveal::All))\n+            .map_err(|layout| EvalErrorKind::Layout(layout).into())\n+    }\n+\n+    pub fn push_stack_frame(\n+        &mut self,\n+        instance: ty::Instance<'tcx>,\n+        span: codemap::Span,\n+        mir: &'tcx mir::Mir<'tcx>,\n+        return_lvalue: Lvalue,\n+        return_to_block: StackPopCleanup,\n+    ) -> EvalResult<'tcx> {\n+        ::log_settings::settings().indentation += 1;\n+\n+        /// Return the set of locals that have a storage annotation anywhere\n+        fn collect_storage_annotations<'tcx>(mir: &'tcx mir::Mir<'tcx>) -> HashSet<mir::Local> {\n+            use rustc::mir::StatementKind::*;\n+\n+            let mut set = HashSet::new();\n+            for block in mir.basic_blocks() {\n+                for stmt in block.statements.iter() {\n+                    match stmt.kind {\n+                        StorageLive(local) |\n+                        StorageDead(local) => {\n+                            set.insert(local);\n+                        }\n+                        _ => {}\n+                    }\n+                }\n+            }\n+            set\n+        }\n+\n+        // Subtract 1 because `local_decls` includes the ReturnMemoryPointer, but we don't store a local\n+        // `Value` for that.\n+        let num_locals = mir.local_decls.len() - 1;\n+\n+        let locals = {\n+            let annotated_locals = collect_storage_annotations(mir);\n+            let mut locals = vec![None; num_locals];\n+            for i in 0..num_locals {\n+                let local = mir::Local::new(i + 1);\n+                if !annotated_locals.contains(&local) {\n+                    locals[i] = Some(Value::ByVal(PrimVal::Undef));\n+                }\n+            }\n+            locals\n+        };\n+\n+        self.stack.push(Frame {\n+            mir,\n+            block: mir::START_BLOCK,\n+            return_to_block,\n+            return_lvalue,\n+            locals,\n+            span,\n+            instance,\n+            stmt: 0,\n+        });\n+\n+        self.memory.cur_frame = self.cur_frame();\n+\n+        if self.stack.len() > self.stack_limit {\n+            err!(StackFrameLimitReached)\n+        } else {\n+            Ok(())\n+        }\n+    }\n+\n+    pub(super) fn pop_stack_frame(&mut self) -> EvalResult<'tcx> {\n+        ::log_settings::settings().indentation -= 1;\n+        self.end_region(None)?;\n+        let frame = self.stack.pop().expect(\n+            \"tried to pop a stack frame, but there were none\",\n+        );\n+        if !self.stack.is_empty() {\n+            // TODO: Is this the correct time to start considering these accesses as originating from the returned-to stack frame?\n+            self.memory.cur_frame = self.cur_frame();\n+        }\n+        match frame.return_to_block {\n+            StackPopCleanup::MarkStatic(mutable) => {\n+                if let Lvalue::Ptr { ptr, .. } = frame.return_lvalue {\n+                    // FIXME: to_ptr()? might be too extreme here, static zsts might reach this under certain conditions\n+                    self.memory.mark_static_initalized(\n+                        ptr.to_ptr()?.alloc_id,\n+                        mutable,\n+                    )?\n+                } else {\n+                    bug!(\"StackPopCleanup::MarkStatic on: {:?}\", frame.return_lvalue);\n+                }\n+            }\n+            StackPopCleanup::Goto(target) => self.goto_block(target),\n+            StackPopCleanup::None => {}\n+        }\n+        // deallocate all locals that are backed by an allocation\n+        for local in frame.locals {\n+            self.deallocate_local(local)?;\n+        }\n+\n+        Ok(())\n+    }\n+\n+    pub fn deallocate_local(&mut self, local: Option<Value>) -> EvalResult<'tcx> {\n+        if let Some(Value::ByRef(ptr)) = local {\n+            trace!(\"deallocating local\");\n+            let ptr = ptr.to_ptr()?;\n+            self.memory.dump_alloc(ptr.alloc_id);\n+            match self.memory.get(ptr.alloc_id)?.kind {\n+                // for a constant like `const FOO: &i32 = &1;` the local containing\n+                // the `1` is referred to by the global. We transitively marked everything\n+                // the global refers to as static itself, so we don't free it here\n+                MemoryKind::Static => {}\n+                MemoryKind::Stack => self.memory.deallocate(ptr, None, MemoryKind::Stack)?,\n+                other => bug!(\"local contained non-stack memory: {:?}\", other),\n+            }\n+        };\n+        Ok(())\n+    }\n+\n+    pub fn assign_discr_and_fields(\n+        &mut self,\n+        dest: Lvalue,\n+        dest_ty: Ty<'tcx>,\n+        discr_offset: u64,\n+        operands: &[mir::Operand<'tcx>],\n+        discr_val: u128,\n+        variant_idx: usize,\n+        discr_size: u64,\n+        discr_signed: bool,\n+    ) -> EvalResult<'tcx> {\n+        // FIXME(solson)\n+        let dest_ptr = self.force_allocation(dest)?.to_ptr()?;\n+\n+        let discr_dest = dest_ptr.offset(discr_offset, &self)?;\n+        self.memory.write_primval(discr_dest, PrimVal::Bytes(discr_val), discr_size, discr_signed)?;\n+\n+        let dest = Lvalue::Ptr {\n+            ptr: PtrAndAlign {\n+                ptr: dest_ptr.into(),\n+                aligned: true,\n+            },\n+            extra: LvalueExtra::DowncastVariant(variant_idx),\n+        };\n+\n+        self.assign_fields(dest, dest_ty, operands)\n+    }\n+\n+    pub fn assign_fields(\n+        &mut self,\n+        dest: Lvalue,\n+        dest_ty: Ty<'tcx>,\n+        operands: &[mir::Operand<'tcx>],\n+    ) -> EvalResult<'tcx> {\n+        if self.type_size(dest_ty)? == Some(0) {\n+            // zst assigning is a nop\n+            return Ok(());\n+        }\n+        if self.ty_to_primval_kind(dest_ty).is_ok() {\n+            assert_eq!(operands.len(), 1);\n+            let value = self.eval_operand(&operands[0])?;\n+            return self.write_value(value, dest);\n+        }\n+        for (field_index, operand) in operands.iter().enumerate() {\n+            let value = self.eval_operand(operand)?;\n+            let field_dest = self.lvalue_field(dest, mir::Field::new(field_index), dest_ty, value.ty)?;\n+            self.write_value(value, field_dest)?;\n+        }\n+        Ok(())\n+    }\n+\n+    /// Evaluate an assignment statement.\n+    ///\n+    /// There is no separate `eval_rvalue` function. Instead, the code for handling each rvalue\n+    /// type writes its results directly into the memory specified by the lvalue.\n+    pub(super) fn eval_rvalue_into_lvalue(\n+        &mut self,\n+        rvalue: &mir::Rvalue<'tcx>,\n+        lvalue: &mir::Lvalue<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        let dest = self.eval_lvalue(lvalue)?;\n+        let dest_ty = self.lvalue_ty(lvalue);\n+        let dest_layout = self.type_layout(dest_ty)?;\n+\n+        use rustc::mir::Rvalue::*;\n+        match *rvalue {\n+            Use(ref operand) => {\n+                let value = self.eval_operand(operand)?.value;\n+                let valty = ValTy {\n+                    value,\n+                    ty: dest_ty,\n+                };\n+                self.write_value(valty, dest)?;\n+            }\n+\n+            BinaryOp(bin_op, ref left, ref right) => {\n+                let left = self.eval_operand(left)?;\n+                let right = self.eval_operand(right)?;\n+                if self.intrinsic_overflowing(\n+                    bin_op,\n+                    left,\n+                    right,\n+                    dest,\n+                    dest_ty,\n+                )?\n+                {\n+                    // There was an overflow in an unchecked binop.  Right now, we consider this an error and bail out.\n+                    // The rationale is that the reason rustc emits unchecked binops in release mode (vs. the checked binops\n+                    // it emits in debug mode) is performance, but it doesn't cost us any performance in miri.\n+                    // If, however, the compiler ever starts transforming unchecked intrinsics into unchecked binops,\n+                    // we have to go back to just ignoring the overflow here.\n+                    return err!(OverflowingMath);\n+                }\n+            }\n+\n+            CheckedBinaryOp(bin_op, ref left, ref right) => {\n+                let left = self.eval_operand(left)?;\n+                let right = self.eval_operand(right)?;\n+                self.intrinsic_with_overflow(\n+                    bin_op,\n+                    left,\n+                    right,\n+                    dest,\n+                    dest_ty,\n+                )?;\n+            }\n+\n+            UnaryOp(un_op, ref operand) => {\n+                let val = self.eval_operand_to_primval(operand)?;\n+                let kind = self.ty_to_primval_kind(dest_ty)?;\n+                self.write_primval(\n+                    dest,\n+                    operator::unary_op(un_op, val, kind)?,\n+                    dest_ty,\n+                )?;\n+            }\n+\n+            // Skip everything for zsts\n+            Aggregate(..) if self.type_size(dest_ty)? == Some(0) => {}\n+\n+            Aggregate(ref kind, ref operands) => {\n+                self.inc_step_counter_and_check_limit(operands.len() as u64)?;\n+                use rustc::ty::layout::Layout::*;\n+                match *dest_layout {\n+                    Univariant { ref variant, .. } => {\n+                        self.write_maybe_aligned_mut(!variant.packed, |ecx| {\n+                            ecx.assign_fields(dest, dest_ty, operands)\n+                        })?;\n+                    }\n+\n+                    Array { .. } => {\n+                        self.assign_fields(dest, dest_ty, operands)?;\n+                    }\n+\n+                    General {\n+                        discr,\n+                        ref variants,\n+                        ..\n+                    } => {\n+                        if let mir::AggregateKind::Adt(adt_def, variant, _, _) = **kind {\n+                            let discr_val = adt_def\n+                                .discriminants(self.tcx)\n+                                .nth(variant)\n+                                .expect(\"broken mir: Adt variant id invalid\")\n+                                .to_u128_unchecked();\n+                            let discr_size = discr.size().bytes();\n+\n+                            self.assign_discr_and_fields(\n+                                dest,\n+                                dest_ty,\n+                                variants[variant].offsets[0].bytes(),\n+                                operands,\n+                                discr_val,\n+                                variant,\n+                                discr_size,\n+                                false,\n+                            )?;\n+                        } else {\n+                            bug!(\"tried to assign {:?} to Layout::General\", kind);\n+                        }\n+                    }\n+\n+                    RawNullablePointer { nndiscr, .. } => {\n+                        if let mir::AggregateKind::Adt(_, variant, _, _) = **kind {\n+                            if nndiscr == variant as u64 {\n+                                assert_eq!(operands.len(), 1);\n+                                let operand = &operands[0];\n+                                let value = self.eval_operand(operand)?;\n+                                self.write_value(value, dest)?;\n+                            } else {\n+                                if let Some(operand) = operands.get(0) {\n+                                    assert_eq!(operands.len(), 1);\n+                                    let operand_ty = self.operand_ty(operand);\n+                                    assert_eq!(self.type_size(operand_ty)?, Some(0));\n+                                }\n+                                self.write_null(dest, dest_ty)?;\n+                            }\n+                        } else {\n+                            bug!(\"tried to assign {:?} to Layout::RawNullablePointer\", kind);\n+                        }\n+                    }\n+\n+                    StructWrappedNullablePointer {\n+                        nndiscr,\n+                        ref discrfield_source,\n+                        ref nonnull,\n+                        ..\n+                    } => {\n+                        if let mir::AggregateKind::Adt(_, variant, _, _) = **kind {\n+                            if nndiscr == variant as u64 {\n+                                self.write_maybe_aligned_mut(!nonnull.packed, |ecx| {\n+                                    ecx.assign_fields(dest, dest_ty, operands)\n+                                })?;\n+                            } else {\n+                                for operand in operands {\n+                                    let operand_ty = self.operand_ty(operand);\n+                                    assert_eq!(self.type_size(operand_ty)?, Some(0));\n+                                }\n+                                self.write_struct_wrapped_null_pointer(\n+                                    dest_ty,\n+                                    nndiscr,\n+                                    discrfield_source,\n+                                    dest,\n+                                )?;\n+                            }\n+                        } else {\n+                            bug!(\"tried to assign {:?} to Layout::RawNullablePointer\", kind);\n+                        }\n+                    }\n+\n+                    CEnum { .. } => {\n+                        assert_eq!(operands.len(), 0);\n+                        if let mir::AggregateKind::Adt(adt_def, variant, _, _) = **kind {\n+                            let n = adt_def\n+                                .discriminants(self.tcx)\n+                                .nth(variant)\n+                                .expect(\"broken mir: Adt variant index invalid\")\n+                                .to_u128_unchecked();\n+                            self.write_primval(dest, PrimVal::Bytes(n), dest_ty)?;\n+                        } else {\n+                            bug!(\"tried to assign {:?} to Layout::CEnum\", kind);\n+                        }\n+                    }\n+\n+                    Vector { count, .. } => {\n+                        debug_assert_eq!(count, operands.len() as u64);\n+                        self.assign_fields(dest, dest_ty, operands)?;\n+                    }\n+\n+                    UntaggedUnion { ref variants } => {\n+                        assert_eq!(operands.len(), 1);\n+                        let operand = &operands[0];\n+                        let value = self.eval_operand(operand)?;\n+                        self.write_maybe_aligned_mut(!variants.packed, |ecx| {\n+                            ecx.write_value(value, dest)\n+                        })?;\n+                    }\n+\n+                    _ => {\n+                        return err!(Unimplemented(format!(\n+                            \"can't handle destination layout {:?} when assigning {:?}\",\n+                            dest_layout,\n+                            kind\n+                        )));\n+                    }\n+                }\n+            }\n+\n+            Repeat(ref operand, _) => {\n+                let (elem_ty, length) = match dest_ty.sty {\n+                    ty::TyArray(elem_ty, n) => (elem_ty, n.val.to_const_int().unwrap().to_u64().unwrap()),\n+                    _ => {\n+                        bug!(\n+                            \"tried to assign array-repeat to non-array type {:?}\",\n+                            dest_ty\n+                        )\n+                    }\n+                };\n+                self.inc_step_counter_and_check_limit(length)?;\n+                let elem_size = self.type_size(elem_ty)?.expect(\n+                    \"repeat element type must be sized\",\n+                );\n+                let value = self.eval_operand(operand)?.value;\n+\n+                // FIXME(solson)\n+                let dest = Pointer::from(self.force_allocation(dest)?.to_ptr()?);\n+\n+                for i in 0..length {\n+                    let elem_dest = dest.offset(i * elem_size, &self)?;\n+                    self.write_value_to_ptr(value, elem_dest, elem_ty)?;\n+                }\n+            }\n+\n+            Len(ref lvalue) => {\n+                // FIXME(CTFE): don't allow computing the length of arrays in const eval\n+                let src = self.eval_lvalue(lvalue)?;\n+                let ty = self.lvalue_ty(lvalue);\n+                let (_, len) = src.elem_ty_and_len(ty);\n+                self.write_primval(\n+                    dest,\n+                    PrimVal::from_u128(len as u128),\n+                    dest_ty,\n+                )?;\n+            }\n+\n+            Ref(_, _, ref lvalue) => {\n+                let src = self.eval_lvalue(lvalue)?;\n+                // We ignore the alignment of the lvalue here -- special handling for packed structs ends\n+                // at the `&` operator.\n+                let (ptr, extra) = self.force_allocation(src)?.to_ptr_extra_aligned();\n+\n+                let val = match extra {\n+                    LvalueExtra::None => ptr.ptr.to_value(),\n+                    LvalueExtra::Length(len) => ptr.ptr.to_value_with_len(len),\n+                    LvalueExtra::Vtable(vtable) => ptr.ptr.to_value_with_vtable(vtable),\n+                    LvalueExtra::DowncastVariant(..) => {\n+                        bug!(\"attempted to take a reference to an enum downcast lvalue\")\n+                    }\n+                };\n+                let valty = ValTy {\n+                    value: val,\n+                    ty: dest_ty,\n+                };\n+                self.write_value(valty, dest)?;\n+            }\n+\n+            NullaryOp(mir::NullOp::Box, ty) => {\n+                M::box_alloc(self, ty, dest)?;\n+            }\n+\n+            NullaryOp(mir::NullOp::SizeOf, ty) => {\n+                let size = self.type_size(ty)?.expect(\n+                    \"SizeOf nullary MIR operator called for unsized type\",\n+                );\n+                self.write_primval(\n+                    dest,\n+                    PrimVal::from_u128(size as u128),\n+                    dest_ty,\n+                )?;\n+            }\n+\n+            Cast(kind, ref operand, cast_ty) => {\n+                debug_assert_eq!(self.monomorphize(cast_ty, self.substs()), dest_ty);\n+                use rustc::mir::CastKind::*;\n+                match kind {\n+                    Unsize => {\n+                        let src = self.eval_operand(operand)?;\n+                        self.unsize_into(src.value, src.ty, dest, dest_ty)?;\n+                    }\n+\n+                    Misc => {\n+                        let src = self.eval_operand(operand)?;\n+                        if self.type_is_fat_ptr(src.ty) {\n+                            match (src.value, self.type_is_fat_ptr(dest_ty)) {\n+                                (Value::ByRef { .. }, _) |\n+                                (Value::ByValPair(..), true) => {\n+                                    let valty = ValTy {\n+                                        value: src.value,\n+                                        ty: dest_ty,\n+                                    };\n+                                    self.write_value(valty, dest)?;\n+                                }\n+                                (Value::ByValPair(data, _), false) => {\n+                                    let valty = ValTy {\n+                                        value: Value::ByVal(data),\n+                                        ty: dest_ty,\n+                                    };\n+                                    self.write_value(valty, dest)?;\n+                                }\n+                                (Value::ByVal(_), _) => bug!(\"expected fat ptr\"),\n+                            }\n+                        } else {\n+                            let src_val = self.value_to_primval(src)?;\n+                            let dest_val = self.cast_primval(src_val, src.ty, dest_ty)?;\n+                            let valty = ValTy {\n+                                value: Value::ByVal(dest_val),\n+                                ty: dest_ty,\n+                            };\n+                            self.write_value(valty, dest)?;\n+                        }\n+                    }\n+\n+                    ReifyFnPointer => {\n+                        match self.operand_ty(operand).sty {\n+                            ty::TyFnDef(def_id, substs) => {\n+                                let instance = resolve(self.tcx, def_id, substs);\n+                                let fn_ptr = self.memory.create_fn_alloc(instance);\n+                                let valty = ValTy {\n+                                    value: Value::ByVal(PrimVal::Ptr(fn_ptr)),\n+                                    ty: dest_ty,\n+                                };\n+                                self.write_value(valty, dest)?;\n+                            }\n+                            ref other => bug!(\"reify fn pointer on {:?}\", other),\n+                        }\n+                    }\n+\n+                    UnsafeFnPointer => {\n+                        match dest_ty.sty {\n+                            ty::TyFnPtr(_) => {\n+                                let mut src = self.eval_operand(operand)?;\n+                                src.ty = dest_ty;\n+                                self.write_value(src, dest)?;\n+                            }\n+                            ref other => bug!(\"fn to unsafe fn cast on {:?}\", other),\n+                        }\n+                    }\n+\n+                    ClosureFnPointer => {\n+                        match self.operand_ty(operand).sty {\n+                            ty::TyClosure(def_id, substs) => {\n+                                let instance = resolve_closure(\n+                                    self.tcx,\n+                                    def_id,\n+                                    substs,\n+                                    ty::ClosureKind::FnOnce,\n+                                );\n+                                let fn_ptr = self.memory.create_fn_alloc(instance);\n+                                let valty = ValTy {\n+                                    value: Value::ByVal(PrimVal::Ptr(fn_ptr)),\n+                                    ty: dest_ty,\n+                                };\n+                                self.write_value(valty, dest)?;\n+                            }\n+                            ref other => bug!(\"closure fn pointer on {:?}\", other),\n+                        }\n+                    }\n+                }\n+            }\n+\n+            Discriminant(ref lvalue) => {\n+                let lval = self.eval_lvalue(lvalue)?;\n+                let ty = self.lvalue_ty(lvalue);\n+                let ptr = self.force_allocation(lval)?.to_ptr()?;\n+                let discr_val = self.read_discriminant_value(ptr, ty)?;\n+                if let ty::TyAdt(adt_def, _) = ty.sty {\n+                    trace!(\"Read discriminant {}, valid discriminants {:?}\", discr_val, adt_def.discriminants(self.tcx).collect::<Vec<_>>());\n+                    if adt_def.discriminants(self.tcx).all(|v| {\n+                        discr_val != v.to_u128_unchecked()\n+                    })\n+                    {\n+                        return err!(InvalidDiscriminant);\n+                    }\n+                    self.write_primval(dest, PrimVal::Bytes(discr_val), dest_ty)?;\n+                } else {\n+                    bug!(\"rustc only generates Rvalue::Discriminant for enums\");\n+                }\n+            }\n+        }\n+\n+        if log_enabled!(::log::LogLevel::Trace) {\n+            self.dump_local(dest);\n+        }\n+\n+        Ok(())\n+    }\n+\n+    pub(crate) fn write_struct_wrapped_null_pointer(\n+        &mut self,\n+        dest_ty: ty::Ty<'tcx>,\n+        nndiscr: u64,\n+        discrfield_source: &layout::FieldPath,\n+        dest: Lvalue,\n+    ) -> EvalResult<'tcx> {\n+        let (offset, TyAndPacked { ty, packed }) = self.nonnull_offset_and_ty(\n+            dest_ty,\n+            nndiscr,\n+            discrfield_source,\n+        )?;\n+        let nonnull = self.force_allocation(dest)?.to_ptr()?.offset(\n+            offset.bytes(),\n+            &self,\n+        )?;\n+        trace!(\"struct wrapped nullable pointer type: {}\", ty);\n+        // only the pointer part of a fat pointer is used for this space optimization\n+        let discr_size = self.type_size(ty)?.expect(\n+            \"bad StructWrappedNullablePointer discrfield\",\n+        );\n+        self.memory.write_maybe_aligned_mut(!packed, |mem| {\n+            // We're writing 0, signedness does not matter\n+            mem.write_primval(nonnull, PrimVal::Bytes(0), discr_size, false)\n+        })\n+    }\n+\n+    pub(super) fn type_is_fat_ptr(&self, ty: Ty<'tcx>) -> bool {\n+        match ty.sty {\n+            ty::TyRawPtr(ref tam) |\n+            ty::TyRef(_, ref tam) => !self.type_is_sized(tam.ty),\n+            ty::TyAdt(def, _) if def.is_box() => !self.type_is_sized(ty.boxed_ty()),\n+            _ => false,\n+        }\n+    }\n+\n+    pub(super) fn nonnull_offset_and_ty(\n+        &self,\n+        ty: Ty<'tcx>,\n+        nndiscr: u64,\n+        discrfield: &[u32],\n+    ) -> EvalResult<'tcx, (Size, TyAndPacked<'tcx>)> {\n+        // Skip the constant 0 at the start meant for LLVM GEP and the outer non-null variant\n+        let path = discrfield.iter().skip(2).map(|&i| i as usize);\n+\n+        // Handle the field index for the outer non-null variant.\n+        let (inner_offset, inner_ty) = match ty.sty {\n+            ty::TyAdt(adt_def, substs) => {\n+                let variant = &adt_def.variants[nndiscr as usize];\n+                let index = discrfield[1];\n+                let field = &variant.fields[index as usize];\n+                (\n+                    self.get_field_offset(ty, index as usize)?,\n+                    field.ty(self.tcx, substs),\n+                )\n+            }\n+            _ => bug!(\"non-enum for StructWrappedNullablePointer: {}\", ty),\n+        };\n+\n+        self.field_path_offset_and_ty(inner_offset, inner_ty, path)\n+    }\n+\n+    fn field_path_offset_and_ty<I: Iterator<Item = usize>>(\n+        &self,\n+        mut offset: Size,\n+        mut ty: Ty<'tcx>,\n+        path: I,\n+    ) -> EvalResult<'tcx, (Size, TyAndPacked<'tcx>)> {\n+        // Skip the initial 0 intended for LLVM GEP.\n+        let mut packed = false;\n+        for field_index in path {\n+            let field_offset = self.get_field_offset(ty, field_index)?;\n+            trace!(\n+                \"field_path_offset_and_ty: {}, {}, {:?}, {:?}\",\n+                field_index,\n+                ty,\n+                field_offset,\n+                offset\n+            );\n+            let field_ty = self.get_field_ty(ty, field_index)?;\n+            ty = field_ty.ty;\n+            packed = packed || field_ty.packed;\n+            offset = offset\n+                .checked_add(field_offset, &self.tcx.data_layout)\n+                .unwrap();\n+        }\n+\n+        Ok((offset, TyAndPacked { ty, packed }))\n+    }\n+    fn get_fat_field(\n+        &self,\n+        pointee_ty: Ty<'tcx>,\n+        field_index: usize,\n+    ) -> EvalResult<'tcx, Ty<'tcx>> {\n+        match (field_index, &self.tcx.struct_tail(pointee_ty).sty) {\n+            (1, &ty::TyStr) |\n+            (1, &ty::TySlice(_)) => Ok(self.tcx.types.usize),\n+            (1, &ty::TyDynamic(..)) |\n+            (0, _) => Ok(self.tcx.mk_imm_ptr(self.tcx.types.u8)),\n+            _ => bug!(\"invalid fat pointee type: {}\", pointee_ty),\n+        }\n+    }\n+\n+    /// Returns the field type and whether the field is packed\n+    pub fn get_field_ty(\n+        &self,\n+        ty: Ty<'tcx>,\n+        field_index: usize,\n+    ) -> EvalResult<'tcx, TyAndPacked<'tcx>> {\n+        match ty.sty {\n+            ty::TyAdt(adt_def, _) if adt_def.is_box() => Ok(TyAndPacked {\n+                ty: self.get_fat_field(ty.boxed_ty(), field_index)?,\n+                packed: false,\n+            }),\n+            ty::TyAdt(adt_def, substs) if adt_def.is_enum() => {\n+                use rustc::ty::layout::Layout::*;\n+                match *self.type_layout(ty)? {\n+                    RawNullablePointer { nndiscr, .. } => Ok(TyAndPacked {\n+                        ty: adt_def.variants[nndiscr as usize].fields[field_index].ty(\n+                            self.tcx,\n+                            substs,\n+                        ),\n+                        packed: false,\n+                    }),\n+                    StructWrappedNullablePointer {\n+                        nndiscr,\n+                        ref nonnull,\n+                        ..\n+                    } => {\n+                        let ty = adt_def.variants[nndiscr as usize].fields[field_index].ty(\n+                            self.tcx,\n+                            substs,\n+                        );\n+                        Ok(TyAndPacked {\n+                            ty,\n+                            packed: nonnull.packed,\n+                        })\n+                    }\n+                    // mir optimizations treat single variant enums as structs\n+                    General { .. } if adt_def.variants.len() == 1 => Ok(TyAndPacked {\n+                        ty: adt_def.variants[0].fields[field_index].ty(self.tcx, substs),\n+                        packed: false,\n+                    }),\n+                    _ => {\n+                        err!(Unimplemented(format!(\n+                            \"get_field_ty can't handle enum type: {:?}, {:?}\",\n+                            ty,\n+                            ty.sty\n+                        )))\n+                    }\n+                }\n+            }\n+            ty::TyAdt(adt_def, substs) => {\n+                let variant_def = adt_def.struct_variant();\n+                use rustc::ty::layout::Layout::*;\n+                match *self.type_layout(ty)? {\n+                    UntaggedUnion { ref variants } => Ok(TyAndPacked {\n+                        ty: variant_def.fields[field_index].ty(self.tcx, substs),\n+                        packed: variants.packed,\n+                    }),\n+                    Univariant { ref variant, .. } => Ok(TyAndPacked {\n+                        ty: variant_def.fields[field_index].ty(self.tcx, substs),\n+                        packed: variant.packed,\n+                    }),\n+                    _ => {\n+                        err!(Unimplemented(format!(\n+                            \"get_field_ty can't handle struct type: {:?}, {:?}\",\n+                            ty,\n+                            ty.sty\n+                        )))\n+                    }\n+                }\n+            }\n+\n+            ty::TyTuple(fields, _) => Ok(TyAndPacked {\n+                ty: fields[field_index],\n+                packed: false,\n+            }),\n+\n+            ty::TyRef(_, ref tam) |\n+            ty::TyRawPtr(ref tam) => Ok(TyAndPacked {\n+                ty: self.get_fat_field(tam.ty, field_index)?,\n+                packed: false,\n+            }),\n+\n+            ty::TyArray(ref inner, _) => Ok(TyAndPacked {\n+                ty: inner,\n+                packed: false,\n+            }),\n+\n+            ty::TyClosure(def_id, ref closure_substs) => Ok(TyAndPacked {\n+                ty: closure_substs.upvar_tys(def_id, self.tcx).nth(field_index).unwrap(),\n+                packed: false,\n+            }),\n+\n+            _ => {\n+                err!(Unimplemented(\n+                    format!(\"can't handle type: {:?}, {:?}\", ty, ty.sty),\n+                ))\n+            }\n+        }\n+    }\n+\n+    fn get_field_offset(&self, ty: Ty<'tcx>, field_index: usize) -> EvalResult<'tcx, Size> {\n+        // Also see lvalue_field in lvalue.rs, which handles more cases but needs an actual value at the given type\n+        let layout = self.type_layout(ty)?;\n+\n+        use rustc::ty::layout::Layout::*;\n+        match *layout {\n+            Univariant { ref variant, .. } => Ok(variant.offsets[field_index]),\n+            FatPointer { .. } => {\n+                let bytes = field_index as u64 * self.memory.pointer_size();\n+                Ok(Size::from_bytes(bytes))\n+            }\n+            StructWrappedNullablePointer { ref nonnull, .. } => Ok(nonnull.offsets[field_index]),\n+            UntaggedUnion { .. } => Ok(Size::from_bytes(0)),\n+            // mir optimizations treat single variant enums as structs\n+            General { ref variants, .. } if variants.len() == 1 => Ok(variants[0].offsets[field_index]),\n+            _ => {\n+                let msg = format!(\n+                    \"get_field_offset: can't handle type: {:?}, with layout: {:?}\",\n+                    ty,\n+                    layout\n+                );\n+                err!(Unimplemented(msg))\n+            }\n+        }\n+    }\n+\n+    pub fn get_field_count(&self, ty: Ty<'tcx>) -> EvalResult<'tcx, u64> {\n+        let layout = self.type_layout(ty)?;\n+\n+        use rustc::ty::layout::Layout::*;\n+        match *layout {\n+            Univariant { ref variant, .. } => Ok(variant.offsets.len() as u64),\n+            FatPointer { .. } => Ok(2),\n+            StructWrappedNullablePointer { ref nonnull, .. } => Ok(nonnull.offsets.len() as u64),\n+            Vector { count, .. } |\n+            Array { count, .. } => Ok(count),\n+            Scalar { .. } => Ok(0),\n+            UntaggedUnion { .. } => Ok(1),\n+            _ => {\n+                let msg = format!(\n+                    \"get_field_count: can't handle type: {:?}, with layout: {:?}\",\n+                    ty,\n+                    layout\n+                );\n+                err!(Unimplemented(msg))\n+            }\n+        }\n+    }\n+\n+    pub(super) fn eval_operand_to_primval(\n+        &mut self,\n+        op: &mir::Operand<'tcx>,\n+    ) -> EvalResult<'tcx, PrimVal> {\n+        let valty = self.eval_operand(op)?;\n+        self.value_to_primval(valty)\n+    }\n+\n+    pub(crate) fn operands_to_args(\n+        &mut self,\n+        ops: &[mir::Operand<'tcx>],\n+    ) -> EvalResult<'tcx, Vec<ValTy<'tcx>>> {\n+        ops.into_iter()\n+            .map(|op| self.eval_operand(op))\n+            .collect()\n+    }\n+\n+    pub fn eval_operand(&mut self, op: &mir::Operand<'tcx>) -> EvalResult<'tcx, ValTy<'tcx>> {\n+        use rustc::mir::Operand::*;\n+        match *op {\n+            Consume(ref lvalue) => {\n+                Ok(ValTy {\n+                    value: self.eval_and_read_lvalue(lvalue)?,\n+                    ty: self.operand_ty(op),\n+                })\n+            },\n+\n+            Constant(ref constant) => {\n+                use rustc::mir::Literal;\n+                let mir::Constant { ref literal, .. } = **constant;\n+                let value = match *literal {\n+                    Literal::Value { ref value } => self.const_to_value(&value.val)?,\n+\n+                    Literal::Promoted { index } => {\n+                        let cid = GlobalId {\n+                            instance: self.frame().instance,\n+                            promoted: Some(index),\n+                        };\n+                        Value::ByRef(*self.globals.get(&cid).expect(\"promoted not cached\"))\n+                    }\n+                };\n+\n+                Ok(ValTy {\n+                    value,\n+                    ty: self.operand_ty(op),\n+                })\n+            }\n+        }\n+    }\n+\n+    pub fn read_discriminant_value(\n+        &self,\n+        adt_ptr: MemoryPointer,\n+        adt_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx, u128> {\n+        use rustc::ty::layout::Layout::*;\n+        let adt_layout = self.type_layout(adt_ty)?;\n+        //trace!(\"read_discriminant_value {:#?}\", adt_layout);\n+\n+        let discr_val = match *adt_layout {\n+            General { discr, .. } => {\n+                let discr_size = discr.size().bytes();\n+                self.memory.read_primval(adt_ptr, discr_size, false)?.to_bytes()?\n+            }\n+\n+            CEnum {\n+                discr,\n+                signed,\n+                ..\n+            } => {\n+                let discr_size = discr.size().bytes();\n+                self.memory.read_primval(adt_ptr, discr_size, signed)?.to_bytes()?\n+            }\n+\n+            RawNullablePointer { nndiscr, value } => {\n+                let discr_size = value.size(&self.tcx.data_layout).bytes();\n+                trace!(\"rawnullablepointer with size {}\", discr_size);\n+                self.read_nonnull_discriminant_value(\n+                    adt_ptr,\n+                    nndiscr as u128,\n+                    discr_size,\n+                )?\n+            }\n+\n+            StructWrappedNullablePointer {\n+                nndiscr,\n+                ref discrfield_source,\n+                ..\n+            } => {\n+                let (offset, TyAndPacked { ty, packed }) = self.nonnull_offset_and_ty(\n+                    adt_ty,\n+                    nndiscr,\n+                    discrfield_source,\n+                )?;\n+                let nonnull = adt_ptr.offset(offset.bytes(), &*self)?;\n+                trace!(\"struct wrapped nullable pointer type: {}\", ty);\n+                // only the pointer part of a fat pointer is used for this space optimization\n+                let discr_size = self.type_size(ty)?.expect(\n+                    \"bad StructWrappedNullablePointer discrfield\",\n+                );\n+                self.read_maybe_aligned(!packed, |ectx| {\n+                    ectx.read_nonnull_discriminant_value(nonnull, nndiscr as u128, discr_size)\n+                })?\n+            }\n+\n+            // The discriminant_value intrinsic returns 0 for non-sum types.\n+            Array { .. } |\n+            FatPointer { .. } |\n+            Scalar { .. } |\n+            Univariant { .. } |\n+            Vector { .. } |\n+            UntaggedUnion { .. } => 0,\n+        };\n+\n+        Ok(discr_val)\n+    }\n+\n+    fn read_nonnull_discriminant_value(\n+        &self,\n+        ptr: MemoryPointer,\n+        nndiscr: u128,\n+        discr_size: u64,\n+    ) -> EvalResult<'tcx, u128> {\n+        trace!(\n+            \"read_nonnull_discriminant_value: {:?}, {}, {}\",\n+            ptr,\n+            nndiscr,\n+            discr_size\n+        );\n+        // We are only interested in 0 vs. non-0, the sign does not matter for this\n+        let null = match self.memory.read_primval(ptr, discr_size, false)? {\n+            PrimVal::Bytes(0) => true,\n+            PrimVal::Bytes(_) |\n+            PrimVal::Ptr(..) => false,\n+            PrimVal::Undef => return err!(ReadUndefBytes),\n+        };\n+        assert!(nndiscr == 0 || nndiscr == 1);\n+        Ok(if !null { nndiscr } else { 1 - nndiscr })\n+    }\n+\n+    pub fn read_global_as_value(&self, gid: GlobalId) -> Value {\n+        Value::ByRef(*self.globals.get(&gid).expect(\"global not cached\"))\n+    }\n+\n+    pub fn operand_ty(&self, operand: &mir::Operand<'tcx>) -> Ty<'tcx> {\n+        self.monomorphize(operand.ty(self.mir(), self.tcx), self.substs())\n+    }\n+\n+    fn copy(&mut self, src: Pointer, dest: Pointer, ty: Ty<'tcx>) -> EvalResult<'tcx> {\n+        let size = self.type_size(ty)?.expect(\n+            \"cannot copy from an unsized type\",\n+        );\n+        let align = self.type_align(ty)?;\n+        self.memory.copy(src, dest, size, align, false)?;\n+        Ok(())\n+    }\n+\n+    pub fn is_packed(&self, ty: Ty<'tcx>) -> EvalResult<'tcx, bool> {\n+        let layout = self.type_layout(ty)?;\n+        use rustc::ty::layout::Layout::*;\n+        Ok(match *layout {\n+            Univariant { ref variant, .. } => variant.packed,\n+\n+            StructWrappedNullablePointer { ref nonnull, .. } => nonnull.packed,\n+\n+            UntaggedUnion { ref variants } => variants.packed,\n+\n+            // can only apply #[repr(packed)] to struct and union\n+            _ => false,\n+        })\n+    }\n+\n+    pub fn force_allocation(&mut self, lvalue: Lvalue) -> EvalResult<'tcx, Lvalue> {\n+        let new_lvalue = match lvalue {\n+            Lvalue::Local { frame, local } => {\n+                // -1 since we don't store the return value\n+                match self.stack[frame].locals[local.index() - 1] {\n+                    None => return err!(DeadLocal),\n+                    Some(Value::ByRef(ptr)) => {\n+                        Lvalue::Ptr {\n+                            ptr,\n+                            extra: LvalueExtra::None,\n+                        }\n+                    }\n+                    Some(val) => {\n+                        let ty = self.stack[frame].mir.local_decls[local].ty;\n+                        let ty = self.monomorphize(ty, self.stack[frame].instance.substs);\n+                        let substs = self.stack[frame].instance.substs;\n+                        let ptr = self.alloc_ptr_with_substs(ty, substs)?;\n+                        self.stack[frame].locals[local.index() - 1] =\n+                            Some(Value::by_ref(ptr.into())); // it stays live\n+                        self.write_value_to_ptr(val, ptr.into(), ty)?;\n+                        Lvalue::from_ptr(ptr)\n+                    }\n+                }\n+            }\n+            Lvalue::Ptr { .. } => lvalue,\n+        };\n+        Ok(new_lvalue)\n+    }\n+\n+    /// ensures this Value is not a ByRef\n+    pub(super) fn follow_by_ref_value(\n+        &self,\n+        value: Value,\n+        ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx, Value> {\n+        match value {\n+            Value::ByRef(PtrAndAlign { ptr, aligned }) => {\n+                self.read_maybe_aligned(aligned, |ectx| ectx.read_value(ptr, ty))\n+            }\n+            other => Ok(other),\n+        }\n+    }\n+\n+    pub fn value_to_primval(\n+        &self,\n+        ValTy { value, ty } : ValTy<'tcx>,\n+    ) -> EvalResult<'tcx, PrimVal> {\n+        match self.follow_by_ref_value(value, ty)? {\n+            Value::ByRef { .. } => bug!(\"follow_by_ref_value can't result in `ByRef`\"),\n+\n+            Value::ByVal(primval) => {\n+                // TODO: Do we really want insta-UB here?\n+                self.ensure_valid_value(primval, ty)?;\n+                Ok(primval)\n+            }\n+\n+            Value::ByValPair(..) => bug!(\"value_to_primval can't work with fat pointers\"),\n+        }\n+    }\n+\n+    pub fn write_null(&mut self, dest: Lvalue, dest_ty: Ty<'tcx>) -> EvalResult<'tcx> {\n+        self.write_primval(dest, PrimVal::Bytes(0), dest_ty)\n+    }\n+\n+    pub fn write_ptr(&mut self, dest: Lvalue, val: Pointer, dest_ty: Ty<'tcx>) -> EvalResult<'tcx> {\n+        let valty = ValTy {\n+            value: val.to_value(),\n+            ty: dest_ty,\n+        };\n+        self.write_value(valty, dest)\n+    }\n+\n+    pub fn write_primval(\n+        &mut self,\n+        dest: Lvalue,\n+        val: PrimVal,\n+        dest_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        let valty = ValTy {\n+            value: Value::ByVal(val),\n+            ty: dest_ty,\n+        };\n+        self.write_value(valty, dest)\n+    }\n+\n+    pub fn write_value(\n+        &mut self,\n+        ValTy { value: src_val, ty: dest_ty } : ValTy<'tcx>,\n+        dest: Lvalue,\n+    ) -> EvalResult<'tcx> {\n+        //trace!(\"Writing {:?} to {:?} at type {:?}\", src_val, dest, dest_ty);\n+        // Note that it is really important that the type here is the right one, and matches the type things are read at.\n+        // In case `src_val` is a `ByValPair`, we don't do any magic here to handle padding properly, which is only\n+        // correct if we never look at this data with the wrong type.\n+\n+        match dest {\n+            Lvalue::Ptr {\n+                ptr: PtrAndAlign { ptr, aligned },\n+                extra,\n+            } => {\n+                assert_eq!(extra, LvalueExtra::None);\n+                self.write_maybe_aligned_mut(\n+                    aligned,\n+                    |ectx| ectx.write_value_to_ptr(src_val, ptr, dest_ty),\n+                )\n+            }\n+\n+            Lvalue::Local { frame, local } => {\n+                let dest = self.stack[frame].get_local(local)?;\n+                self.write_value_possibly_by_val(\n+                    src_val,\n+                    |this, val| this.stack[frame].set_local(local, val),\n+                    dest,\n+                    dest_ty,\n+                )\n+            }\n+        }\n+    }\n+\n+    // The cases here can be a bit subtle. Read carefully!\n+    fn write_value_possibly_by_val<F: FnOnce(&mut Self, Value) -> EvalResult<'tcx>>(\n+        &mut self,\n+        src_val: Value,\n+        write_dest: F,\n+        old_dest_val: Value,\n+        dest_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        if let Value::ByRef(PtrAndAlign {\n+                                ptr: dest_ptr,\n+                                aligned,\n+                            }) = old_dest_val\n+        {\n+            // If the value is already `ByRef` (that is, backed by an `Allocation`),\n+            // then we must write the new value into this allocation, because there may be\n+            // other pointers into the allocation. These other pointers are logically\n+            // pointers into the local variable, and must be able to observe the change.\n+            //\n+            // Thus, it would be an error to replace the `ByRef` with a `ByVal`, unless we\n+            // knew for certain that there were no outstanding pointers to this allocation.\n+            self.write_maybe_aligned_mut(aligned, |ectx| {\n+                ectx.write_value_to_ptr(src_val, dest_ptr, dest_ty)\n+            })?;\n+\n+        } else if let Value::ByRef(PtrAndAlign {\n+                                       ptr: src_ptr,\n+                                       aligned,\n+                                   }) = src_val\n+        {\n+            // If the value is not `ByRef`, then we know there are no pointers to it\n+            // and we can simply overwrite the `Value` in the locals array directly.\n+            //\n+            // In this specific case, where the source value is `ByRef`, we must duplicate\n+            // the allocation, because this is a by-value operation. It would be incorrect\n+            // if they referred to the same allocation, since then a change to one would\n+            // implicitly change the other.\n+            //\n+            // It is a valid optimization to attempt reading a primitive value out of the\n+            // source and write that into the destination without making an allocation, so\n+            // we do so here.\n+            self.read_maybe_aligned_mut(aligned, |ectx| {\n+                if let Ok(Some(src_val)) = ectx.try_read_value(src_ptr, dest_ty) {\n+                    write_dest(ectx, src_val)?;\n+                } else {\n+                    let dest_ptr = ectx.alloc_ptr(dest_ty)?.into();\n+                    ectx.copy(src_ptr, dest_ptr, dest_ty)?;\n+                    write_dest(ectx, Value::by_ref(dest_ptr))?;\n+                }\n+                Ok(())\n+            })?;\n+\n+        } else {\n+            // Finally, we have the simple case where neither source nor destination are\n+            // `ByRef`. We may simply copy the source value over the the destintion.\n+            write_dest(self, src_val)?;\n+        }\n+        Ok(())\n+    }\n+\n+    pub fn write_value_to_ptr(\n+        &mut self,\n+        value: Value,\n+        dest: Pointer,\n+        dest_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        match value {\n+            Value::ByRef(PtrAndAlign { ptr, aligned }) => {\n+                self.read_maybe_aligned_mut(aligned, |ectx| ectx.copy(ptr, dest, dest_ty))\n+            }\n+            Value::ByVal(primval) => {\n+                let size = self.type_size(dest_ty)?.expect(\"dest type must be sized\");\n+                if size == 0 {\n+                    assert!(primval.is_undef());\n+                    Ok(())\n+                } else {\n+                    // TODO: Do we need signedness?\n+                    self.memory.write_primval(dest.to_ptr()?, primval, size, false)\n+                }\n+            }\n+            Value::ByValPair(a, b) => self.write_pair_to_ptr(a, b, dest.to_ptr()?, dest_ty),\n+        }\n+    }\n+\n+    pub fn write_pair_to_ptr(\n+        &mut self,\n+        a: PrimVal,\n+        b: PrimVal,\n+        ptr: MemoryPointer,\n+        mut ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        let mut packed = false;\n+        while self.get_field_count(ty)? == 1 {\n+            let field = self.get_field_ty(ty, 0)?;\n+            ty = field.ty;\n+            packed = packed || field.packed;\n+        }\n+        assert_eq!(self.get_field_count(ty)?, 2);\n+        let field_0 = self.get_field_offset(ty, 0)?;\n+        let field_1 = self.get_field_offset(ty, 1)?;\n+        let field_0_ty = self.get_field_ty(ty, 0)?;\n+        let field_1_ty = self.get_field_ty(ty, 1)?;\n+        assert_eq!(\n+            field_0_ty.packed,\n+            field_1_ty.packed,\n+            \"the two fields must agree on being packed\"\n+        );\n+        packed = packed || field_0_ty.packed;\n+        let field_0_size = self.type_size(field_0_ty.ty)?.expect(\n+            \"pair element type must be sized\",\n+        );\n+        let field_1_size = self.type_size(field_1_ty.ty)?.expect(\n+            \"pair element type must be sized\",\n+        );\n+        let field_0_ptr = ptr.offset(field_0.bytes(), &self)?.into();\n+        let field_1_ptr = ptr.offset(field_1.bytes(), &self)?.into();\n+        // TODO: What about signedess?\n+        self.write_maybe_aligned_mut(!packed, |ectx| {\n+            ectx.memory.write_primval(field_0_ptr, a, field_0_size, false)\n+        })?;\n+        self.write_maybe_aligned_mut(!packed, |ectx| {\n+            ectx.memory.write_primval(field_1_ptr, b, field_1_size, false)\n+        })?;\n+        Ok(())\n+    }\n+\n+    pub fn ty_to_primval_kind(&self, ty: Ty<'tcx>) -> EvalResult<'tcx, PrimValKind> {\n+        use syntax::ast::FloatTy;\n+\n+        let kind = match ty.sty {\n+            ty::TyBool => PrimValKind::Bool,\n+            ty::TyChar => PrimValKind::Char,\n+\n+            ty::TyInt(int_ty) => {\n+                use syntax::ast::IntTy::*;\n+                let size = match int_ty {\n+                    I8 => 1,\n+                    I16 => 2,\n+                    I32 => 4,\n+                    I64 => 8,\n+                    I128 => 16,\n+                    Is => self.memory.pointer_size(),\n+                };\n+                PrimValKind::from_int_size(size)\n+            }\n+\n+            ty::TyUint(uint_ty) => {\n+                use syntax::ast::UintTy::*;\n+                let size = match uint_ty {\n+                    U8 => 1,\n+                    U16 => 2,\n+                    U32 => 4,\n+                    U64 => 8,\n+                    U128 => 16,\n+                    Us => self.memory.pointer_size(),\n+                };\n+                PrimValKind::from_uint_size(size)\n+            }\n+\n+            ty::TyFloat(FloatTy::F32) => PrimValKind::F32,\n+            ty::TyFloat(FloatTy::F64) => PrimValKind::F64,\n+\n+            ty::TyFnPtr(_) => PrimValKind::FnPtr,\n+\n+            ty::TyRef(_, ref tam) |\n+            ty::TyRawPtr(ref tam) if self.type_is_sized(tam.ty) => PrimValKind::Ptr,\n+\n+            ty::TyAdt(def, _) if def.is_box() => PrimValKind::Ptr,\n+\n+            ty::TyAdt(def, substs) => {\n+                use rustc::ty::layout::Layout::*;\n+                match *self.type_layout(ty)? {\n+                    CEnum { discr, signed, .. } => {\n+                        let size = discr.size().bytes();\n+                        if signed {\n+                            PrimValKind::from_int_size(size)\n+                        } else {\n+                            PrimValKind::from_uint_size(size)\n+                        }\n+                    }\n+\n+                    RawNullablePointer { value, .. } => {\n+                        use rustc::ty::layout::Primitive::*;\n+                        match value {\n+                            // TODO(solson): Does signedness matter here? What should the sign be?\n+                            Int(int) => PrimValKind::from_uint_size(int.size().bytes()),\n+                            F32 => PrimValKind::F32,\n+                            F64 => PrimValKind::F64,\n+                            Pointer => PrimValKind::Ptr,\n+                        }\n+                    }\n+\n+                    // represent single field structs as their single field\n+                    Univariant { .. } => {\n+                        // enums with just one variant are no different, but `.struct_variant()` doesn't work for enums\n+                        let variant = &def.variants[0];\n+                        // FIXME: also allow structs with only a single non zst field\n+                        if variant.fields.len() == 1 {\n+                            return self.ty_to_primval_kind(variant.fields[0].ty(self.tcx, substs));\n+                        } else {\n+                            return err!(TypeNotPrimitive(ty));\n+                        }\n+                    }\n+\n+                    _ => return err!(TypeNotPrimitive(ty)),\n+                }\n+            }\n+\n+            _ => return err!(TypeNotPrimitive(ty)),\n+        };\n+\n+        Ok(kind)\n+    }\n+\n+    fn ensure_valid_value(&self, val: PrimVal, ty: Ty<'tcx>) -> EvalResult<'tcx> {\n+        match ty.sty {\n+            ty::TyBool if val.to_bytes()? > 1 => err!(InvalidBool),\n+\n+            ty::TyChar if ::std::char::from_u32(val.to_bytes()? as u32).is_none() => {\n+                err!(InvalidChar(val.to_bytes()? as u32 as u128))\n+            }\n+\n+            _ => Ok(()),\n+        }\n+    }\n+\n+    pub fn read_value(&self, ptr: Pointer, ty: Ty<'tcx>) -> EvalResult<'tcx, Value> {\n+        if let Some(val) = self.try_read_value(ptr, ty)? {\n+            Ok(val)\n+        } else {\n+            bug!(\"primitive read failed for type: {:?}\", ty);\n+        }\n+    }\n+\n+    pub(crate) fn read_ptr(\n+        &self,\n+        ptr: MemoryPointer,\n+        pointee_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx, Value> {\n+        let ptr_size = self.memory.pointer_size();\n+        let p : Pointer = self.memory.read_ptr_sized_unsigned(ptr)?.into();\n+        if self.type_is_sized(pointee_ty) {\n+            Ok(p.to_value())\n+        } else {\n+            trace!(\"reading fat pointer extra of type {}\", pointee_ty);\n+            let extra = ptr.offset(ptr_size, self)?;\n+            match self.tcx.struct_tail(pointee_ty).sty {\n+                ty::TyDynamic(..) => Ok(p.to_value_with_vtable(\n+                    self.memory.read_ptr_sized_unsigned(extra)?.to_ptr()?,\n+                )),\n+                ty::TySlice(..) | ty::TyStr => Ok(\n+                    p.to_value_with_len(self.memory.read_ptr_sized_unsigned(extra)?.to_bytes()? as u64),\n+                ),\n+                _ => bug!(\"unsized primval ptr read from {:?}\", pointee_ty),\n+            }\n+        }\n+    }\n+\n+    fn try_read_value(&self, ptr: Pointer, ty: Ty<'tcx>) -> EvalResult<'tcx, Option<Value>> {\n+        use syntax::ast::FloatTy;\n+\n+        let ptr = ptr.to_ptr()?;\n+        let val = match ty.sty {\n+            ty::TyBool => {\n+                let val = self.memory.read_primval(ptr, 1, false)?;\n+                let val = match val {\n+                    PrimVal::Bytes(0) => false,\n+                    PrimVal::Bytes(1) => true,\n+                    // TODO: This seems a little overeager, should reading at bool type already be insta-UB?\n+                    _ => return err!(InvalidBool),\n+                };\n+                PrimVal::from_bool(val)\n+            }\n+            ty::TyChar => {\n+                let c = self.memory.read_primval(ptr, 4, false)?.to_bytes()? as u32;\n+                match ::std::char::from_u32(c) {\n+                    Some(ch) => PrimVal::from_char(ch),\n+                    None => return err!(InvalidChar(c as u128)),\n+                }\n+            }\n+\n+            ty::TyInt(int_ty) => {\n+                use syntax::ast::IntTy::*;\n+                let size = match int_ty {\n+                    I8 => 1,\n+                    I16 => 2,\n+                    I32 => 4,\n+                    I64 => 8,\n+                    I128 => 16,\n+                    Is => self.memory.pointer_size(),\n+                };\n+                self.memory.read_primval(ptr, size, true)?\n+            }\n+\n+            ty::TyUint(uint_ty) => {\n+                use syntax::ast::UintTy::*;\n+                let size = match uint_ty {\n+                    U8 => 1,\n+                    U16 => 2,\n+                    U32 => 4,\n+                    U64 => 8,\n+                    U128 => 16,\n+                    Us => self.memory.pointer_size(),\n+                };\n+                self.memory.read_primval(ptr, size, false)?\n+            }\n+\n+            ty::TyFloat(FloatTy::F32) => PrimVal::Bytes(self.memory.read_primval(ptr, 4, false)?.to_bytes()?),\n+            ty::TyFloat(FloatTy::F64) => PrimVal::Bytes(self.memory.read_primval(ptr, 8, false)?.to_bytes()?),\n+\n+            ty::TyFnPtr(_) => self.memory.read_ptr_sized_unsigned(ptr)?,\n+            ty::TyRef(_, ref tam) |\n+            ty::TyRawPtr(ref tam) => return self.read_ptr(ptr, tam.ty).map(Some),\n+\n+            ty::TyAdt(def, _) => {\n+                if def.is_box() {\n+                    return self.read_ptr(ptr, ty.boxed_ty()).map(Some);\n+                }\n+                use rustc::ty::layout::Layout::*;\n+                if let CEnum { discr, signed, .. } = *self.type_layout(ty)? {\n+                    let size = discr.size().bytes();\n+                    self.memory.read_primval(ptr, size, signed)?\n+                } else {\n+                    return Ok(None);\n+                }\n+            }\n+\n+            _ => return Ok(None),\n+        };\n+\n+        Ok(Some(Value::ByVal(val)))\n+    }\n+\n+    pub fn frame(&self) -> &Frame<'tcx> {\n+        self.stack.last().expect(\"no call frames exist\")\n+    }\n+\n+    pub(super) fn frame_mut(&mut self) -> &mut Frame<'tcx> {\n+        self.stack.last_mut().expect(\"no call frames exist\")\n+    }\n+\n+    pub(super) fn mir(&self) -> &'tcx mir::Mir<'tcx> {\n+        self.frame().mir\n+    }\n+\n+    pub(super) fn substs(&self) -> &'tcx Substs<'tcx> {\n+        self.frame().instance.substs\n+    }\n+\n+    fn unsize_into_ptr(\n+        &mut self,\n+        src: Value,\n+        src_ty: Ty<'tcx>,\n+        dest: Lvalue,\n+        dest_ty: Ty<'tcx>,\n+        sty: Ty<'tcx>,\n+        dty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        // A<Struct> -> A<Trait> conversion\n+        let (src_pointee_ty, dest_pointee_ty) = self.tcx.struct_lockstep_tails(sty, dty);\n+\n+        match (&src_pointee_ty.sty, &dest_pointee_ty.sty) {\n+            (&ty::TyArray(_, length), &ty::TySlice(_)) => {\n+                let ptr = src.into_ptr(&self.memory)?;\n+                // u64 cast is from usize to u64, which is always good\n+                let valty = ValTy {\n+                    value: ptr.to_value_with_len(length.val.to_const_int().unwrap().to_u64().unwrap() ),\n+                    ty: dest_ty,\n+                };\n+                self.write_value(valty, dest)\n+            }\n+            (&ty::TyDynamic(..), &ty::TyDynamic(..)) => {\n+                // For now, upcasts are limited to changes in marker\n+                // traits, and hence never actually require an actual\n+                // change to the vtable.\n+                let valty = ValTy {\n+                    value: src,\n+                    ty: dest_ty,\n+                };\n+                self.write_value(valty, dest)\n+            }\n+            (_, &ty::TyDynamic(ref data, _)) => {\n+                let trait_ref = data.principal().unwrap().with_self_ty(\n+                    self.tcx,\n+                    src_pointee_ty,\n+                );\n+                let trait_ref = self.tcx.erase_regions(&trait_ref);\n+                let vtable = self.get_vtable(src_pointee_ty, trait_ref)?;\n+                let ptr = src.into_ptr(&self.memory)?;\n+                let valty = ValTy {\n+                    value: ptr.to_value_with_vtable(vtable),\n+                    ty: dest_ty,\n+                };\n+                self.write_value(valty, dest)\n+            }\n+\n+            _ => bug!(\"invalid unsizing {:?} -> {:?}\", src_ty, dest_ty),\n+        }\n+    }\n+\n+    fn unsize_into(\n+        &mut self,\n+        src: Value,\n+        src_ty: Ty<'tcx>,\n+        dest: Lvalue,\n+        dest_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        match (&src_ty.sty, &dest_ty.sty) {\n+            (&ty::TyRef(_, ref s), &ty::TyRef(_, ref d)) |\n+            (&ty::TyRef(_, ref s), &ty::TyRawPtr(ref d)) |\n+            (&ty::TyRawPtr(ref s), &ty::TyRawPtr(ref d)) => {\n+                self.unsize_into_ptr(src, src_ty, dest, dest_ty, s.ty, d.ty)\n+            }\n+            (&ty::TyAdt(def_a, substs_a), &ty::TyAdt(def_b, substs_b)) => {\n+                if def_a.is_box() || def_b.is_box() {\n+                    if !def_a.is_box() || !def_b.is_box() {\n+                        panic!(\"invalid unsizing between {:?} -> {:?}\", src_ty, dest_ty);\n+                    }\n+                    return self.unsize_into_ptr(\n+                        src,\n+                        src_ty,\n+                        dest,\n+                        dest_ty,\n+                        src_ty.boxed_ty(),\n+                        dest_ty.boxed_ty(),\n+                    );\n+                }\n+                if self.ty_to_primval_kind(src_ty).is_ok() {\n+                    // TODO: We ignore the packed flag here\n+                    let sty = self.get_field_ty(src_ty, 0)?.ty;\n+                    let dty = self.get_field_ty(dest_ty, 0)?.ty;\n+                    return self.unsize_into(src, sty, dest, dty);\n+                }\n+                // unsizing of generic struct with pointer fields\n+                // Example: `Arc<T>` -> `Arc<Trait>`\n+                // here we need to increase the size of every &T thin ptr field to a fat ptr\n+\n+                assert_eq!(def_a, def_b);\n+\n+                let src_fields = def_a.variants[0].fields.iter();\n+                let dst_fields = def_b.variants[0].fields.iter();\n+\n+                //let src = adt::MaybeSizedValue::sized(src);\n+                //let dst = adt::MaybeSizedValue::sized(dst);\n+                let src_ptr = match src {\n+                    Value::ByRef(PtrAndAlign { ptr, aligned: true }) => ptr,\n+                    // TODO: Is it possible for unaligned pointers to occur here?\n+                    _ => bug!(\"expected aligned pointer, got {:?}\", src),\n+                };\n+\n+                // FIXME(solson)\n+                let dest = self.force_allocation(dest)?.to_ptr()?;\n+                let iter = src_fields.zip(dst_fields).enumerate();\n+                for (i, (src_f, dst_f)) in iter {\n+                    let src_fty = self.field_ty(substs_a, src_f);\n+                    let dst_fty = self.field_ty(substs_b, dst_f);\n+                    if self.type_size(dst_fty)? == Some(0) {\n+                        continue;\n+                    }\n+                    let src_field_offset = self.get_field_offset(src_ty, i)?.bytes();\n+                    let dst_field_offset = self.get_field_offset(dest_ty, i)?.bytes();\n+                    let src_f_ptr = src_ptr.offset(src_field_offset, &self)?;\n+                    let dst_f_ptr = dest.offset(dst_field_offset, &self)?;\n+                    if src_fty == dst_fty {\n+                        self.copy(src_f_ptr, dst_f_ptr.into(), src_fty)?;\n+                    } else {\n+                        self.unsize_into(\n+                            Value::by_ref(src_f_ptr),\n+                            src_fty,\n+                            Lvalue::from_ptr(dst_f_ptr),\n+                            dst_fty,\n+                        )?;\n+                    }\n+                }\n+                Ok(())\n+            }\n+            _ => {\n+                bug!(\n+                    \"unsize_into: invalid conversion: {:?} -> {:?}\",\n+                    src_ty,\n+                    dest_ty\n+                )\n+            }\n+        }\n+    }\n+\n+    pub fn dump_local(&self, lvalue: Lvalue) {\n+        // Debug output\n+        match lvalue {\n+            Lvalue::Local { frame, local } => {\n+                let mut allocs = Vec::new();\n+                let mut msg = format!(\"{:?}\", local);\n+                if frame != self.cur_frame() {\n+                    write!(msg, \" ({} frames up)\", self.cur_frame() - frame).unwrap();\n+                }\n+                write!(msg, \":\").unwrap();\n+\n+                match self.stack[frame].get_local(local) {\n+                    Err(EvalError { kind: EvalErrorKind::DeadLocal, .. }) => {\n+                        write!(msg, \" is dead\").unwrap();\n+                    }\n+                    Err(err) => {\n+                        panic!(\"Failed to access local: {:?}\", err);\n+                    }\n+                    Ok(Value::ByRef(PtrAndAlign { ptr, aligned })) => {\n+                        match ptr.into_inner_primval() {\n+                            PrimVal::Ptr(ptr) => {\n+                                write!(msg, \" by {}ref:\", if aligned { \"\" } else { \"unaligned \" })\n+                                    .unwrap();\n+                                allocs.push(ptr.alloc_id);\n+                            }\n+                            ptr => write!(msg, \" integral by ref: {:?}\", ptr).unwrap(),\n+                        }\n+                    }\n+                    Ok(Value::ByVal(val)) => {\n+                        write!(msg, \" {:?}\", val).unwrap();\n+                        if let PrimVal::Ptr(ptr) = val {\n+                            allocs.push(ptr.alloc_id);\n+                        }\n+                    }\n+                    Ok(Value::ByValPair(val1, val2)) => {\n+                        write!(msg, \" ({:?}, {:?})\", val1, val2).unwrap();\n+                        if let PrimVal::Ptr(ptr) = val1 {\n+                            allocs.push(ptr.alloc_id);\n+                        }\n+                        if let PrimVal::Ptr(ptr) = val2 {\n+                            allocs.push(ptr.alloc_id);\n+                        }\n+                    }\n+                }\n+\n+                trace!(\"{}\", msg);\n+                self.memory.dump_allocs(allocs);\n+            }\n+            Lvalue::Ptr { ptr: PtrAndAlign { ptr, aligned }, .. } => {\n+                match ptr.into_inner_primval() {\n+                    PrimVal::Ptr(ptr) => {\n+                        trace!(\"by {}ref:\", if aligned { \"\" } else { \"unaligned \" });\n+                        self.memory.dump_alloc(ptr.alloc_id);\n+                    }\n+                    ptr => trace!(\" integral by ref: {:?}\", ptr),\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Convenience function to ensure correct usage of locals\n+    pub fn modify_local<F>(&mut self, frame: usize, local: mir::Local, f: F) -> EvalResult<'tcx>\n+    where\n+        F: FnOnce(&mut Self, Value) -> EvalResult<'tcx, Value>,\n+    {\n+        let val = self.stack[frame].get_local(local)?;\n+        let new_val = f(self, val)?;\n+        self.stack[frame].set_local(local, new_val)?;\n+        // FIXME(solson): Run this when setting to Undef? (See previous version of this code.)\n+        // if let Value::ByRef(ptr) = self.stack[frame].get_local(local) {\n+        //     self.memory.deallocate(ptr)?;\n+        // }\n+        Ok(())\n+    }\n+\n+    pub fn report(&self, e: &mut EvalError) {\n+        if let Some(ref mut backtrace) = e.backtrace {\n+            let mut trace_text = \"\\n\\nAn error occurred in miri:\\n\".to_string();\n+            let mut skip_init = true;\n+            backtrace.resolve();\n+            'frames: for (i, frame) in backtrace.frames().iter().enumerate() {\n+                for symbol in frame.symbols() {\n+                    if let Some(name) = symbol.name() {\n+                        // unmangle the symbol via `to_string`\n+                        let name = name.to_string();\n+                        if name.starts_with(\"miri::after_analysis\") {\n+                            // don't report initialization gibberish\n+                            break 'frames;\n+                        } else if name.starts_with(\"backtrace::capture::Backtrace::new\")\n+                            // debug mode produces funky symbol names\n+                            || name.starts_with(\"backtrace::capture::{{impl}}::new\")\n+                        {\n+                            // don't report backtrace internals\n+                            skip_init = false;\n+                            continue 'frames;\n+                        }\n+                    }\n+                }\n+                if skip_init {\n+                    continue;\n+                }\n+                for symbol in frame.symbols() {\n+                    write!(trace_text, \"{}: \", i).unwrap();\n+                    if let Some(name) = symbol.name() {\n+                        write!(trace_text, \"{}\\n\", name).unwrap();\n+                    } else {\n+                        write!(trace_text, \"<unknown>\\n\").unwrap();\n+                    }\n+                    write!(trace_text, \"\\tat \").unwrap();\n+                    if let Some(file_path) = symbol.filename() {\n+                        write!(trace_text, \"{}\", file_path.display()).unwrap();\n+                    } else {\n+                        write!(trace_text, \"<unknown_file>\").unwrap();\n+                    }\n+                    if let Some(line) = symbol.lineno() {\n+                        write!(trace_text, \":{}\\n\", line).unwrap();\n+                    } else {\n+                        write!(trace_text, \"\\n\").unwrap();\n+                    }\n+                }\n+            }\n+            error!(\"{}\", trace_text);\n+        }\n+        if let Some(frame) = self.stack().last() {\n+            let block = &frame.mir.basic_blocks()[frame.block];\n+            let span = if frame.stmt < block.statements.len() {\n+                block.statements[frame.stmt].source_info.span\n+            } else {\n+                block.terminator().source_info.span\n+            };\n+            let mut err = self.tcx.sess.struct_span_err(span, &e.to_string());\n+            for &Frame { instance, span, .. } in self.stack().iter().rev() {\n+                if self.tcx.def_key(instance.def_id()).disambiguated_data.data ==\n+                    DefPathData::ClosureExpr\n+                {\n+                    err.span_note(span, \"inside call to closure\");\n+                    continue;\n+                }\n+                err.span_note(span, &format!(\"inside call to {}\", instance));\n+            }\n+            err.emit();\n+        } else {\n+            self.tcx.sess.err(&e.to_string());\n+        }\n+    }\n+}\n+\n+impl<'tcx> Frame<'tcx> {\n+    pub fn get_local(&self, local: mir::Local) -> EvalResult<'tcx, Value> {\n+        // Subtract 1 because we don't store a value for the ReturnPointer, the local with index 0.\n+        self.locals[local.index() - 1].ok_or(EvalErrorKind::DeadLocal.into())\n+    }\n+\n+    fn set_local(&mut self, local: mir::Local, value: Value) -> EvalResult<'tcx> {\n+        // Subtract 1 because we don't store a value for the ReturnPointer, the local with index 0.\n+        match self.locals[local.index() - 1] {\n+            None => err!(DeadLocal),\n+            Some(ref mut local) => {\n+                *local = value;\n+                Ok(())\n+            }\n+        }\n+    }\n+\n+    pub fn storage_live(&mut self, local: mir::Local) -> EvalResult<'tcx, Option<Value>> {\n+        trace!(\"{:?} is now live\", local);\n+\n+        let old = self.locals[local.index() - 1];\n+        self.locals[local.index() - 1] = Some(Value::ByVal(PrimVal::Undef)); // StorageLive *always* kills the value that's currently stored\n+        return Ok(old);\n+    }\n+\n+    /// Returns the old value of the local\n+    pub fn storage_dead(&mut self, local: mir::Local) -> EvalResult<'tcx, Option<Value>> {\n+        trace!(\"{:?} is now dead\", local);\n+\n+        let old = self.locals[local.index() - 1];\n+        self.locals[local.index() - 1] = None;\n+        return Ok(old);\n+    }\n+}\n+\n+// TODO(solson): Upstream these methods into rustc::ty::layout.\n+\n+pub(super) trait IntegerExt {\n+    fn size(self) -> Size;\n+}\n+\n+impl IntegerExt for layout::Integer {\n+    fn size(self) -> Size {\n+        use rustc::ty::layout::Integer::*;\n+        match self {\n+            I1 | I8 => Size::from_bits(8),\n+            I16 => Size::from_bits(16),\n+            I32 => Size::from_bits(32),\n+            I64 => Size::from_bits(64),\n+            I128 => Size::from_bits(128),\n+        }\n+    }\n+}\n+\n+/// FIXME: expose trans::monomorphize::resolve_closure\n+pub fn resolve_closure<'a, 'tcx>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    def_id: DefId,\n+    substs: ty::ClosureSubsts<'tcx>,\n+    requested_kind: ty::ClosureKind,\n+) -> ty::Instance<'tcx> {\n+    let actual_kind = tcx.closure_kind(def_id);\n+    match needs_fn_once_adapter_shim(actual_kind, requested_kind) {\n+        Ok(true) => fn_once_adapter_instance(tcx, def_id, substs),\n+        _ => ty::Instance::new(def_id, substs.substs),\n+    }\n+}\n+\n+fn fn_once_adapter_instance<'a, 'tcx>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    closure_did: DefId,\n+    substs: ty::ClosureSubsts<'tcx>,\n+) -> ty::Instance<'tcx> {\n+    debug!(\"fn_once_adapter_shim({:?}, {:?})\", closure_did, substs);\n+    let fn_once = tcx.lang_items().fn_once_trait().unwrap();\n+    let call_once = tcx.associated_items(fn_once)\n+        .find(|it| it.kind == ty::AssociatedKind::Method)\n+        .unwrap()\n+        .def_id;\n+    let def = ty::InstanceDef::ClosureOnceShim { call_once };\n+\n+    let self_ty = tcx.mk_closure_from_closure_substs(closure_did, substs);\n+\n+    let sig = tcx.fn_sig(closure_did).subst(tcx, substs.substs);\n+    let sig = tcx.erase_late_bound_regions_and_normalize(&sig);\n+    assert_eq!(sig.inputs().len(), 1);\n+    let substs = tcx.mk_substs(\n+        [Kind::from(self_ty), Kind::from(sig.inputs()[0])]\n+            .iter()\n+            .cloned(),\n+    );\n+\n+    debug!(\"fn_once_adapter_shim: self_ty={:?} sig={:?}\", self_ty, sig);\n+    ty::Instance { def, substs }\n+}\n+\n+fn needs_fn_once_adapter_shim(\n+    actual_closure_kind: ty::ClosureKind,\n+    trait_closure_kind: ty::ClosureKind,\n+) -> Result<bool, ()> {\n+    match (actual_closure_kind, trait_closure_kind) {\n+        (ty::ClosureKind::Fn, ty::ClosureKind::Fn) |\n+        (ty::ClosureKind::FnMut, ty::ClosureKind::FnMut) |\n+        (ty::ClosureKind::FnOnce, ty::ClosureKind::FnOnce) => {\n+            // No adapter needed.\n+            Ok(false)\n+        }\n+        (ty::ClosureKind::Fn, ty::ClosureKind::FnMut) => {\n+            // The closure fn `llfn` is a `fn(&self, ...)`.  We want a\n+            // `fn(&mut self, ...)`. In fact, at trans time, these are\n+            // basically the same thing, so we can just return llfn.\n+            Ok(false)\n+        }\n+        (ty::ClosureKind::Fn, ty::ClosureKind::FnOnce) |\n+        (ty::ClosureKind::FnMut, ty::ClosureKind::FnOnce) => {\n+            // The closure fn `llfn` is a `fn(&self, ...)` or `fn(&mut\n+            // self, ...)`.  We want a `fn(self, ...)`. We can produce\n+            // this by doing something like:\n+            //\n+            //     fn call_once(self, ...) { call_mut(&self, ...) }\n+            //     fn call_once(mut self, ...) { call_mut(&mut self, ...) }\n+            //\n+            // These are both the same at trans time.\n+            Ok(true)\n+        }\n+        _ => Err(()),\n+    }\n+}\n+\n+/// The point where linking happens. Resolve a (def_id, substs)\n+/// pair to an instance.\n+pub fn resolve<'a, 'tcx>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    def_id: DefId,\n+    substs: &'tcx Substs<'tcx>,\n+) -> ty::Instance<'tcx> {\n+    debug!(\"resolve(def_id={:?}, substs={:?})\", def_id, substs);\n+    let result = if let Some(trait_def_id) = tcx.trait_of_item(def_id) {\n+        debug!(\" => associated item, attempting to find impl\");\n+        let item = tcx.associated_item(def_id);\n+        resolve_associated_item(tcx, &item, trait_def_id, substs)\n+    } else {\n+        let item_type = def_ty(tcx, def_id, substs);\n+        let def = match item_type.sty {\n+            ty::TyFnDef(..)\n+                if {\n+                       let f = item_type.fn_sig(tcx);\n+                       f.abi() == Abi::RustIntrinsic || f.abi() == Abi::PlatformIntrinsic\n+                   } => {\n+                debug!(\" => intrinsic\");\n+                ty::InstanceDef::Intrinsic(def_id)\n+            }\n+            _ => {\n+                if Some(def_id) == tcx.lang_items().drop_in_place_fn() {\n+                    let ty = substs.type_at(0);\n+                    if needs_drop_glue(tcx, ty) {\n+                        debug!(\" => nontrivial drop glue\");\n+                        ty::InstanceDef::DropGlue(def_id, Some(ty))\n+                    } else {\n+                        debug!(\" => trivial drop glue\");\n+                        ty::InstanceDef::DropGlue(def_id, None)\n+                    }\n+                } else {\n+                    debug!(\" => free item\");\n+                    ty::InstanceDef::Item(def_id)\n+                }\n+            }\n+        };\n+        ty::Instance { def, substs }\n+    };\n+    debug!(\n+        \"resolve(def_id={:?}, substs={:?}) = {}\",\n+        def_id,\n+        substs,\n+        result\n+    );\n+    result\n+}\n+\n+pub fn needs_drop_glue<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>, t: Ty<'tcx>) -> bool {\n+    assert!(t.is_normalized_for_trans());\n+\n+    let t = tcx.erase_regions(&t);\n+\n+    // FIXME (#22815): note that type_needs_drop conservatively\n+    // approximates in some cases and may say a type expression\n+    // requires drop glue when it actually does not.\n+    //\n+    // (In this case it is not clear whether any harm is done, i.e.\n+    // erroneously returning `true` in some cases where we could have\n+    // returned `false` does not appear unsound. The impact on\n+    // code quality is unknown at this time.)\n+\n+    let env = ty::ParamEnv::empty(Reveal::All);\n+    if !t.needs_drop(tcx, env) {\n+        return false;\n+    }\n+    match t.sty {\n+        ty::TyAdt(def, _) if def.is_box() => {\n+            let typ = t.boxed_ty();\n+            if !typ.needs_drop(tcx, env) && type_is_sized(tcx, typ) {\n+                let layout = t.layout(tcx, ty::ParamEnv::empty(Reveal::All)).unwrap();\n+                // `Box<ZeroSizeType>` does not allocate.\n+                layout.size(&tcx.data_layout).bytes() != 0\n+            } else {\n+                true\n+            }\n+        }\n+        _ => true,\n+    }\n+}\n+\n+fn resolve_associated_item<'a, 'tcx>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    trait_item: &ty::AssociatedItem,\n+    trait_id: DefId,\n+    rcvr_substs: &'tcx Substs<'tcx>,\n+) -> ty::Instance<'tcx> {\n+    let def_id = trait_item.def_id;\n+    debug!(\n+        \"resolve_associated_item(trait_item={:?}, \\\n+                                    trait_id={:?}, \\\n+                                    rcvr_substs={:?})\",\n+        def_id,\n+        trait_id,\n+        rcvr_substs\n+    );\n+\n+    let trait_ref = ty::TraitRef::from_method(tcx, trait_id, rcvr_substs);\n+    let vtbl = tcx.trans_fulfill_obligation(DUMMY_SP, ty::Binder(trait_ref));\n+\n+    // Now that we know which impl is being used, we can dispatch to\n+    // the actual function:\n+    match vtbl {\n+        ::rustc::traits::VtableImpl(impl_data) => {\n+            let (def_id, substs) =\n+                ::rustc::traits::find_associated_item(tcx, trait_item, rcvr_substs, &impl_data);\n+            let substs = tcx.erase_regions(&substs);\n+            ty::Instance::new(def_id, substs)\n+        }\n+        ::rustc::traits::VtableGenerator(closure_data) => {\n+            ty::Instance {\n+                def: ty::InstanceDef::Item(closure_data.closure_def_id),\n+                substs: closure_data.substs.substs\n+            }\n+        }\n+        ::rustc::traits::VtableClosure(closure_data) => {\n+            let trait_closure_kind = tcx.lang_items().fn_trait_kind(trait_id).unwrap();\n+            resolve_closure(\n+                tcx,\n+                closure_data.closure_def_id,\n+                closure_data.substs,\n+                trait_closure_kind,\n+            )\n+        }\n+        ::rustc::traits::VtableFnPointer(ref data) => {\n+            ty::Instance {\n+                def: ty::InstanceDef::FnPtrShim(trait_item.def_id, data.fn_ty),\n+                substs: rcvr_substs,\n+            }\n+        }\n+        ::rustc::traits::VtableObject(ref data) => {\n+            let index = tcx.get_vtable_index_of_object_method(data, def_id);\n+            ty::Instance {\n+                def: ty::InstanceDef::Virtual(def_id, index),\n+                substs: rcvr_substs,\n+            }\n+        }\n+        ::rustc::traits::VtableBuiltin(..) if Some(trait_id) == tcx.lang_items().clone_trait() => {\n+            ty::Instance {\n+                def: ty::InstanceDef::CloneShim(def_id, trait_ref.self_ty()),\n+                substs: rcvr_substs\n+            }\n+        }\n+        _ => bug!(\"static call to invalid vtable: {:?}\", vtbl),\n+    }\n+}\n+\n+pub fn def_ty<'a, 'tcx>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    def_id: DefId,\n+    substs: &'tcx Substs<'tcx>,\n+) -> Ty<'tcx> {\n+    let ty = tcx.type_of(def_id);\n+    apply_param_substs(tcx, substs, &ty)\n+}\n+\n+/// Monomorphizes a type from the AST by first applying the in-scope\n+/// substitutions and then normalizing any associated types.\n+pub fn apply_param_substs<'a, 'tcx, T>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    param_substs: &Substs<'tcx>,\n+    value: &T,\n+) -> T\n+where\n+    T: ::rustc::infer::TransNormalize<'tcx>,\n+{\n+    debug!(\n+        \"apply_param_substs(param_substs={:?}, value={:?})\",\n+        param_substs,\n+        value\n+    );\n+    let substituted = value.subst(tcx, param_substs);\n+    let substituted = tcx.erase_regions(&substituted);\n+    AssociatedTypeNormalizer { tcx }.fold(&substituted)\n+}\n+\n+\n+struct AssociatedTypeNormalizer<'a, 'tcx: 'a> {\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+}\n+\n+impl<'a, 'tcx> AssociatedTypeNormalizer<'a, 'tcx> {\n+    fn fold<T: TypeFoldable<'tcx>>(&mut self, value: &T) -> T {\n+        if !value.has_projections() {\n+            value.clone()\n+        } else {\n+            value.fold_with(self)\n+        }\n+    }\n+}\n+\n+impl<'a, 'tcx> ::rustc::ty::fold::TypeFolder<'tcx, 'tcx> for AssociatedTypeNormalizer<'a, 'tcx> {\n+    fn tcx<'c>(&'c self) -> TyCtxt<'c, 'tcx, 'tcx> {\n+        self.tcx\n+    }\n+\n+    fn fold_ty(&mut self, ty: Ty<'tcx>) -> Ty<'tcx> {\n+        if !ty.has_projections() {\n+            ty\n+        } else {\n+            self.tcx.normalize_associated_type(&ty)\n+        }\n+    }\n+}\n+\n+fn type_is_sized<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>, ty: Ty<'tcx>) -> bool {\n+    // generics are weird, don't run this function on a generic\n+    assert!(!ty.needs_subst());\n+    ty.is_sized(tcx, ty::ParamEnv::empty(Reveal::All), DUMMY_SP)\n+}\n+\n+pub fn resolve_drop_in_place<'a, 'tcx>(\n+    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    ty: Ty<'tcx>,\n+) -> ty::Instance<'tcx> {\n+    let def_id = tcx.require_lang_item(::rustc::middle::lang_items::DropInPlaceFnLangItem);\n+    let substs = tcx.intern_substs(&[Kind::from(ty)]);\n+    resolve(tcx, def_id, substs)\n+}"}, {"sha": "36b396a7a2ba5ff71816e912ef0395c5f0e0e156", "filename": "src/librustc/mir/interpret/lvalue.rs", "status": "added", "additions": 506, "deletions": 0, "changes": 506, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Flvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Flvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Flvalue.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,506 @@\n+use rustc::mir;\n+use rustc::ty::layout::{Size, Align};\n+use rustc::ty::{self, Ty};\n+use rustc_data_structures::indexed_vec::Idx;\n+\n+use super::{EvalResult, EvalContext, MemoryPointer, PrimVal, Value, Pointer, Machine, PtrAndAlign, ValTy};\n+\n+#[derive(Copy, Clone, Debug)]\n+pub enum Lvalue {\n+    /// An lvalue referring to a value allocated in the `Memory` system.\n+    Ptr {\n+        /// An lvalue may have an invalid (integral or undef) pointer,\n+        /// since it might be turned back into a reference\n+        /// before ever being dereferenced.\n+        ptr: PtrAndAlign,\n+        extra: LvalueExtra,\n+    },\n+\n+    /// An lvalue referring to a value on the stack. Represented by a stack frame index paired with\n+    /// a Mir local index.\n+    Local { frame: usize, local: mir::Local },\n+}\n+\n+#[derive(Copy, Clone, Debug, Eq, PartialEq)]\n+pub enum LvalueExtra {\n+    None,\n+    Length(u64),\n+    Vtable(MemoryPointer),\n+    DowncastVariant(usize),\n+}\n+\n+/// Uniquely identifies a specific constant or static.\n+#[derive(Copy, Clone, Debug, Eq, PartialEq, Hash)]\n+pub struct GlobalId<'tcx> {\n+    /// For a constant or static, the `Instance` of the item itself.\n+    /// For a promoted global, the `Instance` of the function they belong to.\n+    pub instance: ty::Instance<'tcx>,\n+\n+    /// The index for promoted globals within their function's `Mir`.\n+    pub promoted: Option<mir::Promoted>,\n+}\n+\n+impl<'tcx> Lvalue {\n+    /// Produces an Lvalue that will error if attempted to be read from\n+    pub fn undef() -> Self {\n+        Self::from_primval_ptr(PrimVal::Undef.into())\n+    }\n+\n+    pub fn from_primval_ptr(ptr: Pointer) -> Self {\n+        Lvalue::Ptr {\n+            ptr: PtrAndAlign { ptr, aligned: true },\n+            extra: LvalueExtra::None,\n+        }\n+    }\n+\n+    pub fn from_ptr(ptr: MemoryPointer) -> Self {\n+        Self::from_primval_ptr(ptr.into())\n+    }\n+\n+    pub(super) fn to_ptr_extra_aligned(self) -> (PtrAndAlign, LvalueExtra) {\n+        match self {\n+            Lvalue::Ptr { ptr, extra } => (ptr, extra),\n+            _ => bug!(\"to_ptr_and_extra: expected Lvalue::Ptr, got {:?}\", self),\n+\n+        }\n+    }\n+\n+    pub fn to_ptr(self) -> EvalResult<'tcx, MemoryPointer> {\n+        let (ptr, extra) = self.to_ptr_extra_aligned();\n+        // At this point, we forget about the alignment information -- the lvalue has been turned into a reference,\n+        // and no matter where it came from, it now must be aligned.\n+        assert_eq!(extra, LvalueExtra::None);\n+        ptr.to_ptr()\n+    }\n+\n+    pub(super) fn elem_ty_and_len(self, ty: Ty<'tcx>) -> (Ty<'tcx>, u64) {\n+        match ty.sty {\n+            ty::TyArray(elem, n) => (elem, n.val.to_const_int().unwrap().to_u64().unwrap() as u64),\n+\n+            ty::TySlice(elem) => {\n+                match self {\n+                    Lvalue::Ptr { extra: LvalueExtra::Length(len), .. } => (elem, len),\n+                    _ => {\n+                        bug!(\n+                            \"elem_ty_and_len of a TySlice given non-slice lvalue: {:?}\",\n+                            self\n+                        )\n+                    }\n+                }\n+            }\n+\n+            _ => bug!(\"elem_ty_and_len expected array or slice, got {:?}\", ty),\n+        }\n+    }\n+}\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    /// Reads a value from the lvalue without going through the intermediate step of obtaining\n+    /// a `miri::Lvalue`\n+    pub fn try_read_lvalue(\n+        &mut self,\n+        lvalue: &mir::Lvalue<'tcx>,\n+    ) -> EvalResult<'tcx, Option<Value>> {\n+        use rustc::mir::Lvalue::*;\n+        match *lvalue {\n+            // Might allow this in the future, right now there's no way to do this from Rust code anyway\n+            Local(mir::RETURN_POINTER) => err!(ReadFromReturnPointer),\n+            // Directly reading a local will always succeed\n+            Local(local) => self.frame().get_local(local).map(Some),\n+            // Directly reading a static will always succeed\n+            Static(ref static_) => {\n+                let instance = ty::Instance::mono(self.tcx, static_.def_id);\n+                let cid = GlobalId {\n+                    instance,\n+                    promoted: None,\n+                };\n+                Ok(Some(Value::ByRef(\n+                    *self.globals.get(&cid).expect(\"global not cached\"),\n+                )))\n+            }\n+            Projection(ref proj) => self.try_read_lvalue_projection(proj),\n+        }\n+    }\n+\n+    fn try_read_lvalue_projection(\n+        &mut self,\n+        proj: &mir::LvalueProjection<'tcx>,\n+    ) -> EvalResult<'tcx, Option<Value>> {\n+        use rustc::mir::ProjectionElem::*;\n+        let base = match self.try_read_lvalue(&proj.base)? {\n+            Some(base) => base,\n+            None => return Ok(None),\n+        };\n+        let base_ty = self.lvalue_ty(&proj.base);\n+        match proj.elem {\n+            Field(field, _) => match (field.index(), base) {\n+                // the only field of a struct\n+                (0, Value::ByVal(val)) => Ok(Some(Value::ByVal(val))),\n+                // split fat pointers, 2 element tuples, ...\n+                (0...1, Value::ByValPair(a, b)) if self.get_field_count(base_ty)? == 2 => {\n+                    let val = [a, b][field.index()];\n+                    Ok(Some(Value::ByVal(val)))\n+                },\n+                // the only field of a struct is a fat pointer\n+                (0, Value::ByValPair(..)) => Ok(Some(base)),\n+                _ => Ok(None),\n+            },\n+            // The NullablePointer cases should work fine, need to take care for normal enums\n+            Downcast(..) |\n+            Subslice { .. } |\n+            // reading index 0 or index 1 from a ByVal or ByVal pair could be optimized\n+            ConstantIndex { .. } | Index(_) |\n+            // No way to optimize this projection any better than the normal lvalue path\n+            Deref => Ok(None),\n+        }\n+    }\n+\n+    /// Returns a value and (in case of a ByRef) if we are supposed to use aligned accesses.\n+    pub(super) fn eval_and_read_lvalue(\n+        &mut self,\n+        lvalue: &mir::Lvalue<'tcx>,\n+    ) -> EvalResult<'tcx, Value> {\n+        // Shortcut for things like accessing a fat pointer's field,\n+        // which would otherwise (in the `eval_lvalue` path) require moving a `ByValPair` to memory\n+        // and returning an `Lvalue::Ptr` to it\n+        if let Some(val) = self.try_read_lvalue(lvalue)? {\n+            return Ok(val);\n+        }\n+        let lvalue = self.eval_lvalue(lvalue)?;\n+        self.read_lvalue(lvalue)\n+    }\n+\n+    pub fn read_lvalue(&self, lvalue: Lvalue) -> EvalResult<'tcx, Value> {\n+        match lvalue {\n+            Lvalue::Ptr { ptr, extra } => {\n+                assert_eq!(extra, LvalueExtra::None);\n+                Ok(Value::ByRef(ptr))\n+            }\n+            Lvalue::Local { frame, local } => self.stack[frame].get_local(local),\n+        }\n+    }\n+\n+    pub fn eval_lvalue(&mut self, mir_lvalue: &mir::Lvalue<'tcx>) -> EvalResult<'tcx, Lvalue> {\n+        use rustc::mir::Lvalue::*;\n+        let lvalue = match *mir_lvalue {\n+            Local(mir::RETURN_POINTER) => self.frame().return_lvalue,\n+            Local(local) => Lvalue::Local {\n+                frame: self.cur_frame(),\n+                local,\n+            },\n+\n+            Static(ref static_) => {\n+                let instance = ty::Instance::mono(self.tcx, static_.def_id);\n+                let gid = GlobalId {\n+                    instance,\n+                    promoted: None,\n+                };\n+                Lvalue::Ptr {\n+                    ptr: *self.globals.get(&gid).expect(\"uncached global\"),\n+                    extra: LvalueExtra::None,\n+                }\n+            }\n+\n+            Projection(ref proj) => {\n+                let ty = self.lvalue_ty(&proj.base);\n+                let lvalue = self.eval_lvalue(&proj.base)?;\n+                return self.eval_lvalue_projection(lvalue, ty, &proj.elem);\n+            }\n+        };\n+\n+        if log_enabled!(::log::LogLevel::Trace) {\n+            self.dump_local(lvalue);\n+        }\n+\n+        Ok(lvalue)\n+    }\n+\n+    pub fn lvalue_field(\n+        &mut self,\n+        base: Lvalue,\n+        field: mir::Field,\n+        base_ty: Ty<'tcx>,\n+        field_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx, Lvalue> {\n+        use rustc::ty::layout::Layout::*;\n+\n+        let base_layout = self.type_layout(base_ty)?;\n+        let field_index = field.index();\n+        let (offset, packed) = match *base_layout {\n+            Univariant { ref variant, .. } => (variant.offsets[field_index], variant.packed),\n+\n+            // mir optimizations treat single variant enums as structs\n+            General { ref variants, .. } if variants.len() == 1 => {\n+                (variants[0].offsets[field_index], variants[0].packed)\n+            }\n+\n+            General { ref variants, .. } => {\n+                let (_, base_extra) = base.to_ptr_extra_aligned();\n+                if let LvalueExtra::DowncastVariant(variant_idx) = base_extra {\n+                    // +1 for the discriminant, which is field 0\n+                    assert!(!variants[variant_idx].packed);\n+                    (variants[variant_idx].offsets[field_index + 1], false)\n+                } else {\n+                    bug!(\"field access on enum had no variant index\");\n+                }\n+            }\n+\n+            RawNullablePointer { .. } => {\n+                assert_eq!(field_index, 0);\n+                return Ok(base);\n+            }\n+\n+            StructWrappedNullablePointer { ref nonnull, .. } => {\n+                (nonnull.offsets[field_index], nonnull.packed)\n+            }\n+\n+            UntaggedUnion { .. } => return Ok(base),\n+\n+            Vector { element, count } => {\n+                let field = field_index as u64;\n+                assert!(field < count);\n+                let elem_size = element.size(&self.tcx.data_layout).bytes();\n+                (Size::from_bytes(field * elem_size), false)\n+            }\n+\n+            // We treat arrays + fixed sized indexing like field accesses\n+            Array { .. } => {\n+                let field = field_index as u64;\n+                let elem_size = match base_ty.sty {\n+                    ty::TyArray(elem_ty, n) => {\n+                        assert!(field < n.val.to_const_int().unwrap().to_u64().unwrap() as u64);\n+                        self.type_size(elem_ty)?.expect(\"array elements are sized\") as u64\n+                    }\n+                    _ => {\n+                        bug!(\n+                            \"lvalue_field: got Array layout but non-array type {:?}\",\n+                            base_ty\n+                        )\n+                    }\n+                };\n+                (Size::from_bytes(field * elem_size), false)\n+            }\n+\n+            FatPointer { .. } => {\n+                let bytes = field_index as u64 * self.memory.pointer_size();\n+                let offset = Size::from_bytes(bytes);\n+                (offset, false)\n+            }\n+\n+            _ => bug!(\"field access on non-product type: {:?}\", base_layout),\n+        };\n+\n+        // Do not allocate in trivial cases\n+        let (base_ptr, base_extra) = match base {\n+            Lvalue::Ptr { ptr, extra } => (ptr, extra),\n+            Lvalue::Local { frame, local } => {\n+                match self.stack[frame].get_local(local)? {\n+                    // in case the type has a single field, just return the value\n+                    Value::ByVal(_)\n+                        if self.get_field_count(base_ty).map(|c| c == 1).unwrap_or(\n+                            false,\n+                        ) => {\n+                        assert_eq!(\n+                            offset.bytes(),\n+                            0,\n+                            \"ByVal can only have 1 non zst field with offset 0\"\n+                        );\n+                        return Ok(base);\n+                    }\n+                    Value::ByRef { .. } |\n+                    Value::ByValPair(..) |\n+                    Value::ByVal(_) => self.force_allocation(base)?.to_ptr_extra_aligned(),\n+                }\n+            }\n+        };\n+\n+        let offset = match base_extra {\n+            LvalueExtra::Vtable(tab) => {\n+                let (_, align) = self.size_and_align_of_dst(\n+                    base_ty,\n+                    base_ptr.ptr.to_value_with_vtable(tab),\n+                )?;\n+                offset\n+                    .abi_align(Align::from_bytes(align, align).unwrap())\n+                    .bytes()\n+            }\n+            _ => offset.bytes(),\n+        };\n+\n+        let mut ptr = base_ptr.offset(offset, &self)?;\n+        // if we were unaligned, stay unaligned\n+        // no matter what we were, if we are packed, we must not be aligned anymore\n+        ptr.aligned &= !packed;\n+\n+        let field_ty = self.monomorphize(field_ty, self.substs());\n+\n+        let extra = if self.type_is_sized(field_ty) {\n+            LvalueExtra::None\n+        } else {\n+            match base_extra {\n+                LvalueExtra::None => bug!(\"expected fat pointer\"),\n+                LvalueExtra::DowncastVariant(..) => {\n+                    bug!(\"Rust doesn't support unsized fields in enum variants\")\n+                }\n+                LvalueExtra::Vtable(_) |\n+                LvalueExtra::Length(_) => {}\n+            }\n+            base_extra\n+        };\n+\n+        Ok(Lvalue::Ptr { ptr, extra })\n+    }\n+\n+    pub(super) fn val_to_lvalue(&self, val: Value, ty: Ty<'tcx>) -> EvalResult<'tcx, Lvalue> {\n+        Ok(match self.tcx.struct_tail(ty).sty {\n+            ty::TyDynamic(..) => {\n+                let (ptr, vtable) = val.into_ptr_vtable_pair(&self.memory)?;\n+                Lvalue::Ptr {\n+                    ptr: PtrAndAlign { ptr, aligned: true },\n+                    extra: LvalueExtra::Vtable(vtable),\n+                }\n+            }\n+            ty::TyStr | ty::TySlice(_) => {\n+                let (ptr, len) = val.into_slice(&self.memory)?;\n+                Lvalue::Ptr {\n+                    ptr: PtrAndAlign { ptr, aligned: true },\n+                    extra: LvalueExtra::Length(len),\n+                }\n+            }\n+            _ => Lvalue::from_primval_ptr(val.into_ptr(&self.memory)?),\n+        })\n+    }\n+\n+    pub(super) fn lvalue_index(\n+        &mut self,\n+        base: Lvalue,\n+        outer_ty: Ty<'tcx>,\n+        n: u64,\n+    ) -> EvalResult<'tcx, Lvalue> {\n+        // Taking the outer type here may seem odd; it's needed because for array types, the outer type gives away the length.\n+        let base = self.force_allocation(base)?;\n+        let (base_ptr, _) = base.to_ptr_extra_aligned();\n+\n+        let (elem_ty, len) = base.elem_ty_and_len(outer_ty);\n+        let elem_size = self.type_size(elem_ty)?.expect(\n+            \"slice element must be sized\",\n+        );\n+        assert!(\n+            n < len,\n+            \"Tried to access element {} of array/slice with length {}\",\n+            n,\n+            len\n+        );\n+        let ptr = base_ptr.offset(n * elem_size, self.memory.layout)?;\n+        Ok(Lvalue::Ptr {\n+            ptr,\n+            extra: LvalueExtra::None,\n+        })\n+    }\n+\n+    pub(super) fn eval_lvalue_projection(\n+        &mut self,\n+        base: Lvalue,\n+        base_ty: Ty<'tcx>,\n+        proj_elem: &mir::ProjectionElem<'tcx, mir::Local, Ty<'tcx>>,\n+    ) -> EvalResult<'tcx, Lvalue> {\n+        use rustc::mir::ProjectionElem::*;\n+        let (ptr, extra) = match *proj_elem {\n+            Field(field, field_ty) => {\n+                return self.lvalue_field(base, field, base_ty, field_ty);\n+            }\n+\n+            Downcast(_, variant) => {\n+                let base_layout = self.type_layout(base_ty)?;\n+                // FIXME(solson)\n+                let base = self.force_allocation(base)?;\n+                let (base_ptr, base_extra) = base.to_ptr_extra_aligned();\n+\n+                use rustc::ty::layout::Layout::*;\n+                let extra = match *base_layout {\n+                    General { .. } => LvalueExtra::DowncastVariant(variant),\n+                    RawNullablePointer { .. } |\n+                    StructWrappedNullablePointer { .. } => base_extra,\n+                    _ => bug!(\"variant downcast on non-aggregate: {:?}\", base_layout),\n+                };\n+                (base_ptr, extra)\n+            }\n+\n+            Deref => {\n+                let val = self.read_lvalue(base)?;\n+\n+                let pointee_type = match base_ty.sty {\n+                    ty::TyRawPtr(ref tam) |\n+                    ty::TyRef(_, ref tam) => tam.ty,\n+                    ty::TyAdt(def, _) if def.is_box() => base_ty.boxed_ty(),\n+                    _ => bug!(\"can only deref pointer types\"),\n+                };\n+\n+                trace!(\"deref to {} on {:?}\", pointee_type, val);\n+\n+                return self.val_to_lvalue(val, pointee_type);\n+            }\n+\n+            Index(local) => {\n+                let value = self.frame().get_local(local)?;\n+                let ty = self.tcx.types.usize;\n+                let n = self.value_to_primval(ValTy { value, ty })?.to_u64()?;\n+                return self.lvalue_index(base, base_ty, n);\n+            }\n+\n+            ConstantIndex {\n+                offset,\n+                min_length,\n+                from_end,\n+            } => {\n+                // FIXME(solson)\n+                let base = self.force_allocation(base)?;\n+                let (base_ptr, _) = base.to_ptr_extra_aligned();\n+\n+                let (elem_ty, n) = base.elem_ty_and_len(base_ty);\n+                let elem_size = self.type_size(elem_ty)?.expect(\n+                    \"sequence element must be sized\",\n+                );\n+                assert!(n >= min_length as u64);\n+\n+                let index = if from_end {\n+                    n - u64::from(offset)\n+                } else {\n+                    u64::from(offset)\n+                };\n+\n+                let ptr = base_ptr.offset(index * elem_size, &self)?;\n+                (ptr, LvalueExtra::None)\n+            }\n+\n+            Subslice { from, to } => {\n+                // FIXME(solson)\n+                let base = self.force_allocation(base)?;\n+                let (base_ptr, _) = base.to_ptr_extra_aligned();\n+\n+                let (elem_ty, n) = base.elem_ty_and_len(base_ty);\n+                let elem_size = self.type_size(elem_ty)?.expect(\n+                    \"slice element must be sized\",\n+                );\n+                assert!(u64::from(from) <= n - u64::from(to));\n+                let ptr = base_ptr.offset(u64::from(from) * elem_size, &self)?;\n+                // sublicing arrays produces arrays\n+                let extra = if self.type_is_sized(base_ty) {\n+                    LvalueExtra::None\n+                } else {\n+                    LvalueExtra::Length(n - u64::from(to) - u64::from(from))\n+                };\n+                (ptr, extra)\n+            }\n+        };\n+\n+        Ok(Lvalue::Ptr { ptr, extra })\n+    }\n+\n+    pub fn lvalue_ty(&self, lvalue: &mir::Lvalue<'tcx>) -> Ty<'tcx> {\n+        self.monomorphize(\n+            lvalue.ty(self.mir(), self.tcx).to_ty(self.tcx),\n+            self.substs(),\n+        )\n+    }\n+}"}, {"sha": "3df5d1b6a31bea95e3ec97fa8d00864747bb45a2", "filename": "src/librustc/mir/interpret/machine.rs", "status": "added", "additions": 82, "deletions": 0, "changes": 82, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fmachine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fmachine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fmachine.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,82 @@\n+//! This module contains everything needed to instantiate an interpreter.\n+//! This separation exists to ensure that no fancy miri features like\n+//! interpreting common C functions leak into CTFE.\n+\n+use super::{EvalResult, EvalContext, Lvalue, PrimVal, ValTy};\n+\n+use rustc::{mir, ty};\n+use syntax::codemap::Span;\n+use syntax::ast::Mutability;\n+\n+/// Methods of this trait signifies a point where CTFE evaluation would fail\n+/// and some use case dependent behaviour can instead be applied\n+pub trait Machine<'tcx>: Sized {\n+    /// Additional data that can be accessed via the EvalContext\n+    type Data;\n+\n+    /// Additional data that can be accessed via the Memory\n+    type MemoryData;\n+\n+    /// Additional memory kinds a machine wishes to distinguish from the builtin ones\n+    type MemoryKinds: ::std::fmt::Debug + PartialEq + Copy + Clone;\n+\n+    /// Entry point to all function calls.\n+    ///\n+    /// Returns Ok(true) when the function was handled completely\n+    /// e.g. due to missing mir\n+    ///\n+    /// Returns Ok(false) if a new stack frame was pushed\n+    fn eval_fn_call<'a>(\n+        ecx: &mut EvalContext<'a, 'tcx, Self>,\n+        instance: ty::Instance<'tcx>,\n+        destination: Option<(Lvalue, mir::BasicBlock)>,\n+        args: &[ValTy<'tcx>],\n+        span: Span,\n+        sig: ty::FnSig<'tcx>,\n+    ) -> EvalResult<'tcx, bool>;\n+\n+    /// directly process an intrinsic without pushing a stack frame.\n+    fn call_intrinsic<'a>(\n+        ecx: &mut EvalContext<'a, 'tcx, Self>,\n+        instance: ty::Instance<'tcx>,\n+        args: &[ValTy<'tcx>],\n+        dest: Lvalue,\n+        dest_ty: ty::Ty<'tcx>,\n+        dest_layout: &'tcx ty::layout::Layout,\n+        target: mir::BasicBlock,\n+    ) -> EvalResult<'tcx>;\n+\n+    /// Called for all binary operations except on float types.\n+    ///\n+    /// Returns `None` if the operation should be handled by the integer\n+    /// op code in order to share more code between machines\n+    ///\n+    /// Returns a (value, overflowed) pair if the operation succeeded\n+    fn try_ptr_op<'a>(\n+        ecx: &EvalContext<'a, 'tcx, Self>,\n+        bin_op: mir::BinOp,\n+        left: PrimVal,\n+        left_ty: ty::Ty<'tcx>,\n+        right: PrimVal,\n+        right_ty: ty::Ty<'tcx>,\n+    ) -> EvalResult<'tcx, Option<(PrimVal, bool)>>;\n+\n+    /// Called when trying to mark machine defined `MemoryKinds` as static\n+    fn mark_static_initialized(m: Self::MemoryKinds) -> EvalResult<'tcx>;\n+\n+    /// Heap allocations via the `box` keyword\n+    ///\n+    /// Returns a pointer to the allocated memory\n+    fn box_alloc<'a>(\n+        ecx: &mut EvalContext<'a, 'tcx, Self>,\n+        ty: ty::Ty<'tcx>,\n+        dest: Lvalue,\n+    ) -> EvalResult<'tcx>;\n+\n+    /// Called when trying to access a global declared with a `linkage` attribute\n+    fn global_item_with_linkage<'a>(\n+        ecx: &mut EvalContext<'a, 'tcx, Self>,\n+        instance: ty::Instance<'tcx>,\n+        mutability: Mutability,\n+    ) -> EvalResult<'tcx>;\n+}"}, {"sha": "bde79294adda50781d83abf8db995c44224352ae", "filename": "src/librustc/mir/interpret/memory.rs", "status": "added", "additions": 1700, "deletions": 0, "changes": 1700, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fmemory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fmemory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fmemory.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,1700 @@\n+use byteorder::{ReadBytesExt, WriteBytesExt, LittleEndian, BigEndian};\n+use std::collections::{btree_map, BTreeMap, HashMap, HashSet, VecDeque};\n+use std::{fmt, iter, ptr, mem, io};\n+use std::cell::Cell;\n+\n+use rustc::ty::Instance;\n+use rustc::ty::layout::{self, TargetDataLayout, HasDataLayout};\n+use syntax::ast::Mutability;\n+use rustc::middle::region;\n+\n+use super::{EvalResult, EvalErrorKind, PrimVal, Pointer, EvalContext, DynamicLifetime, Machine,\n+            RangeMap, AbsLvalue};\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Locks\n+////////////////////////////////////////////////////////////////////////////////\n+\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum AccessKind {\n+    Read,\n+    Write,\n+}\n+\n+/// Information about a lock that is currently held.\n+#[derive(Clone, Debug)]\n+struct LockInfo<'tcx> {\n+    /// Stores for which lifetimes (of the original write lock) we got\n+    /// which suspensions.\n+    suspended: HashMap<WriteLockId<'tcx>, Vec<region::Scope>>,\n+    /// The current state of the lock that's actually effective.\n+    active: Lock,\n+}\n+\n+/// Write locks are identified by a stack frame and an \"abstract\" (untyped) lvalue.\n+/// It may be tempting to use the lifetime as identifier, but that does not work\n+/// for two reasons:\n+/// * First of all, due to subtyping, the same lock may be referred to with different\n+///   lifetimes.\n+/// * Secondly, different write locks may actually have the same lifetime.  See `test2`\n+///   in `run-pass/many_shr_bor.rs`.\n+/// The Id is \"captured\" when the lock is first suspended; at that point, the borrow checker\n+/// considers the path frozen and hence the Id remains stable.\n+#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+struct WriteLockId<'tcx> {\n+    frame: usize,\n+    path: AbsLvalue<'tcx>,\n+}\n+\n+#[derive(Clone, Debug, PartialEq)]\n+pub enum Lock {\n+    NoLock,\n+    WriteLock(DynamicLifetime),\n+    ReadLock(Vec<DynamicLifetime>), // This should never be empty -- that would be a read lock held and nobody there to release it...\n+}\n+use self::Lock::*;\n+\n+impl<'tcx> Default for LockInfo<'tcx> {\n+    fn default() -> Self {\n+        LockInfo::new(NoLock)\n+    }\n+}\n+\n+impl<'tcx> LockInfo<'tcx> {\n+    fn new(lock: Lock) -> LockInfo<'tcx> {\n+        LockInfo {\n+            suspended: HashMap::new(),\n+            active: lock,\n+        }\n+    }\n+\n+    fn access_permitted(&self, frame: Option<usize>, access: AccessKind) -> bool {\n+        use self::AccessKind::*;\n+        match (&self.active, access) {\n+            (&NoLock, _) => true,\n+            (&ReadLock(ref lfts), Read) => {\n+                assert!(!lfts.is_empty(), \"Someone left an empty read lock behind.\");\n+                // Read access to read-locked region is okay, no matter who's holding the read lock.\n+                true\n+            }\n+            (&WriteLock(ref lft), _) => {\n+                // All access is okay if we are the ones holding it\n+                Some(lft.frame) == frame\n+            }\n+            _ => false, // Nothing else is okay.\n+        }\n+    }\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Allocations and pointers\n+////////////////////////////////////////////////////////////////////////////////\n+\n+#[derive(Copy, Clone, Eq, Hash, Ord, PartialEq, PartialOrd)]\n+pub struct AllocId(u64);\n+\n+#[derive(Debug)]\n+pub enum AllocIdKind {\n+    /// We can't ever have more than `usize::max_value` functions at the same time\n+    /// since we never \"deallocate\" functions\n+    Function(usize),\n+    /// Locals and heap allocations (also statics for now, but those will get their\n+    /// own variant soonish).\n+    Runtime(u64),\n+}\n+\n+impl AllocIdKind {\n+    pub fn into_alloc_id(self) -> AllocId {\n+        match self {\n+            AllocIdKind::Function(n) => AllocId(n as u64),\n+            AllocIdKind::Runtime(n) => AllocId((1 << 63) | n),\n+        }\n+    }\n+}\n+\n+impl AllocId {\n+    /// Currently yields the top bit to discriminate the `AllocIdKind`s\n+    fn discriminant(self) -> u64 {\n+        self.0 >> 63\n+    }\n+    /// Yields everything but the discriminant bits\n+    pub fn index(self) -> u64 {\n+        self.0 & ((1 << 63) - 1)\n+    }\n+    pub fn into_alloc_id_kind(self) -> AllocIdKind {\n+        match self.discriminant() {\n+            0 => AllocIdKind::Function(self.index() as usize),\n+            1 => AllocIdKind::Runtime(self.index()),\n+            n => bug!(\"got discriminant {} for AllocId\", n),\n+        }\n+    }\n+}\n+\n+impl fmt::Display for AllocId {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self.into_alloc_id_kind())\n+    }\n+}\n+\n+impl fmt::Debug for AllocId {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self.into_alloc_id_kind())\n+    }\n+}\n+\n+#[derive(Debug)]\n+pub struct Allocation<'tcx, M> {\n+    /// The actual bytes of the allocation.\n+    /// Note that the bytes of a pointer represent the offset of the pointer\n+    pub bytes: Vec<u8>,\n+    /// Maps from byte addresses to allocations.\n+    /// Only the first byte of a pointer is inserted into the map.\n+    pub relocations: BTreeMap<u64, AllocId>,\n+    /// Denotes undefined memory. Reading from undefined memory is forbidden in miri\n+    pub undef_mask: UndefMask,\n+    /// The alignment of the allocation to detect unaligned reads.\n+    pub align: u64,\n+    /// Whether the allocation may be modified.\n+    pub mutable: Mutability,\n+    /// Use the `mark_static_initalized` method of `Memory` to ensure that an error occurs, if the memory of this\n+    /// allocation is modified or deallocated in the future.\n+    /// Helps guarantee that stack allocations aren't deallocated via `rust_deallocate`\n+    pub kind: MemoryKind<M>,\n+    /// Memory regions that are locked by some function\n+    locks: RangeMap<LockInfo<'tcx>>,\n+}\n+\n+impl<'tcx, M> Allocation<'tcx, M> {\n+    fn check_locks(\n+        &self,\n+        frame: Option<usize>,\n+        offset: u64,\n+        len: u64,\n+        access: AccessKind,\n+    ) -> Result<(), LockInfo<'tcx>> {\n+        if len == 0 {\n+            return Ok(());\n+        }\n+        for lock in self.locks.iter(offset, len) {\n+            // Check if the lock is in conflict with the access.\n+            if !lock.access_permitted(frame, access) {\n+                return Err(lock.clone());\n+            }\n+        }\n+        Ok(())\n+    }\n+}\n+\n+#[derive(Debug, PartialEq, Copy, Clone)]\n+pub enum MemoryKind<T> {\n+    /// Error if deallocated except during a stack pop\n+    Stack,\n+    /// Static in the process of being initialized.\n+    /// The difference is important: An immutable static referring to a\n+    /// mutable initialized static will freeze immutably and would not\n+    /// be able to distinguish already initialized statics from uninitialized ones\n+    UninitializedStatic,\n+    /// May never be deallocated\n+    Static,\n+    /// Additional memory kinds a machine wishes to distinguish from the builtin ones\n+    Machine(T),\n+}\n+\n+#[derive(Copy, Clone, Debug, Eq, PartialEq)]\n+pub struct MemoryPointer {\n+    pub alloc_id: AllocId,\n+    pub offset: u64,\n+}\n+\n+impl<'tcx> MemoryPointer {\n+    pub fn new(alloc_id: AllocId, offset: u64) -> Self {\n+        MemoryPointer { alloc_id, offset }\n+    }\n+\n+    pub(crate) fn wrapping_signed_offset<C: HasDataLayout>(self, i: i64, cx: C) -> Self {\n+        MemoryPointer::new(\n+            self.alloc_id,\n+            cx.data_layout().wrapping_signed_offset(self.offset, i),\n+        )\n+    }\n+\n+    pub fn overflowing_signed_offset<C: HasDataLayout>(self, i: i128, cx: C) -> (Self, bool) {\n+        let (res, over) = cx.data_layout().overflowing_signed_offset(self.offset, i);\n+        (MemoryPointer::new(self.alloc_id, res), over)\n+    }\n+\n+    pub(crate) fn signed_offset<C: HasDataLayout>(self, i: i64, cx: C) -> EvalResult<'tcx, Self> {\n+        Ok(MemoryPointer::new(\n+            self.alloc_id,\n+            cx.data_layout().signed_offset(self.offset, i)?,\n+        ))\n+    }\n+\n+    pub fn overflowing_offset<C: HasDataLayout>(self, i: u64, cx: C) -> (Self, bool) {\n+        let (res, over) = cx.data_layout().overflowing_offset(self.offset, i);\n+        (MemoryPointer::new(self.alloc_id, res), over)\n+    }\n+\n+    pub fn offset<C: HasDataLayout>(self, i: u64, cx: C) -> EvalResult<'tcx, Self> {\n+        Ok(MemoryPointer::new(\n+            self.alloc_id,\n+            cx.data_layout().offset(self.offset, i)?,\n+        ))\n+    }\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Top-level interpreter memory\n+////////////////////////////////////////////////////////////////////////////////\n+\n+pub struct Memory<'a, 'tcx, M: Machine<'tcx>> {\n+    /// Additional data required by the Machine\n+    pub data: M::MemoryData,\n+\n+    /// Actual memory allocations (arbitrary bytes, may contain pointers into other allocations).\n+    alloc_map: HashMap<u64, Allocation<'tcx, M::MemoryKinds>>,\n+\n+    /// The AllocId to assign to the next new regular allocation. Always incremented, never gets smaller.\n+    next_alloc_id: u64,\n+\n+    /// Number of virtual bytes allocated.\n+    memory_usage: u64,\n+\n+    /// Maximum number of virtual bytes that may be allocated.\n+    memory_size: u64,\n+\n+    /// Function \"allocations\". They exist solely so pointers have something to point to, and\n+    /// we can figure out what they point to.\n+    functions: Vec<Instance<'tcx>>,\n+\n+    /// Inverse map of `functions` so we don't allocate a new pointer every time we need one\n+    function_alloc_cache: HashMap<Instance<'tcx>, AllocId>,\n+\n+    /// Target machine data layout to emulate.\n+    pub layout: &'a TargetDataLayout,\n+\n+    /// A cache for basic byte allocations keyed by their contents. This is used to deduplicate\n+    /// allocations for string and bytestring literals.\n+    literal_alloc_cache: HashMap<Vec<u8>, AllocId>,\n+\n+    /// To avoid having to pass flags to every single memory access, we have some global state saying whether\n+    /// alignment checking is currently enforced for read and/or write accesses.\n+    reads_are_aligned: Cell<bool>,\n+    writes_are_aligned: Cell<bool>,\n+\n+    /// The current stack frame.  Used to check accesses against locks.\n+    pub(super) cur_frame: usize,\n+}\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> Memory<'a, 'tcx, M> {\n+    pub fn new(layout: &'a TargetDataLayout, max_memory: u64, data: M::MemoryData) -> Self {\n+        Memory {\n+            data,\n+            alloc_map: HashMap::new(),\n+            functions: Vec::new(),\n+            function_alloc_cache: HashMap::new(),\n+            next_alloc_id: 0,\n+            layout,\n+            memory_size: max_memory,\n+            memory_usage: 0,\n+            literal_alloc_cache: HashMap::new(),\n+            reads_are_aligned: Cell::new(true),\n+            writes_are_aligned: Cell::new(true),\n+            cur_frame: usize::max_value(),\n+        }\n+    }\n+\n+    pub fn allocations<'x>(\n+        &'x self,\n+    ) -> impl Iterator<Item = (AllocId, &'x Allocation<M::MemoryKinds>)> {\n+        self.alloc_map.iter().map(|(&id, alloc)| {\n+            (AllocIdKind::Runtime(id).into_alloc_id(), alloc)\n+        })\n+    }\n+\n+    pub fn create_fn_alloc(&mut self, instance: Instance<'tcx>) -> MemoryPointer {\n+        if let Some(&alloc_id) = self.function_alloc_cache.get(&instance) {\n+            return MemoryPointer::new(alloc_id, 0);\n+        }\n+        let id = self.functions.len();\n+        debug!(\"creating fn ptr: {}\", id);\n+        self.functions.push(instance);\n+        let alloc_id = AllocIdKind::Function(id).into_alloc_id();\n+        self.function_alloc_cache.insert(instance, alloc_id);\n+        MemoryPointer::new(alloc_id, 0)\n+    }\n+\n+    pub fn allocate_cached(&mut self, bytes: &[u8]) -> EvalResult<'tcx, MemoryPointer> {\n+        if let Some(&alloc_id) = self.literal_alloc_cache.get(bytes) {\n+            return Ok(MemoryPointer::new(alloc_id, 0));\n+        }\n+\n+        let ptr = self.allocate(\n+            bytes.len() as u64,\n+            1,\n+            MemoryKind::UninitializedStatic,\n+        )?;\n+        self.write_bytes(ptr.into(), bytes)?;\n+        self.mark_static_initalized(\n+            ptr.alloc_id,\n+            Mutability::Immutable,\n+        )?;\n+        self.literal_alloc_cache.insert(\n+            bytes.to_vec(),\n+            ptr.alloc_id,\n+        );\n+        Ok(ptr)\n+    }\n+\n+    pub fn allocate(\n+        &mut self,\n+        size: u64,\n+        align: u64,\n+        kind: MemoryKind<M::MemoryKinds>,\n+    ) -> EvalResult<'tcx, MemoryPointer> {\n+        assert_ne!(align, 0);\n+        assert!(align.is_power_of_two());\n+\n+        if self.memory_size - self.memory_usage < size {\n+            return err!(OutOfMemory {\n+                allocation_size: size,\n+                memory_size: self.memory_size,\n+                memory_usage: self.memory_usage,\n+            });\n+        }\n+        self.memory_usage += size;\n+        assert_eq!(size as usize as u64, size);\n+        let alloc = Allocation {\n+            bytes: vec![0; size as usize],\n+            relocations: BTreeMap::new(),\n+            undef_mask: UndefMask::new(size),\n+            align,\n+            kind,\n+            mutable: Mutability::Mutable,\n+            locks: RangeMap::new(),\n+        };\n+        let id = self.next_alloc_id;\n+        self.next_alloc_id += 1;\n+        self.alloc_map.insert(id, alloc);\n+        Ok(MemoryPointer::new(\n+            AllocIdKind::Runtime(id).into_alloc_id(),\n+            0,\n+        ))\n+    }\n+\n+    pub fn reallocate(\n+        &mut self,\n+        ptr: MemoryPointer,\n+        old_size: u64,\n+        old_align: u64,\n+        new_size: u64,\n+        new_align: u64,\n+        kind: MemoryKind<M::MemoryKinds>,\n+    ) -> EvalResult<'tcx, MemoryPointer> {\n+        use std::cmp::min;\n+\n+        if ptr.offset != 0 {\n+            return err!(ReallocateNonBasePtr);\n+        }\n+        if let Ok(alloc) = self.get(ptr.alloc_id) {\n+            if alloc.kind != kind {\n+                return err!(ReallocatedWrongMemoryKind(\n+                    format!(\"{:?}\", alloc.kind),\n+                    format!(\"{:?}\", kind),\n+                ));\n+            }\n+        }\n+\n+        // For simplicities' sake, we implement reallocate as \"alloc, copy, dealloc\"\n+        let new_ptr = self.allocate(new_size, new_align, kind)?;\n+        self.copy(\n+            ptr.into(),\n+            new_ptr.into(),\n+            min(old_size, new_size),\n+            min(old_align, new_align),\n+            /*nonoverlapping*/\n+            true,\n+        )?;\n+        self.deallocate(ptr, Some((old_size, old_align)), kind)?;\n+\n+        Ok(new_ptr)\n+    }\n+\n+    pub fn deallocate(\n+        &mut self,\n+        ptr: MemoryPointer,\n+        size_and_align: Option<(u64, u64)>,\n+        kind: MemoryKind<M::MemoryKinds>,\n+    ) -> EvalResult<'tcx> {\n+        if ptr.offset != 0 {\n+            return err!(DeallocateNonBasePtr);\n+        }\n+\n+        let alloc_id = match ptr.alloc_id.into_alloc_id_kind() {\n+            AllocIdKind::Function(_) => {\n+                return err!(DeallocatedWrongMemoryKind(\n+                    \"function\".to_string(),\n+                    format!(\"{:?}\", kind),\n+                ))\n+            }\n+            AllocIdKind::Runtime(id) => id,\n+        };\n+\n+        let alloc = match self.alloc_map.remove(&alloc_id) {\n+            Some(alloc) => alloc,\n+            None => return err!(DoubleFree),\n+        };\n+\n+        // It is okay for us to still holds locks on deallocation -- for example, we could store data we own\n+        // in a local, and the local could be deallocated (from StorageDead) before the function returns.\n+        // However, we should check *something*.  For now, we make sure that there is no conflicting write\n+        // lock by another frame.  We *have* to permit deallocation if we hold a read lock.\n+        // TODO: Figure out the exact rules here.\n+        alloc\n+            .check_locks(\n+                Some(self.cur_frame),\n+                0,\n+                alloc.bytes.len() as u64,\n+                AccessKind::Read,\n+            )\n+            .map_err(|lock| {\n+                EvalErrorKind::DeallocatedLockedMemory {\n+                    ptr,\n+                    lock: lock.active,\n+                }\n+            })?;\n+\n+        if alloc.kind != kind {\n+            return err!(DeallocatedWrongMemoryKind(\n+                format!(\"{:?}\", alloc.kind),\n+                format!(\"{:?}\", kind),\n+            ));\n+        }\n+        if let Some((size, align)) = size_and_align {\n+            if size != alloc.bytes.len() as u64 || align != alloc.align {\n+                return err!(IncorrectAllocationInformation);\n+            }\n+        }\n+\n+        self.memory_usage -= alloc.bytes.len() as u64;\n+        debug!(\"deallocated : {}\", ptr.alloc_id);\n+\n+        Ok(())\n+    }\n+\n+    pub fn pointer_size(&self) -> u64 {\n+        self.layout.pointer_size.bytes()\n+    }\n+\n+    pub fn endianess(&self) -> layout::Endian {\n+        self.layout.endian\n+    }\n+\n+    /// Check that the pointer is aligned AND non-NULL.\n+    pub fn check_align(&self, ptr: Pointer, align: u64, access: Option<AccessKind>) -> EvalResult<'tcx> {\n+        // Check non-NULL/Undef, extract offset\n+        let (offset, alloc_align) = match ptr.into_inner_primval() {\n+            PrimVal::Ptr(ptr) => {\n+                let alloc = self.get(ptr.alloc_id)?;\n+                (ptr.offset, alloc.align)\n+            }\n+            PrimVal::Bytes(bytes) => {\n+                let v = ((bytes as u128) % (1 << self.pointer_size())) as u64;\n+                if v == 0 {\n+                    return err!(InvalidNullPointerUsage);\n+                }\n+                (v, align) // the base address if the \"integer allocation\" is 0 and hence always aligned\n+            }\n+            PrimVal::Undef => return err!(ReadUndefBytes),\n+        };\n+        // See if alignment checking is disabled\n+        let enforce_alignment = match access {\n+            Some(AccessKind::Read) => self.reads_are_aligned.get(),\n+            Some(AccessKind::Write) => self.writes_are_aligned.get(),\n+            None => true,\n+        };\n+        if !enforce_alignment {\n+            return Ok(());\n+        }\n+        // Check alignment\n+        if alloc_align < align {\n+            return err!(AlignmentCheckFailed {\n+                has: alloc_align,\n+                required: align,\n+            });\n+        }\n+        if offset % align == 0 {\n+            Ok(())\n+        } else {\n+            err!(AlignmentCheckFailed {\n+                has: offset % align,\n+                required: align,\n+            })\n+        }\n+    }\n+\n+    pub fn check_bounds(&self, ptr: MemoryPointer, access: bool) -> EvalResult<'tcx> {\n+        let alloc = self.get(ptr.alloc_id)?;\n+        let allocation_size = alloc.bytes.len() as u64;\n+        if ptr.offset > allocation_size {\n+            return err!(PointerOutOfBounds {\n+                ptr,\n+                access,\n+                allocation_size,\n+            });\n+        }\n+        Ok(())\n+    }\n+}\n+\n+/// Locking\n+impl<'a, 'tcx, M: Machine<'tcx>> Memory<'a, 'tcx, M> {\n+    pub(crate) fn check_locks(\n+        &self,\n+        ptr: MemoryPointer,\n+        len: u64,\n+        access: AccessKind,\n+    ) -> EvalResult<'tcx> {\n+        if len == 0 {\n+            return Ok(());\n+        }\n+        let alloc = self.get(ptr.alloc_id)?;\n+        let frame = self.cur_frame;\n+        alloc\n+            .check_locks(Some(frame), ptr.offset, len, access)\n+            .map_err(|lock| {\n+                EvalErrorKind::MemoryLockViolation {\n+                    ptr,\n+                    len,\n+                    frame,\n+                    access,\n+                    lock: lock.active,\n+                }.into()\n+            })\n+    }\n+\n+    /// Acquire the lock for the given lifetime\n+    pub(crate) fn acquire_lock(\n+        &mut self,\n+        ptr: MemoryPointer,\n+        len: u64,\n+        region: Option<region::Scope>,\n+        kind: AccessKind,\n+    ) -> EvalResult<'tcx> {\n+        let frame = self.cur_frame;\n+        assert!(len > 0);\n+        trace!(\n+            \"Frame {} acquiring {:?} lock at {:?}, size {} for region {:?}\",\n+            frame,\n+            kind,\n+            ptr,\n+            len,\n+            region\n+        );\n+        self.check_bounds(ptr.offset(len, self.layout)?, true)?; // if ptr.offset is in bounds, then so is ptr (because offset checks for overflow)\n+        let alloc = self.get_mut_unchecked(ptr.alloc_id)?;\n+\n+        // Iterate over our range and acquire the lock.  If the range is already split into pieces,\n+        // we have to manipulate all of them.\n+        let lifetime = DynamicLifetime { frame, region };\n+        for lock in alloc.locks.iter_mut(ptr.offset, len) {\n+            if !lock.access_permitted(None, kind) {\n+                return err!(MemoryAcquireConflict {\n+                    ptr,\n+                    len,\n+                    kind,\n+                    lock: lock.active.clone(),\n+                });\n+            }\n+            // See what we have to do\n+            match (&mut lock.active, kind) {\n+                (active @ &mut NoLock, AccessKind::Write) => {\n+                    *active = WriteLock(lifetime);\n+                }\n+                (active @ &mut NoLock, AccessKind::Read) => {\n+                    *active = ReadLock(vec![lifetime]);\n+                }\n+                (&mut ReadLock(ref mut lifetimes), AccessKind::Read) => {\n+                    lifetimes.push(lifetime);\n+                }\n+                _ => bug!(\"We already checked that there is no conflicting lock\"),\n+            }\n+        }\n+        Ok(())\n+    }\n+\n+    /// Release or suspend a write lock of the given lifetime prematurely.\n+    /// When releasing, if there is a read lock or someone else's write lock, that's an error.\n+    /// If no lock is held, that's fine.  This can happen when e.g. a local is initialized\n+    /// from a constant, and then suspended.\n+    /// When suspending, the same cases are fine; we just register an additional suspension.\n+    pub(crate) fn suspend_write_lock(\n+        &mut self,\n+        ptr: MemoryPointer,\n+        len: u64,\n+        lock_path: &AbsLvalue<'tcx>,\n+        suspend: Option<region::Scope>,\n+    ) -> EvalResult<'tcx> {\n+        assert!(len > 0);\n+        let cur_frame = self.cur_frame;\n+        let alloc = self.get_mut_unchecked(ptr.alloc_id)?;\n+\n+        'locks: for lock in alloc.locks.iter_mut(ptr.offset, len) {\n+            let is_our_lock = match lock.active {\n+                WriteLock(lft) =>\n+                    // Double-check that we are holding the lock.\n+                    // (Due to subtyping, checking the region would not make any sense.)\n+                    lft.frame == cur_frame,\n+                ReadLock(_) | NoLock => false,\n+            };\n+            if is_our_lock {\n+                trace!(\"Releasing {:?}\", lock.active);\n+                // Disable the lock\n+                lock.active = NoLock;\n+            } else {\n+                trace!(\n+                    \"Not touching {:?} as it is not our lock\",\n+                    lock.active,\n+                );\n+            }\n+            // Check if we want to register a suspension\n+            if let Some(suspend_region) = suspend {\n+                let lock_id = WriteLockId {\n+                    frame: cur_frame,\n+                    path: lock_path.clone(),\n+                };\n+                trace!(\"Adding suspension to {:?}\", lock_id);\n+                let mut new_suspension = false;\n+                lock.suspended\n+                    .entry(lock_id)\n+                    // Remember whether we added a new suspension or not\n+                    .or_insert_with(|| { new_suspension = true; Vec::new() })\n+                    .push(suspend_region);\n+                // If the suspension is new, we should have owned this.\n+                // If there already was a suspension, we should NOT have owned this.\n+                if new_suspension == is_our_lock {\n+                    // All is well\n+                    continue 'locks;\n+                }\n+            } else {\n+                if !is_our_lock {\n+                    // All is well.\n+                    continue 'locks;\n+                }\n+            }\n+            // If we get here, releasing this is an error except for NoLock.\n+            if lock.active != NoLock {\n+                return err!(InvalidMemoryLockRelease {\n+                    ptr,\n+                    len,\n+                    frame: cur_frame,\n+                    lock: lock.active.clone(),\n+                });\n+            }\n+        }\n+\n+        Ok(())\n+    }\n+\n+    /// Release a suspension from the write lock.  If this is the last suspension or if there is no suspension, acquire the lock.\n+    pub(crate) fn recover_write_lock(\n+        &mut self,\n+        ptr: MemoryPointer,\n+        len: u64,\n+        lock_path: &AbsLvalue<'tcx>,\n+        lock_region: Option<region::Scope>,\n+        suspended_region: region::Scope,\n+    ) -> EvalResult<'tcx> {\n+        assert!(len > 0);\n+        let cur_frame = self.cur_frame;\n+        let lock_id = WriteLockId {\n+            frame: cur_frame,\n+            path: lock_path.clone(),\n+        };\n+        let alloc = self.get_mut_unchecked(ptr.alloc_id)?;\n+\n+        for lock in alloc.locks.iter_mut(ptr.offset, len) {\n+            // Check if we have a suspension here\n+            let (got_the_lock, remove_suspension) = match lock.suspended.get_mut(&lock_id) {\n+                None => {\n+                    trace!(\"No suspension around, we can just acquire\");\n+                    (true, false)\n+                }\n+                Some(suspensions) => {\n+                    trace!(\"Found suspension of {:?}, removing it\", lock_id);\n+                    // That's us!  Remove suspension (it should be in there).  The same suspension can\n+                    // occur multiple times (when there are multiple shared borrows of this that have the same\n+                    // lifetime); only remove one of them.\n+                    let idx = match suspensions.iter().enumerate().find(|&(_, re)| re == &suspended_region) {\n+                        None => // TODO: Can the user trigger this?\n+                            bug!(\"We have this lock suspended, but not for the given region.\"),\n+                        Some((idx, _)) => idx\n+                    };\n+                    suspensions.remove(idx);\n+                    let got_lock = suspensions.is_empty();\n+                    if got_lock {\n+                        trace!(\"All suspensions are gone, we can have the lock again\");\n+                    }\n+                    (got_lock, got_lock)\n+                }\n+            };\n+            if remove_suspension {\n+                // with NLL, we could do that up in the match above...\n+                assert!(got_the_lock);\n+                lock.suspended.remove(&lock_id);\n+            }\n+            if got_the_lock {\n+                match lock.active {\n+                    ref mut active @ NoLock => {\n+                        *active = WriteLock(\n+                            DynamicLifetime {\n+                                frame: cur_frame,\n+                                region: lock_region,\n+                            }\n+                        );\n+                    }\n+                    _ => {\n+                        return err!(MemoryAcquireConflict {\n+                            ptr,\n+                            len,\n+                            kind: AccessKind::Write,\n+                            lock: lock.active.clone(),\n+                        })\n+                    }\n+                }\n+            }\n+        }\n+\n+        Ok(())\n+    }\n+\n+    pub(crate) fn locks_lifetime_ended(&mut self, ending_region: Option<region::Scope>) {\n+        let cur_frame = self.cur_frame;\n+        trace!(\n+            \"Releasing frame {} locks that expire at {:?}\",\n+            cur_frame,\n+            ending_region\n+        );\n+        let has_ended = |lifetime: &DynamicLifetime| -> bool {\n+            if lifetime.frame != cur_frame {\n+                return false;\n+            }\n+            match ending_region {\n+                None => true, // When a function ends, we end *all* its locks. It's okay for a function to still have lifetime-related locks\n+                // when it returns, that can happen e.g. with NLL when a lifetime can, but does not have to, extend beyond the\n+                // end of a function.  Same for a function still having recoveries.\n+                Some(ending_region) => lifetime.region == Some(ending_region),\n+            }\n+        };\n+\n+        for alloc in self.alloc_map.values_mut() {\n+            for lock in alloc.locks.iter_mut_all() {\n+                // Delete everything that ends now -- i.e., keep only all the other lifetimes.\n+                let lock_ended = match lock.active {\n+                    WriteLock(ref lft) => has_ended(lft),\n+                    ReadLock(ref mut lfts) => {\n+                        lfts.retain(|lft| !has_ended(lft));\n+                        lfts.is_empty()\n+                    }\n+                    NoLock => false,\n+                };\n+                if lock_ended {\n+                    lock.active = NoLock;\n+                }\n+                // Also clean up suspended write locks when the function returns\n+                if ending_region.is_none() {\n+                    lock.suspended.retain(|id, _suspensions| id.frame != cur_frame);\n+                }\n+            }\n+            // Clean up the map\n+            alloc.locks.retain(|lock| match lock.active {\n+                NoLock => lock.suspended.len() > 0,\n+                _ => true,\n+            });\n+        }\n+    }\n+}\n+\n+/// Allocation accessors\n+impl<'a, 'tcx, M: Machine<'tcx>> Memory<'a, 'tcx, M> {\n+    pub fn get(&self, id: AllocId) -> EvalResult<'tcx, &Allocation<'tcx, M::MemoryKinds>> {\n+        match id.into_alloc_id_kind() {\n+            AllocIdKind::Function(_) => err!(DerefFunctionPointer),\n+            AllocIdKind::Runtime(id) => {\n+                match self.alloc_map.get(&id) {\n+                    Some(alloc) => Ok(alloc),\n+                    None => err!(DanglingPointerDeref),\n+                }\n+            }\n+        }\n+    }\n+\n+    fn get_mut_unchecked(\n+        &mut self,\n+        id: AllocId,\n+    ) -> EvalResult<'tcx, &mut Allocation<'tcx, M::MemoryKinds>> {\n+        match id.into_alloc_id_kind() {\n+            AllocIdKind::Function(_) => err!(DerefFunctionPointer),\n+            AllocIdKind::Runtime(id) => {\n+                match self.alloc_map.get_mut(&id) {\n+                    Some(alloc) => Ok(alloc),\n+                    None => err!(DanglingPointerDeref),\n+                }\n+            }\n+        }\n+    }\n+\n+    fn get_mut(&mut self, id: AllocId) -> EvalResult<'tcx, &mut Allocation<'tcx, M::MemoryKinds>> {\n+        let alloc = self.get_mut_unchecked(id)?;\n+        if alloc.mutable == Mutability::Mutable {\n+            Ok(alloc)\n+        } else {\n+            err!(ModifiedConstantMemory)\n+        }\n+    }\n+\n+    pub fn get_fn(&self, ptr: MemoryPointer) -> EvalResult<'tcx, Instance<'tcx>> {\n+        if ptr.offset != 0 {\n+            return err!(InvalidFunctionPointer);\n+        }\n+        debug!(\"reading fn ptr: {}\", ptr.alloc_id);\n+        match ptr.alloc_id.into_alloc_id_kind() {\n+            AllocIdKind::Function(id) => Ok(self.functions[id]),\n+            AllocIdKind::Runtime(_) => err!(ExecuteMemory),\n+        }\n+    }\n+\n+    /// For debugging, print an allocation and all allocations it points to, recursively.\n+    pub fn dump_alloc(&self, id: AllocId) {\n+        self.dump_allocs(vec![id]);\n+    }\n+\n+    /// For debugging, print a list of allocations and all allocations they point to, recursively.\n+    pub fn dump_allocs(&self, mut allocs: Vec<AllocId>) {\n+        use std::fmt::Write;\n+        allocs.sort();\n+        allocs.dedup();\n+        let mut allocs_to_print = VecDeque::from(allocs);\n+        let mut allocs_seen = HashSet::new();\n+\n+        while let Some(id) = allocs_to_print.pop_front() {\n+            let mut msg = format!(\"Alloc {:<5} \", format!(\"{}:\", id));\n+            let prefix_len = msg.len();\n+            let mut relocations = vec![];\n+\n+            let alloc = match id.into_alloc_id_kind() {\n+                AllocIdKind::Function(id) => {\n+                    trace!(\"{} {}\", msg, self.functions[id]);\n+                    continue;\n+                }\n+                AllocIdKind::Runtime(id) => {\n+                    match self.alloc_map.get(&id) {\n+                        Some(a) => a,\n+                        None => {\n+                            trace!(\"{} (deallocated)\", msg);\n+                            continue;\n+                        }\n+                    }\n+                }\n+            };\n+\n+            for i in 0..(alloc.bytes.len() as u64) {\n+                if let Some(&target_id) = alloc.relocations.get(&i) {\n+                    if allocs_seen.insert(target_id) {\n+                        allocs_to_print.push_back(target_id);\n+                    }\n+                    relocations.push((i, target_id));\n+                }\n+                if alloc.undef_mask.is_range_defined(i, i + 1) {\n+                    // this `as usize` is fine, since `i` came from a `usize`\n+                    write!(msg, \"{:02x} \", alloc.bytes[i as usize]).unwrap();\n+                } else {\n+                    msg.push_str(\"__ \");\n+                }\n+            }\n+\n+            let immutable = match (alloc.kind, alloc.mutable) {\n+                (MemoryKind::UninitializedStatic, _) => {\n+                    \" (static in the process of initialization)\".to_owned()\n+                }\n+                (MemoryKind::Static, Mutability::Mutable) => \" (static mut)\".to_owned(),\n+                (MemoryKind::Static, Mutability::Immutable) => \" (immutable)\".to_owned(),\n+                (MemoryKind::Machine(m), _) => format!(\" ({:?})\", m),\n+                (MemoryKind::Stack, _) => \" (stack)\".to_owned(),\n+            };\n+            trace!(\n+                \"{}({} bytes, alignment {}){}\",\n+                msg,\n+                alloc.bytes.len(),\n+                alloc.align,\n+                immutable\n+            );\n+\n+            if !relocations.is_empty() {\n+                msg.clear();\n+                write!(msg, \"{:1$}\", \"\", prefix_len).unwrap(); // Print spaces.\n+                let mut pos = 0;\n+                let relocation_width = (self.pointer_size() - 1) * 3;\n+                for (i, target_id) in relocations {\n+                    // this `as usize` is fine, since we can't print more chars than `usize::MAX`\n+                    write!(msg, \"{:1$}\", \"\", ((i - pos) * 3) as usize).unwrap();\n+                    let target = format!(\"({})\", target_id);\n+                    // this `as usize` is fine, since we can't print more chars than `usize::MAX`\n+                    write!(msg, \"\u2514{0:\u2500^1$}\u2518 \", target, relocation_width as usize).unwrap();\n+                    pos = i + self.pointer_size();\n+                }\n+                trace!(\"{}\", msg);\n+            }\n+        }\n+    }\n+\n+    pub fn leak_report(&self) -> usize {\n+        trace!(\"### LEAK REPORT ###\");\n+        let leaks: Vec<_> = self.alloc_map\n+            .iter()\n+            .filter_map(|(&key, val)| if val.kind != MemoryKind::Static {\n+                Some(AllocIdKind::Runtime(key).into_alloc_id())\n+            } else {\n+                None\n+            })\n+            .collect();\n+        let n = leaks.len();\n+        self.dump_allocs(leaks);\n+        n\n+    }\n+}\n+\n+/// Byte accessors\n+impl<'a, 'tcx, M: Machine<'tcx>> Memory<'a, 'tcx, M> {\n+    fn get_bytes_unchecked(\n+        &self,\n+        ptr: MemoryPointer,\n+        size: u64,\n+        align: u64,\n+    ) -> EvalResult<'tcx, &[u8]> {\n+        // Zero-sized accesses can use dangling pointers, but they still have to be aligned and non-NULL\n+        self.check_align(ptr.into(), align, Some(AccessKind::Read))?;\n+        if size == 0 {\n+            return Ok(&[]);\n+        }\n+        self.check_locks(ptr, size, AccessKind::Read)?;\n+        self.check_bounds(ptr.offset(size, self)?, true)?; // if ptr.offset is in bounds, then so is ptr (because offset checks for overflow)\n+        let alloc = self.get(ptr.alloc_id)?;\n+        assert_eq!(ptr.offset as usize as u64, ptr.offset);\n+        assert_eq!(size as usize as u64, size);\n+        let offset = ptr.offset as usize;\n+        Ok(&alloc.bytes[offset..offset + size as usize])\n+    }\n+\n+    fn get_bytes_unchecked_mut(\n+        &mut self,\n+        ptr: MemoryPointer,\n+        size: u64,\n+        align: u64,\n+    ) -> EvalResult<'tcx, &mut [u8]> {\n+        // Zero-sized accesses can use dangling pointers, but they still have to be aligned and non-NULL\n+        self.check_align(ptr.into(), align, Some(AccessKind::Write))?;\n+        if size == 0 {\n+            return Ok(&mut []);\n+        }\n+        self.check_locks(ptr, size, AccessKind::Write)?;\n+        self.check_bounds(ptr.offset(size, self.layout)?, true)?; // if ptr.offset is in bounds, then so is ptr (because offset checks for overflow)\n+        let alloc = self.get_mut(ptr.alloc_id)?;\n+        assert_eq!(ptr.offset as usize as u64, ptr.offset);\n+        assert_eq!(size as usize as u64, size);\n+        let offset = ptr.offset as usize;\n+        Ok(&mut alloc.bytes[offset..offset + size as usize])\n+    }\n+\n+    fn get_bytes(&self, ptr: MemoryPointer, size: u64, align: u64) -> EvalResult<'tcx, &[u8]> {\n+        assert_ne!(size, 0);\n+        if self.relocations(ptr, size)?.count() != 0 {\n+            return err!(ReadPointerAsBytes);\n+        }\n+        self.check_defined(ptr, size)?;\n+        self.get_bytes_unchecked(ptr, size, align)\n+    }\n+\n+    fn get_bytes_mut(\n+        &mut self,\n+        ptr: MemoryPointer,\n+        size: u64,\n+        align: u64,\n+    ) -> EvalResult<'tcx, &mut [u8]> {\n+        assert_ne!(size, 0);\n+        self.clear_relocations(ptr, size)?;\n+        self.mark_definedness(ptr.into(), size, true)?;\n+        self.get_bytes_unchecked_mut(ptr, size, align)\n+    }\n+}\n+\n+/// Reading and writing\n+impl<'a, 'tcx, M: Machine<'tcx>> Memory<'a, 'tcx, M> {\n+    /// mark an allocation pointed to by a static as static and initialized\n+    fn mark_inner_allocation_initialized(\n+        &mut self,\n+        alloc: AllocId,\n+        mutability: Mutability,\n+    ) -> EvalResult<'tcx> {\n+        // relocations into other statics are not \"inner allocations\"\n+        if self.get(alloc).ok().map_or(false, |alloc| {\n+            alloc.kind != MemoryKind::UninitializedStatic\n+        })\n+        {\n+            self.mark_static_initalized(alloc, mutability)?;\n+        }\n+        Ok(())\n+    }\n+\n+    /// mark an allocation as static and initialized, either mutable or not\n+    pub fn mark_static_initalized(\n+        &mut self,\n+        alloc_id: AllocId,\n+        mutability: Mutability,\n+    ) -> EvalResult<'tcx> {\n+        trace!(\n+            \"mark_static_initalized {:?}, mutability: {:?}\",\n+            alloc_id,\n+            mutability\n+        );\n+        // do not use `self.get_mut(alloc_id)` here, because we might have already marked a\n+        // sub-element or have circular pointers (e.g. `Rc`-cycles)\n+        let alloc_id = match alloc_id.into_alloc_id_kind() {\n+            AllocIdKind::Function(_) => return Ok(()),\n+            AllocIdKind::Runtime(id) => id,\n+        };\n+        let relocations = match self.alloc_map.get_mut(&alloc_id) {\n+            Some(&mut Allocation {\n+                     ref mut relocations,\n+                     ref mut kind,\n+                     ref mut mutable,\n+                     ..\n+                 }) => {\n+                match *kind {\n+                    // const eval results can refer to \"locals\".\n+                    // E.g. `const Foo: &u32 = &1;` refers to the temp local that stores the `1`\n+                    MemoryKind::Stack |\n+                    // The entire point of this function\n+                    MemoryKind::UninitializedStatic => {},\n+                    MemoryKind::Machine(m) => M::mark_static_initialized(m)?,\n+                    MemoryKind::Static => {\n+                        trace!(\"mark_static_initalized: skipping already initialized static referred to by static currently being initialized\");\n+                        return Ok(());\n+                    },\n+                }\n+                *kind = MemoryKind::Static;\n+                *mutable = mutability;\n+                // take out the relocations vector to free the borrow on self, so we can call\n+                // mark recursively\n+                mem::replace(relocations, Default::default())\n+            }\n+            None => return err!(DanglingPointerDeref),\n+        };\n+        // recurse into inner allocations\n+        for &alloc in relocations.values() {\n+            self.mark_inner_allocation_initialized(alloc, mutability)?;\n+        }\n+        // put back the relocations\n+        self.alloc_map\n+            .get_mut(&alloc_id)\n+            .expect(\"checked above\")\n+            .relocations = relocations;\n+        Ok(())\n+    }\n+\n+    pub fn copy(\n+        &mut self,\n+        src: Pointer,\n+        dest: Pointer,\n+        size: u64,\n+        align: u64,\n+        nonoverlapping: bool,\n+    ) -> EvalResult<'tcx> {\n+        // Empty accesses don't need to be valid pointers, but they should still be aligned\n+        self.check_align(src, align, Some(AccessKind::Read))?;\n+        self.check_align(dest, align, Some(AccessKind::Write))?;\n+        if size == 0 {\n+            return Ok(());\n+        }\n+        let src = src.to_ptr()?;\n+        let dest = dest.to_ptr()?;\n+        self.check_relocation_edges(src, size)?;\n+\n+        // first copy the relocations to a temporary buffer, because\n+        // `get_bytes_mut` will clear the relocations, which is correct,\n+        // since we don't want to keep any relocations at the target.\n+\n+        let relocations: Vec<_> = self.relocations(src, size)?\n+            .map(|(&offset, &alloc_id)| {\n+                // Update relocation offsets for the new positions in the destination allocation.\n+                (offset + dest.offset - src.offset, alloc_id)\n+            })\n+            .collect();\n+\n+        let src_bytes = self.get_bytes_unchecked(src, size, align)?.as_ptr();\n+        let dest_bytes = self.get_bytes_mut(dest, size, align)?.as_mut_ptr();\n+\n+        // SAFE: The above indexing would have panicked if there weren't at least `size` bytes\n+        // behind `src` and `dest`. Also, we use the overlapping-safe `ptr::copy` if `src` and\n+        // `dest` could possibly overlap.\n+        unsafe {\n+            assert_eq!(size as usize as u64, size);\n+            if src.alloc_id == dest.alloc_id {\n+                if nonoverlapping {\n+                    if (src.offset <= dest.offset && src.offset + size > dest.offset) ||\n+                        (dest.offset <= src.offset && dest.offset + size > src.offset)\n+                    {\n+                        return err!(Intrinsic(\n+                            format!(\"copy_nonoverlapping called on overlapping ranges\"),\n+                        ));\n+                    }\n+                }\n+                ptr::copy(src_bytes, dest_bytes, size as usize);\n+            } else {\n+                ptr::copy_nonoverlapping(src_bytes, dest_bytes, size as usize);\n+            }\n+        }\n+\n+        self.copy_undef_mask(src, dest, size)?;\n+        // copy back the relocations\n+        self.get_mut(dest.alloc_id)?.relocations.extend(relocations);\n+\n+        Ok(())\n+    }\n+\n+    pub fn read_c_str(&self, ptr: MemoryPointer) -> EvalResult<'tcx, &[u8]> {\n+        let alloc = self.get(ptr.alloc_id)?;\n+        assert_eq!(ptr.offset as usize as u64, ptr.offset);\n+        let offset = ptr.offset as usize;\n+        match alloc.bytes[offset..].iter().position(|&c| c == 0) {\n+            Some(size) => {\n+                if self.relocations(ptr, (size + 1) as u64)?.count() != 0 {\n+                    return err!(ReadPointerAsBytes);\n+                }\n+                self.check_defined(ptr, (size + 1) as u64)?;\n+                self.check_locks(ptr, (size + 1) as u64, AccessKind::Read)?;\n+                Ok(&alloc.bytes[offset..offset + size])\n+            }\n+            None => err!(UnterminatedCString(ptr)),\n+        }\n+    }\n+\n+    pub fn read_bytes(&self, ptr: Pointer, size: u64) -> EvalResult<'tcx, &[u8]> {\n+        // Empty accesses don't need to be valid pointers, but they should still be non-NULL\n+        self.check_align(ptr, 1, Some(AccessKind::Read))?;\n+        if size == 0 {\n+            return Ok(&[]);\n+        }\n+        self.get_bytes(ptr.to_ptr()?, size, 1)\n+    }\n+\n+    pub fn write_bytes(&mut self, ptr: Pointer, src: &[u8]) -> EvalResult<'tcx> {\n+        // Empty accesses don't need to be valid pointers, but they should still be non-NULL\n+        self.check_align(ptr, 1, Some(AccessKind::Write))?;\n+        if src.is_empty() {\n+            return Ok(());\n+        }\n+        let bytes = self.get_bytes_mut(ptr.to_ptr()?, src.len() as u64, 1)?;\n+        bytes.clone_from_slice(src);\n+        Ok(())\n+    }\n+\n+    pub fn write_repeat(&mut self, ptr: Pointer, val: u8, count: u64) -> EvalResult<'tcx> {\n+        // Empty accesses don't need to be valid pointers, but they should still be non-NULL\n+        self.check_align(ptr, 1, Some(AccessKind::Write))?;\n+        if count == 0 {\n+            return Ok(());\n+        }\n+        let bytes = self.get_bytes_mut(ptr.to_ptr()?, count, 1)?;\n+        for b in bytes {\n+            *b = val;\n+        }\n+        Ok(())\n+    }\n+\n+    pub fn read_primval(&self, ptr: MemoryPointer, size: u64, signed: bool) -> EvalResult<'tcx, PrimVal> {\n+        self.check_relocation_edges(ptr, size)?; // Make sure we don't read part of a pointer as a pointer\n+        let endianess = self.endianess();\n+        let bytes = self.get_bytes_unchecked(ptr, size, self.int_align(size))?;\n+        // Undef check happens *after* we established that the alignment is correct.\n+        // We must not return Ok() for unaligned pointers!\n+        if self.check_defined(ptr, size).is_err() {\n+            return Ok(PrimVal::Undef.into());\n+        }\n+        // Now we do the actual reading\n+        let bytes = if signed {\n+            read_target_int(endianess, bytes).unwrap() as u128\n+        } else {\n+            read_target_uint(endianess, bytes).unwrap()\n+        };\n+        // See if we got a pointer\n+        if size != self.pointer_size() {\n+            if self.relocations(ptr, size)?.count() != 0 {\n+                return err!(ReadPointerAsBytes);\n+            }\n+        } else {\n+            let alloc = self.get(ptr.alloc_id)?;\n+            match alloc.relocations.get(&ptr.offset) {\n+                Some(&alloc_id) => return Ok(PrimVal::Ptr(MemoryPointer::new(alloc_id, bytes as u64))),\n+                None => {},\n+            }\n+        }\n+        // We don't. Just return the bytes.\n+        Ok(PrimVal::Bytes(bytes))\n+    }\n+\n+    pub fn read_ptr_sized_unsigned(&self, ptr: MemoryPointer) -> EvalResult<'tcx, PrimVal> {\n+        self.read_primval(ptr, self.pointer_size(), false)\n+    }\n+\n+    pub fn write_primval(&mut self, ptr: MemoryPointer, val: PrimVal, size: u64, signed: bool) -> EvalResult<'tcx> {\n+        let endianess = self.endianess();\n+\n+        let bytes = match val {\n+            PrimVal::Ptr(val) => {\n+                assert_eq!(size, self.pointer_size());\n+                val.offset as u128\n+            }\n+\n+            PrimVal::Bytes(bytes) => {\n+                // We need to mask here, or the byteorder crate can die when given a u64 larger\n+                // than fits in an integer of the requested size.\n+                let mask = match size {\n+                    1 => !0u8 as u128,\n+                    2 => !0u16 as u128,\n+                    4 => !0u32 as u128,\n+                    8 => !0u64 as u128,\n+                    16 => !0,\n+                    n => bug!(\"unexpected PrimVal::Bytes size: {}\", n),\n+                };\n+                bytes & mask\n+            }\n+\n+            PrimVal::Undef => {\n+                self.mark_definedness(PrimVal::Ptr(ptr).into(), size, false)?;\n+                return Ok(());\n+            }\n+        };\n+\n+        {\n+            let align = self.int_align(size);\n+            let dst = self.get_bytes_mut(ptr, size, align)?;\n+            if signed {\n+                write_target_int(endianess, dst, bytes as i128).unwrap();\n+            } else {\n+                write_target_uint(endianess, dst, bytes).unwrap();\n+            }\n+        }\n+\n+        // See if we have to also write a relocation\n+        match val {\n+            PrimVal::Ptr(val) => {\n+                self.get_mut(ptr.alloc_id)?.relocations.insert(\n+                    ptr.offset,\n+                    val.alloc_id,\n+                );\n+            }\n+            _ => {}\n+        }\n+\n+        Ok(())\n+    }\n+\n+    pub fn write_ptr_sized_unsigned(&mut self, ptr: MemoryPointer, val: PrimVal) -> EvalResult<'tcx> {\n+        let ptr_size = self.pointer_size();\n+        self.write_primval(ptr, val, ptr_size, false)\n+    }\n+\n+    fn int_align(&self, size: u64) -> u64 {\n+        // We assume pointer-sized integers have the same alignment as pointers.\n+        // We also assume signed and unsigned integers of the same size have the same alignment.\n+        match size {\n+            1 => self.layout.i8_align.abi(),\n+            2 => self.layout.i16_align.abi(),\n+            4 => self.layout.i32_align.abi(),\n+            8 => self.layout.i64_align.abi(),\n+            16 => self.layout.i128_align.abi(),\n+            _ => bug!(\"bad integer size: {}\", size),\n+        }\n+    }\n+}\n+\n+/// Relocations\n+impl<'a, 'tcx, M: Machine<'tcx>> Memory<'a, 'tcx, M> {\n+    fn relocations(\n+        &self,\n+        ptr: MemoryPointer,\n+        size: u64,\n+    ) -> EvalResult<'tcx, btree_map::Range<u64, AllocId>> {\n+        let start = ptr.offset.saturating_sub(self.pointer_size() - 1);\n+        let end = ptr.offset + size;\n+        Ok(self.get(ptr.alloc_id)?.relocations.range(start..end))\n+    }\n+\n+    fn clear_relocations(&mut self, ptr: MemoryPointer, size: u64) -> EvalResult<'tcx> {\n+        // Find all relocations overlapping the given range.\n+        let keys: Vec<_> = self.relocations(ptr, size)?.map(|(&k, _)| k).collect();\n+        if keys.is_empty() {\n+            return Ok(());\n+        }\n+\n+        // Find the start and end of the given range and its outermost relocations.\n+        let start = ptr.offset;\n+        let end = start + size;\n+        let first = *keys.first().unwrap();\n+        let last = *keys.last().unwrap() + self.pointer_size();\n+\n+        let alloc = self.get_mut(ptr.alloc_id)?;\n+\n+        // Mark parts of the outermost relocations as undefined if they partially fall outside the\n+        // given range.\n+        if first < start {\n+            alloc.undef_mask.set_range(first, start, false);\n+        }\n+        if last > end {\n+            alloc.undef_mask.set_range(end, last, false);\n+        }\n+\n+        // Forget all the relocations.\n+        for k in keys {\n+            alloc.relocations.remove(&k);\n+        }\n+\n+        Ok(())\n+    }\n+\n+    fn check_relocation_edges(&self, ptr: MemoryPointer, size: u64) -> EvalResult<'tcx> {\n+        let overlapping_start = self.relocations(ptr, 0)?.count();\n+        let overlapping_end = self.relocations(ptr.offset(size, self.layout)?, 0)?.count();\n+        if overlapping_start + overlapping_end != 0 {\n+            return err!(ReadPointerAsBytes);\n+        }\n+        Ok(())\n+    }\n+}\n+\n+/// Undefined bytes\n+impl<'a, 'tcx, M: Machine<'tcx>> Memory<'a, 'tcx, M> {\n+    // FIXME(solson): This is a very naive, slow version.\n+    fn copy_undef_mask(\n+        &mut self,\n+        src: MemoryPointer,\n+        dest: MemoryPointer,\n+        size: u64,\n+    ) -> EvalResult<'tcx> {\n+        // The bits have to be saved locally before writing to dest in case src and dest overlap.\n+        assert_eq!(size as usize as u64, size);\n+        let mut v = Vec::with_capacity(size as usize);\n+        for i in 0..size {\n+            let defined = self.get(src.alloc_id)?.undef_mask.get(src.offset + i);\n+            v.push(defined);\n+        }\n+        for (i, defined) in v.into_iter().enumerate() {\n+            self.get_mut(dest.alloc_id)?.undef_mask.set(\n+                dest.offset +\n+                    i as u64,\n+                defined,\n+            );\n+        }\n+        Ok(())\n+    }\n+\n+    fn check_defined(&self, ptr: MemoryPointer, size: u64) -> EvalResult<'tcx> {\n+        let alloc = self.get(ptr.alloc_id)?;\n+        if !alloc.undef_mask.is_range_defined(\n+            ptr.offset,\n+            ptr.offset + size,\n+        )\n+        {\n+            return err!(ReadUndefBytes);\n+        }\n+        Ok(())\n+    }\n+\n+    pub fn mark_definedness(\n+        &mut self,\n+        ptr: Pointer,\n+        size: u64,\n+        new_state: bool,\n+    ) -> EvalResult<'tcx> {\n+        if size == 0 {\n+            return Ok(());\n+        }\n+        let ptr = ptr.to_ptr()?;\n+        let alloc = self.get_mut(ptr.alloc_id)?;\n+        alloc.undef_mask.set_range(\n+            ptr.offset,\n+            ptr.offset + size,\n+            new_state,\n+        );\n+        Ok(())\n+    }\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Methods to access integers in the target endianess\n+////////////////////////////////////////////////////////////////////////////////\n+\n+fn write_target_uint(\n+    endianess: layout::Endian,\n+    mut target: &mut [u8],\n+    data: u128,\n+) -> Result<(), io::Error> {\n+    let len = target.len();\n+    match endianess {\n+        layout::Endian::Little => target.write_uint128::<LittleEndian>(data, len),\n+        layout::Endian::Big => target.write_uint128::<BigEndian>(data, len),\n+    }\n+}\n+fn write_target_int(\n+    endianess: layout::Endian,\n+    mut target: &mut [u8],\n+    data: i128,\n+) -> Result<(), io::Error> {\n+    let len = target.len();\n+    match endianess {\n+        layout::Endian::Little => target.write_int128::<LittleEndian>(data, len),\n+        layout::Endian::Big => target.write_int128::<BigEndian>(data, len),\n+    }\n+}\n+\n+fn read_target_uint(endianess: layout::Endian, mut source: &[u8]) -> Result<u128, io::Error> {\n+    match endianess {\n+        layout::Endian::Little => source.read_uint128::<LittleEndian>(source.len()),\n+        layout::Endian::Big => source.read_uint128::<BigEndian>(source.len()),\n+    }\n+}\n+\n+fn read_target_int(endianess: layout::Endian, mut source: &[u8]) -> Result<i128, io::Error> {\n+    match endianess {\n+        layout::Endian::Little => source.read_int128::<LittleEndian>(source.len()),\n+        layout::Endian::Big => source.read_int128::<BigEndian>(source.len()),\n+    }\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Undefined byte tracking\n+////////////////////////////////////////////////////////////////////////////////\n+\n+type Block = u64;\n+const BLOCK_SIZE: u64 = 64;\n+\n+#[derive(Clone, Debug)]\n+pub struct UndefMask {\n+    blocks: Vec<Block>,\n+    len: u64,\n+}\n+\n+impl UndefMask {\n+    fn new(size: u64) -> Self {\n+        let mut m = UndefMask {\n+            blocks: vec![],\n+            len: 0,\n+        };\n+        m.grow(size, false);\n+        m\n+    }\n+\n+    /// Check whether the range `start..end` (end-exclusive) is entirely defined.\n+    pub fn is_range_defined(&self, start: u64, end: u64) -> bool {\n+        if end > self.len {\n+            return false;\n+        }\n+        for i in start..end {\n+            if !self.get(i) {\n+                return false;\n+            }\n+        }\n+        true\n+    }\n+\n+    fn set_range(&mut self, start: u64, end: u64, new_state: bool) {\n+        let len = self.len;\n+        if end > len {\n+            self.grow(end - len, new_state);\n+        }\n+        self.set_range_inbounds(start, end, new_state);\n+    }\n+\n+    fn set_range_inbounds(&mut self, start: u64, end: u64, new_state: bool) {\n+        for i in start..end {\n+            self.set(i, new_state);\n+        }\n+    }\n+\n+    fn get(&self, i: u64) -> bool {\n+        let (block, bit) = bit_index(i);\n+        (self.blocks[block] & 1 << bit) != 0\n+    }\n+\n+    fn set(&mut self, i: u64, new_state: bool) {\n+        let (block, bit) = bit_index(i);\n+        if new_state {\n+            self.blocks[block] |= 1 << bit;\n+        } else {\n+            self.blocks[block] &= !(1 << bit);\n+        }\n+    }\n+\n+    fn grow(&mut self, amount: u64, new_state: bool) {\n+        let unused_trailing_bits = self.blocks.len() as u64 * BLOCK_SIZE - self.len;\n+        if amount > unused_trailing_bits {\n+            let additional_blocks = amount / BLOCK_SIZE + 1;\n+            assert_eq!(additional_blocks as usize as u64, additional_blocks);\n+            self.blocks.extend(\n+                iter::repeat(0).take(additional_blocks as usize),\n+            );\n+        }\n+        let start = self.len;\n+        self.len += amount;\n+        self.set_range_inbounds(start, start + amount, new_state);\n+    }\n+}\n+\n+fn bit_index(bits: u64) -> (usize, usize) {\n+    let a = bits / BLOCK_SIZE;\n+    let b = bits % BLOCK_SIZE;\n+    assert_eq!(a as usize as u64, a);\n+    assert_eq!(b as usize as u64, b);\n+    (a as usize, b as usize)\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Unaligned accesses\n+////////////////////////////////////////////////////////////////////////////////\n+\n+pub trait HasMemory<'a, 'tcx, M: Machine<'tcx>> {\n+    fn memory_mut(&mut self) -> &mut Memory<'a, 'tcx, M>;\n+    fn memory(&self) -> &Memory<'a, 'tcx, M>;\n+\n+    // These are not supposed to be overriden.\n+    fn read_maybe_aligned<F, T>(&self, aligned: bool, f: F) -> EvalResult<'tcx, T>\n+    where\n+        F: FnOnce(&Self) -> EvalResult<'tcx, T>,\n+    {\n+        let old = self.memory().reads_are_aligned.get();\n+        // Do alignment checking if *all* nested calls say it has to be aligned.\n+        self.memory().reads_are_aligned.set(old && aligned);\n+        let t = f(self);\n+        self.memory().reads_are_aligned.set(old);\n+        t\n+    }\n+\n+    fn read_maybe_aligned_mut<F, T>(&mut self, aligned: bool, f: F) -> EvalResult<'tcx, T>\n+    where\n+        F: FnOnce(&mut Self) -> EvalResult<'tcx, T>,\n+    {\n+        let old = self.memory().reads_are_aligned.get();\n+        // Do alignment checking if *all* nested calls say it has to be aligned.\n+        self.memory().reads_are_aligned.set(old && aligned);\n+        let t = f(self);\n+        self.memory().reads_are_aligned.set(old);\n+        t\n+    }\n+\n+    fn write_maybe_aligned_mut<F, T>(&mut self, aligned: bool, f: F) -> EvalResult<'tcx, T>\n+    where\n+        F: FnOnce(&mut Self) -> EvalResult<'tcx, T>,\n+    {\n+        let old = self.memory().writes_are_aligned.get();\n+        // Do alignment checking if *all* nested calls say it has to be aligned.\n+        self.memory().writes_are_aligned.set(old && aligned);\n+        let t = f(self);\n+        self.memory().writes_are_aligned.set(old);\n+        t\n+    }\n+}\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> HasMemory<'a, 'tcx, M> for Memory<'a, 'tcx, M> {\n+    #[inline]\n+    fn memory_mut(&mut self) -> &mut Memory<'a, 'tcx, M> {\n+        self\n+    }\n+\n+    #[inline]\n+    fn memory(&self) -> &Memory<'a, 'tcx, M> {\n+        self\n+    }\n+}\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> HasMemory<'a, 'tcx, M> for EvalContext<'a, 'tcx, M> {\n+    #[inline]\n+    fn memory_mut(&mut self) -> &mut Memory<'a, 'tcx, M> {\n+        &mut self.memory\n+    }\n+\n+    #[inline]\n+    fn memory(&self) -> &Memory<'a, 'tcx, M> {\n+        &self.memory\n+    }\n+}\n+\n+////////////////////////////////////////////////////////////////////////////////\n+// Pointer arithmetic\n+////////////////////////////////////////////////////////////////////////////////\n+\n+pub trait PointerArithmetic: layout::HasDataLayout {\n+    // These are not supposed to be overriden.\n+\n+    //// Trunace the given value to the pointer size; also return whether there was an overflow\n+    fn truncate_to_ptr(self, val: u128) -> (u64, bool) {\n+        let max_ptr_plus_1 = 1u128 << self.data_layout().pointer_size.bits();\n+        ((val % max_ptr_plus_1) as u64, val >= max_ptr_plus_1)\n+    }\n+\n+    // Overflow checking only works properly on the range from -u64 to +u64.\n+    fn overflowing_signed_offset(self, val: u64, i: i128) -> (u64, bool) {\n+        // FIXME: is it possible to over/underflow here?\n+        if i < 0 {\n+            // trickery to ensure that i64::min_value() works fine\n+            // this formula only works for true negative values, it panics for zero!\n+            let n = u64::max_value() - (i as u64) + 1;\n+            val.overflowing_sub(n)\n+        } else {\n+            self.overflowing_offset(val, i as u64)\n+        }\n+    }\n+\n+    fn overflowing_offset(self, val: u64, i: u64) -> (u64, bool) {\n+        let (res, over1) = val.overflowing_add(i);\n+        let (res, over2) = self.truncate_to_ptr(res as u128);\n+        (res, over1 || over2)\n+    }\n+\n+    fn signed_offset<'tcx>(self, val: u64, i: i64) -> EvalResult<'tcx, u64> {\n+        let (res, over) = self.overflowing_signed_offset(val, i as i128);\n+        if over { err!(OverflowingMath) } else { Ok(res) }\n+    }\n+\n+    fn offset<'tcx>(self, val: u64, i: u64) -> EvalResult<'tcx, u64> {\n+        let (res, over) = self.overflowing_offset(val, i);\n+        if over { err!(OverflowingMath) } else { Ok(res) }\n+    }\n+\n+    fn wrapping_signed_offset(self, val: u64, i: i64) -> u64 {\n+        self.overflowing_signed_offset(val, i as i128).0\n+    }\n+}\n+\n+impl<T: layout::HasDataLayout> PointerArithmetic for T {}\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> layout::HasDataLayout for &'a Memory<'a, 'tcx, M> {\n+    #[inline]\n+    fn data_layout(&self) -> &TargetDataLayout {\n+        self.layout\n+    }\n+}\n+impl<'a, 'tcx, M: Machine<'tcx>> layout::HasDataLayout for &'a EvalContext<'a, 'tcx, M> {\n+    #[inline]\n+    fn data_layout(&self) -> &TargetDataLayout {\n+        self.memory().layout\n+    }\n+}\n+\n+impl<'c, 'b, 'a, 'tcx, M: Machine<'tcx>> layout::HasDataLayout\n+    for &'c &'b mut EvalContext<'a, 'tcx, M> {\n+    #[inline]\n+    fn data_layout(&self) -> &TargetDataLayout {\n+        self.memory().layout\n+    }\n+}"}, {"sha": "08837c4fb6d7805abab6ebb03141285421e0d749", "filename": "src/librustc/mir/interpret/mod.rs", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fmod.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,42 @@\n+//! An interpreter for MIR used in CTFE and by miri\n+\n+#[macro_export]\n+macro_rules! err {\n+    ($($tt:tt)*) => { Err($crate::interpret::EvalErrorKind::$($tt)*.into()) };\n+}\n+\n+mod cast;\n+mod const_eval;\n+mod error;\n+mod eval_context;\n+mod lvalue;\n+mod validation;\n+mod machine;\n+mod memory;\n+mod operator;\n+mod range_map;\n+mod step;\n+mod terminator;\n+mod traits;\n+mod value;\n+\n+pub use self::error::{EvalError, EvalResult, EvalErrorKind};\n+\n+pub use self::eval_context::{EvalContext, Frame, ResourceLimits, StackPopCleanup, DynamicLifetime,\n+                             TyAndPacked, PtrAndAlign, ValTy};\n+\n+pub use self::lvalue::{Lvalue, LvalueExtra, GlobalId};\n+\n+pub use self::memory::{AllocId, Memory, MemoryPointer, MemoryKind, HasMemory, AccessKind, AllocIdKind};\n+\n+use self::memory::{PointerArithmetic, Lock};\n+\n+use self::range_map::RangeMap;\n+\n+pub use self::value::{PrimVal, PrimValKind, Value, Pointer};\n+\n+pub use self::const_eval::{eval_body_as_integer, eval_body_as_primval};\n+\n+pub use self::machine::Machine;\n+\n+pub use self::validation::{ValidationQuery, AbsLvalue};"}, {"sha": "7fe4691ffff0cabc4faa30b1087c836ae43b41d2", "filename": "src/librustc/mir/interpret/operator.rs", "status": "added", "additions": 268, "deletions": 0, "changes": 268, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Foperator.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Foperator.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Foperator.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,268 @@\n+use rustc::mir;\n+use rustc::ty::Ty;\n+use rustc_const_math::ConstFloat;\n+use syntax::ast::FloatTy;\n+use std::cmp::Ordering;\n+\n+use super::{EvalResult, EvalContext, Lvalue, Machine, ValTy};\n+\n+use super::value::{PrimVal, PrimValKind, Value, bytes_to_f32, bytes_to_f64, f32_to_bytes,\n+                   f64_to_bytes};\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    fn binop_with_overflow(\n+        &mut self,\n+        op: mir::BinOp,\n+        left: ValTy<'tcx>,\n+        right: ValTy<'tcx>,\n+    ) -> EvalResult<'tcx, (PrimVal, bool)> {\n+        let left_val = self.value_to_primval(left)?;\n+        let right_val = self.value_to_primval(right)?;\n+        self.binary_op(op, left_val, left.ty, right_val, right.ty)\n+    }\n+\n+    /// Applies the binary operation `op` to the two operands and writes a tuple of the result\n+    /// and a boolean signifying the potential overflow to the destination.\n+    pub fn intrinsic_with_overflow(\n+        &mut self,\n+        op: mir::BinOp,\n+        left: ValTy<'tcx>,\n+        right: ValTy<'tcx>,\n+        dest: Lvalue,\n+        dest_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        let (val, overflowed) = self.binop_with_overflow(op, left, right)?;\n+        let val = Value::ByValPair(val, PrimVal::from_bool(overflowed));\n+        let valty = ValTy {\n+            value: val,\n+            ty: dest_ty,\n+        };\n+        self.write_value(valty, dest)\n+    }\n+\n+    /// Applies the binary operation `op` to the arguments and writes the result to the\n+    /// destination. Returns `true` if the operation overflowed.\n+    pub fn intrinsic_overflowing(\n+        &mut self,\n+        op: mir::BinOp,\n+        left: ValTy<'tcx>,\n+        right: ValTy<'tcx>,\n+        dest: Lvalue,\n+        dest_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx, bool> {\n+        let (val, overflowed) = self.binop_with_overflow(op, left, right)?;\n+        self.write_primval(dest, val, dest_ty)?;\n+        Ok(overflowed)\n+    }\n+}\n+\n+macro_rules! overflow {\n+    ($op:ident, $l:expr, $r:expr) => ({\n+        let (val, overflowed) = $l.$op($r);\n+        let primval = PrimVal::Bytes(val as u128);\n+        Ok((primval, overflowed))\n+    })\n+}\n+\n+macro_rules! int_arithmetic {\n+    ($kind:expr, $int_op:ident, $l:expr, $r:expr) => ({\n+        let l = $l;\n+        let r = $r;\n+        use super::PrimValKind::*;\n+        match $kind {\n+            I8  => overflow!($int_op, l as i8,  r as i8),\n+            I16 => overflow!($int_op, l as i16, r as i16),\n+            I32 => overflow!($int_op, l as i32, r as i32),\n+            I64 => overflow!($int_op, l as i64, r as i64),\n+            I128 => overflow!($int_op, l as i128, r as i128),\n+            U8  => overflow!($int_op, l as u8,  r as u8),\n+            U16 => overflow!($int_op, l as u16, r as u16),\n+            U32 => overflow!($int_op, l as u32, r as u32),\n+            U64 => overflow!($int_op, l as u64, r as u64),\n+            U128 => overflow!($int_op, l as u128, r as u128),\n+            _ => bug!(\"int_arithmetic should only be called on int primvals\"),\n+        }\n+    })\n+}\n+\n+macro_rules! int_shift {\n+    ($kind:expr, $int_op:ident, $l:expr, $r:expr) => ({\n+        let l = $l;\n+        let r = $r;\n+        let r_wrapped = r as u32;\n+        match $kind {\n+            I8  => overflow!($int_op, l as i8,  r_wrapped),\n+            I16 => overflow!($int_op, l as i16, r_wrapped),\n+            I32 => overflow!($int_op, l as i32, r_wrapped),\n+            I64 => overflow!($int_op, l as i64, r_wrapped),\n+            I128 => overflow!($int_op, l as i128, r_wrapped),\n+            U8  => overflow!($int_op, l as u8,  r_wrapped),\n+            U16 => overflow!($int_op, l as u16, r_wrapped),\n+            U32 => overflow!($int_op, l as u32, r_wrapped),\n+            U64 => overflow!($int_op, l as u64, r_wrapped),\n+            U128 => overflow!($int_op, l as u128, r_wrapped),\n+            _ => bug!(\"int_shift should only be called on int primvals\"),\n+        }.map(|(val, over)| (val, over || r != r_wrapped as u128))\n+    })\n+}\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    /// Returns the result of the specified operation and whether it overflowed.\n+    pub fn binary_op(\n+        &self,\n+        bin_op: mir::BinOp,\n+        left: PrimVal,\n+        left_ty: Ty<'tcx>,\n+        right: PrimVal,\n+        right_ty: Ty<'tcx>,\n+    ) -> EvalResult<'tcx, (PrimVal, bool)> {\n+        use rustc::mir::BinOp::*;\n+        use super::PrimValKind::*;\n+\n+        let left_kind = self.ty_to_primval_kind(left_ty)?;\n+        let right_kind = self.ty_to_primval_kind(right_ty)?;\n+        //trace!(\"Running binary op {:?}: {:?} ({:?}), {:?} ({:?})\", bin_op, left, left_kind, right, right_kind);\n+\n+        // I: Handle operations that support pointers\n+        if !left_kind.is_float() && !right_kind.is_float() {\n+            if let Some(handled) = M::try_ptr_op(self, bin_op, left, left_ty, right, right_ty)? {\n+                return Ok(handled);\n+            }\n+        }\n+\n+        // II: From now on, everything must be bytes, no pointers\n+        let l = left.to_bytes()?;\n+        let r = right.to_bytes()?;\n+\n+        // These ops can have an RHS with a different numeric type.\n+        if right_kind.is_int() && (bin_op == Shl || bin_op == Shr) {\n+            return match bin_op {\n+                Shl => int_shift!(left_kind, overflowing_shl, l, r),\n+                Shr => int_shift!(left_kind, overflowing_shr, l, r),\n+                _ => bug!(\"it has already been checked that this is a shift op\"),\n+            };\n+        }\n+\n+        if left_kind != right_kind {\n+            let msg = format!(\n+                \"unimplemented binary op {:?}: {:?} ({:?}), {:?} ({:?})\",\n+                bin_op,\n+                left,\n+                left_kind,\n+                right,\n+                right_kind\n+            );\n+            return err!(Unimplemented(msg));\n+        }\n+\n+        let float_op = |op, l, r, ty| {\n+            let l = ConstFloat {\n+                bits: l,\n+                ty,\n+            };\n+            let r = ConstFloat {\n+                bits: r,\n+                ty,\n+            };\n+            match op {\n+                Eq => PrimVal::from_bool(l.try_cmp(r).unwrap() == Ordering::Equal),\n+                Ne => PrimVal::from_bool(l.try_cmp(r).unwrap() != Ordering::Equal),\n+                Lt => PrimVal::from_bool(l.try_cmp(r).unwrap() == Ordering::Less),\n+                Le => PrimVal::from_bool(l.try_cmp(r).unwrap() != Ordering::Greater),\n+                Gt => PrimVal::from_bool(l.try_cmp(r).unwrap() == Ordering::Greater),\n+                Ge => PrimVal::from_bool(l.try_cmp(r).unwrap() != Ordering::Less),\n+                Add => PrimVal::Bytes((l + r).unwrap().bits),\n+                Sub => PrimVal::Bytes((l - r).unwrap().bits),\n+                Mul => PrimVal::Bytes((l * r).unwrap().bits),\n+                Div => PrimVal::Bytes((l / r).unwrap().bits),\n+                Rem => PrimVal::Bytes((l % r).unwrap().bits),\n+                _ => bug!(\"invalid float op: `{:?}`\", op),\n+            }\n+        };\n+\n+        let val = match (bin_op, left_kind) {\n+            (_, F32) => float_op(bin_op, l, r, FloatTy::F32),\n+            (_, F64) => float_op(bin_op, l, r, FloatTy::F64),\n+\n+\n+            (Eq, _) => PrimVal::from_bool(l == r),\n+            (Ne, _) => PrimVal::from_bool(l != r),\n+\n+            (Lt, k) if k.is_signed_int() => PrimVal::from_bool((l as i128) < (r as i128)),\n+            (Lt, _) => PrimVal::from_bool(l < r),\n+            (Le, k) if k.is_signed_int() => PrimVal::from_bool((l as i128) <= (r as i128)),\n+            (Le, _) => PrimVal::from_bool(l <= r),\n+            (Gt, k) if k.is_signed_int() => PrimVal::from_bool((l as i128) > (r as i128)),\n+            (Gt, _) => PrimVal::from_bool(l > r),\n+            (Ge, k) if k.is_signed_int() => PrimVal::from_bool((l as i128) >= (r as i128)),\n+            (Ge, _) => PrimVal::from_bool(l >= r),\n+\n+            (BitOr, _) => PrimVal::Bytes(l | r),\n+            (BitAnd, _) => PrimVal::Bytes(l & r),\n+            (BitXor, _) => PrimVal::Bytes(l ^ r),\n+\n+            (Add, k) if k.is_int() => return int_arithmetic!(k, overflowing_add, l, r),\n+            (Sub, k) if k.is_int() => return int_arithmetic!(k, overflowing_sub, l, r),\n+            (Mul, k) if k.is_int() => return int_arithmetic!(k, overflowing_mul, l, r),\n+            (Div, k) if k.is_int() => return int_arithmetic!(k, overflowing_div, l, r),\n+            (Rem, k) if k.is_int() => return int_arithmetic!(k, overflowing_rem, l, r),\n+\n+            _ => {\n+                let msg = format!(\n+                    \"unimplemented binary op {:?}: {:?} ({:?}), {:?} ({:?})\",\n+                    bin_op,\n+                    left,\n+                    left_kind,\n+                    right,\n+                    right_kind\n+                );\n+                return err!(Unimplemented(msg));\n+            }\n+        };\n+\n+        Ok((val, false))\n+    }\n+}\n+\n+pub fn unary_op<'tcx>(\n+    un_op: mir::UnOp,\n+    val: PrimVal,\n+    val_kind: PrimValKind,\n+) -> EvalResult<'tcx, PrimVal> {\n+    use rustc::mir::UnOp::*;\n+    use super::PrimValKind::*;\n+\n+    let bytes = val.to_bytes()?;\n+\n+    let result_bytes = match (un_op, val_kind) {\n+        (Not, Bool) => !val.to_bool()? as u128,\n+\n+        (Not, U8) => !(bytes as u8) as u128,\n+        (Not, U16) => !(bytes as u16) as u128,\n+        (Not, U32) => !(bytes as u32) as u128,\n+        (Not, U64) => !(bytes as u64) as u128,\n+        (Not, U128) => !bytes,\n+\n+        (Not, I8) => !(bytes as i8) as u128,\n+        (Not, I16) => !(bytes as i16) as u128,\n+        (Not, I32) => !(bytes as i32) as u128,\n+        (Not, I64) => !(bytes as i64) as u128,\n+        (Not, I128) => !(bytes as i128) as u128,\n+\n+        (Neg, I8) => -(bytes as i8) as u128,\n+        (Neg, I16) => -(bytes as i16) as u128,\n+        (Neg, I32) => -(bytes as i32) as u128,\n+        (Neg, I64) => -(bytes as i64) as u128,\n+        (Neg, I128) => -(bytes as i128) as u128,\n+\n+        (Neg, F32) => f32_to_bytes(-bytes_to_f32(bytes)),\n+        (Neg, F64) => f64_to_bytes(-bytes_to_f64(bytes)),\n+\n+        _ => {\n+            let msg = format!(\"unimplemented unary op: {:?}, {:?}\", un_op, val);\n+            return err!(Unimplemented(msg));\n+        }\n+    };\n+\n+    Ok(PrimVal::Bytes(result_bytes))\n+}"}, {"sha": "5cdcbe35121a579e26b6d74c461b5cb64ece2676", "filename": "src/librustc/mir/interpret/range_map.rs", "status": "added", "additions": 250, "deletions": 0, "changes": 250, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Frange_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Frange_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Frange_map.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,250 @@\n+//! Implements a map from integer indices to data.\n+//! Rather than storing data for every index, internally, this maps entire ranges to the data.\n+//! To this end, the APIs all work on ranges, not on individual integers. Ranges are split as\n+//! necessary (e.g. when [0,5) is first associated with X, and then [1,2) is mutated).\n+//! Users must not depend on whether a range is coalesced or not, even though this is observable\n+//! via the iteration APIs.\n+use std::collections::BTreeMap;\n+use std::ops;\n+\n+#[derive(Clone, Debug)]\n+pub struct RangeMap<T> {\n+    map: BTreeMap<Range, T>,\n+}\n+\n+// The derived `Ord` impl sorts first by the first field, then, if the fields are the same,\n+// by the second field.\n+// This is exactly what we need for our purposes, since a range query on a BTReeSet/BTreeMap will give us all\n+// `MemoryRange`s whose `start` is <= than the one we're looking for, but not > the end of the range we're checking.\n+// At the same time the `end` is irrelevant for the sorting and range searching, but used for the check.\n+// This kind of search breaks, if `end < start`, so don't do that!\n+#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Debug)]\n+struct Range {\n+    start: u64,\n+    end: u64, // Invariant: end > start\n+}\n+\n+impl Range {\n+    fn range(offset: u64, len: u64) -> ops::Range<Range> {\n+        assert!(len > 0);\n+        // We select all elements that are within\n+        // the range given by the offset into the allocation and the length.\n+        // This is sound if all ranges that intersect with the argument range, are in the\n+        // resulting range of ranges.\n+        let left = Range {\n+            // lowest range to include `offset`\n+            start: 0,\n+            end: offset + 1,\n+        };\n+        let right = Range {\n+            // lowest (valid) range not to include `offset+len`\n+            start: offset + len,\n+            end: offset + len + 1,\n+        };\n+        left..right\n+    }\n+\n+    /// Tests if all of [offset, offset+len) are contained in this range.\n+    fn overlaps(&self, offset: u64, len: u64) -> bool {\n+        assert!(len > 0);\n+        offset < self.end && offset + len >= self.start\n+    }\n+}\n+\n+impl<T> RangeMap<T> {\n+    pub fn new() -> RangeMap<T> {\n+        RangeMap { map: BTreeMap::new() }\n+    }\n+\n+    fn iter_with_range<'a>(\n+        &'a self,\n+        offset: u64,\n+        len: u64,\n+    ) -> impl Iterator<Item = (&'a Range, &'a T)> + 'a {\n+        assert!(len > 0);\n+        self.map.range(Range::range(offset, len)).filter_map(\n+            move |(range,\n+                   data)| {\n+                if range.overlaps(offset, len) {\n+                    Some((range, data))\n+                } else {\n+                    None\n+                }\n+            },\n+        )\n+    }\n+\n+    pub fn iter<'a>(&'a self, offset: u64, len: u64) -> impl Iterator<Item = &'a T> + 'a {\n+        self.iter_with_range(offset, len).map(|(_, data)| data)\n+    }\n+\n+    fn split_entry_at(&mut self, offset: u64)\n+    where\n+        T: Clone,\n+    {\n+        let range = match self.iter_with_range(offset, 1).next() {\n+            Some((&range, _)) => range,\n+            None => return,\n+        };\n+        assert!(\n+            range.start <= offset && range.end > offset,\n+            \"We got a range that doesn't even contain what we asked for.\"\n+        );\n+        // There is an entry overlapping this position, see if we have to split it\n+        if range.start < offset {\n+            let data = self.map.remove(&range).unwrap();\n+            let old = self.map.insert(\n+                Range {\n+                    start: range.start,\n+                    end: offset,\n+                },\n+                data.clone(),\n+            );\n+            assert!(old.is_none());\n+            let old = self.map.insert(\n+                Range {\n+                    start: offset,\n+                    end: range.end,\n+                },\n+                data,\n+            );\n+            assert!(old.is_none());\n+        }\n+    }\n+\n+    pub fn iter_mut_all<'a>(&'a mut self) -> impl Iterator<Item = &'a mut T> + 'a {\n+        self.map.values_mut()\n+    }\n+\n+    /// Provide mutable iteration over everything in the given range.  As a side-effect,\n+    /// this will split entries in the map that are only partially hit by the given range,\n+    /// to make sure that when they are mutated, the effect is constrained to the given range.\n+    pub fn iter_mut_with_gaps<'a>(\n+        &'a mut self,\n+        offset: u64,\n+        len: u64,\n+    ) -> impl Iterator<Item = &'a mut T> + 'a\n+    where\n+        T: Clone,\n+    {\n+        assert!(len > 0);\n+        // Preparation: Split first and last entry as needed.\n+        self.split_entry_at(offset);\n+        self.split_entry_at(offset + len);\n+        // Now we can provide a mutable iterator\n+        self.map.range_mut(Range::range(offset, len)).filter_map(\n+            move |(&range, data)| {\n+                if range.overlaps(offset, len) {\n+                    assert!(\n+                        offset <= range.start && offset + len >= range.end,\n+                        \"The splitting went wrong\"\n+                    );\n+                    Some(data)\n+                } else {\n+                    // Skip this one\n+                    None\n+                }\n+            },\n+        )\n+    }\n+\n+    /// Provide a mutable iterator over everything in the given range, with the same side-effects as\n+    /// iter_mut_with_gaps.  Furthermore, if there are gaps between ranges, fill them with the given default.\n+    /// This is also how you insert.\n+    pub fn iter_mut<'a>(&'a mut self, offset: u64, len: u64) -> impl Iterator<Item = &'a mut T> + 'a\n+    where\n+        T: Clone + Default,\n+    {\n+        // Do a first iteration to collect the gaps\n+        let mut gaps = Vec::new();\n+        let mut last_end = offset;\n+        for (range, _) in self.iter_with_range(offset, len) {\n+            if last_end < range.start {\n+                gaps.push(Range {\n+                    start: last_end,\n+                    end: range.start,\n+                });\n+            }\n+            last_end = range.end;\n+        }\n+        if last_end < offset + len {\n+            gaps.push(Range {\n+                start: last_end,\n+                end: offset + len,\n+            });\n+        }\n+\n+        // Add default for all gaps\n+        for gap in gaps {\n+            let old = self.map.insert(gap, Default::default());\n+            assert!(old.is_none());\n+        }\n+\n+        // Now provide mutable iteration\n+        self.iter_mut_with_gaps(offset, len)\n+    }\n+\n+    pub fn retain<F>(&mut self, mut f: F)\n+    where\n+        F: FnMut(&T) -> bool,\n+    {\n+        let mut remove = Vec::new();\n+        for (range, data) in self.map.iter() {\n+            if !f(data) {\n+                remove.push(*range);\n+            }\n+        }\n+\n+        for range in remove {\n+            self.map.remove(&range);\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    /// Query the map at every offset in the range and collect the results.\n+    fn to_vec<T: Copy>(map: &RangeMap<T>, offset: u64, len: u64) -> Vec<T> {\n+        (offset..offset + len)\n+            .into_iter()\n+            .map(|i| *map.iter(i, 1).next().unwrap())\n+            .collect()\n+    }\n+\n+    #[test]\n+    fn basic_insert() {\n+        let mut map = RangeMap::<i32>::new();\n+        // Insert\n+        for x in map.iter_mut(10, 1) {\n+            *x = 42;\n+        }\n+        // Check\n+        assert_eq!(to_vec(&map, 10, 1), vec![42]);\n+    }\n+\n+    #[test]\n+    fn gaps() {\n+        let mut map = RangeMap::<i32>::new();\n+        for x in map.iter_mut(11, 1) {\n+            *x = 42;\n+        }\n+        for x in map.iter_mut(15, 1) {\n+            *x = 42;\n+        }\n+\n+        // Now request a range that needs three gaps filled\n+        for x in map.iter_mut(10, 10) {\n+            if *x != 42 {\n+                *x = 23;\n+            }\n+        }\n+\n+        assert_eq!(\n+            to_vec(&map, 10, 10),\n+            vec![23, 42, 23, 23, 23, 42, 23, 23, 23, 23]\n+        );\n+        assert_eq!(to_vec(&map, 13, 5), vec![23, 23, 42, 23, 23]);\n+    }\n+}"}, {"sha": "c701ebfbf4c7519dae19211aeaffde65ee6de734", "filename": "src/librustc/mir/interpret/step.rs", "status": "added", "additions": 402, "deletions": 0, "changes": 402, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fstep.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fstep.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fstep.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,402 @@\n+//! This module contains the `EvalContext` methods for executing a single step of the interpreter.\n+//!\n+//! The main entry point is the `step` method.\n+\n+use rustc::hir::def_id::DefId;\n+use rustc::hir;\n+use rustc::mir::visit::{Visitor, LvalueContext};\n+use rustc::mir;\n+use rustc::traits::Reveal;\n+use rustc::ty;\n+use rustc::ty::layout::Layout;\n+use rustc::ty::subst::Substs;\n+use rustc::middle::const_val::ConstVal;\n+\n+use super::{EvalResult, EvalContext, StackPopCleanup, PtrAndAlign, GlobalId, Lvalue,\n+            MemoryKind, Machine, PrimVal};\n+\n+use syntax::codemap::Span;\n+use syntax::ast::Mutability;\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    pub fn inc_step_counter_and_check_limit(&mut self, n: u64) -> EvalResult<'tcx> {\n+        self.steps_remaining = self.steps_remaining.saturating_sub(n);\n+        if self.steps_remaining > 0 {\n+            Ok(())\n+        } else {\n+            err!(ExecutionTimeLimitReached)\n+        }\n+    }\n+\n+    /// Returns true as long as there are more things to do.\n+    pub fn step(&mut self) -> EvalResult<'tcx, bool> {\n+        self.inc_step_counter_and_check_limit(1)?;\n+        if self.stack.is_empty() {\n+            return Ok(false);\n+        }\n+\n+        let block = self.frame().block;\n+        let stmt_id = self.frame().stmt;\n+        let mir = self.mir();\n+        let basic_block = &mir.basic_blocks()[block];\n+\n+        if let Some(stmt) = basic_block.statements.get(stmt_id) {\n+            let mut new = Ok(0);\n+            ConstantExtractor {\n+                span: stmt.source_info.span,\n+                instance: self.frame().instance,\n+                ecx: self,\n+                mir,\n+                new_constants: &mut new,\n+            }.visit_statement(\n+                block,\n+                stmt,\n+                mir::Location {\n+                    block,\n+                    statement_index: stmt_id,\n+                },\n+            );\n+            // if ConstantExtractor added new frames, we don't execute anything here\n+            // but await the next call to step\n+            if new? == 0 {\n+                self.statement(stmt)?;\n+            }\n+            return Ok(true);\n+        }\n+\n+        let terminator = basic_block.terminator();\n+        let mut new = Ok(0);\n+        ConstantExtractor {\n+            span: terminator.source_info.span,\n+            instance: self.frame().instance,\n+            ecx: self,\n+            mir,\n+            new_constants: &mut new,\n+        }.visit_terminator(\n+            block,\n+            terminator,\n+            mir::Location {\n+                block,\n+                statement_index: stmt_id,\n+            },\n+        );\n+        // if ConstantExtractor added new frames, we don't execute anything here\n+        // but await the next call to step\n+        if new? == 0 {\n+            self.terminator(terminator)?;\n+        }\n+        Ok(true)\n+    }\n+\n+    fn statement(&mut self, stmt: &mir::Statement<'tcx>) -> EvalResult<'tcx> {\n+        trace!(\"{:?}\", stmt);\n+\n+        use rustc::mir::StatementKind::*;\n+\n+        // Some statements (e.g. box) push new stack frames.  We have to record the stack frame number\n+        // *before* executing the statement.\n+        let frame_idx = self.cur_frame();\n+\n+        match stmt.kind {\n+            Assign(ref lvalue, ref rvalue) => self.eval_rvalue_into_lvalue(rvalue, lvalue)?,\n+\n+            SetDiscriminant {\n+                ref lvalue,\n+                variant_index,\n+            } => {\n+                let dest = self.eval_lvalue(lvalue)?;\n+                let dest_ty = self.lvalue_ty(lvalue);\n+                let dest_layout = self.type_layout(dest_ty)?;\n+\n+                match *dest_layout {\n+                    Layout::General { discr, .. } => {\n+                        let discr_size = discr.size().bytes();\n+                        let dest_ptr = self.force_allocation(dest)?.to_ptr()?;\n+                        self.memory.write_primval(\n+                            dest_ptr,\n+                            PrimVal::Bytes(variant_index as u128),\n+                            discr_size,\n+                            false\n+                        )?\n+                    }\n+\n+                    Layout::RawNullablePointer { nndiscr, .. } => {\n+                        if variant_index as u64 != nndiscr {\n+                            self.write_null(dest, dest_ty)?;\n+                        }\n+                    }\n+\n+                    Layout::StructWrappedNullablePointer {\n+                        nndiscr,\n+                        ref discrfield_source,\n+                        ..\n+                    } => {\n+                        if variant_index as u64 != nndiscr {\n+                            self.write_struct_wrapped_null_pointer(\n+                                dest_ty,\n+                                nndiscr,\n+                                discrfield_source,\n+                                dest,\n+                            )?;\n+                        }\n+                    }\n+\n+                    _ => {\n+                        bug!(\n+                            \"SetDiscriminant on {} represented as {:#?}\",\n+                            dest_ty,\n+                            dest_layout\n+                        )\n+                    }\n+                }\n+            }\n+\n+            // Mark locals as alive\n+            StorageLive(local) => {\n+                let old_val = self.frame_mut().storage_live(local)?;\n+                self.deallocate_local(old_val)?;\n+            }\n+\n+            // Mark locals as dead\n+            StorageDead(local) => {\n+                let old_val = self.frame_mut().storage_dead(local)?;\n+                self.deallocate_local(old_val)?;\n+            }\n+\n+            // Validity checks.\n+            Validate(op, ref lvalues) => {\n+                for operand in lvalues {\n+                    self.validation_op(op, operand)?;\n+                }\n+            }\n+            EndRegion(ce) => {\n+                self.end_region(Some(ce))?;\n+            }\n+\n+            // Defined to do nothing. These are added by optimization passes, to avoid changing the\n+            // size of MIR constantly.\n+            Nop => {}\n+\n+            InlineAsm { .. } => return err!(InlineAsm),\n+        }\n+\n+        self.stack[frame_idx].stmt += 1;\n+        Ok(())\n+    }\n+\n+    fn terminator(&mut self, terminator: &mir::Terminator<'tcx>) -> EvalResult<'tcx> {\n+        trace!(\"{:?}\", terminator.kind);\n+        self.eval_terminator(terminator)?;\n+        if !self.stack.is_empty() {\n+            trace!(\"// {:?}\", self.frame().block);\n+        }\n+        Ok(())\n+    }\n+\n+    /// returns `true` if a stackframe was pushed\n+    fn global_item(\n+        &mut self,\n+        def_id: DefId,\n+        substs: &'tcx Substs<'tcx>,\n+        span: Span,\n+        mutability: Mutability,\n+    ) -> EvalResult<'tcx, bool> {\n+        let instance = self.resolve_associated_const(def_id, substs);\n+        let cid = GlobalId {\n+            instance,\n+            promoted: None,\n+        };\n+        if self.globals.contains_key(&cid) {\n+            return Ok(false);\n+        }\n+        if self.tcx.has_attr(def_id, \"linkage\") {\n+            M::global_item_with_linkage(self, cid.instance, mutability)?;\n+            return Ok(false);\n+        }\n+        let mir = self.load_mir(instance.def)?;\n+        let size = self.type_size_with_substs(mir.return_ty, substs)?.expect(\n+            \"unsized global\",\n+        );\n+        let align = self.type_align_with_substs(mir.return_ty, substs)?;\n+        let ptr = self.memory.allocate(\n+            size,\n+            align,\n+            MemoryKind::UninitializedStatic,\n+        )?;\n+        let aligned = !self.is_packed(mir.return_ty)?;\n+        self.globals.insert(\n+            cid,\n+            PtrAndAlign {\n+                ptr: ptr.into(),\n+                aligned,\n+            },\n+        );\n+        let internally_mutable = !mir.return_ty.is_freeze(\n+            self.tcx,\n+            ty::ParamEnv::empty(Reveal::All),\n+            span,\n+        );\n+        let mutability = if mutability == Mutability::Mutable || internally_mutable {\n+            Mutability::Mutable\n+        } else {\n+            Mutability::Immutable\n+        };\n+        let cleanup = StackPopCleanup::MarkStatic(mutability);\n+        let name = ty::tls::with(|tcx| tcx.item_path_str(def_id));\n+        trace!(\"pushing stack frame for global: {}\", name);\n+        self.push_stack_frame(\n+            instance,\n+            span,\n+            mir,\n+            Lvalue::from_ptr(ptr),\n+            cleanup,\n+        )?;\n+        Ok(true)\n+    }\n+}\n+\n+// WARNING: This code pushes new stack frames.  Make sure that any methods implemented on this\n+// type don't ever access ecx.stack[ecx.cur_frame()], as that will change. This includes, e.g.,\n+// using the current stack frame's substitution.\n+// Basically don't call anything other than `load_mir`, `alloc_ptr`, `push_stack_frame`.\n+struct ConstantExtractor<'a, 'b: 'a, 'tcx: 'b, M: Machine<'tcx> + 'a> {\n+    span: Span,\n+    ecx: &'a mut EvalContext<'b, 'tcx, M>,\n+    mir: &'tcx mir::Mir<'tcx>,\n+    instance: ty::Instance<'tcx>,\n+    new_constants: &'a mut EvalResult<'tcx, u64>,\n+}\n+\n+impl<'a, 'b, 'tcx, M: Machine<'tcx>> ConstantExtractor<'a, 'b, 'tcx, M> {\n+    fn try<F: FnOnce(&mut Self) -> EvalResult<'tcx, bool>>(&mut self, f: F) {\n+        // previous constant errored\n+        let n = match *self.new_constants {\n+            Ok(n) => n,\n+            Err(_) => return,\n+        };\n+        match f(self) {\n+            // everything ok + a new stackframe\n+            Ok(true) => *self.new_constants = Ok(n + 1),\n+            // constant correctly evaluated, but no new stackframe\n+            Ok(false) => {}\n+            // constant eval errored\n+            Err(err) => *self.new_constants = Err(err),\n+        }\n+    }\n+}\n+\n+impl<'a, 'b, 'tcx, M: Machine<'tcx>> Visitor<'tcx> for ConstantExtractor<'a, 'b, 'tcx, M> {\n+    fn visit_constant(&mut self, constant: &mir::Constant<'tcx>, location: mir::Location) {\n+        self.super_constant(constant, location);\n+        match constant.literal {\n+            // already computed by rustc\n+            mir::Literal::Value { value: &ty::Const { val: ConstVal::Unevaluated(def_id, substs), .. } } => {\n+                self.try(|this| {\n+                    this.ecx.global_item(\n+                        def_id,\n+                        substs,\n+                        constant.span,\n+                        Mutability::Immutable,\n+                    )\n+                });\n+            }\n+            mir::Literal::Value { .. } => {}\n+            mir::Literal::Promoted { index } => {\n+                let cid = GlobalId {\n+                    instance: self.instance,\n+                    promoted: Some(index),\n+                };\n+                if self.ecx.globals.contains_key(&cid) {\n+                    return;\n+                }\n+                let mir = &self.mir.promoted[index];\n+                self.try(|this| {\n+                    let size = this.ecx\n+                        .type_size_with_substs(mir.return_ty, this.instance.substs)?\n+                        .expect(\"unsized global\");\n+                    let align = this.ecx.type_align_with_substs(\n+                        mir.return_ty,\n+                        this.instance.substs,\n+                    )?;\n+                    let ptr = this.ecx.memory.allocate(\n+                        size,\n+                        align,\n+                        MemoryKind::UninitializedStatic,\n+                    )?;\n+                    let aligned = !this.ecx.is_packed(mir.return_ty)?;\n+                    this.ecx.globals.insert(\n+                        cid,\n+                        PtrAndAlign {\n+                            ptr: ptr.into(),\n+                            aligned,\n+                        },\n+                    );\n+                    trace!(\"pushing stack frame for {:?}\", index);\n+                    this.ecx.push_stack_frame(\n+                        this.instance,\n+                        constant.span,\n+                        mir,\n+                        Lvalue::from_ptr(ptr),\n+                        StackPopCleanup::MarkStatic(Mutability::Immutable),\n+                    )?;\n+                    Ok(true)\n+                });\n+            }\n+        }\n+    }\n+\n+    fn visit_lvalue(\n+        &mut self,\n+        lvalue: &mir::Lvalue<'tcx>,\n+        context: LvalueContext<'tcx>,\n+        location: mir::Location,\n+    ) {\n+        self.super_lvalue(lvalue, context, location);\n+        if let mir::Lvalue::Static(ref static_) = *lvalue {\n+            let def_id = static_.def_id;\n+            let substs = self.ecx.tcx.intern_substs(&[]);\n+            let span = self.span;\n+            if let Some(node_item) = self.ecx.tcx.hir.get_if_local(def_id) {\n+                if let hir::map::Node::NodeItem(&hir::Item { ref node, .. }) = node_item {\n+                    if let hir::ItemStatic(_, m, _) = *node {\n+                        self.try(|this| {\n+                            this.ecx.global_item(\n+                                def_id,\n+                                substs,\n+                                span,\n+                                if m == hir::MutMutable {\n+                                    Mutability::Mutable\n+                                } else {\n+                                    Mutability::Immutable\n+                                },\n+                            )\n+                        });\n+                        return;\n+                    } else {\n+                        bug!(\"static def id doesn't point to static\");\n+                    }\n+                } else {\n+                    bug!(\"static def id doesn't point to item\");\n+                }\n+            } else {\n+                let def = self.ecx.tcx.describe_def(def_id).expect(\"static not found\");\n+                if let hir::def::Def::Static(_, mutable) = def {\n+                    self.try(|this| {\n+                        this.ecx.global_item(\n+                            def_id,\n+                            substs,\n+                            span,\n+                            if mutable {\n+                                Mutability::Mutable\n+                            } else {\n+                                Mutability::Immutable\n+                            },\n+                        )\n+                    });\n+                } else {\n+                    bug!(\"static found but isn't a static: {:?}\", def);\n+                }\n+            }\n+        }\n+    }\n+}"}, {"sha": "6596cf951fd9eb99433a7ad582160d72234035a2", "filename": "src/librustc/mir/interpret/terminator/drop.rs", "status": "added", "additions": 83, "deletions": 0, "changes": 83, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fterminator%2Fdrop.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fterminator%2Fdrop.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fterminator%2Fdrop.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,83 @@\n+use rustc::mir::BasicBlock;\n+use rustc::ty::{self, Ty};\n+use syntax::codemap::Span;\n+\n+use interpret::{EvalResult, EvalContext, Lvalue, LvalueExtra, PrimVal, Value,\n+                Machine, ValTy};\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    pub(crate) fn drop_lvalue(\n+        &mut self,\n+        lval: Lvalue,\n+        instance: ty::Instance<'tcx>,\n+        ty: Ty<'tcx>,\n+        span: Span,\n+        target: BasicBlock,\n+    ) -> EvalResult<'tcx> {\n+        trace!(\"drop_lvalue: {:#?}\", lval);\n+        // We take the address of the object.  This may well be unaligned, which is fine for us here.\n+        // However, unaligned accesses will probably make the actual drop implementation fail -- a problem shared\n+        // by rustc.\n+        let val = match self.force_allocation(lval)? {\n+            Lvalue::Ptr {\n+                ptr,\n+                extra: LvalueExtra::Vtable(vtable),\n+            } => ptr.ptr.to_value_with_vtable(vtable),\n+            Lvalue::Ptr {\n+                ptr,\n+                extra: LvalueExtra::Length(len),\n+            } => ptr.ptr.to_value_with_len(len),\n+            Lvalue::Ptr {\n+                ptr,\n+                extra: LvalueExtra::None,\n+            } => ptr.ptr.to_value(),\n+            _ => bug!(\"force_allocation broken\"),\n+        };\n+        self.drop(val, instance, ty, span, target)\n+    }\n+\n+    fn drop(\n+        &mut self,\n+        arg: Value,\n+        instance: ty::Instance<'tcx>,\n+        ty: Ty<'tcx>,\n+        span: Span,\n+        target: BasicBlock,\n+    ) -> EvalResult<'tcx> {\n+        trace!(\"drop: {:#?}, {:?}, {:?}\", arg, ty.sty, instance.def);\n+\n+        let instance = match ty.sty {\n+            ty::TyDynamic(..) => {\n+                let vtable = match arg {\n+                    Value::ByValPair(_, PrimVal::Ptr(vtable)) => vtable,\n+                    _ => bug!(\"expected fat ptr, got {:?}\", arg),\n+                };\n+                match self.read_drop_type_from_vtable(vtable)? {\n+                    Some(func) => func,\n+                    // no drop fn -> bail out\n+                    None => {\n+                        self.goto_block(target);\n+                        return Ok(())\n+                    },\n+                }\n+            }\n+            _ => instance,\n+        };\n+\n+        // the drop function expects a reference to the value\n+        let valty = ValTy {\n+            value: arg,\n+            ty: self.tcx.mk_mut_ptr(ty),\n+        };\n+\n+        let fn_sig = self.tcx.fn_sig(instance.def_id()).skip_binder().clone();\n+\n+        self.eval_fn_call(\n+            instance,\n+            Some((Lvalue::undef(), target)),\n+            &vec![valty],\n+            span,\n+            fn_sig,\n+        )\n+    }\n+}"}, {"sha": "e01777cdb4e76f1d275a5950faa1965758525b15", "filename": "src/librustc/mir/interpret/terminator/mod.rs", "status": "added", "additions": 411, "deletions": 0, "changes": 411, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fterminator%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fterminator%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fterminator%2Fmod.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,411 @@\n+use rustc::mir;\n+use rustc::ty::{self, TypeVariants};\n+use rustc::ty::layout::Layout;\n+use syntax::codemap::Span;\n+use syntax::abi::Abi;\n+\n+use super::{EvalResult, EvalContext, eval_context,\n+            PtrAndAlign, Lvalue, PrimVal, Value, Machine, ValTy};\n+\n+use rustc_data_structures::indexed_vec::Idx;\n+\n+mod drop;\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    pub fn goto_block(&mut self, target: mir::BasicBlock) {\n+        self.frame_mut().block = target;\n+        self.frame_mut().stmt = 0;\n+    }\n+\n+    pub(super) fn eval_terminator(\n+        &mut self,\n+        terminator: &mir::Terminator<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        use rustc::mir::TerminatorKind::*;\n+        match terminator.kind {\n+            Return => {\n+                self.dump_local(self.frame().return_lvalue);\n+                self.pop_stack_frame()?\n+            }\n+\n+            Goto { target } => self.goto_block(target),\n+\n+            SwitchInt {\n+                ref discr,\n+                ref values,\n+                ref targets,\n+                ..\n+            } => {\n+                // FIXME(CTFE): forbid branching\n+                let discr_val = self.eval_operand(discr)?;\n+                let discr_prim = self.value_to_primval(discr_val)?;\n+\n+                // Branch to the `otherwise` case by default, if no match is found.\n+                let mut target_block = targets[targets.len() - 1];\n+\n+                for (index, const_int) in values.iter().enumerate() {\n+                    let prim = PrimVal::Bytes(const_int.to_u128_unchecked());\n+                    if discr_prim.to_bytes()? == prim.to_bytes()? {\n+                        target_block = targets[index];\n+                        break;\n+                    }\n+                }\n+\n+                self.goto_block(target_block);\n+            }\n+\n+            Call {\n+                ref func,\n+                ref args,\n+                ref destination,\n+                ..\n+            } => {\n+                let destination = match *destination {\n+                    Some((ref lv, target)) => Some((self.eval_lvalue(lv)?, target)),\n+                    None => None,\n+                };\n+\n+                let func_ty = self.operand_ty(func);\n+                let (fn_def, sig) = match func_ty.sty {\n+                    ty::TyFnPtr(sig) => {\n+                        let fn_ptr = self.eval_operand_to_primval(func)?.to_ptr()?;\n+                        let instance = self.memory.get_fn(fn_ptr)?;\n+                        let instance_ty = instance.def.def_ty(self.tcx);\n+                        let instance_ty = self.monomorphize(instance_ty, instance.substs);\n+                        match instance_ty.sty {\n+                            ty::TyFnDef(..) => {\n+                                let real_sig = instance_ty.fn_sig(self.tcx);\n+                                let sig = self.tcx.erase_late_bound_regions_and_normalize(&sig);\n+                                let real_sig = self.tcx.erase_late_bound_regions_and_normalize(&real_sig);\n+                                if !self.check_sig_compat(sig, real_sig)? {\n+                                    return err!(FunctionPointerTyMismatch(real_sig, sig));\n+                                }\n+                            }\n+                            ref other => bug!(\"instance def ty: {:?}\", other),\n+                        }\n+                        (instance, sig)\n+                    }\n+                    ty::TyFnDef(def_id, substs) => (\n+                        eval_context::resolve(self.tcx, def_id, substs),\n+                        func_ty.fn_sig(self.tcx),\n+                    ),\n+                    _ => {\n+                        let msg = format!(\"can't handle callee of type {:?}\", func_ty);\n+                        return err!(Unimplemented(msg));\n+                    }\n+                };\n+                let args = self.operands_to_args(args)?;\n+                let sig = self.tcx.erase_late_bound_regions_and_normalize(&sig);\n+                self.eval_fn_call(\n+                    fn_def,\n+                    destination,\n+                    &args,\n+                    terminator.source_info.span,\n+                    sig,\n+                )?;\n+            }\n+\n+            Drop {\n+                ref location,\n+                target,\n+                ..\n+            } => {\n+                // FIXME(CTFE): forbid drop in const eval\n+                let lval = self.eval_lvalue(location)?;\n+                let ty = self.lvalue_ty(location);\n+                let ty = eval_context::apply_param_substs(self.tcx, self.substs(), &ty);\n+                trace!(\"TerminatorKind::drop: {:?}, type {}\", location, ty);\n+\n+                let instance = eval_context::resolve_drop_in_place(self.tcx, ty);\n+                self.drop_lvalue(\n+                    lval,\n+                    instance,\n+                    ty,\n+                    terminator.source_info.span,\n+                    target,\n+                )?;\n+            }\n+\n+            Assert {\n+                ref cond,\n+                expected,\n+                ref msg,\n+                target,\n+                ..\n+            } => {\n+                let cond_val = self.eval_operand_to_primval(cond)?.to_bool()?;\n+                if expected == cond_val {\n+                    self.goto_block(target);\n+                } else {\n+                    use rustc::mir::AssertMessage::*;\n+                    return match *msg {\n+                        BoundsCheck { ref len, ref index } => {\n+                            let span = terminator.source_info.span;\n+                            let len = self.eval_operand_to_primval(len)\n+                                .expect(\"can't eval len\")\n+                                .to_u64()?;\n+                            let index = self.eval_operand_to_primval(index)\n+                                .expect(\"can't eval index\")\n+                                .to_u64()?;\n+                            err!(ArrayIndexOutOfBounds(span, len, index))\n+                        }\n+                        Math(ref err) => {\n+                            err!(Math(terminator.source_info.span, err.clone()))\n+                        }\n+                        GeneratorResumedAfterReturn |\n+                        GeneratorResumedAfterPanic => unimplemented!(),\n+                    };\n+                }\n+            }\n+\n+            Yield { .. } => unimplemented!(\"{:#?}\", terminator.kind),\n+            GeneratorDrop => unimplemented!(),\n+            DropAndReplace { .. } => unimplemented!(),\n+            Resume => unimplemented!(),\n+            Unreachable => return err!(Unreachable),\n+        }\n+\n+        Ok(())\n+    }\n+\n+    /// Decides whether it is okay to call the method with signature `real_sig` using signature `sig`.\n+    /// FIXME: This should take into account the platform-dependent ABI description.\n+    fn check_sig_compat(\n+        &mut self,\n+        sig: ty::FnSig<'tcx>,\n+        real_sig: ty::FnSig<'tcx>,\n+    ) -> EvalResult<'tcx, bool> {\n+        fn check_ty_compat<'tcx>(ty: ty::Ty<'tcx>, real_ty: ty::Ty<'tcx>) -> bool {\n+            if ty == real_ty {\n+                return true;\n+            } // This is actually a fast pointer comparison\n+            return match (&ty.sty, &real_ty.sty) {\n+                // Permit changing the pointer type of raw pointers and references as well as\n+                // mutability of raw pointers.\n+                // TODO: Should not be allowed when fat pointers are involved.\n+                (&TypeVariants::TyRawPtr(_), &TypeVariants::TyRawPtr(_)) => true,\n+                (&TypeVariants::TyRef(_, _), &TypeVariants::TyRef(_, _)) => {\n+                    ty.is_mutable_pointer() == real_ty.is_mutable_pointer()\n+                }\n+                // rule out everything else\n+                _ => false,\n+            };\n+        }\n+\n+        if sig.abi == real_sig.abi && sig.variadic == real_sig.variadic &&\n+            sig.inputs_and_output.len() == real_sig.inputs_and_output.len() &&\n+            sig.inputs_and_output\n+                .iter()\n+                .zip(real_sig.inputs_and_output)\n+                .all(|(ty, real_ty)| check_ty_compat(ty, real_ty))\n+        {\n+            // Definitely good.\n+            return Ok(true);\n+        }\n+\n+        if sig.variadic || real_sig.variadic {\n+            // We're not touching this\n+            return Ok(false);\n+        }\n+\n+        // We need to allow what comes up when a non-capturing closure is cast to a fn().\n+        match (sig.abi, real_sig.abi) {\n+            (Abi::Rust, Abi::RustCall) // check the ABIs.  This makes the test here non-symmetric.\n+                if check_ty_compat(sig.output(), real_sig.output()) && real_sig.inputs_and_output.len() == 3 => {\n+                // First argument of real_sig must be a ZST\n+                let fst_ty = real_sig.inputs_and_output[0];\n+                let layout = self.type_layout(fst_ty)?;\n+                let size = layout.size(&self.tcx.data_layout).bytes();\n+                if size == 0 {\n+                    // Second argument must be a tuple matching the argument list of sig\n+                    let snd_ty = real_sig.inputs_and_output[1];\n+                    match snd_ty.sty {\n+                        TypeVariants::TyTuple(tys, _) if sig.inputs().len() == tys.len() =>\n+                            if sig.inputs().iter().zip(tys).all(|(ty, real_ty)| check_ty_compat(ty, real_ty)) {\n+                                return Ok(true)\n+                            },\n+                        _ => {}\n+                    }\n+                }\n+            }\n+            _ => {}\n+        };\n+\n+        // Nope, this doesn't work.\n+        return Ok(false);\n+    }\n+\n+    fn eval_fn_call(\n+        &mut self,\n+        instance: ty::Instance<'tcx>,\n+        destination: Option<(Lvalue, mir::BasicBlock)>,\n+        args: &[ValTy<'tcx>],\n+        span: Span,\n+        sig: ty::FnSig<'tcx>,\n+    ) -> EvalResult<'tcx> {\n+        trace!(\"eval_fn_call: {:#?}\", instance);\n+        match instance.def {\n+            ty::InstanceDef::Intrinsic(..) => {\n+                let (ret, target) = match destination {\n+                    Some(dest) => dest,\n+                    _ => return err!(Unreachable),\n+                };\n+                let ty = sig.output();\n+                let layout = self.type_layout(ty)?;\n+                M::call_intrinsic(self, instance, args, ret, ty, layout, target)?;\n+                self.dump_local(ret);\n+                Ok(())\n+            }\n+            // FIXME: figure out why we can't just go through the shim\n+            ty::InstanceDef::ClosureOnceShim { .. } => {\n+                if M::eval_fn_call(self, instance, destination, args, span, sig)? {\n+                    return Ok(());\n+                }\n+                let mut arg_locals = self.frame().mir.args_iter();\n+                match sig.abi {\n+                    // closure as closure once\n+                    Abi::RustCall => {\n+                        for (arg_local, &valty) in arg_locals.zip(args) {\n+                            let dest = self.eval_lvalue(&mir::Lvalue::Local(arg_local))?;\n+                            self.write_value(valty, dest)?;\n+                        }\n+                    }\n+                    // non capture closure as fn ptr\n+                    // need to inject zst ptr for closure object (aka do nothing)\n+                    // and need to pack arguments\n+                    Abi::Rust => {\n+                        trace!(\n+                            \"arg_locals: {:?}\",\n+                            self.frame().mir.args_iter().collect::<Vec<_>>()\n+                        );\n+                        trace!(\"args: {:?}\", args);\n+                        let local = arg_locals.nth(1).unwrap();\n+                        for (i, &valty) in args.into_iter().enumerate() {\n+                            let dest = self.eval_lvalue(&mir::Lvalue::Local(local).field(\n+                                mir::Field::new(i),\n+                                valty.ty,\n+                            ))?;\n+                            self.write_value(valty, dest)?;\n+                        }\n+                    }\n+                    _ => bug!(\"bad ABI for ClosureOnceShim: {:?}\", sig.abi),\n+                }\n+                Ok(())\n+            }\n+            ty::InstanceDef::FnPtrShim(..) |\n+            ty::InstanceDef::DropGlue(..) |\n+            ty::InstanceDef::CloneShim(..) |\n+            ty::InstanceDef::Item(_) => {\n+                // Push the stack frame, and potentially be entirely done if the call got hooked\n+                if M::eval_fn_call(self, instance, destination, args, span, sig)? {\n+                    return Ok(());\n+                }\n+\n+                // Pass the arguments\n+                let mut arg_locals = self.frame().mir.args_iter();\n+                trace!(\"ABI: {:?}\", sig.abi);\n+                trace!(\n+                    \"arg_locals: {:?}\",\n+                    self.frame().mir.args_iter().collect::<Vec<_>>()\n+                );\n+                trace!(\"args: {:?}\", args);\n+                match sig.abi {\n+                    Abi::RustCall => {\n+                        assert_eq!(args.len(), 2);\n+\n+                        {\n+                            // write first argument\n+                            let first_local = arg_locals.next().unwrap();\n+                            let dest = self.eval_lvalue(&mir::Lvalue::Local(first_local))?;\n+                            self.write_value(args[0], dest)?;\n+                        }\n+\n+                        // unpack and write all other args\n+                        let layout = self.type_layout(args[1].ty)?;\n+                        if let (&ty::TyTuple(fields, _),\n+                                &Layout::Univariant { ref variant, .. }) = (&args[1].ty.sty, layout)\n+                        {\n+                            trace!(\"fields: {:?}\", fields);\n+                            if self.frame().mir.args_iter().count() == fields.len() + 1 {\n+                                let offsets = variant.offsets.iter().map(|s| s.bytes());\n+                                match args[1].value {\n+                                    Value::ByRef(PtrAndAlign { ptr, aligned }) => {\n+                                        assert!(\n+                                            aligned,\n+                                            \"Unaligned ByRef-values cannot occur as function arguments\"\n+                                        );\n+                                        for ((offset, ty), arg_local) in\n+                                            offsets.zip(fields).zip(arg_locals)\n+                                        {\n+                                            let arg = Value::by_ref(ptr.offset(offset, &self)?);\n+                                            let dest =\n+                                                self.eval_lvalue(&mir::Lvalue::Local(arg_local))?;\n+                                            trace!(\n+                                                \"writing arg {:?} to {:?} (type: {})\",\n+                                                arg,\n+                                                dest,\n+                                                ty\n+                                            );\n+                                            let valty = ValTy {\n+                                                value: arg,\n+                                                ty,\n+                                            };\n+                                            self.write_value(valty, dest)?;\n+                                        }\n+                                    }\n+                                    Value::ByVal(PrimVal::Undef) => {}\n+                                    other => {\n+                                        assert_eq!(fields.len(), 1);\n+                                        let dest = self.eval_lvalue(&mir::Lvalue::Local(\n+                                            arg_locals.next().unwrap(),\n+                                        ))?;\n+                                        let valty = ValTy {\n+                                            value: other,\n+                                            ty: fields[0],\n+                                        };\n+                                        self.write_value(valty, dest)?;\n+                                    }\n+                                }\n+                            } else {\n+                                trace!(\"manual impl of rust-call ABI\");\n+                                // called a manual impl of a rust-call function\n+                                let dest = self.eval_lvalue(\n+                                    &mir::Lvalue::Local(arg_locals.next().unwrap()),\n+                                )?;\n+                                self.write_value(args[1], dest)?;\n+                            }\n+                        } else {\n+                            bug!(\n+                                \"rust-call ABI tuple argument was {:#?}, {:#?}\",\n+                                args[1].ty,\n+                                layout\n+                            );\n+                        }\n+                    }\n+                    _ => {\n+                        for (arg_local, &valty) in arg_locals.zip(args) {\n+                            let dest = self.eval_lvalue(&mir::Lvalue::Local(arg_local))?;\n+                            self.write_value(valty, dest)?;\n+                        }\n+                    }\n+                }\n+                Ok(())\n+            }\n+            // cannot use the shim here, because that will only result in infinite recursion\n+            ty::InstanceDef::Virtual(_, idx) => {\n+                let ptr_size = self.memory.pointer_size();\n+                let (ptr, vtable) = args[0].into_ptr_vtable_pair(&self.memory)?;\n+                let fn_ptr = self.memory.read_ptr_sized_unsigned(\n+                    vtable.offset(ptr_size * (idx as u64 + 3), &self)?\n+                )?.to_ptr()?;\n+                let instance = self.memory.get_fn(fn_ptr)?;\n+                let mut args = args.to_vec();\n+                let ty = self.get_field_ty(args[0].ty, 0)?.ty; // TODO: packed flag is ignored\n+                args[0].ty = ty;\n+                args[0].value = ptr.to_value();\n+                // recurse with concrete function\n+                self.eval_fn_call(instance, destination, &args, span, sig)\n+            }\n+        }\n+    }\n+}"}, {"sha": "3f7e10a9eaff0bd929247141b34c3f4690300bfb", "filename": "src/librustc/mir/interpret/traits.rs", "status": "added", "additions": 137, "deletions": 0, "changes": 137, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Ftraits.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Ftraits.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Ftraits.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,137 @@\n+use rustc::traits::{self, Reveal};\n+use rustc::hir::def_id::DefId;\n+use rustc::ty::subst::Substs;\n+use rustc::ty::{self, Ty};\n+use syntax::codemap::DUMMY_SP;\n+use syntax::ast::{self, Mutability};\n+\n+use super::{EvalResult, EvalContext, eval_context, MemoryPointer, MemoryKind, Value, PrimVal,\n+            Machine};\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    pub(crate) fn fulfill_obligation(\n+        &self,\n+        trait_ref: ty::PolyTraitRef<'tcx>,\n+    ) -> traits::Vtable<'tcx, ()> {\n+        // Do the initial selection for the obligation. This yields the shallow result we are\n+        // looking for -- that is, what specific impl.\n+        self.tcx.infer_ctxt().enter(|infcx| {\n+            let mut selcx = traits::SelectionContext::new(&infcx);\n+\n+            let obligation = traits::Obligation::new(\n+                traits::ObligationCause::misc(DUMMY_SP, ast::DUMMY_NODE_ID),\n+                ty::ParamEnv::empty(Reveal::All),\n+                trait_ref.to_poly_trait_predicate(),\n+            );\n+            let selection = selcx.select(&obligation).unwrap().unwrap();\n+\n+            // Currently, we use a fulfillment context to completely resolve all nested obligations.\n+            // This is because they can inform the inference of the impl's type parameters.\n+            let mut fulfill_cx = traits::FulfillmentContext::new();\n+            let vtable = selection.map(|predicate| {\n+                fulfill_cx.register_predicate_obligation(&infcx, predicate);\n+            });\n+            infcx.drain_fulfillment_cx_or_panic(DUMMY_SP, &mut fulfill_cx, &vtable)\n+        })\n+    }\n+\n+    /// Creates a dynamic vtable for the given type and vtable origin. This is used only for\n+    /// objects.\n+    ///\n+    /// The `trait_ref` encodes the erased self type. Hence if we are\n+    /// making an object `Foo<Trait>` from a value of type `Foo<T>`, then\n+    /// `trait_ref` would map `T:Trait`.\n+    pub fn get_vtable(\n+        &mut self,\n+        ty: Ty<'tcx>,\n+        trait_ref: ty::PolyTraitRef<'tcx>,\n+    ) -> EvalResult<'tcx, MemoryPointer> {\n+        debug!(\"get_vtable(trait_ref={:?})\", trait_ref);\n+\n+        let size = self.type_size(trait_ref.self_ty())?.expect(\n+            \"can't create a vtable for an unsized type\",\n+        );\n+        let align = self.type_align(trait_ref.self_ty())?;\n+\n+        let ptr_size = self.memory.pointer_size();\n+        let methods = ::rustc::traits::get_vtable_methods(self.tcx, trait_ref);\n+        let vtable = self.memory.allocate(\n+            ptr_size * (3 + methods.count() as u64),\n+            ptr_size,\n+            MemoryKind::UninitializedStatic,\n+        )?;\n+\n+        let drop = eval_context::resolve_drop_in_place(self.tcx, ty);\n+        let drop = self.memory.create_fn_alloc(drop);\n+        self.memory.write_ptr_sized_unsigned(vtable, PrimVal::Ptr(drop))?;\n+\n+        let size_ptr = vtable.offset(ptr_size, &self)?;\n+        self.memory.write_ptr_sized_unsigned(size_ptr, PrimVal::Bytes(size as u128))?;\n+        let align_ptr = vtable.offset(ptr_size * 2, &self)?;\n+        self.memory.write_ptr_sized_unsigned(align_ptr, PrimVal::Bytes(align as u128))?;\n+\n+        for (i, method) in ::rustc::traits::get_vtable_methods(self.tcx, trait_ref).enumerate() {\n+            if let Some((def_id, substs)) = method {\n+                let instance = eval_context::resolve(self.tcx, def_id, substs);\n+                let fn_ptr = self.memory.create_fn_alloc(instance);\n+                let method_ptr = vtable.offset(ptr_size * (3 + i as u64), &self)?;\n+                self.memory.write_ptr_sized_unsigned(method_ptr, PrimVal::Ptr(fn_ptr))?;\n+            }\n+        }\n+\n+        self.memory.mark_static_initalized(\n+            vtable.alloc_id,\n+            Mutability::Mutable,\n+        )?;\n+\n+        Ok(vtable)\n+    }\n+\n+    pub fn read_drop_type_from_vtable(\n+        &self,\n+        vtable: MemoryPointer,\n+    ) -> EvalResult<'tcx, Option<ty::Instance<'tcx>>> {\n+        // we don't care about the pointee type, we just want a pointer\n+        match self.read_ptr(vtable, self.tcx.mk_nil_ptr())? {\n+            // some values don't need to call a drop impl, so the value is null\n+            Value::ByVal(PrimVal::Bytes(0)) => Ok(None),\n+            Value::ByVal(PrimVal::Ptr(drop_fn)) => self.memory.get_fn(drop_fn).map(Some),\n+            _ => err!(ReadBytesAsPointer),\n+        }\n+    }\n+\n+    pub fn read_size_and_align_from_vtable(\n+        &self,\n+        vtable: MemoryPointer,\n+    ) -> EvalResult<'tcx, (u64, u64)> {\n+        let pointer_size = self.memory.pointer_size();\n+        let size = self.memory.read_ptr_sized_unsigned(vtable.offset(pointer_size, self)?)?.to_bytes()? as u64;\n+        let align = self.memory.read_ptr_sized_unsigned(\n+            vtable.offset(pointer_size * 2, self)?\n+        )?.to_bytes()? as u64;\n+        Ok((size, align))\n+    }\n+\n+    pub(crate) fn resolve_associated_const(\n+        &self,\n+        def_id: DefId,\n+        substs: &'tcx Substs<'tcx>,\n+    ) -> ty::Instance<'tcx> {\n+        if let Some(trait_id) = self.tcx.trait_of_item(def_id) {\n+            let trait_ref = ty::Binder(ty::TraitRef::new(trait_id, substs));\n+            let vtable = self.fulfill_obligation(trait_ref);\n+            if let traits::VtableImpl(vtable_impl) = vtable {\n+                let name = self.tcx.item_name(def_id);\n+                let assoc_const_opt = self.tcx.associated_items(vtable_impl.impl_def_id).find(\n+                    |item| {\n+                        item.kind == ty::AssociatedKind::Const && item.name == name\n+                    },\n+                );\n+                if let Some(assoc_const) = assoc_const_opt {\n+                    return ty::Instance::new(assoc_const.def_id, vtable_impl.substs);\n+                }\n+            }\n+        }\n+        ty::Instance::new(def_id, substs)\n+    }\n+}"}, {"sha": "9be9341ee239b9477045b1ec461bcaa7974ef644", "filename": "src/librustc/mir/interpret/validation.rs", "status": "added", "additions": 727, "deletions": 0, "changes": 727, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fvalidation.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fvalidation.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fvalidation.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,727 @@\n+use rustc::hir::{self, Mutability};\n+use rustc::hir::Mutability::*;\n+use rustc::mir::{self, ValidationOp, ValidationOperand};\n+use rustc::ty::{self, Ty, TypeFoldable, TyCtxt};\n+use rustc::ty::subst::{Substs, Subst};\n+use rustc::traits;\n+use rustc::infer::InferCtxt;\n+use rustc::traits::Reveal;\n+use rustc::middle::region;\n+use rustc_data_structures::indexed_vec::Idx;\n+\n+use super::{EvalError, EvalResult, EvalErrorKind, EvalContext, DynamicLifetime, AccessKind, Value,\n+            Lvalue, LvalueExtra, Machine, ValTy};\n+\n+pub type ValidationQuery<'tcx> = ValidationOperand<'tcx, (AbsLvalue<'tcx>, Lvalue)>;\n+\n+#[derive(Copy, Clone, Debug, PartialEq)]\n+enum ValidationMode {\n+    Acquire,\n+    /// Recover because the given region ended\n+    Recover(region::Scope),\n+    ReleaseUntil(Option<region::Scope>),\n+}\n+\n+impl ValidationMode {\n+    fn acquiring(self) -> bool {\n+        use self::ValidationMode::*;\n+        match self {\n+            Acquire | Recover(_) => true,\n+            ReleaseUntil(_) => false,\n+        }\n+    }\n+}\n+\n+// Abstract lvalues\n+#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+pub enum AbsLvalue<'tcx> {\n+    Local(mir::Local),\n+    Static(hir::def_id::DefId),\n+    Projection(Box<AbsLvalueProjection<'tcx>>),\n+}\n+\n+type AbsLvalueProjection<'tcx> = mir::Projection<'tcx, AbsLvalue<'tcx>, u64, ()>;\n+type AbsLvalueElem<'tcx> = mir::ProjectionElem<'tcx, u64, ()>;\n+\n+impl<'tcx> AbsLvalue<'tcx> {\n+    pub fn field(self, f: mir::Field) -> AbsLvalue<'tcx> {\n+        self.elem(mir::ProjectionElem::Field(f, ()))\n+    }\n+\n+    pub fn deref(self) -> AbsLvalue<'tcx> {\n+        self.elem(mir::ProjectionElem::Deref)\n+    }\n+\n+    pub fn downcast(self, adt_def: &'tcx ty::AdtDef, variant_index: usize) -> AbsLvalue<'tcx> {\n+        self.elem(mir::ProjectionElem::Downcast(adt_def, variant_index))\n+    }\n+\n+    pub fn index(self, index: u64) -> AbsLvalue<'tcx> {\n+        self.elem(mir::ProjectionElem::Index(index))\n+    }\n+\n+    fn elem(self, elem: AbsLvalueElem<'tcx>) -> AbsLvalue<'tcx> {\n+        AbsLvalue::Projection(Box::new(AbsLvalueProjection {\n+            base: self,\n+            elem,\n+        }))\n+    }\n+}\n+\n+impl<'a, 'tcx, M: Machine<'tcx>> EvalContext<'a, 'tcx, M> {\n+    fn abstract_lvalue_projection(&self, proj: &mir::LvalueProjection<'tcx>) -> EvalResult<'tcx, AbsLvalueProjection<'tcx>> {\n+        use self::mir::ProjectionElem::*;\n+\n+        let elem = match proj.elem {\n+            Deref => Deref,\n+            Field(f, _) => Field(f, ()),\n+            Index(v) => {\n+                let value = self.frame().get_local(v)?;\n+                let ty = self.tcx.types.usize;\n+                let n = self.value_to_primval(ValTy { value, ty })?.to_u64()?;\n+                Index(n)\n+            },\n+            ConstantIndex { offset, min_length, from_end } =>\n+                ConstantIndex { offset, min_length, from_end },\n+            Subslice { from, to } =>\n+                Subslice { from, to },\n+            Downcast(adt, sz) => Downcast(adt, sz),\n+        };\n+        Ok(AbsLvalueProjection {\n+            base: self.abstract_lvalue(&proj.base)?,\n+            elem\n+        })\n+    }\n+\n+    fn abstract_lvalue(&self, lval: &mir::Lvalue<'tcx>) -> EvalResult<'tcx, AbsLvalue<'tcx>> {\n+        Ok(match lval {\n+            &mir::Lvalue::Local(l) => AbsLvalue::Local(l),\n+            &mir::Lvalue::Static(ref s) => AbsLvalue::Static(s.def_id),\n+            &mir::Lvalue::Projection(ref p) =>\n+                AbsLvalue::Projection(Box::new(self.abstract_lvalue_projection(&*p)?)),\n+        })\n+    }\n+\n+    // Validity checks\n+    pub(crate) fn validation_op(\n+        &mut self,\n+        op: ValidationOp,\n+        operand: &ValidationOperand<'tcx, mir::Lvalue<'tcx>>,\n+    ) -> EvalResult<'tcx> {\n+        // If mir-emit-validate is set to 0 (i.e., disabled), we may still see validation commands\n+        // because other crates may have been compiled with mir-emit-validate > 0.  Ignore those\n+        // commands.  This makes mir-emit-validate also a flag to control whether miri will do\n+        // validation or not.\n+        if self.tcx.sess.opts.debugging_opts.mir_emit_validate == 0 {\n+            return Ok(());\n+        }\n+        debug_assert!(self.memory.cur_frame == self.cur_frame());\n+\n+        // HACK: Determine if this method is whitelisted and hence we do not perform any validation.\n+        // We currently insta-UB on anything passing around uninitialized memory, so we have to whitelist\n+        // the places that are allowed to do that.\n+        // The second group is stuff libstd does that is forbidden even under relaxed validation.\n+        {\n+            // The regexp we use for filtering\n+            use regex::Regex;\n+            lazy_static! {\n+                static ref RE: Regex = Regex::new(\"^(\\\n+                    (std|alloc::heap::__core)::mem::(uninitialized|forget)::|\\\n+                    <(std|alloc)::heap::Heap as (std::heap|alloc::allocator)::Alloc>::|\\\n+                    <(std|alloc::heap::__core)::mem::ManuallyDrop<T>><.*>::new$|\\\n+                    <(std|alloc::heap::__core)::mem::ManuallyDrop<T> as std::ops::DerefMut><.*>::deref_mut$|\\\n+                    (std|alloc::heap::__core)::ptr::read::|\\\n+                    \\\n+                    <std::sync::Arc<T>><.*>::inner$|\\\n+                    <std::sync::Arc<T>><.*>::drop_slow$|\\\n+                    (std::heap|alloc::allocator)::Layout::for_value::|\\\n+                    (std|alloc::heap::__core)::mem::(size|align)_of_val::\\\n+                )\").unwrap();\n+            }\n+            // Now test\n+            let name = self.stack[self.cur_frame()].instance.to_string();\n+            if RE.is_match(&name) {\n+                return Ok(());\n+            }\n+        }\n+\n+        // We need to monomorphize ty *without* erasing lifetimes\n+        let ty = operand.ty.subst(self.tcx, self.substs());\n+        let lval = self.eval_lvalue(&operand.lval)?;\n+        let abs_lval = self.abstract_lvalue(&operand.lval)?;\n+        let query = ValidationQuery {\n+            lval: (abs_lval, lval),\n+            ty,\n+            re: operand.re,\n+            mutbl: operand.mutbl,\n+        };\n+\n+        // Check the mode, and also perform mode-specific operations\n+        let mode = match op {\n+            ValidationOp::Acquire => ValidationMode::Acquire,\n+            ValidationOp::Release => ValidationMode::ReleaseUntil(None),\n+            ValidationOp::Suspend(scope) => {\n+                if query.mutbl == MutMutable {\n+                    let lft = DynamicLifetime {\n+                        frame: self.cur_frame(),\n+                        region: Some(scope), // Notably, we only ever suspend things for given regions.\n+                        // Suspending for the entire function does not make any sense.\n+                    };\n+                    trace!(\"Suspending {:?} until {:?}\", query, scope);\n+                    self.suspended.entry(lft).or_insert_with(Vec::new).push(\n+                        query.clone(),\n+                    );\n+                }\n+                ValidationMode::ReleaseUntil(Some(scope))\n+            }\n+        };\n+        self.validate(query, mode)\n+    }\n+\n+    /// Release locks and executes suspensions of the given region (or the entire fn, in case of None).\n+    pub(crate) fn end_region(&mut self, scope: Option<region::Scope>) -> EvalResult<'tcx> {\n+        debug_assert!(self.memory.cur_frame == self.cur_frame());\n+        self.memory.locks_lifetime_ended(scope);\n+        match scope {\n+            Some(scope) => {\n+                // Recover suspended lvals\n+                let lft = DynamicLifetime {\n+                    frame: self.cur_frame(),\n+                    region: Some(scope),\n+                };\n+                if let Some(queries) = self.suspended.remove(&lft) {\n+                    for query in queries {\n+                        trace!(\"Recovering {:?} from suspension\", query);\n+                        self.validate(query, ValidationMode::Recover(scope))?;\n+                    }\n+                }\n+            }\n+            None => {\n+                // Clean suspension table of current frame\n+                let cur_frame = self.cur_frame();\n+                self.suspended.retain(|lft, _| {\n+                    lft.frame != cur_frame // keep only what is in the other (lower) frames\n+                });\n+            }\n+        }\n+        Ok(())\n+    }\n+\n+    fn normalize_type_unerased(&self, ty: Ty<'tcx>) -> Ty<'tcx> {\n+        return normalize_associated_type(self.tcx, &ty);\n+\n+        use syntax::codemap::{Span, DUMMY_SP};\n+\n+        // We copy a bunch of stuff from rustc/infer/mod.rs to be able to tweak its behavior\n+        fn normalize_projections_in<'a, 'gcx, 'tcx, T>(\n+            self_: &InferCtxt<'a, 'gcx, 'tcx>,\n+            param_env: ty::ParamEnv<'tcx>,\n+            value: &T,\n+        ) -> T::Lifted\n+        where\n+            T: TypeFoldable<'tcx> + ty::Lift<'gcx>,\n+        {\n+            let mut selcx = traits::SelectionContext::new(self_);\n+            let cause = traits::ObligationCause::dummy();\n+            let traits::Normalized {\n+                value: result,\n+                obligations,\n+            } = traits::normalize(&mut selcx, param_env, cause, value);\n+\n+            let mut fulfill_cx = traits::FulfillmentContext::new();\n+\n+            for obligation in obligations {\n+                fulfill_cx.register_predicate_obligation(self_, obligation);\n+            }\n+\n+            drain_fulfillment_cx_or_panic(self_, DUMMY_SP, &mut fulfill_cx, &result)\n+        }\n+\n+        fn drain_fulfillment_cx_or_panic<'a, 'gcx, 'tcx, T>(\n+            self_: &InferCtxt<'a, 'gcx, 'tcx>,\n+            span: Span,\n+            fulfill_cx: &mut traits::FulfillmentContext<'tcx>,\n+            result: &T,\n+        ) -> T::Lifted\n+        where\n+            T: TypeFoldable<'tcx> + ty::Lift<'gcx>,\n+        {\n+            // In principle, we only need to do this so long as `result`\n+            // contains unbound type parameters. It could be a slight\n+            // optimization to stop iterating early.\n+            match fulfill_cx.select_all_or_error(self_) {\n+                Ok(()) => { }\n+                Err(errors) => {\n+                    span_bug!(\n+                        span,\n+                        \"Encountered errors `{:?}` resolving bounds after type-checking\",\n+                        errors\n+                    );\n+                }\n+            }\n+\n+            let result = self_.resolve_type_vars_if_possible(result);\n+            let result = self_.tcx.fold_regions(\n+                &result,\n+                &mut false,\n+                |r, _| match *r {\n+                    ty::ReVar(_) => self_.tcx.types.re_erased,\n+                    _ => r,\n+                },\n+            );\n+\n+            match self_.tcx.lift_to_global(&result) {\n+                Some(result) => result,\n+                None => {\n+                    span_bug!(span, \"Uninferred types/regions in `{:?}`\", result);\n+                }\n+            }\n+        }\n+\n+        trait MyTransNormalize<'gcx>: TypeFoldable<'gcx> {\n+            fn my_trans_normalize<'a, 'tcx>(\n+                &self,\n+                infcx: &InferCtxt<'a, 'gcx, 'tcx>,\n+                param_env: ty::ParamEnv<'tcx>,\n+            ) -> Self;\n+        }\n+\n+        macro_rules! items { ($($item:item)+) => ($($item)+) }\n+        macro_rules! impl_trans_normalize {\n+            ($lt_gcx:tt, $($ty:ty),+) => {\n+                items!($(impl<$lt_gcx> MyTransNormalize<$lt_gcx> for $ty {\n+                    fn my_trans_normalize<'a, 'tcx>(&self,\n+                                                infcx: &InferCtxt<'a, $lt_gcx, 'tcx>,\n+                                                param_env: ty::ParamEnv<'tcx>)\n+                                                -> Self {\n+                        normalize_projections_in(infcx, param_env, self)\n+                    }\n+                })+);\n+            }\n+        }\n+\n+        impl_trans_normalize!('gcx,\n+            Ty<'gcx>,\n+            &'gcx Substs<'gcx>,\n+            ty::FnSig<'gcx>,\n+            ty::PolyFnSig<'gcx>,\n+            ty::ClosureSubsts<'gcx>,\n+            ty::PolyTraitRef<'gcx>,\n+            ty::ExistentialTraitRef<'gcx>\n+        );\n+\n+        fn normalize_associated_type<'a, 'tcx, T>(self_: TyCtxt<'a, 'tcx, 'tcx>, value: &T) -> T\n+        where\n+            T: MyTransNormalize<'tcx>,\n+        {\n+            let param_env = ty::ParamEnv::empty(Reveal::All);\n+\n+            if !value.has_projections() {\n+                return value.clone();\n+            }\n+\n+            self_.infer_ctxt().enter(|infcx| {\n+                value.my_trans_normalize(&infcx, param_env)\n+            })\n+        }\n+    }\n+\n+    fn validate_variant(\n+        &mut self,\n+        query: ValidationQuery<'tcx>,\n+        variant: &ty::VariantDef,\n+        subst: &ty::subst::Substs<'tcx>,\n+        mode: ValidationMode,\n+    ) -> EvalResult<'tcx> {\n+        // TODO: Maybe take visibility/privacy into account.\n+        for (idx, field_def) in variant.fields.iter().enumerate() {\n+            let field_ty = field_def.ty(self.tcx, subst);\n+            let field = mir::Field::new(idx);\n+            let field_lvalue = self.lvalue_field(query.lval.1, field, query.ty, field_ty)?;\n+            self.validate(\n+                ValidationQuery {\n+                    lval: (query.lval.0.clone().field(field), field_lvalue),\n+                    ty: field_ty,\n+                    ..query\n+                },\n+                mode,\n+            )?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn validate_ptr(\n+        &mut self,\n+        val: Value,\n+        abs_lval: AbsLvalue<'tcx>,\n+        pointee_ty: Ty<'tcx>,\n+        re: Option<region::Scope>,\n+        mutbl: Mutability,\n+        mode: ValidationMode,\n+    ) -> EvalResult<'tcx> {\n+        // Check alignment and non-NULLness\n+        let (_, align) = self.size_and_align_of_dst(pointee_ty, val)?;\n+        let ptr = val.into_ptr(&self.memory)?;\n+        self.memory.check_align(ptr, align, None)?;\n+\n+        // Recurse\n+        let pointee_lvalue = self.val_to_lvalue(val, pointee_ty)?;\n+        self.validate(\n+            ValidationQuery {\n+                lval: (abs_lval.deref(), pointee_lvalue),\n+                ty: pointee_ty,\n+                re,\n+                mutbl,\n+            },\n+            mode,\n+        )\n+    }\n+\n+    /// Validate the lvalue at the given type. If `acquire` is false, just do a release of all write locks\n+    fn validate(\n+        &mut self,\n+        mut query: ValidationQuery<'tcx>,\n+        mode: ValidationMode,\n+    ) -> EvalResult<'tcx> {\n+        use rustc::ty::TypeVariants::*;\n+        use rustc::ty::RegionKind::*;\n+        use rustc::ty::AdtKind;\n+\n+        // No point releasing shared stuff.\n+        if !mode.acquiring() && query.mutbl == MutImmutable {\n+            return Ok(());\n+        }\n+        // When we recover, we may see data whose validity *just* ended.  Do not acquire it.\n+        if let ValidationMode::Recover(ending_ce) = mode {\n+            if query.re == Some(ending_ce) {\n+                return Ok(());\n+            }\n+        }\n+\n+        query.ty = self.normalize_type_unerased(&query.ty);\n+        trace!(\"{:?} on {:?}\", mode, query);\n+\n+        // Decide whether this type *owns* the memory it covers (like integers), or whether it\n+        // just assembles pieces (that each own their memory) together to a larger whole.\n+        // TODO: Currently, we don't acquire locks for padding and discriminants. We should.\n+        let is_owning = match query.ty.sty {\n+            TyInt(_) | TyUint(_) | TyRawPtr(_) | TyBool | TyFloat(_) | TyChar | TyStr |\n+            TyRef(..) | TyFnPtr(..) | TyFnDef(..) | TyNever => true,\n+            TyAdt(adt, _) if adt.is_box() => true,\n+            TySlice(_) | TyAdt(_, _) | TyTuple(..) | TyClosure(..) | TyArray(..) |\n+            TyDynamic(..) | TyGenerator(..) => false,\n+            TyParam(_) | TyInfer(_) | TyProjection(_) | TyAnon(..) | TyError => {\n+                bug!(\"I got an incomplete/unnormalized type for validation\")\n+            }\n+        };\n+        if is_owning {\n+            // We need to lock.  So we need memory.  So we have to force_acquire.\n+            // Tracking the same state for locals not backed by memory would just duplicate too\n+            // much machinery.\n+            // FIXME: We ignore alignment.\n+            let (ptr, extra) = self.force_allocation(query.lval.1)?.to_ptr_extra_aligned();\n+            // Determine the size\n+            // FIXME: Can we reuse size_and_align_of_dst for Lvalues?\n+            let len = match self.type_size(query.ty)? {\n+                Some(size) => {\n+                    assert_eq!(extra, LvalueExtra::None, \"Got a fat ptr to a sized type\");\n+                    size\n+                }\n+                None => {\n+                    // The only unsized typ we concider \"owning\" is TyStr.\n+                    assert_eq!(\n+                        query.ty.sty,\n+                        TyStr,\n+                        \"Found a surprising unsized owning type\"\n+                    );\n+                    // The extra must be the length, in bytes.\n+                    match extra {\n+                        LvalueExtra::Length(len) => len,\n+                        _ => bug!(\"TyStr must have a length as extra\"),\n+                    }\n+                }\n+            };\n+            // Handle locking\n+            if len > 0 {\n+                let ptr = ptr.to_ptr()?;\n+                match query.mutbl {\n+                    MutImmutable => {\n+                        if mode.acquiring() {\n+                            self.memory.acquire_lock(\n+                                ptr,\n+                                len,\n+                                query.re,\n+                                AccessKind::Read,\n+                            )?;\n+                        }\n+                    }\n+                    // No releasing of read locks, ever.\n+                    MutMutable => {\n+                        match mode {\n+                            ValidationMode::Acquire => {\n+                                self.memory.acquire_lock(\n+                                    ptr,\n+                                    len,\n+                                    query.re,\n+                                    AccessKind::Write,\n+                                )?\n+                            }\n+                            ValidationMode::Recover(ending_ce) => {\n+                                self.memory.recover_write_lock(\n+                                    ptr,\n+                                    len,\n+                                    &query.lval.0,\n+                                    query.re,\n+                                    ending_ce,\n+                                )?\n+                            }\n+                            ValidationMode::ReleaseUntil(suspended_ce) => {\n+                                self.memory.suspend_write_lock(\n+                                    ptr,\n+                                    len,\n+                                    &query.lval.0,\n+                                    suspended_ce,\n+                                )?\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        let res = do catch {\n+            match query.ty.sty {\n+                TyInt(_) | TyUint(_) | TyRawPtr(_) => {\n+                    if mode.acquiring() {\n+                        // Make sure we can read this.\n+                        let val = self.read_lvalue(query.lval.1)?;\n+                        self.follow_by_ref_value(val, query.ty)?;\n+                        // FIXME: It would be great to rule out Undef here, but that doesn't actually work.\n+                        // Passing around undef data is a thing that e.g. Vec::extend_with does.\n+                    }\n+                    Ok(())\n+                }\n+                TyBool | TyFloat(_) | TyChar => {\n+                    if mode.acquiring() {\n+                        let val = self.read_lvalue(query.lval.1)?;\n+                        let val = self.value_to_primval(ValTy { value: val, ty: query.ty })?;\n+                        val.to_bytes()?;\n+                        // TODO: Check if these are valid bool/float/codepoint/UTF-8\n+                    }\n+                    Ok(())\n+                }\n+                TyNever => err!(ValidationFailure(format!(\"The empty type is never valid.\"))),\n+                TyRef(region,\n+                    ty::TypeAndMut {\n+                        ty: pointee_ty,\n+                        mutbl,\n+                    }) => {\n+                    let val = self.read_lvalue(query.lval.1)?;\n+                    // Sharing restricts our context\n+                    if mutbl == MutImmutable {\n+                        query.mutbl = MutImmutable;\n+                    }\n+                    // Inner lifetimes *outlive* outer ones, so only if we have no lifetime restriction yet,\n+                    // we record the region of this borrow to the context.\n+                    if query.re == None {\n+                        match *region {\n+                            ReScope(scope) => query.re = Some(scope),\n+                            // It is possible for us to encounter erased lifetimes here because the lifetimes in\n+                            // this functions' Subst will be erased.\n+                            _ => {}\n+                        }\n+                    }\n+                    self.validate_ptr(val, query.lval.0, pointee_ty, query.re, query.mutbl, mode)\n+                }\n+                TyAdt(adt, _) if adt.is_box() => {\n+                    let val = self.read_lvalue(query.lval.1)?;\n+                    self.validate_ptr(val, query.lval.0, query.ty.boxed_ty(), query.re, query.mutbl, mode)\n+                }\n+                TyFnPtr(_sig) => {\n+                    let ptr = self.read_lvalue(query.lval.1)?\n+                        .into_ptr(&self.memory)?\n+                        .to_ptr()?;\n+                    self.memory.get_fn(ptr)?;\n+                    // TODO: Check if the signature matches (should be the same check as what terminator/mod.rs already does on call?).\n+                    Ok(())\n+                }\n+                TyFnDef(..) => {\n+                    // This is a zero-sized type with all relevant data sitting in the type.\n+                    // There is nothing to validate.\n+                    Ok(())\n+                }\n+\n+                // Compound types\n+                TyStr => {\n+                    // TODO: Validate strings\n+                    Ok(())\n+                }\n+                TySlice(elem_ty) => {\n+                    let len = match query.lval.1 {\n+                        Lvalue::Ptr { extra: LvalueExtra::Length(len), .. } => len,\n+                        _ => {\n+                            bug!(\n+                                \"acquire_valid of a TySlice given non-slice lvalue: {:?}\",\n+                                query.lval\n+                            )\n+                        }\n+                    };\n+                    for i in 0..len {\n+                        let inner_lvalue = self.lvalue_index(query.lval.1, query.ty, i)?;\n+                        self.validate(\n+                            ValidationQuery {\n+                                lval: (query.lval.0.clone().index(i), inner_lvalue),\n+                                ty: elem_ty,\n+                                ..query\n+                            },\n+                            mode,\n+                        )?;\n+                    }\n+                    Ok(())\n+                }\n+                TyArray(elem_ty, len) => {\n+                    let len = len.val.to_const_int().unwrap().to_u64().unwrap();\n+                    for i in 0..len {\n+                        let inner_lvalue = self.lvalue_index(query.lval.1, query.ty, i as u64)?;\n+                        self.validate(\n+                            ValidationQuery {\n+                                lval: (query.lval.0.clone().index(i as u64), inner_lvalue),\n+                                ty: elem_ty,\n+                                ..query\n+                            },\n+                            mode,\n+                        )?;\n+                    }\n+                    Ok(())\n+                }\n+                TyDynamic(_data, _region) => {\n+                    // Check that this is a valid vtable\n+                    let vtable = match query.lval.1 {\n+                        Lvalue::Ptr { extra: LvalueExtra::Vtable(vtable), .. } => vtable,\n+                        _ => {\n+                            bug!(\n+                                \"acquire_valid of a TyDynamic given non-trait-object lvalue: {:?}\",\n+                                query.lval\n+                            )\n+                        }\n+                    };\n+                    self.read_size_and_align_from_vtable(vtable)?;\n+                    // TODO: Check that the vtable contains all the function pointers we expect it to have.\n+                    // Trait objects cannot have any operations performed\n+                    // on them directly.  We cannot, in general, even acquire any locks as the trait object *could*\n+                    // contain an UnsafeCell.  If we call functions to get access to data, we will validate\n+                    // their return values.  So, it doesn't seem like there's anything else to do.\n+                    Ok(())\n+                }\n+                TyAdt(adt, subst) => {\n+                    if Some(adt.did) == self.tcx.lang_items().unsafe_cell_type() &&\n+                        query.mutbl == MutImmutable\n+                    {\n+                        // No locks for shared unsafe cells.  Also no other validation, the only field is private anyway.\n+                        return Ok(());\n+                    }\n+\n+                    match adt.adt_kind() {\n+                        AdtKind::Enum => {\n+                            // TODO: Can we get the discriminant without forcing an allocation?\n+                            let ptr = self.force_allocation(query.lval.1)?.to_ptr()?;\n+                            let discr = self.read_discriminant_value(ptr, query.ty)?;\n+\n+                            // Get variant index for discriminant\n+                            let variant_idx = adt.discriminants(self.tcx).position(|variant_discr| {\n+                                variant_discr.to_u128_unchecked() == discr\n+                            });\n+                            let variant_idx = match variant_idx {\n+                                Some(val) => val,\n+                                None => return err!(InvalidDiscriminant),\n+                            };\n+                            let variant = &adt.variants[variant_idx];\n+\n+                            if variant.fields.len() > 0 {\n+                                // Downcast to this variant, if needed\n+                                let lval = if adt.variants.len() > 1 {\n+                                    (\n+                                        query.lval.0.downcast(adt, variant_idx),\n+                                        self.eval_lvalue_projection(\n+                                            query.lval.1,\n+                                            query.ty,\n+                                            &mir::ProjectionElem::Downcast(adt, variant_idx),\n+                                        )?,\n+                                    )\n+                                } else {\n+                                    query.lval\n+                                };\n+\n+                                // Recursively validate the fields\n+                                self.validate_variant(\n+                                    ValidationQuery { lval, ..query },\n+                                    variant,\n+                                    subst,\n+                                    mode,\n+                                )\n+                            } else {\n+                                // No fields, nothing left to check.  Downcasting may fail, e.g. in case of a CEnum.\n+                                Ok(())\n+                            }\n+                        }\n+                        AdtKind::Struct => {\n+                            self.validate_variant(query, adt.struct_variant(), subst, mode)\n+                        }\n+                        AdtKind::Union => {\n+                            // No guarantees are provided for union types.\n+                            // TODO: Make sure that all access to union fields is unsafe; otherwise, we may have some checking to do (but what exactly?)\n+                            Ok(())\n+                        }\n+                    }\n+                }\n+                TyTuple(ref types, _) => {\n+                    for (idx, field_ty) in types.iter().enumerate() {\n+                        let field = mir::Field::new(idx);\n+                        let field_lvalue = self.lvalue_field(query.lval.1, field, query.ty, field_ty)?;\n+                        self.validate(\n+                            ValidationQuery {\n+                                lval: (query.lval.0.clone().field(field), field_lvalue),\n+                                ty: field_ty,\n+                                ..query\n+                            },\n+                            mode,\n+                        )?;\n+                    }\n+                    Ok(())\n+                }\n+                TyClosure(def_id, ref closure_substs) => {\n+                    for (idx, field_ty) in closure_substs.upvar_tys(def_id, self.tcx).enumerate() {\n+                        let field = mir::Field::new(idx);\n+                        let field_lvalue = self.lvalue_field(query.lval.1, field, query.ty, field_ty)?;\n+                        self.validate(\n+                            ValidationQuery {\n+                                lval: (query.lval.0.clone().field(field), field_lvalue),\n+                                ty: field_ty,\n+                                ..query\n+                            },\n+                            mode,\n+                        )?;\n+                    }\n+                    // TODO: Check if the signature matches (should be the same check as what terminator/mod.rs already does on call?).\n+                    // Is there other things we can/should check?  Like vtable pointers?\n+                    Ok(())\n+                }\n+                // FIXME: generators aren't validated right now\n+                TyGenerator(..) => Ok(()),\n+                _ => bug!(\"We already established that this is a type we support. ({})\", query.ty),\n+            }\n+        };\n+        match res {\n+            // ReleaseUntil(None) of an uninitalized variable is a NOP.  This is needed because\n+            // we have to release the return value of a function; due to destination-passing-style\n+            // the callee may directly write there.\n+            // TODO: Ideally we would know whether the destination is already initialized, and only\n+            // release if it is.  But of course that can't even always be statically determined.\n+            Err(EvalError { kind: EvalErrorKind::ReadUndefBytes, .. })\n+                if mode == ValidationMode::ReleaseUntil(None) => {\n+                return Ok(());\n+            }\n+            res => res,\n+        }\n+    }\n+}"}, {"sha": "e052ec1e391cd30f1a69449602b5ebdf446d5319", "filename": "src/librustc/mir/interpret/value.rs", "status": "added", "additions": 405, "deletions": 0, "changes": 405, "blob_url": "https://github.com/rust-lang/rust/blob/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/df5e122edaab52250b0a49ccf30a2f56948b75aa/src%2Flibrustc%2Fmir%2Finterpret%2Fvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmir%2Finterpret%2Fvalue.rs?ref=df5e122edaab52250b0a49ccf30a2f56948b75aa", "patch": "@@ -0,0 +1,405 @@\n+#![allow(unknown_lints)]\n+\n+use rustc::ty::layout::HasDataLayout;\n+\n+use super::{EvalResult, Memory, MemoryPointer, HasMemory, PointerArithmetic, Machine, PtrAndAlign};\n+\n+pub(super) fn bytes_to_f32(bytes: u128) -> f32 {\n+    f32::from_bits(bytes as u32)\n+}\n+\n+pub(super) fn bytes_to_f64(bytes: u128) -> f64 {\n+    f64::from_bits(bytes as u64)\n+}\n+\n+pub(super) fn f32_to_bytes(f: f32) -> u128 {\n+    f.to_bits() as u128\n+}\n+\n+pub(super) fn f64_to_bytes(f: f64) -> u128 {\n+    f.to_bits() as u128\n+}\n+\n+/// A `Value` represents a single self-contained Rust value.\n+///\n+/// A `Value` can either refer to a block of memory inside an allocation (`ByRef`) or to a primitve\n+/// value held directly, outside of any allocation (`ByVal`).  For `ByRef`-values, we remember\n+/// whether the pointer is supposed to be aligned or not (also see Lvalue).\n+///\n+/// For optimization of a few very common cases, there is also a representation for a pair of\n+/// primitive values (`ByValPair`). It allows Miri to avoid making allocations for checked binary\n+/// operations and fat pointers. This idea was taken from rustc's trans.\n+#[derive(Clone, Copy, Debug)]\n+pub enum Value {\n+    ByRef(PtrAndAlign),\n+    ByVal(PrimVal),\n+    ByValPair(PrimVal, PrimVal),\n+}\n+\n+/// A wrapper type around `PrimVal` that cannot be turned back into a `PrimVal` accidentally.\n+/// This type clears up a few APIs where having a `PrimVal` argument for something that is\n+/// potentially an integer pointer or a pointer to an allocation was unclear.\n+///\n+/// I (@oli-obk) believe it is less easy to mix up generic primvals and primvals that are just\n+/// the representation of pointers. Also all the sites that convert between primvals and pointers\n+/// are explicit now (and rare!)\n+#[derive(Clone, Copy, Debug)]\n+pub struct Pointer {\n+    primval: PrimVal,\n+}\n+\n+impl<'tcx> Pointer {\n+    pub fn null() -> Self {\n+        PrimVal::Bytes(0).into()\n+    }\n+    pub fn to_ptr(self) -> EvalResult<'tcx, MemoryPointer> {\n+        self.primval.to_ptr()\n+    }\n+    pub fn into_inner_primval(self) -> PrimVal {\n+        self.primval\n+    }\n+\n+    pub fn signed_offset<C: HasDataLayout>(self, i: i64, cx: C) -> EvalResult<'tcx, Self> {\n+        let layout = cx.data_layout();\n+        match self.primval {\n+            PrimVal::Bytes(b) => {\n+                assert_eq!(b as u64 as u128, b);\n+                Ok(Pointer::from(\n+                    PrimVal::Bytes(layout.signed_offset(b as u64, i)? as u128),\n+                ))\n+            }\n+            PrimVal::Ptr(ptr) => ptr.signed_offset(i, layout).map(Pointer::from),\n+            PrimVal::Undef => err!(ReadUndefBytes),\n+        }\n+    }\n+\n+    pub fn offset<C: HasDataLayout>(self, i: u64, cx: C) -> EvalResult<'tcx, Self> {\n+        let layout = cx.data_layout();\n+        match self.primval {\n+            PrimVal::Bytes(b) => {\n+                assert_eq!(b as u64 as u128, b);\n+                Ok(Pointer::from(\n+                    PrimVal::Bytes(layout.offset(b as u64, i)? as u128),\n+                ))\n+            }\n+            PrimVal::Ptr(ptr) => ptr.offset(i, layout).map(Pointer::from),\n+            PrimVal::Undef => err!(ReadUndefBytes),\n+        }\n+    }\n+\n+    pub fn wrapping_signed_offset<C: HasDataLayout>(self, i: i64, cx: C) -> EvalResult<'tcx, Self> {\n+        let layout = cx.data_layout();\n+        match self.primval {\n+            PrimVal::Bytes(b) => {\n+                assert_eq!(b as u64 as u128, b);\n+                Ok(Pointer::from(PrimVal::Bytes(\n+                    layout.wrapping_signed_offset(b as u64, i) as u128,\n+                )))\n+            }\n+            PrimVal::Ptr(ptr) => Ok(Pointer::from(ptr.wrapping_signed_offset(i, layout))),\n+            PrimVal::Undef => err!(ReadUndefBytes),\n+        }\n+    }\n+\n+    pub fn is_null(self) -> EvalResult<'tcx, bool> {\n+        match self.primval {\n+            PrimVal::Bytes(b) => Ok(b == 0),\n+            PrimVal::Ptr(_) => Ok(false),\n+            PrimVal::Undef => err!(ReadUndefBytes),\n+        }\n+    }\n+\n+    pub fn to_value_with_len(self, len: u64) -> Value {\n+        Value::ByValPair(self.primval, PrimVal::from_u128(len as u128))\n+    }\n+\n+    pub fn to_value_with_vtable(self, vtable: MemoryPointer) -> Value {\n+        Value::ByValPair(self.primval, PrimVal::Ptr(vtable))\n+    }\n+\n+    pub fn to_value(self) -> Value {\n+        Value::ByVal(self.primval)\n+    }\n+}\n+\n+impl ::std::convert::From<PrimVal> for Pointer {\n+    fn from(primval: PrimVal) -> Self {\n+        Pointer { primval }\n+    }\n+}\n+\n+impl ::std::convert::From<MemoryPointer> for Pointer {\n+    fn from(ptr: MemoryPointer) -> Self {\n+        PrimVal::Ptr(ptr).into()\n+    }\n+}\n+\n+/// A `PrimVal` represents an immediate, primitive value existing outside of a\n+/// `memory::Allocation`. It is in many ways like a small chunk of a `Allocation`, up to 8 bytes in\n+/// size. Like a range of bytes in an `Allocation`, a `PrimVal` can either represent the raw bytes\n+/// of a simple value, a pointer into another `Allocation`, or be undefined.\n+#[derive(Clone, Copy, Debug)]\n+pub enum PrimVal {\n+    /// The raw bytes of a simple value.\n+    Bytes(u128),\n+\n+    /// A pointer into an `Allocation`. An `Allocation` in the `memory` module has a list of\n+    /// relocations, but a `PrimVal` is only large enough to contain one, so we just represent the\n+    /// relocation and its associated offset together as a `MemoryPointer` here.\n+    Ptr(MemoryPointer),\n+\n+    /// An undefined `PrimVal`, for representing values that aren't safe to examine, but are safe\n+    /// to copy around, just like undefined bytes in an `Allocation`.\n+    Undef,\n+}\n+\n+#[derive(Clone, Copy, Debug, PartialEq)]\n+pub enum PrimValKind {\n+    I8, I16, I32, I64, I128,\n+    U8, U16, U32, U64, U128,\n+    F32, F64,\n+    Ptr, FnPtr,\n+    Bool,\n+    Char,\n+}\n+\n+impl<'a, 'tcx: 'a> Value {\n+    #[inline]\n+    pub fn by_ref(ptr: Pointer) -> Self {\n+        Value::ByRef(PtrAndAlign { ptr, aligned: true })\n+    }\n+\n+    /// Convert the value into a pointer (or a pointer-sized integer).  If the value is a ByRef,\n+    /// this may have to perform a load.\n+    pub fn into_ptr<M: Machine<'tcx>>(\n+        &self,\n+        mem: &Memory<'a, 'tcx, M>,\n+    ) -> EvalResult<'tcx, Pointer> {\n+        use self::Value::*;\n+        Ok(match *self {\n+            ByRef(PtrAndAlign { ptr, aligned }) => {\n+                mem.read_maybe_aligned(aligned, |mem| mem.read_ptr_sized_unsigned(ptr.to_ptr()?))?\n+            }\n+            ByVal(ptr) |\n+            ByValPair(ptr, _) => ptr,\n+        }.into())\n+    }\n+\n+    pub(super) fn into_ptr_vtable_pair<M: Machine<'tcx>>(\n+        &self,\n+        mem: &Memory<'a, 'tcx, M>,\n+    ) -> EvalResult<'tcx, (Pointer, MemoryPointer)> {\n+        use self::Value::*;\n+        match *self {\n+            ByRef(PtrAndAlign {\n+                      ptr: ref_ptr,\n+                      aligned,\n+                  }) => {\n+                mem.read_maybe_aligned(aligned, |mem| {\n+                    let ptr = mem.read_ptr_sized_unsigned(ref_ptr.to_ptr()?)?.into();\n+                    let vtable = mem.read_ptr_sized_unsigned(\n+                        ref_ptr.offset(mem.pointer_size(), mem.layout)?.to_ptr()?,\n+                    )?.to_ptr()?;\n+                    Ok((ptr, vtable))\n+                })\n+            }\n+\n+            ByValPair(ptr, vtable) => Ok((ptr.into(), vtable.to_ptr()?)),\n+\n+            ByVal(PrimVal::Undef) => err!(ReadUndefBytes),\n+            _ => bug!(\"expected ptr and vtable, got {:?}\", self),\n+        }\n+    }\n+\n+    pub(super) fn into_slice<M: Machine<'tcx>>(\n+        &self,\n+        mem: &Memory<'a, 'tcx, M>,\n+    ) -> EvalResult<'tcx, (Pointer, u64)> {\n+        use self::Value::*;\n+        match *self {\n+            ByRef(PtrAndAlign {\n+                      ptr: ref_ptr,\n+                      aligned,\n+                  }) => {\n+                mem.read_maybe_aligned(aligned, |mem| {\n+                    let ptr = mem.read_ptr_sized_unsigned(ref_ptr.to_ptr()?)?.into();\n+                    let len = mem.read_ptr_sized_unsigned(\n+                        ref_ptr.offset(mem.pointer_size(), mem.layout)?.to_ptr()?,\n+                    )?.to_bytes()? as u64;\n+                    Ok((ptr, len))\n+                })\n+            }\n+            ByValPair(ptr, val) => {\n+                let len = val.to_u128()?;\n+                assert_eq!(len as u64 as u128, len);\n+                Ok((ptr.into(), len as u64))\n+            }\n+            ByVal(PrimVal::Undef) => err!(ReadUndefBytes),\n+            ByVal(_) => bug!(\"expected ptr and length, got {:?}\", self),\n+        }\n+    }\n+}\n+\n+impl<'tcx> PrimVal {\n+    pub fn from_u128(n: u128) -> Self {\n+        PrimVal::Bytes(n)\n+    }\n+\n+    pub fn from_i128(n: i128) -> Self {\n+        PrimVal::Bytes(n as u128)\n+    }\n+\n+    pub fn from_f32(f: f32) -> Self {\n+        PrimVal::Bytes(f32_to_bytes(f))\n+    }\n+\n+    pub fn from_f64(f: f64) -> Self {\n+        PrimVal::Bytes(f64_to_bytes(f))\n+    }\n+\n+    pub fn from_bool(b: bool) -> Self {\n+        PrimVal::Bytes(b as u128)\n+    }\n+\n+    pub fn from_char(c: char) -> Self {\n+        PrimVal::Bytes(c as u128)\n+    }\n+\n+    pub fn to_bytes(self) -> EvalResult<'tcx, u128> {\n+        match self {\n+            PrimVal::Bytes(b) => Ok(b),\n+            PrimVal::Ptr(_) => err!(ReadPointerAsBytes),\n+            PrimVal::Undef => err!(ReadUndefBytes),\n+        }\n+    }\n+\n+    pub fn to_ptr(self) -> EvalResult<'tcx, MemoryPointer> {\n+        match self {\n+            PrimVal::Bytes(_) => err!(ReadBytesAsPointer),\n+            PrimVal::Ptr(p) => Ok(p),\n+            PrimVal::Undef => err!(ReadUndefBytes),\n+        }\n+    }\n+\n+    pub fn is_bytes(self) -> bool {\n+        match self {\n+            PrimVal::Bytes(_) => true,\n+            _ => false,\n+        }\n+    }\n+\n+    pub fn is_ptr(self) -> bool {\n+        match self {\n+            PrimVal::Ptr(_) => true,\n+            _ => false,\n+        }\n+    }\n+\n+    pub fn is_undef(self) -> bool {\n+        match self {\n+            PrimVal::Undef => true,\n+            _ => false,\n+        }\n+    }\n+\n+    pub fn to_u128(self) -> EvalResult<'tcx, u128> {\n+        self.to_bytes()\n+    }\n+\n+    pub fn to_u64(self) -> EvalResult<'tcx, u64> {\n+        self.to_bytes().map(|b| {\n+            assert_eq!(b as u64 as u128, b);\n+            b as u64\n+        })\n+    }\n+\n+    pub fn to_i32(self) -> EvalResult<'tcx, i32> {\n+        self.to_bytes().map(|b| {\n+            assert_eq!(b as i32 as u128, b);\n+            b as i32\n+        })\n+    }\n+\n+    pub fn to_i128(self) -> EvalResult<'tcx, i128> {\n+        self.to_bytes().map(|b| b as i128)\n+    }\n+\n+    pub fn to_i64(self) -> EvalResult<'tcx, i64> {\n+        self.to_bytes().map(|b| {\n+            assert_eq!(b as i64 as u128, b);\n+            b as i64\n+        })\n+    }\n+\n+    pub fn to_f32(self) -> EvalResult<'tcx, f32> {\n+        self.to_bytes().map(bytes_to_f32)\n+    }\n+\n+    pub fn to_f64(self) -> EvalResult<'tcx, f64> {\n+        self.to_bytes().map(bytes_to_f64)\n+    }\n+\n+    pub fn to_bool(self) -> EvalResult<'tcx, bool> {\n+        match self.to_bytes()? {\n+            0 => Ok(false),\n+            1 => Ok(true),\n+            _ => err!(InvalidBool),\n+        }\n+    }\n+}\n+\n+impl PrimValKind {\n+    pub fn is_int(self) -> bool {\n+        use self::PrimValKind::*;\n+        match self {\n+            I8 | I16 | I32 | I64 | I128 | U8 | U16 | U32 | U64 | U128 => true,\n+            _ => false,\n+        }\n+    }\n+\n+    pub fn is_signed_int(self) -> bool {\n+        use self::PrimValKind::*;\n+        match self {\n+            I8 | I16 | I32 | I64 | I128 => true,\n+            _ => false,\n+        }\n+    }\n+\n+    pub fn is_float(self) -> bool {\n+        use self::PrimValKind::*;\n+        match self {\n+            F32 | F64 => true,\n+            _ => false,\n+        }\n+    }\n+\n+    pub fn from_uint_size(size: u64) -> Self {\n+        match size {\n+            1 => PrimValKind::U8,\n+            2 => PrimValKind::U16,\n+            4 => PrimValKind::U32,\n+            8 => PrimValKind::U64,\n+            16 => PrimValKind::U128,\n+            _ => bug!(\"can't make uint with size {}\", size),\n+        }\n+    }\n+\n+    pub fn from_int_size(size: u64) -> Self {\n+        match size {\n+            1 => PrimValKind::I8,\n+            2 => PrimValKind::I16,\n+            4 => PrimValKind::I32,\n+            8 => PrimValKind::I64,\n+            16 => PrimValKind::I128,\n+            _ => bug!(\"can't make int with size {}\", size),\n+        }\n+    }\n+\n+    pub fn is_ptr(self) -> bool {\n+        use self::PrimValKind::*;\n+        match self {\n+            Ptr | FnPtr => true,\n+            _ => false,\n+        }\n+    }\n+}"}]}