{"sha": "b0e121d9d588b334eaa1b68a127f5ee0fcda4296", "node_id": "MDY6Q29tbWl0NzI0NzEyOmIwZTEyMWQ5ZDU4OGIzMzRlYWExYjY4YTEyN2Y1ZWUwZmNkYTQyOTY=", "commit": {"author": {"name": "Mark Rousskov", "email": "mark.simulacrum@gmail.com", "date": "2020-03-21T14:16:01Z"}, "committer": {"name": "Mark Rousskov", "email": "mark.simulacrum@gmail.com", "date": "2020-03-21T15:22:00Z"}, "message": "Shrink bitset words through functional mapping\n\nPreviously, all words in the (deduplicated) bitset would be stored raw -- a full\n64 bits (8 bytes). Now, those words that are equivalent to others through a\nspecific mapping are stored separately and \"mapped\" to the original when\nloading; this shrinks the table sizes significantly, as each mapped word is\nstored in 2 bytes (a 4x decrease from the previous).\n\nThe new encoding is also potentially non-optimal: the \"mapped\" byte is\nfrequently repeated, as in practice many mapped words use the same base word.\n\nCurrently we only support two forms of mapping: rotation and inversion. Note\nthat these are both guaranteed to map transitively if at all, and supporting\nmappings for which this is not true may require a more interesting algorithm for\nchoosing the optimal pairing.\n\nUpdated sizes:\n\nAlphabetic     : 2622 bytes     (-  414 bytes)\nCase_Ignorable : 1803 bytes     (-  330 bytes)\nCased          : 808 bytes      (-  126 bytes)\nCc             : 32 bytes\nGrapheme_Extend: 1508 bytes     (-  252 bytes)\nLowercase      : 901 bytes      (-   84 bytes)\nN              : 1064 bytes     (-  156 bytes)\nUppercase      : 838 bytes      (-   96 bytes)\nWhite_Space    : 91 bytes       (-    6 bytes)\nTotal table sizes: 9667 bytes   (-1,464 bytes)", "tree": {"sha": "d3bc693f5f4e5d894ccfcf1173052c381b8ff0f5", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/d3bc693f5f4e5d894ccfcf1173052c381b8ff0f5"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b0e121d9d588b334eaa1b68a127f5ee0fcda4296", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b0e121d9d588b334eaa1b68a127f5ee0fcda4296", "html_url": "https://github.com/rust-lang/rust/commit/b0e121d9d588b334eaa1b68a127f5ee0fcda4296", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/comments", "author": {"login": "Mark-Simulacrum", "id": 5047365, "node_id": "MDQ6VXNlcjUwNDczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/5047365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mark-Simulacrum", "html_url": "https://github.com/Mark-Simulacrum", "followers_url": "https://api.github.com/users/Mark-Simulacrum/followers", "following_url": "https://api.github.com/users/Mark-Simulacrum/following{/other_user}", "gists_url": "https://api.github.com/users/Mark-Simulacrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mark-Simulacrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mark-Simulacrum/subscriptions", "organizations_url": "https://api.github.com/users/Mark-Simulacrum/orgs", "repos_url": "https://api.github.com/users/Mark-Simulacrum/repos", "events_url": "https://api.github.com/users/Mark-Simulacrum/events{/privacy}", "received_events_url": "https://api.github.com/users/Mark-Simulacrum/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Mark-Simulacrum", "id": 5047365, "node_id": "MDQ6VXNlcjUwNDczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/5047365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mark-Simulacrum", "html_url": "https://github.com/Mark-Simulacrum", "followers_url": "https://api.github.com/users/Mark-Simulacrum/followers", "following_url": "https://api.github.com/users/Mark-Simulacrum/following{/other_user}", "gists_url": "https://api.github.com/users/Mark-Simulacrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mark-Simulacrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mark-Simulacrum/subscriptions", "organizations_url": "https://api.github.com/users/Mark-Simulacrum/orgs", "repos_url": "https://api.github.com/users/Mark-Simulacrum/repos", "events_url": "https://api.github.com/users/Mark-Simulacrum/events{/privacy}", "received_events_url": "https://api.github.com/users/Mark-Simulacrum/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "6c7691a37bf485b28fecb6856e6ede8fa952f99e", "url": "https://api.github.com/repos/rust-lang/rust/commits/6c7691a37bf485b28fecb6856e6ede8fa952f99e", "html_url": "https://github.com/rust-lang/rust/commit/6c7691a37bf485b28fecb6856e6ede8fa952f99e"}], "stats": {"total": 1643, "additions": 1211, "deletions": 432}, "files": [{"sha": "2a41685a4809683dcc2262fef7a79049927bf73b", "filename": "src/libcore/unicode/mod.rs", "status": "modified", "additions": 24, "deletions": 4, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/src%2Flibcore%2Funicode%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/src%2Flibcore%2Funicode%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Fmod.rs?ref=b0e121d9d588b334eaa1b68a127f5ee0fcda4296", "patch": "@@ -34,12 +34,19 @@ pub use unicode_data::uppercase::lookup as Uppercase;\n pub use unicode_data::white_space::lookup as White_Space;\n \n #[inline(always)]\n-fn range_search<const N: usize, const CHUNK_SIZE: usize, const N1: usize, const N2: usize>(\n+fn range_search<\n+    const N: usize,\n+    const CHUNK_SIZE: usize,\n+    const N1: usize,\n+    const CANONICAL: usize,\n+    const CANONICALIZED: usize,\n+>(\n     needle: u32,\n     chunk_idx_map: &[u8; N],\n     (last_chunk_idx, last_chunk_mapping): (u16, u8),\n     bitset_chunk_idx: &[[u8; CHUNK_SIZE]; N1],\n-    bitset: &[u64; N2],\n+    bitset_canonical: &[u64; CANONICAL],\n+    bitset_canonicalized: &[(u8, u8); CANONICALIZED],\n ) -> bool {\n     let bucket_idx = (needle / 64) as usize;\n     let chunk_map_idx = bucket_idx / CHUNK_SIZE;\n@@ -53,7 +60,20 @@ fn range_search<const N: usize, const CHUNK_SIZE: usize, const N1: usize, const\n     } else {\n         chunk_idx_map[chunk_map_idx]\n     };\n-    let idx = bitset_chunk_idx[(chunk_idx as usize)][chunk_piece];\n-    let word = bitset[(idx as usize)];\n+    let idx = bitset_chunk_idx[(chunk_idx as usize)][chunk_piece] as usize;\n+    let word = if idx < CANONICAL {\n+        bitset_canonical[idx]\n+    } else {\n+        let (real_idx, mapping) = bitset_canonicalized[idx - CANONICAL];\n+        let mut word = bitset_canonical[real_idx as usize];\n+        let should_invert = mapping & (1 << 7) != 0;\n+        if should_invert {\n+            word = !word;\n+        }\n+        // Unset the inversion bit\n+        let rotate_by = mapping & !(1 << 7);\n+        word = word.rotate_left(rotate_by as u32);\n+        word\n+    };\n     (word & (1 << (needle % 64) as u64)) != 0\n }"}, {"sha": "555a0437f7bcb8a8826ef5b20f990200ba4e8ca9", "filename": "src/libcore/unicode/unicode_data.rs", "status": "modified", "additions": 938, "deletions": 409, "changes": 1347, "blob_url": "https://github.com/rust-lang/rust/blob/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/src%2Flibcore%2Funicode%2Funicode_data.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/src%2Flibcore%2Funicode%2Funicode_data.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Funicode_data.rs?ref=b0e121d9d588b334eaa1b68a127f5ee0fcda4296"}, {"sha": "5e8865fc9e3b5691120025b6c9604e1c5a1029a9", "filename": "src/tools/unicode-table-generator/src/main.rs", "status": "modified", "additions": 24, "deletions": 4, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fmain.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fmain.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fmain.rs?ref=b0e121d9d588b334eaa1b68a127f5ee0fcda4296", "patch": "@@ -254,12 +254,19 @@ fn generate_tests(data_path: &str, ranges: &[(&str, Vec<Range<u32>>)]) -> String\n     s.push_str(\n         \"\n #[inline(always)]\n-fn range_search<const N: usize, const CHUNK_SIZE: usize, const N1: usize, const N2: usize>(\n+fn range_search<\n+    const N: usize,\n+    const CHUNK_SIZE: usize,\n+    const N1: usize,\n+    const CANONICAL: usize,\n+    const CANONICALIZED: usize,\n+>(\n     needle: u32,\n     chunk_idx_map: &[u8; N],\n     (last_chunk_idx, last_chunk_mapping): (u16, u8),\n     bitset_chunk_idx: &[[u8; CHUNK_SIZE]; N1],\n-    bitset: &[u64; N2],\n+    bitset_canonical: &[u64; CANONICAL],\n+    bitset_canonicalized: &[(u8, u8); CANONICALIZED],\n ) -> bool {\n     let bucket_idx = (needle / 64) as usize;\n     let chunk_map_idx = bucket_idx / CHUNK_SIZE;\n@@ -273,8 +280,21 @@ fn range_search<const N: usize, const CHUNK_SIZE: usize, const N1: usize, const\n     } else {\n         chunk_idx_map[chunk_map_idx]\n     };\n-    let idx = bitset_chunk_idx[(chunk_idx as usize)][chunk_piece];\n-    let word = bitset[(idx as usize)];\n+    let idx = bitset_chunk_idx[(chunk_idx as usize)][chunk_piece] as usize;\n+    let word = if idx < CANONICAL {\n+        bitset_canonical[idx]\n+    } else {\n+        let (real_idx, mapping) = bitset_canonicalized[idx - CANONICAL];\n+        let mut word = bitset_canonical[real_idx as usize];\n+        let should_invert = mapping & (1 << 7) != 0;\n+        if should_invert {\n+            word = !word;\n+        }\n+        // Unset the inversion bit\n+        let rotate_by = mapping & !(1 << 7);\n+        word = word.rotate_left(rotate_by as u32);\n+        word\n+    };\n     (word & (1 << (needle % 64) as u64)) != 0\n }\n     \","}, {"sha": "38b36c34042287168a6936faf9ae918d5c7fd390", "filename": "src/tools/unicode-table-generator/src/raw_emitter.rs", "status": "modified", "additions": 225, "deletions": 15, "changes": 240, "blob_url": "https://github.com/rust-lang/rust/blob/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fraw_emitter.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b0e121d9d588b334eaa1b68a127f5ee0fcda4296/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fraw_emitter.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fraw_emitter.rs?ref=b0e121d9d588b334eaa1b68a127f5ee0fcda4296", "patch": "@@ -22,8 +22,9 @@\n //! mapping into two separate sets; currently this is not dealt with).\n //!\n //! With that scheme, we now have a single byte for every 64 codepoints. We\n-//! further group these by 16 (arbitrarily chosen), and again deduplicate and\n-//! store in an array (u8 -> [u8; 16]).\n+//! further group these by some constant N (between 1 and 64 per group), and\n+//! again deduplicate and store in an array (u8 -> [u8; N]). The constant is\n+//! chosen to be optimal in bytes-in-memory for the given dataset.\n //!\n //! The indices into this array represent ranges of 64*16 = 1024 codepoints.\n //!\n@@ -37,9 +38,9 @@\n //! down considerably.\n \n use crate::fmt_list;\n-use std::collections::{BTreeSet, HashMap};\n+use std::collections::{BTreeMap, BTreeSet, HashMap};\n use std::convert::TryFrom;\n-use std::fmt::Write;\n+use std::fmt::{self, Write};\n use std::ops::Range;\n \n #[derive(Clone)]\n@@ -61,20 +62,20 @@ impl RawEmitter {\n     }\n \n     fn emit_bitset(&mut self, words: &[u64]) {\n+        let mut words = words.to_vec();\n+        // Ensure that there's a zero word in the dataset, used for padding and\n+        // such.\n+        words.push(0);\n         let unique_words =\n             words.iter().cloned().collect::<BTreeSet<_>>().into_iter().collect::<Vec<_>>();\n         if unique_words.len() > u8::max_value() as usize {\n             panic!(\"cannot pack {} into 8 bits\", unique_words.len());\n         }\n         // needed for the chunk mapping to work\n         assert_eq!(unique_words[0], 0, \"has a zero word\");\n+        let canonicalized = Canonicalized::canonicalize(&unique_words);\n \n-        let word_indices = unique_words\n-            .iter()\n-            .cloned()\n-            .enumerate()\n-            .map(|(idx, word)| (word, u8::try_from(idx).unwrap()))\n-            .collect::<HashMap<_, _>>();\n+        let word_indices = canonicalized.unique_mapping.clone();\n         let compressed_words = words.iter().map(|w| word_indices[w]).collect::<Vec<u8>>();\n \n         let mut best = None;\n@@ -91,14 +92,32 @@ impl RawEmitter {\n         }\n         self.emit_chunk_map(word_indices[&0], &compressed_words, best.unwrap().0);\n \n+        struct Bits(u64);\n+        impl fmt::Debug for Bits {\n+            fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                write!(f, \"0b{:064b}\", self.0)\n+            }\n+        }\n+\n+        writeln!(\n+            &mut self.file,\n+            \"static BITSET_CANONICAL: [u64; {}] = [{}];\",\n+            canonicalized.canonical_words.len(),\n+            fmt_list(canonicalized.canonical_words.iter().map(|v| Bits(*v))),\n+        )\n+        .unwrap();\n+        self.bytes_used += 8 * canonicalized.canonical_words.len();\n         writeln!(\n             &mut self.file,\n-            \"static BITSET: [u64; {}] = [{}];\",\n-            unique_words.len(),\n-            fmt_list(&unique_words),\n+            \"static BITSET_MAPPING: [(u8, u8); {}] = [{}];\",\n+            canonicalized.canonicalized_words.len(),\n+            fmt_list(&canonicalized.canonicalized_words),\n         )\n         .unwrap();\n-        self.bytes_used += 8 * unique_words.len();\n+        // 8 bit index into shifted words, 7 bits for shift + optional flip\n+        // We only need it for the words that we removed by applying a shift and\n+        // flip to them.\n+        self.bytes_used += 2 * canonicalized.canonicalized_words.len();\n     }\n \n     fn emit_chunk_map(&mut self, zero_at: u8, compressed_words: &[u8], chunk_length: usize) {\n@@ -170,7 +189,8 @@ impl RawEmitter {\n         writeln!(&mut self.file, \"        &BITSET_CHUNKS_MAP,\").unwrap();\n         writeln!(&mut self.file, \"        BITSET_LAST_CHUNK_MAP,\").unwrap();\n         writeln!(&mut self.file, \"        &BITSET_INDEX_CHUNKS,\").unwrap();\n-        writeln!(&mut self.file, \"        &BITSET,\").unwrap();\n+        writeln!(&mut self.file, \"        &BITSET_CANONICAL,\").unwrap();\n+        writeln!(&mut self.file, \"        &BITSET_MAPPING,\").unwrap();\n         writeln!(&mut self.file, \"    )\").unwrap();\n         writeln!(&mut self.file, \"}}\").unwrap();\n     }\n@@ -196,3 +216,193 @@ pub fn emit_codepoints(emitter: &mut RawEmitter, ranges: &[Range<u32>]) {\n     emitter.blank_line();\n     emitter.emit_lookup();\n }\n+\n+struct Canonicalized {\n+    canonical_words: Vec<u64>,\n+    canonicalized_words: Vec<(u8, u8)>,\n+\n+    /// Maps an input unique word to the associated index (u8) which is into\n+    /// canonical_words or canonicalized_words (in order).\n+    unique_mapping: HashMap<u64, u8>,\n+}\n+\n+impl Canonicalized {\n+    fn canonicalize(unique_words: &[u64]) -> Self {\n+        #[derive(Copy, Clone, Debug)]\n+        enum Mapping {\n+            Rotate(u32),\n+            Invert,\n+            RotateAndInvert(u32),\n+        }\n+\n+        // key is the word being mapped to\n+        let mut mappings: BTreeMap<u64, Vec<(u64, Mapping)>> = BTreeMap::new();\n+        for &a in unique_words {\n+            'b: for &b in unique_words {\n+                // skip self\n+                if a == b {\n+                    continue;\n+                }\n+\n+                // All possible distinct rotations\n+                for rotation in 1..64 {\n+                    if a.rotate_right(rotation) == b {\n+                        mappings.entry(b).or_default().push((a, Mapping::Rotate(rotation)));\n+                        // We're not interested in further mappings between a and b\n+                        continue 'b;\n+                    }\n+                }\n+\n+                if (!a) == b {\n+                    mappings.entry(b).or_default().push((a, Mapping::Invert));\n+                    // We're not interested in further mappings between a and b\n+                    continue 'b;\n+                }\n+\n+                // All possible distinct rotations, inverted\n+                for rotation in 1..64 {\n+                    if (!a.rotate_right(rotation)) == b {\n+                        mappings\n+                            .entry(b)\n+                            .or_default()\n+                            .push((a, Mapping::RotateAndInvert(rotation)));\n+                        // We're not interested in further mappings between a and b\n+                        continue 'b;\n+                    }\n+                }\n+            }\n+        }\n+        // These are the bitset words which will be represented \"raw\" (as a u64)\n+        let mut canonical_words = Vec::new();\n+        // These are mapped words, which will be represented by an index into\n+        // the canonical_words and a Mapping; u16 when encoded.\n+        let mut canonicalized_words = Vec::new();\n+        let mut unique_mapping = HashMap::new();\n+\n+        #[derive(Debug, PartialEq, Eq)]\n+        enum UniqueMapping {\n+            Canonical(usize),\n+            Canonicalized(usize),\n+        }\n+\n+        while let Some((&to, _)) = mappings.iter().max_by_key(|m| m.1.len()) {\n+            // Get the mapping with the most entries. Currently, no mapping can\n+            // only exist transitively (i.e., there is no A, B, C such that A\n+            // does not map to C and but A maps to B maps to C), so this is\n+            // guaranteed to be acceptable.\n+            //\n+            // In the future, we may need a more sophisticated algorithm to\n+            // identify which keys to prefer as canonical.\n+            let mapped_from = mappings.remove(&to).unwrap();\n+            for (from, how) in &mapped_from {\n+                // Remove the entries which mapped to this one.\n+                // Noting that it should be associated with the Nth canonical word.\n+                //\n+                // We do not assert that this is present, because there may be\n+                // no mappings to the `from` word; that's fine.\n+                mappings.remove(from);\n+                assert_eq!(\n+                    unique_mapping\n+                        .insert(*from, UniqueMapping::Canonicalized(canonicalized_words.len())),\n+                    None\n+                );\n+                canonicalized_words.push((canonical_words.len(), *how));\n+\n+                // Remove the now-canonicalized word from other mappings,\n+                // to ensure that we deprioritize them in the next iteration of\n+                // the while loop.\n+                for (_, mapped) in &mut mappings {\n+                    let mut i = 0;\n+                    while i != mapped.len() {\n+                        if mapped[i].0 == *from {\n+                            mapped.remove(i);\n+                        } else {\n+                            i += 1;\n+                        }\n+                    }\n+                }\n+            }\n+            assert!(\n+                unique_mapping\n+                    .insert(to, UniqueMapping::Canonical(canonical_words.len()))\n+                    .is_none()\n+            );\n+            canonical_words.push(to);\n+\n+            // Remove the now-canonical word from other mappings, to ensure that\n+            // we deprioritize them in the next iteration of the while loop.\n+            for (_, mapped) in &mut mappings {\n+                let mut i = 0;\n+                while i != mapped.len() {\n+                    if mapped[i].0 == to {\n+                        mapped.remove(i);\n+                    } else {\n+                        i += 1;\n+                    }\n+                }\n+            }\n+        }\n+\n+        // Any words which we couldn't shrink, just stick into the canonical\n+        // words.\n+        //\n+        // FIXME: work harder -- there are more possibilities for mapping\n+        // functions (e.g., multiplication, shifting instead of rotation, etc.)\n+        // We'll probably always have some slack though so this loop will still\n+        // be needed.\n+        for &w in unique_words {\n+            if !unique_mapping.contains_key(&w) {\n+                assert!(\n+                    unique_mapping\n+                        .insert(w, UniqueMapping::Canonical(canonical_words.len()))\n+                        .is_none()\n+                );\n+                canonical_words.push(w);\n+            }\n+        }\n+        assert_eq!(canonicalized_words.len() + canonical_words.len(), unique_words.len());\n+        assert_eq!(unique_mapping.len(), unique_words.len());\n+\n+        let unique_mapping = unique_mapping\n+            .into_iter()\n+            .map(|(key, value)| {\n+                (\n+                    key,\n+                    match value {\n+                        UniqueMapping::Canonicalized(idx) => {\n+                            u8::try_from(canonical_words.len() + idx).unwrap()\n+                        }\n+                        UniqueMapping::Canonical(idx) => u8::try_from(idx).unwrap(),\n+                    },\n+                )\n+            })\n+            .collect::<HashMap<_, _>>();\n+\n+        let mut distinct_indices = BTreeSet::new();\n+        for &w in unique_words {\n+            let idx = unique_mapping.get(&w).unwrap();\n+            assert!(distinct_indices.insert(idx));\n+        }\n+\n+        let canonicalized_words = canonicalized_words\n+            .into_iter()\n+            .map(|v| {\n+                (\n+                    u8::try_from(v.0).unwrap(),\n+                    match v.1 {\n+                        Mapping::RotateAndInvert(amount) => {\n+                            assert!(amount < (1 << 7));\n+                            1 << 7 | (amount as u8)\n+                        }\n+                        Mapping::Rotate(amount) => {\n+                            assert!(amount < (1 << 7));\n+                            amount as u8\n+                        }\n+                        Mapping::Invert => 1 << 7,\n+                    },\n+                )\n+            })\n+            .collect::<Vec<(u8, u8)>>();\n+        Canonicalized { unique_mapping, canonical_words, canonicalized_words }\n+    }\n+}"}]}