{"sha": "cfad9d12f32634160e5aaecc41fb4097658b2bb2", "node_id": "C_kwDOAAsO6NoAKGNmYWQ5ZDEyZjMyNjM0MTYwZTVhYWVjYzQxZmI0MDk3NjU4YjJiYjI", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-07-03T14:39:22Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-07-03T14:39:22Z"}, "message": "Auto merge of #1935 - saethlin:optimize-sb, r=RalfJung\n\nOptimizing Stacked Borrows (part 1?): Cache locations of Tags in a Borrow Stack\n\nBefore this PR, a profile of Miri under almost any workload points quite squarely at these regions of code as being incredibly hot (each being ~40% of cycles):\n\nhttps://github.com/rust-lang/miri/blob/dadcbebfbd017aac2358cf652a4bd71a91694edc/src/stacked_borrows.rs#L259-L269\n\nhttps://github.com/rust-lang/miri/blob/dadcbebfbd017aac2358cf652a4bd71a91694edc/src/stacked_borrows.rs#L362-L369\n\nThis code is one of at least three reasons that stacked borrows analysis is super-linear: These are both linear in the number of borrows in the stack and they are positioned along the most commonly-taken paths.\n\nI'm addressing the first loop (which is in `Stack::find_granting`) by adding a very very simple sort of LRU cache implemented on a `VecDeque`, which maps recently-looked-up tags to their position in the stack. For `Untagged` access we fall back to the same sort of linear search. But as far as I can tell there are never enough `Untagged` items to be significant.\n\nI'm addressing the second loop by keeping track of the region of stack where there could be items granting `Permission::Unique`. This optimization is incredibly effective because `Read` access tends to dominate and many trips through this code path now skip the loop entirely.\n\nThese optimizations result in pretty enormous improvements:\nWithout raw pointer tagging, `mse` 34.5s -> 2.4s, `serde1` 5.6s -> 3.6s\nWith raw pointer tagging, `mse` 35.3s -> 2.4s, `serde1` 5.7s -> 3.6s\n\nAnd there is hardly any impact on memory usage:\nMemory usage on `mse` 844 MB -> 848 MB, `serde1` 184 MB -> 184 MB (jitter on these is a few MB).", "tree": {"sha": "21d491231672af0c4028240a8dd6947715fc297e", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/21d491231672af0c4028240a8dd6947715fc297e"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/cfad9d12f32634160e5aaecc41fb4097658b2bb2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/cfad9d12f32634160e5aaecc41fb4097658b2bb2", "html_url": "https://github.com/rust-lang/rust/commit/cfad9d12f32634160e5aaecc41fb4097658b2bb2", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/cfad9d12f32634160e5aaecc41fb4097658b2bb2/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "5382f463a653c42eb37bfa05d9f59d6142da2d26", "url": "https://api.github.com/repos/rust-lang/rust/commits/5382f463a653c42eb37bfa05d9f59d6142da2d26", "html_url": "https://github.com/rust-lang/rust/commit/5382f463a653c42eb37bfa05d9f59d6142da2d26"}, {"sha": "b004a03bdb6affd7b07322114967bd1693208a69", "url": "https://api.github.com/repos/rust-lang/rust/commits/b004a03bdb6affd7b07322114967bd1693208a69", "html_url": "https://github.com/rust-lang/rust/commit/b004a03bdb6affd7b07322114967bd1693208a69"}], "stats": {"total": 529, "additions": 421, "deletions": 108}, "files": [{"sha": "f612e7a3e9e50d58df1baef9ffc4c86cebc52b4f", "filename": "Cargo.toml", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cfad9d12f32634160e5aaecc41fb4097658b2bb2/Cargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/cfad9d12f32634160e5aaecc41fb4097658b2bb2/Cargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.toml?ref=cfad9d12f32634160e5aaecc41fb4097658b2bb2", "patch": "@@ -50,3 +50,9 @@ rustc_private = true\n [[test]]\n name = \"compiletest\"\n harness = false\n+\n+[features]\n+default = [\"stack-cache\"]\n+# Will be enabled on CI via `--all-features`.\n+expensive-debug-assertions = []\n+stack-cache = []"}, {"sha": "f3e6e0eef70a3481771acb84f570b700073fed40", "filename": "src/lib.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/cfad9d12f32634160e5aaecc41fb4097658b2bb2/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cfad9d12f32634160e5aaecc41fb4097658b2bb2/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=cfad9d12f32634160e5aaecc41fb4097658b2bb2", "patch": "@@ -90,8 +90,8 @@ pub use crate::mono_hash_map::MonoHashMap;\n pub use crate::operator::EvalContextExt as OperatorEvalContextExt;\n pub use crate::range_map::RangeMap;\n pub use crate::stacked_borrows::{\n-    CallId, EvalContextExt as StackedBorEvalContextExt, Item, Permission, SbTag, SbTagExtra, Stack,\n-    Stacks,\n+    stack::Stack, CallId, EvalContextExt as StackedBorEvalContextExt, Item, Permission, SbTag,\n+    SbTagExtra, Stacks,\n };\n pub use crate::sync::{CondvarId, EvalContextExt as SyncEvalContextExt, MutexId, RwLockId};\n pub use crate::thread::{"}, {"sha": "3fa001eabfd27dc35fea67f3d9465bdb38889735", "filename": "src/stacked_borrows.rs", "status": "modified", "additions": 30, "deletions": 105, "changes": 135, "blob_url": "https://github.com/rust-lang/rust/blob/cfad9d12f32634160e5aaecc41fb4097658b2bb2/src%2Fstacked_borrows.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cfad9d12f32634160e5aaecc41fb4097658b2bb2/src%2Fstacked_borrows.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fstacked_borrows.rs?ref=cfad9d12f32634160e5aaecc41fb4097658b2bb2", "patch": "@@ -23,6 +23,9 @@ use crate::*;\n pub mod diagnostics;\n use diagnostics::{AllocHistory, TagHistory};\n \n+pub mod stack;\n+use stack::Stack;\n+\n pub type CallId = NonZeroU64;\n \n // Even reading memory can have effects on the stack, so we need a `RefCell` here.\n@@ -111,23 +114,6 @@ impl fmt::Debug for Item {\n     }\n }\n \n-/// Extra per-location state.\n-#[derive(Clone, Debug, PartialEq, Eq)]\n-pub struct Stack {\n-    /// Used *mostly* as a stack; never empty.\n-    /// Invariants:\n-    /// * Above a `SharedReadOnly` there can only be more `SharedReadOnly`.\n-    /// * No tag occurs in the stack more than once.\n-    borrows: Vec<Item>,\n-    /// If this is `Some(id)`, then the actual current stack is unknown. This can happen when\n-    /// wildcard pointers are used to access this location. What we do know is that `borrows` are at\n-    /// the top of the stack, and below it are arbitrarily many items whose `tag` is strictly less\n-    /// than `id`.\n-    /// When the bottom is unknown, `borrows` always has a `SharedReadOnly` or `Unique` at the bottom;\n-    /// we never have the unknown-to-known boundary in an SRW group.\n-    unknown_bottom: Option<SbTag>,\n-}\n-\n /// Extra per-allocation state.\n #[derive(Clone, Debug)]\n pub struct Stacks {\n@@ -298,65 +284,10 @@ impl Permission {\n \n /// Core per-location operations: access, dealloc, reborrow.\n impl<'tcx> Stack {\n-    /// Find the item granting the given kind of access to the given tag, and return where\n-    /// it is on the stack. For wildcard tags, the given index is approximate, but if *no*\n-    /// index is given it means the match was *not* in the known part of the stack.\n-    /// `Ok(None)` indicates it matched the \"unknown\" part of the stack.\n-    /// `Err` indicates it was not found.\n-    fn find_granting(\n-        &self,\n-        access: AccessKind,\n-        tag: SbTagExtra,\n-        exposed_tags: &FxHashSet<SbTag>,\n-    ) -> Result<Option<usize>, ()> {\n-        let SbTagExtra::Concrete(tag) = tag else {\n-            // Handle the wildcard case.\n-            // Go search the stack for an exposed tag.\n-            if let Some(idx) =\n-                self.borrows\n-                    .iter()\n-                    .enumerate() // we also need to know *where* in the stack\n-                    .rev() // search top-to-bottom\n-                    .find_map(|(idx, item)| {\n-                        // If the item fits and *might* be this wildcard, use it.\n-                        if item.perm.grants(access) && exposed_tags.contains(&item.tag) {\n-                            Some(idx)\n-                        } else {\n-                            None\n-                        }\n-                    })\n-            {\n-                return Ok(Some(idx));\n-            }\n-            // If we couldn't find it in the stack, check the unknown bottom.\n-            return if self.unknown_bottom.is_some() { Ok(None) } else { Err(()) };\n-        };\n-\n-        if let Some(idx) =\n-            self.borrows\n-                .iter()\n-                .enumerate() // we also need to know *where* in the stack\n-                .rev() // search top-to-bottom\n-                // Return permission of first item that grants access.\n-                // We require a permission with the right tag, ensuring U3 and F3.\n-                .find_map(|(idx, item)| {\n-                    if tag == item.tag && item.perm.grants(access) { Some(idx) } else { None }\n-                })\n-        {\n-            return Ok(Some(idx));\n-        }\n-\n-        // Couldn't find it in the stack; but if there is an unknown bottom it might be there.\n-        let found = self.unknown_bottom.is_some_and(|&unknown_limit| {\n-            tag.0 < unknown_limit.0 // unknown_limit is an upper bound for what can be in the unknown bottom.\n-        });\n-        if found { Ok(None) } else { Err(()) }\n-    }\n-\n     /// Find the first write-incompatible item above the given one --\n     /// i.e, find the height to which the stack will be truncated when writing to `granting`.\n     fn find_first_write_incompatible(&self, granting: usize) -> usize {\n-        let perm = self.borrows[granting].perm;\n+        let perm = self.get(granting).unwrap().perm;\n         match perm {\n             Permission::SharedReadOnly => bug!(\"Cannot use SharedReadOnly for writing\"),\n             Permission::Disabled => bug!(\"Cannot use Disabled for anything\"),\n@@ -367,7 +298,7 @@ impl<'tcx> Stack {\n             Permission::SharedReadWrite => {\n                 // The SharedReadWrite *just* above us are compatible, to skip those.\n                 let mut idx = granting + 1;\n-                while let Some(item) = self.borrows.get(idx) {\n+                while let Some(item) = self.get(idx) {\n                     if item.perm == Permission::SharedReadWrite {\n                         // Go on.\n                         idx += 1;\n@@ -462,16 +393,16 @@ impl<'tcx> Stack {\n                 // There is a SRW group boundary between the unknown and the known, so everything is incompatible.\n                 0\n             };\n-            for item in self.borrows.drain(first_incompatible_idx..).rev() {\n-                trace!(\"access: popping item {:?}\", item);\n+            self.pop_items_after(first_incompatible_idx, |item| {\n                 Stack::item_popped(\n                     &item,\n                     Some((tag, alloc_range, offset, access)),\n                     global,\n                     alloc_history,\n                 )?;\n                 alloc_history.log_invalidation(item.tag, alloc_range, current_span);\n-            }\n+                Ok(())\n+            })?;\n         } else {\n             // On a read, *disable* all `Unique` above the granting item.  This ensures U2 for read accesses.\n             // The reason this is not following the stack discipline (by removing the first Unique and\n@@ -488,44 +419,39 @@ impl<'tcx> Stack {\n                 // We are reading from something in the unknown part. That means *all* `Unique` we know about are dead now.\n                 0\n             };\n-            for idx in (first_incompatible_idx..self.borrows.len()).rev() {\n-                let item = &mut self.borrows[idx];\n-\n-                if item.perm == Permission::Unique {\n-                    trace!(\"access: disabling item {:?}\", item);\n-                    Stack::item_popped(\n-                        item,\n-                        Some((tag, alloc_range, offset, access)),\n-                        global,\n-                        alloc_history,\n-                    )?;\n-                    item.perm = Permission::Disabled;\n-                    alloc_history.log_invalidation(item.tag, alloc_range, current_span);\n-                }\n-            }\n+            self.disable_uniques_starting_at(first_incompatible_idx, |item| {\n+                Stack::item_popped(\n+                    &item,\n+                    Some((tag, alloc_range, offset, access)),\n+                    global,\n+                    alloc_history,\n+                )?;\n+                alloc_history.log_invalidation(item.tag, alloc_range, current_span);\n+                Ok(())\n+            })?;\n         }\n \n         // If this was an approximate action, we now collapse everything into an unknown.\n         if granting_idx.is_none() || matches!(tag, SbTagExtra::Wildcard) {\n             // Compute the upper bound of the items that remain.\n             // (This is why we did all the work above: to reduce the items we have to consider here.)\n             let mut max = NonZeroU64::new(1).unwrap();\n-            for item in &self.borrows {\n+            for i in 0..self.len() {\n+                let item = self.get(i).unwrap();\n                 // Skip disabled items, they cannot be matched anyway.\n                 if !matches!(item.perm, Permission::Disabled) {\n                     // We are looking for a strict upper bound, so add 1 to this tag.\n                     max = cmp::max(item.tag.0.checked_add(1).unwrap(), max);\n                 }\n             }\n-            if let Some(unk) = self.unknown_bottom {\n+            if let Some(unk) = self.unknown_bottom() {\n                 max = cmp::max(unk.0, max);\n             }\n             // Use `max` as new strict upper bound for everything.\n             trace!(\n                 \"access: forgetting stack to upper bound {max} due to wildcard or unknown access\"\n             );\n-            self.borrows.clear();\n-            self.unknown_bottom = Some(SbTag(max));\n+            self.set_unknown_bottom(SbTag(max));\n         }\n \n         // Done.\n@@ -553,8 +479,9 @@ impl<'tcx> Stack {\n             )\n         })?;\n \n-        // Step 2: Remove all items.  Also checks for protectors.\n-        for item in self.borrows.drain(..).rev() {\n+        // Step 2: Consider all items removed. This checks for protectors.\n+        for idx in (0..self.len()).rev() {\n+            let item = self.get(idx).unwrap();\n             Stack::item_popped(&item, None, global, alloc_history)?;\n         }\n         Ok(())\n@@ -602,8 +529,7 @@ impl<'tcx> Stack {\n                 // The new thing is SRW anyway, so we cannot push it \"on top of the unkown part\"\n                 // (for all we know, it might join an SRW group inside the unknown).\n                 trace!(\"reborrow: forgetting stack entirely due to SharedReadWrite reborrow from wildcard or unknown\");\n-                self.borrows.clear();\n-                self.unknown_bottom = Some(global.next_ptr_tag);\n+                self.set_unknown_bottom(global.next_ptr_tag);\n                 return Ok(());\n             };\n \n@@ -630,19 +556,18 @@ impl<'tcx> Stack {\n             // on top of `derived_from`, and we want the new item at the top so that we\n             // get the strongest possible guarantees.\n             // This ensures U1 and F1.\n-            self.borrows.len()\n+            self.len()\n         };\n \n         // Put the new item there. As an optimization, deduplicate if it is equal to one of its new neighbors.\n         // `new_idx` might be 0 if we just cleared the entire stack.\n-        if self.borrows.get(new_idx) == Some(&new)\n-            || (new_idx > 0 && self.borrows[new_idx - 1] == new)\n+        if self.get(new_idx) == Some(new) || (new_idx > 0 && self.get(new_idx - 1).unwrap() == new)\n         {\n             // Optimization applies, done.\n             trace!(\"reborrow: avoiding adding redundant item {:?}\", new);\n         } else {\n             trace!(\"reborrow: adding item {:?}\", new);\n-            self.borrows.insert(new_idx, new);\n+            self.insert(new_idx, new);\n         }\n         Ok(())\n     }\n@@ -654,7 +579,7 @@ impl<'tcx> Stacks {\n     /// Creates new stack with initial tag.\n     fn new(size: Size, perm: Permission, tag: SbTag) -> Self {\n         let item = Item { perm, tag, protector: None };\n-        let stack = Stack { borrows: vec![item], unknown_bottom: None };\n+        let stack = Stack::new(item);\n \n         Stacks {\n             stacks: RangeMap::new(size, stack),"}, {"sha": "a7b8e5f13cea4162b429c1ce486078adf21f3e8d", "filename": "src/stacked_borrows/diagnostics.rs", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/cfad9d12f32634160e5aaecc41fb4097658b2bb2/src%2Fstacked_borrows%2Fdiagnostics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cfad9d12f32634160e5aaecc41fb4097658b2bb2/src%2Fstacked_borrows%2Fdiagnostics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fstacked_borrows%2Fdiagnostics.rs?ref=cfad9d12f32634160e5aaecc41fb4097658b2bb2", "patch": "@@ -185,7 +185,10 @@ fn operation_summary(\n \n fn error_cause(stack: &Stack, tag: SbTagExtra) -> &'static str {\n     if let SbTagExtra::Concrete(tag) = tag {\n-        if stack.borrows.iter().any(|item| item.tag == tag && item.perm != Permission::Disabled) {\n+        if (0..stack.len())\n+            .map(|i| stack.get(i).unwrap())\n+            .any(|item| item.tag == tag && item.perm != Permission::Disabled)\n+        {\n             \", but that tag only grants SharedReadOnly permission for this location\"\n         } else {\n             \", but that tag does not exist in the borrow stack for this location\""}, {"sha": "ccdd85eafd8eb53985c40b4d5056d9fd2edda497", "filename": "src/stacked_borrows/stack.rs", "status": "added", "additions": 379, "deletions": 0, "changes": 379, "blob_url": "https://github.com/rust-lang/rust/blob/cfad9d12f32634160e5aaecc41fb4097658b2bb2/src%2Fstacked_borrows%2Fstack.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cfad9d12f32634160e5aaecc41fb4097658b2bb2/src%2Fstacked_borrows%2Fstack.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fstacked_borrows%2Fstack.rs?ref=cfad9d12f32634160e5aaecc41fb4097658b2bb2", "patch": "@@ -0,0 +1,379 @@\n+use crate::stacked_borrows::{AccessKind, Item, Permission, SbTag, SbTagExtra};\n+use rustc_data_structures::fx::FxHashSet;\n+#[cfg(feature = \"stack-cache\")]\n+use std::ops::Range;\n+\n+/// Exactly what cache size we should use is a difficult tradeoff. There will always be some\n+/// workload which has a `SbTag` working set which exceeds the size of the cache, and ends up\n+/// falling back to linear searches of the borrow stack very often.\n+/// The cost of making this value too large is that the loop in `Stack::insert` which ensures the\n+/// entries in the cache stay correct after an insert becomes expensive.\n+#[cfg(feature = \"stack-cache\")]\n+const CACHE_LEN: usize = 32;\n+\n+/// Extra per-location state.\n+#[derive(Clone, Debug)]\n+pub struct Stack {\n+    /// Used *mostly* as a stack; never empty.\n+    /// Invariants:\n+    /// * Above a `SharedReadOnly` there can only be more `SharedReadOnly`.\n+    /// * Except for `Untagged`, no tag occurs in the stack more than once.\n+    borrows: Vec<Item>,\n+    /// If this is `Some(id)`, then the actual current stack is unknown. This can happen when\n+    /// wildcard pointers are used to access this location. What we do know is that `borrows` are at\n+    /// the top of the stack, and below it are arbitrarily many items whose `tag` is strictly less\n+    /// than `id`.\n+    /// When the bottom is unknown, `borrows` always has a `SharedReadOnly` or `Unique` at the bottom;\n+    /// we never have the unknown-to-known boundary in an SRW group.\n+    unknown_bottom: Option<SbTag>,\n+\n+    /// A small LRU cache of searches of the borrow stack.\n+    #[cfg(feature = \"stack-cache\")]\n+    cache: StackCache,\n+    /// On a read, we need to disable all `Unique` above the granting item. We can avoid most of\n+    /// this scan by keeping track of the region of the borrow stack that may contain `Unique`s.\n+    #[cfg(feature = \"stack-cache\")]\n+    unique_range: Range<usize>,\n+}\n+\n+/// A very small cache of searches of the borrow stack\n+/// This maps tags to locations in the borrow stack. Any use of this still needs to do a\n+/// probably-cold random access into the borrow stack to figure out what `Permission` an\n+/// `SbTag` grants. We could avoid this by also storing the `Permission` in the cache, but\n+/// most lookups into the cache are immediately followed by access of the full borrow stack anyway.\n+///\n+/// It may seem like maintaining this cache is a waste for small stacks, but\n+/// (a) iterating over small fixed-size arrays is super fast, and (b) empirically this helps *a lot*,\n+/// probably because runtime is dominated by large stacks.\n+#[cfg(feature = \"stack-cache\")]\n+#[derive(Clone, Debug)]\n+struct StackCache {\n+    tags: [SbTag; CACHE_LEN], // Hot in find_granting\n+    idx: [usize; CACHE_LEN],  // Hot in grant\n+}\n+\n+#[cfg(feature = \"stack-cache\")]\n+impl StackCache {\n+    /// When a tag is used, we call this function to add or refresh it in the cache.\n+    ///\n+    /// We use the position in the cache to represent how recently a tag was used; the first position\n+    /// is the most recently used tag. So an add shifts every element towards the end, and inserts\n+    /// the new element at the start. We lose the last element.\n+    /// This strategy is effective at keeping the most-accessed tags in the cache, but it costs a\n+    /// linear shift across the entire cache when we add a new tag.\n+    fn add(&mut self, idx: usize, tag: SbTag) {\n+        self.tags.copy_within(0..CACHE_LEN - 1, 1);\n+        self.tags[0] = tag;\n+        self.idx.copy_within(0..CACHE_LEN - 1, 1);\n+        self.idx[0] = idx;\n+    }\n+}\n+\n+impl PartialEq for Stack {\n+    fn eq(&self, other: &Self) -> bool {\n+        // All the semantics of Stack are in self.borrows, everything else is caching\n+        self.borrows == other.borrows\n+    }\n+}\n+\n+impl Eq for Stack {}\n+\n+impl<'tcx> Stack {\n+    /// Panics if any of the caching mechanisms have broken,\n+    /// - The StackCache indices don't refer to the parallel tags,\n+    /// - There are no Unique tags outside of first_unique..last_unique\n+    #[cfg(feature = \"expensive-debug-assertions\")]\n+    fn verify_cache_consistency(&self) {\n+        // Only a full cache needs to be valid. Also see the comments in find_granting_cache\n+        // and set_unknown_bottom.\n+        if self.borrows.len() >= CACHE_LEN {\n+            for (tag, stack_idx) in self.cache.tags.iter().zip(self.cache.idx.iter()) {\n+                assert_eq!(self.borrows[*stack_idx].tag, *tag);\n+            }\n+        }\n+\n+        for (idx, item) in self.borrows.iter().enumerate() {\n+            if item.perm == Permission::Unique {\n+                assert!(\n+                    self.unique_range.contains(&idx),\n+                    \"{:?} {:?}\",\n+                    self.unique_range,\n+                    self.borrows\n+                );\n+            }\n+        }\n+    }\n+\n+    /// Find the item granting the given kind of access to the given tag, and return where\n+    /// it is on the stack. For wildcard tags, the given index is approximate, but if *no*\n+    /// index is given it means the match was *not* in the known part of the stack.\n+    /// `Ok(None)` indicates it matched the \"unknown\" part of the stack.\n+    /// `Err` indicates it was not found.\n+    pub(super) fn find_granting(\n+        &mut self,\n+        access: AccessKind,\n+        tag: SbTagExtra,\n+        exposed_tags: &FxHashSet<SbTag>,\n+    ) -> Result<Option<usize>, ()> {\n+        #[cfg(feature = \"expensive-debug-assertions\")]\n+        self.verify_cache_consistency();\n+\n+        let SbTagExtra::Concrete(tag) = tag else {\n+            // Handle the wildcard case.\n+            // Go search the stack for an exposed tag.\n+            if let Some(idx) =\n+                self.borrows\n+                    .iter()\n+                    .enumerate() // we also need to know *where* in the stack\n+                    .rev() // search top-to-bottom\n+                    .find_map(|(idx, item)| {\n+                        // If the item fits and *might* be this wildcard, use it.\n+                        if item.perm.grants(access) && exposed_tags.contains(&item.tag) {\n+                            Some(idx)\n+                        } else {\n+                            None\n+                        }\n+                    })\n+            {\n+                return Ok(Some(idx));\n+            }\n+            // If we couldn't find it in the stack, check the unknown bottom.\n+            return if self.unknown_bottom.is_some() { Ok(None) } else { Err(()) };\n+        };\n+\n+        if let Some(idx) = self.find_granting_tagged(access, tag) {\n+            return Ok(Some(idx));\n+        }\n+\n+        // Couldn't find it in the stack; but if there is an unknown bottom it might be there.\n+        let found = self.unknown_bottom.is_some_and(|&unknown_limit| {\n+            tag.0 < unknown_limit.0 // unknown_limit is an upper bound for what can be in the unknown bottom.\n+        });\n+        if found { Ok(None) } else { Err(()) }\n+    }\n+\n+    fn find_granting_tagged(&mut self, access: AccessKind, tag: SbTag) -> Option<usize> {\n+        #[cfg(feature = \"stack-cache\")]\n+        if let Some(idx) = self.find_granting_cache(access, tag) {\n+            return Some(idx);\n+        }\n+\n+        // If we didn't find the tag in the cache, fall back to a linear search of the\n+        // whole stack, and add the tag to the cache.\n+        for (stack_idx, item) in self.borrows.iter().enumerate().rev() {\n+            if tag == item.tag && item.perm.grants(access) {\n+                #[cfg(feature = \"stack-cache\")]\n+                self.cache.add(stack_idx, tag);\n+                return Some(stack_idx);\n+            }\n+        }\n+        None\n+    }\n+\n+    #[cfg(feature = \"stack-cache\")]\n+    fn find_granting_cache(&mut self, access: AccessKind, tag: SbTag) -> Option<usize> {\n+        // This looks like a common-sense optimization; we're going to do a linear search of the\n+        // cache or the borrow stack to scan the shorter of the two. This optimization is miniscule\n+        // and this check actually ensures we do not access an invalid cache.\n+        // When a stack is created and when tags are removed from the top of the borrow stack, we\n+        // need some valid value to populate the cache. In both cases, we try to use the bottom\n+        // item. But when the stack is cleared in `set_unknown_bottom` there is nothing we could\n+        // place in the cache that is correct. But due to the way we populate the cache in\n+        // `StackCache::add`, we know that when the borrow stack has grown larger than the cache,\n+        // every slot in the cache is valid.\n+        if self.borrows.len() <= CACHE_LEN {\n+            return None;\n+        }\n+        // Search the cache for the tag we're looking up\n+        let cache_idx = self.cache.tags.iter().position(|t| *t == tag)?;\n+        let stack_idx = self.cache.idx[cache_idx];\n+        // If we found the tag, look up its position in the stack to see if it grants\n+        // the required permission\n+        if self.borrows[stack_idx].perm.grants(access) {\n+            // If it does, and it's not already in the most-recently-used position, re-insert it at\n+            // the most-recently-used position. This technically reduces the efficiency of the\n+            // cache by duplicating elements, but current benchmarks do not seem to benefit from\n+            // avoiding this duplication.\n+            // But if the tag is in position 1, avoiding the duplicating add is trivial.\n+            if cache_idx == 1 {\n+                self.cache.tags.swap(0, 1);\n+                self.cache.idx.swap(0, 1);\n+            } else if cache_idx > 1 {\n+                self.cache.add(stack_idx, tag);\n+            }\n+            Some(stack_idx)\n+        } else {\n+            // Tag is in the cache, but it doesn't grant the required permission\n+            None\n+        }\n+    }\n+\n+    pub fn insert(&mut self, new_idx: usize, new: Item) {\n+        self.borrows.insert(new_idx, new);\n+\n+        #[cfg(feature = \"stack-cache\")]\n+        self.insert_cache(new_idx, new);\n+    }\n+\n+    #[cfg(feature = \"stack-cache\")]\n+    fn insert_cache(&mut self, new_idx: usize, new: Item) {\n+        // Adjust the possibly-unique range if an insert occurs before or within it\n+        if self.unique_range.start >= new_idx {\n+            self.unique_range.start += 1;\n+        }\n+        if self.unique_range.end >= new_idx {\n+            self.unique_range.end += 1;\n+        }\n+        if new.perm == Permission::Unique {\n+            // Make sure the possibly-unique range contains the new borrow\n+            self.unique_range.start = self.unique_range.start.min(new_idx);\n+            self.unique_range.end = self.unique_range.end.max(new_idx + 1);\n+        }\n+\n+        // The above insert changes the meaning of every index in the cache >= new_idx, so now\n+        // we need to find every one of those indexes and increment it.\n+        // But if the insert is at the end (equivalent to a push), we can skip this step because\n+        // it didn't change the position of any other tags.\n+        if new_idx != self.borrows.len() - 1 {\n+            for idx in &mut self.cache.idx {\n+                if *idx >= new_idx {\n+                    *idx += 1;\n+                }\n+            }\n+        }\n+\n+        // This primes the cache for the next access, which is almost always the just-added tag.\n+        self.cache.add(new_idx, new.tag);\n+\n+        #[cfg(feature = \"expensive-debug-assertions\")]\n+        self.verify_cache_consistency();\n+    }\n+\n+    /// Construct a new `Stack` using the passed `Item` as the base tag.\n+    pub fn new(item: Item) -> Self {\n+        Stack {\n+            borrows: vec![item],\n+            unknown_bottom: None,\n+            #[cfg(feature = \"stack-cache\")]\n+            cache: StackCache { idx: [0; CACHE_LEN], tags: [item.tag; CACHE_LEN] },\n+            #[cfg(feature = \"stack-cache\")]\n+            unique_range: if item.perm == Permission::Unique { 0..1 } else { 0..0 },\n+        }\n+    }\n+\n+    pub fn get(&self, idx: usize) -> Option<Item> {\n+        self.borrows.get(idx).cloned()\n+    }\n+\n+    #[allow(clippy::len_without_is_empty)] // Stacks are never empty\n+    pub fn len(&self) -> usize {\n+        self.borrows.len()\n+    }\n+\n+    pub fn unknown_bottom(&self) -> Option<SbTag> {\n+        self.unknown_bottom\n+    }\n+\n+    pub fn set_unknown_bottom(&mut self, tag: SbTag) {\n+        // We clear the borrow stack but the lookup cache doesn't support clearing per se. Instead,\n+        // there is a check explained in `find_granting_cache` which protects against accessing the\n+        // cache when it has been cleared and not yet refilled.\n+        self.borrows.clear();\n+        self.unknown_bottom = Some(tag);\n+    }\n+\n+    /// Find all `Unique` elements in this borrow stack above `granting_idx`, pass a copy of them\n+    /// to the `visitor`, then set their `Permission` to `Disabled`.\n+    pub fn disable_uniques_starting_at<V: FnMut(Item) -> crate::InterpResult<'tcx>>(\n+        &mut self,\n+        disable_start: usize,\n+        mut visitor: V,\n+    ) -> crate::InterpResult<'tcx> {\n+        #[cfg(feature = \"stack-cache\")]\n+        let unique_range = self.unique_range.clone();\n+        #[cfg(not(feature = \"stack-cache\"))]\n+        let unique_range = 0..self.len();\n+\n+        if disable_start <= unique_range.end {\n+            let lower = unique_range.start.max(disable_start);\n+            let upper = (unique_range.end + 1).min(self.borrows.len());\n+            for item in &mut self.borrows[lower..upper] {\n+                if item.perm == Permission::Unique {\n+                    log::trace!(\"access: disabling item {:?}\", item);\n+                    visitor(*item)?;\n+                    item.perm = Permission::Disabled;\n+                }\n+            }\n+        }\n+\n+        #[cfg(feature = \"stack-cache\")]\n+        if disable_start < self.unique_range.start {\n+            // We disabled all Unique items\n+            self.unique_range.start = 0;\n+            self.unique_range.end = 0;\n+        } else {\n+            // Truncate the range to disable_start. This is + 2 because we are only removing\n+            // elements after disable_start, and this range does not include the end.\n+            self.unique_range.end = self.unique_range.end.min(disable_start + 1);\n+        }\n+\n+        #[cfg(feature = \"expensive-debug-assertions\")]\n+        self.verify_cache_consistency();\n+\n+        Ok(())\n+    }\n+\n+    /// Produces an iterator which iterates over `range` in reverse, and when dropped removes that\n+    /// range of `Item`s from this `Stack`.\n+    pub fn pop_items_after<V: FnMut(Item) -> crate::InterpResult<'tcx>>(\n+        &mut self,\n+        start: usize,\n+        mut visitor: V,\n+    ) -> crate::InterpResult<'tcx> {\n+        while self.borrows.len() > start {\n+            let item = self.borrows.pop().unwrap();\n+            visitor(item)?;\n+        }\n+\n+        #[cfg(feature = \"stack-cache\")]\n+        if !self.borrows.is_empty() {\n+            // After we remove from the borrow stack, every aspect of our caching may be invalid, but it is\n+            // also possible that the whole cache is still valid. So we call this method to repair what\n+            // aspects of the cache are now invalid, instead of resetting the whole thing to a trivially\n+            // valid default state.\n+            let base_tag = self.borrows[0].tag;\n+            let mut removed = 0;\n+            let mut cursor = 0;\n+            // Remove invalid entries from the cache by rotating them to the end of the cache, then\n+            // keep track of how many invalid elements there are and overwrite them with the base tag.\n+            // The base tag here serves as a harmless default value.\n+            for _ in 0..CACHE_LEN - 1 {\n+                if self.cache.idx[cursor] >= start {\n+                    self.cache.idx[cursor..CACHE_LEN - removed].rotate_left(1);\n+                    self.cache.tags[cursor..CACHE_LEN - removed].rotate_left(1);\n+                    removed += 1;\n+                } else {\n+                    cursor += 1;\n+                }\n+            }\n+            for i in CACHE_LEN - removed - 1..CACHE_LEN {\n+                self.cache.idx[i] = 0;\n+                self.cache.tags[i] = base_tag;\n+            }\n+\n+            if start < self.unique_range.start.saturating_sub(1) {\n+                // We removed all the Unique items\n+                self.unique_range = 0..0;\n+            } else {\n+                // Ensure the range doesn't extend past the new top of the stack\n+                self.unique_range.end = self.unique_range.end.min(start + 1);\n+            }\n+        } else {\n+            self.unique_range = 0..0;\n+        }\n+\n+        #[cfg(feature = \"expensive-debug-assertions\")]\n+        self.verify_cache_consistency();\n+        Ok(())\n+    }\n+}"}]}