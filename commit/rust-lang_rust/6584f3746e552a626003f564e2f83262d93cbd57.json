{"sha": "6584f3746e552a626003f564e2f83262d93cbd57", "node_id": "MDY6Q29tbWl0NzI0NzEyOjY1ODRmMzc0NmU1NTJhNjI2MDAzZjU2NGUyZjgzMjYyZDkzY2JkNTc=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-03-29T03:21:45Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-03-29T03:21:45Z"}, "message": "auto merge of #13170 : eddyb/rust/syntax-cleanup, r=alexcrichton\n\nRemoves all Cell's/RefCell's from lexer::Reader implementations and a couple @.", "tree": {"sha": "b5ee7f3495c68acd4fb712ca4f762b41bff384fb", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b5ee7f3495c68acd4fb712ca4f762b41bff384fb"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6584f3746e552a626003f564e2f83262d93cbd57", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6584f3746e552a626003f564e2f83262d93cbd57", "html_url": "https://github.com/rust-lang/rust/commit/6584f3746e552a626003f564e2f83262d93cbd57", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6584f3746e552a626003f564e2f83262d93cbd57/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b334f7c3cc3fcab8170ccb46a1f9a12702a2582c", "url": "https://api.github.com/repos/rust-lang/rust/commits/b334f7c3cc3fcab8170ccb46a1f9a12702a2582c", "html_url": "https://github.com/rust-lang/rust/commit/b334f7c3cc3fcab8170ccb46a1f9a12702a2582c"}, {"sha": "b0e3cb5f32ec25efb9af65293f9d776b6374b657", "url": "https://api.github.com/repos/rust-lang/rust/commits/b0e3cb5f32ec25efb9af65293f9d776b6374b657", "html_url": "https://github.com/rust-lang/rust/commit/b0e3cb5f32ec25efb9af65293f9d776b6374b657"}], "stats": {"total": 931, "additions": 445, "deletions": 486}, "files": [{"sha": "138c163d61217ce159e8585586880391db881db4", "filename": "src/librustc/metadata/creader.rs", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Fcreader.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Fcreader.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmetadata%2Fcreader.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -23,6 +23,7 @@ use metadata::loader;\n use metadata::loader::Os;\n \n use std::cell::RefCell;\n+use std::rc::Rc;\n use collections::HashMap;\n use syntax::ast;\n use syntax::abi;\n@@ -41,7 +42,7 @@ use syntax::visit;\n pub fn read_crates(sess: &Session,\n                    krate: &ast::Crate,\n                    os: loader::Os,\n-                   intr: @IdentInterner) {\n+                   intr: Rc<IdentInterner>) {\n     let mut e = Env {\n         sess: sess,\n         os: os,\n@@ -114,7 +115,7 @@ struct Env<'a> {\n     os: loader::Os,\n     crate_cache: @RefCell<Vec<cache_entry>>,\n     next_crate_num: ast::CrateNum,\n-    intr: @IdentInterner\n+    intr: Rc<IdentInterner>\n }\n \n fn visit_crate(e: &Env, c: &ast::Crate) {\n@@ -295,7 +296,7 @@ fn resolve_crate(e: &mut Env,\n                 id_hash: id_hash,\n                 hash: hash.map(|a| &*a),\n                 os: e.os,\n-                intr: e.intr,\n+                intr: e.intr.clone(),\n                 rejected_via_hash: false,\n             };\n             let loader::Library {"}, {"sha": "046782de3348696107d63db47131d91f8684cdd0", "filename": "src/librustc/metadata/csearch.rs", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Fcsearch.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Fcsearch.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmetadata%2Fcsearch.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -63,7 +63,7 @@ pub fn each_child_of_item(cstore: &cstore::CStore,\n     let get_crate_data: decoder::GetCrateDataCb = |cnum| {\n         cstore.get_crate_data(cnum)\n     };\n-    decoder::each_child_of_item(cstore.intr,\n+    decoder::each_child_of_item(cstore.intr.clone(),\n                                 crate_data,\n                                 def_id.node,\n                                 get_crate_data,\n@@ -80,7 +80,7 @@ pub fn each_top_level_item_of_crate(cstore: &cstore::CStore,\n     let get_crate_data: decoder::GetCrateDataCb = |cnum| {\n         cstore.get_crate_data(cnum)\n     };\n-    decoder::each_top_level_item_of_crate(cstore.intr,\n+    decoder::each_top_level_item_of_crate(cstore.intr.clone(),\n                                           crate_data,\n                                           get_crate_data,\n                                           callback)\n@@ -118,27 +118,27 @@ pub fn get_enum_variants(tcx: &ty::ctxt, def: ast::DefId)\n                       -> Vec<@ty::VariantInfo> {\n     let cstore = &tcx.sess.cstore;\n     let cdata = cstore.get_crate_data(def.krate);\n-    return decoder::get_enum_variants(cstore.intr, cdata, def.node, tcx)\n+    return decoder::get_enum_variants(cstore.intr.clone(), cdata, def.node, tcx)\n }\n \n /// Returns information about the given implementation.\n pub fn get_impl(tcx: &ty::ctxt, impl_def_id: ast::DefId)\n                 -> ty::Impl {\n     let cdata = tcx.sess.cstore.get_crate_data(impl_def_id.krate);\n-    decoder::get_impl(tcx.sess.cstore.intr, cdata, impl_def_id.node, tcx)\n+    decoder::get_impl(tcx.sess.cstore.intr.clone(), cdata, impl_def_id.node, tcx)\n }\n \n pub fn get_method(tcx: &ty::ctxt, def: ast::DefId) -> ty::Method {\n     let cdata = tcx.sess.cstore.get_crate_data(def.krate);\n-    decoder::get_method(tcx.sess.cstore.intr, cdata, def.node, tcx)\n+    decoder::get_method(tcx.sess.cstore.intr.clone(), cdata, def.node, tcx)\n }\n \n pub fn get_method_name_and_explicit_self(cstore: &cstore::CStore,\n                                          def: ast::DefId)\n                                      -> (ast::Ident, ast::ExplicitSelf_)\n {\n     let cdata = cstore.get_crate_data(def.krate);\n-    decoder::get_method_name_and_explicit_self(cstore.intr, cdata, def.node)\n+    decoder::get_method_name_and_explicit_self(cstore.intr.clone(), cdata, def.node)\n }\n \n pub fn get_trait_method_def_ids(cstore: &cstore::CStore,\n@@ -158,7 +158,7 @@ pub fn get_provided_trait_methods(tcx: &ty::ctxt,\n                                -> Vec<@ty::Method> {\n     let cstore = &tcx.sess.cstore;\n     let cdata = cstore.get_crate_data(def.krate);\n-    decoder::get_provided_trait_methods(cstore.intr, cdata, def.node, tcx)\n+    decoder::get_provided_trait_methods(cstore.intr.clone(), cdata, def.node, tcx)\n }\n \n pub fn get_supertraits(tcx: &ty::ctxt, def: ast::DefId) -> Vec<@ty::TraitRef> {\n@@ -177,7 +177,7 @@ pub fn get_static_methods_if_impl(cstore: &cstore::CStore,\n                                   def: ast::DefId)\n                                -> Option<Vec<StaticMethodInfo> > {\n     let cdata = cstore.get_crate_data(def.krate);\n-    decoder::get_static_methods_if_impl(cstore.intr, cdata, def.node)\n+    decoder::get_static_methods_if_impl(cstore.intr.clone(), cdata, def.node)\n }\n \n pub fn get_item_attrs(cstore: &cstore::CStore,\n@@ -191,7 +191,7 @@ pub fn get_struct_fields(cstore: &cstore::CStore,\n                          def: ast::DefId)\n                       -> Vec<ty::field_ty> {\n     let cdata = cstore.get_crate_data(def.krate);\n-    decoder::get_struct_fields(cstore.intr, cdata, def.node)\n+    decoder::get_struct_fields(cstore.intr.clone(), cdata, def.node)\n }\n \n pub fn get_type(tcx: &ty::ctxt,\n@@ -251,7 +251,7 @@ pub fn get_impl_method(cstore: &cstore::CStore,\n                        mname: ast::Ident)\n                     -> Option<ast::DefId> {\n     let cdata = cstore.get_crate_data(def.krate);\n-    decoder::get_impl_method(cstore.intr, cdata, def.node, mname)\n+    decoder::get_impl_method(cstore.intr.clone(), cdata, def.node, mname)\n }\n \n pub fn get_item_visibility(cstore: &cstore::CStore,"}, {"sha": "f57b2610d43963f9dede386a4cafdea39c367400", "filename": "src/librustc/metadata/cstore.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Fcstore.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Fcstore.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmetadata%2Fcstore.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -19,6 +19,7 @@ use metadata::loader;\n \n use std::cell::RefCell;\n use std::c_vec::CVec;\n+use std::rc::Rc;\n use collections::HashMap;\n use syntax::ast;\n use syntax::parse::token::IdentInterner;\n@@ -70,14 +71,14 @@ pub struct CStore {\n     priv used_crate_sources: RefCell<Vec<CrateSource> >,\n     priv used_libraries: RefCell<Vec<(~str, NativeLibaryKind)> >,\n     priv used_link_args: RefCell<Vec<~str> >,\n-    intr: @IdentInterner\n+    intr: Rc<IdentInterner>\n }\n \n // Map from NodeId's of local extern crate statements to crate numbers\n type extern_mod_crate_map = HashMap<ast::NodeId, ast::CrateNum>;\n \n impl CStore {\n-    pub fn new(intr: @IdentInterner) -> CStore {\n+    pub fn new(intr: Rc<IdentInterner>) -> CStore {\n         CStore {\n             metas: RefCell::new(HashMap::new()),\n             extern_mod_crate_map: RefCell::new(HashMap::new()),"}, {"sha": "d901959dc3fca3fe7168343115e43abbe6c73cf0", "filename": "src/librustc/metadata/decoder.rs", "status": "modified", "additions": 24, "deletions": 24, "changes": 48, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Fdecoder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Fdecoder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmetadata%2Fdecoder.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -278,7 +278,7 @@ fn item_region_param_defs(item_doc: ebml::Doc, cdata: Cmd)\n     reader::tagged_docs(item_doc, tag_region_param_def, |rp_doc| {\n             let ident_str_doc = reader::get_doc(rp_doc,\n                                                 tag_region_param_def_ident);\n-            let ident = item_name(token::get_ident_interner(), ident_str_doc);\n+            let ident = item_name(&*token::get_ident_interner(), ident_str_doc);\n             let def_id_doc = reader::get_doc(rp_doc,\n                                              tag_region_param_def_def_id);\n             let def_id = reader::with_doc_data(def_id_doc, parse_def_id);\n@@ -460,13 +460,13 @@ pub fn get_impl_vtables(cdata: Cmd,\n }\n \n \n-pub fn get_impl_method(intr: @IdentInterner, cdata: Cmd, id: ast::NodeId,\n+pub fn get_impl_method(intr: Rc<IdentInterner>, cdata: Cmd, id: ast::NodeId,\n                        name: ast::Ident) -> Option<ast::DefId> {\n     let items = reader::get_doc(reader::Doc(cdata.data()), tag_items);\n     let mut found = None;\n     reader::tagged_docs(find_item(id, items), tag_item_impl_method, |mid| {\n         let m_did = reader::with_doc_data(mid, parse_def_id);\n-        if item_name(intr, find_item(m_did.node, items)) == name {\n+        if item_name(&*intr, find_item(m_did.node, items)) == name {\n             found = Some(translate_def_id(cdata, m_did));\n         }\n         true\n@@ -509,7 +509,7 @@ pub fn each_lang_item(cdata: Cmd, f: |ast::NodeId, uint| -> bool) -> bool {\n     })\n }\n \n-fn each_child_of_item_or_crate(intr: @IdentInterner,\n+fn each_child_of_item_or_crate(intr: Rc<IdentInterner>,\n                                cdata: Cmd,\n                                item_doc: ebml::Doc,\n                                get_crate_data: GetCrateDataCb,\n@@ -536,7 +536,7 @@ fn each_child_of_item_or_crate(intr: @IdentInterner,\n             None => {}\n             Some(child_item_doc) => {\n                 // Hand off the item to the callback.\n-                let child_name = item_name(intr, child_item_doc);\n+                let child_name = item_name(&*intr, child_item_doc);\n                 let def_like = item_to_def_like(child_item_doc,\n                                                 child_def_id,\n                                                 cdata.cnum);\n@@ -577,7 +577,7 @@ fn each_child_of_item_or_crate(intr: @IdentInterner,\n                                     // Hand off the static method\n                                     // to the callback.\n                                     let static_method_name =\n-                                        item_name(intr, impl_method_doc);\n+                                        item_name(&*intr, impl_method_doc);\n                                     let static_method_def_like =\n                                         item_to_def_like(impl_method_doc,\n                                                          impl_method_def_id,\n@@ -638,7 +638,7 @@ fn each_child_of_item_or_crate(intr: @IdentInterner,\n }\n \n /// Iterates over each child of the given item.\n-pub fn each_child_of_item(intr: @IdentInterner,\n+pub fn each_child_of_item(intr: Rc<IdentInterner>,\n                           cdata: Cmd,\n                           id: ast::NodeId,\n                           get_crate_data: GetCrateDataCb,\n@@ -659,7 +659,7 @@ pub fn each_child_of_item(intr: @IdentInterner,\n }\n \n /// Iterates over all the top-level crate items.\n-pub fn each_top_level_item_of_crate(intr: @IdentInterner,\n+pub fn each_top_level_item_of_crate(intr: Rc<IdentInterner>,\n                                     cdata: Cmd,\n                                     get_crate_data: GetCrateDataCb,\n                                     callback: |DefLike,\n@@ -711,7 +711,7 @@ pub fn maybe_get_item_ast(cdata: Cmd, tcx: &ty::ctxt, id: ast::NodeId,\n     }\n }\n \n-pub fn get_enum_variants(intr: @IdentInterner, cdata: Cmd, id: ast::NodeId,\n+pub fn get_enum_variants(intr: Rc<IdentInterner>, cdata: Cmd, id: ast::NodeId,\n                      tcx: &ty::ctxt) -> Vec<@ty::VariantInfo> {\n     let data = cdata.data();\n     let items = reader::get_doc(reader::Doc(data), tag_items);\n@@ -723,7 +723,7 @@ pub fn get_enum_variants(intr: @IdentInterner, cdata: Cmd, id: ast::NodeId,\n         let item = find_item(did.node, items);\n         let ctor_ty = item_type(ast::DefId { krate: cdata.cnum, node: id},\n                                 item, tcx, cdata);\n-        let name = item_name(intr, item);\n+        let name = item_name(&*intr, item);\n         let arg_tys = match ty::get(ctor_ty).sty {\n           ty::ty_bare_fn(ref f) => f.sig.inputs.clone(),\n           _ => Vec::new(), // Nullary enum variant.\n@@ -770,20 +770,20 @@ fn get_explicit_self(item: ebml::Doc) -> ast::ExplicitSelf_ {\n     }\n }\n \n-fn item_impl_methods(intr: @IdentInterner, cdata: Cmd, item: ebml::Doc,\n+fn item_impl_methods(intr: Rc<IdentInterner>, cdata: Cmd, item: ebml::Doc,\n                      tcx: &ty::ctxt) -> Vec<@ty::Method> {\n     let mut rslt = Vec::new();\n     reader::tagged_docs(item, tag_item_impl_method, |doc| {\n         let m_did = reader::with_doc_data(doc, parse_def_id);\n-        rslt.push(@get_method(intr, cdata, m_did.node, tcx));\n+        rslt.push(@get_method(intr.clone(), cdata, m_did.node, tcx));\n         true\n     });\n \n     rslt\n }\n \n /// Returns information about the given implementation.\n-pub fn get_impl(intr: @IdentInterner, cdata: Cmd, impl_id: ast::NodeId,\n+pub fn get_impl(intr: Rc<IdentInterner>, cdata: Cmd, impl_id: ast::NodeId,\n                tcx: &ty::ctxt)\n                 -> ty::Impl {\n     let data = cdata.data();\n@@ -793,23 +793,23 @@ pub fn get_impl(intr: @IdentInterner, cdata: Cmd, impl_id: ast::NodeId,\n             krate: cdata.cnum,\n             node: impl_id,\n         },\n-        ident: item_name(intr, impl_item),\n+        ident: item_name(&*intr, impl_item),\n         methods: item_impl_methods(intr, cdata, impl_item, tcx),\n     }\n }\n \n pub fn get_method_name_and_explicit_self(\n-    intr: @IdentInterner,\n+    intr: Rc<IdentInterner>,\n     cdata: Cmd,\n     id: ast::NodeId) -> (ast::Ident, ast::ExplicitSelf_)\n {\n     let method_doc = lookup_item(id, cdata.data());\n-    let name = item_name(intr, method_doc);\n+    let name = item_name(&*intr, method_doc);\n     let explicit_self = get_explicit_self(method_doc);\n     (name, explicit_self)\n }\n \n-pub fn get_method(intr: @IdentInterner, cdata: Cmd, id: ast::NodeId,\n+pub fn get_method(intr: Rc<IdentInterner>, cdata: Cmd, id: ast::NodeId,\n                   tcx: &ty::ctxt) -> ty::Method\n {\n     let method_doc = lookup_item(id, cdata.data());\n@@ -823,7 +823,7 @@ pub fn get_method(intr: @IdentInterner, cdata: Cmd, id: ast::NodeId,\n         _ => ImplContainer(container_id),\n     };\n \n-    let name = item_name(intr, method_doc);\n+    let name = item_name(&*intr, method_doc);\n     let type_param_defs = item_ty_param_defs(method_doc, tcx, cdata,\n                                              tag_item_method_tps);\n     let rp_defs = item_region_param_defs(method_doc, cdata);\n@@ -867,7 +867,7 @@ pub fn get_item_variances(cdata: Cmd, id: ast::NodeId) -> ty::ItemVariances {\n     unwrap_(Decodable::decode(&mut decoder))\n }\n \n-pub fn get_provided_trait_methods(intr: @IdentInterner, cdata: Cmd,\n+pub fn get_provided_trait_methods(intr: Rc<IdentInterner>, cdata: Cmd,\n                                   id: ast::NodeId, tcx: &ty::ctxt) ->\n         Vec<@ty::Method> {\n     let data = cdata.data();\n@@ -879,7 +879,7 @@ pub fn get_provided_trait_methods(intr: @IdentInterner, cdata: Cmd,\n         let mth = lookup_item(did.node, data);\n \n         if item_method_sort(mth) == 'p' {\n-            result.push(@get_method(intr, cdata, did.node, tcx));\n+            result.push(@get_method(intr.clone(), cdata, did.node, tcx));\n         }\n         true\n     });\n@@ -921,7 +921,7 @@ pub fn get_type_name_if_impl(cdata: Cmd,\n     ret\n }\n \n-pub fn get_static_methods_if_impl(intr: @IdentInterner,\n+pub fn get_static_methods_if_impl(intr: Rc<IdentInterner>,\n                                   cdata: Cmd,\n                                   node_id: ast::NodeId)\n                                -> Option<Vec<StaticMethodInfo> > {\n@@ -957,7 +957,7 @@ pub fn get_static_methods_if_impl(intr: @IdentInterner,\n                 }\n \n                 static_impl_methods.push(StaticMethodInfo {\n-                    ident: item_name(intr, impl_method_doc),\n+                    ident: item_name(&*intr, impl_method_doc),\n                     def_id: item_def_id(impl_method_doc, cdata),\n                     purity: purity,\n                     vis: item_visibility(impl_method_doc),\n@@ -1009,7 +1009,7 @@ fn struct_field_family_to_visibility(family: Family) -> ast::Visibility {\n     }\n }\n \n-pub fn get_struct_fields(intr: @IdentInterner, cdata: Cmd, id: ast::NodeId)\n+pub fn get_struct_fields(intr: Rc<IdentInterner>, cdata: Cmd, id: ast::NodeId)\n     -> Vec<ty::field_ty> {\n     let data = cdata.data();\n     let item = lookup_item(id, data);\n@@ -1018,7 +1018,7 @@ pub fn get_struct_fields(intr: @IdentInterner, cdata: Cmd, id: ast::NodeId)\n         let f = item_family(an_item);\n         if f == PublicField || f == PrivateField || f == InheritedField {\n             // FIXME #6993: name should be of type Name, not Ident\n-            let name = item_name(intr, an_item);\n+            let name = item_name(&*intr, an_item);\n             let did = item_def_id(an_item, cdata);\n             result.push(ty::field_ty {\n                 name: name.name,"}, {"sha": "6de1bf69f6da90b314670b9f3a033de792102d43", "filename": "src/librustc/metadata/loader.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Floader.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustc%2Fmetadata%2Floader.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmetadata%2Floader.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -29,6 +29,7 @@ use std::cast;\n use std::cmp;\n use std::io;\n use std::os::consts::{macos, freebsd, linux, android, win32};\n+use std::rc::Rc;\n use std::str;\n use std::slice;\n \n@@ -52,7 +53,7 @@ pub struct Context<'a> {\n     id_hash: &'a str,\n     hash: Option<&'a Svh>,\n     os: Os,\n-    intr: @IdentInterner,\n+    intr: Rc<IdentInterner>,\n     rejected_via_hash: bool,\n }\n "}, {"sha": "58fc92bf345af3cc7d350426d0c48f16c60a569d", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -44,7 +44,7 @@ pub fn highlight(src: &str, class: Option<&str>) -> ~str {\n /// it's used. All source code emission is done as slices from the source map,\n /// not from the tokens themselves, in order to stay true to the original\n /// source.\n-fn doit(sess: &parse::ParseSess, lexer: lexer::StringReader, class: Option<&str>,\n+fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader, class: Option<&str>,\n         out: &mut Writer) -> io::IoResult<()> {\n     use syntax::parse::lexer::Reader;\n \n@@ -55,7 +55,7 @@ fn doit(sess: &parse::ParseSess, lexer: lexer::StringReader, class: Option<&str>\n     let mut is_macro_nonterminal = false;\n     loop {\n         let next = lexer.next_token();\n-        let test = if next.tok == t::EOF {lexer.pos.get()} else {next.sp.lo};\n+        let test = if next.tok == t::EOF {lexer.pos} else {next.sp.lo};\n \n         // The lexer consumes all whitespace and non-doc-comments when iterating\n         // between tokens. If this token isn't directly adjacent to our last"}, {"sha": "7c3eb7742d20ebe1703994721eb1197897a6ce6d", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -581,14 +581,16 @@ pub enum TokenTree {\n     TTTok(Span, ::parse::token::Token),\n     // a delimited sequence (the delimiters appear as the first\n     // and last elements of the vector)\n-    TTDelim(@Vec<TokenTree> ),\n+    // FIXME(eddyb) #6308 Use Rc<[TokenTree]> after DST.\n+    TTDelim(Rc<Vec<TokenTree>>),\n \n     // These only make sense for right-hand-sides of MBE macros:\n \n     // a kleene-style repetition sequence with a span, a TTForest,\n     // an optional separator, and a boolean where true indicates\n     // zero or more (..), and false indicates one or more (+).\n-    TTSeq(Span, @Vec<TokenTree> , Option<::parse::token::Token>, bool),\n+    // FIXME(eddyb) #6308 Use Rc<[TokenTree]> after DST.\n+    TTSeq(Span, Rc<Vec<TokenTree>>, Option<::parse::token::Token>, bool),\n \n     // a syntactic variable that will be filled in by macro expansion.\n     TTNonterminal(Span, Ident)"}, {"sha": "c9e444a9b8caf2a762b7d6987712765dc9d79269", "filename": "src/libsyntax/ext/log_syntax.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fext%2Flog_syntax.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fext%2Flog_syntax.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Flog_syntax.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -13,14 +13,16 @@ use codemap;\n use ext::base;\n use print;\n \n+use std::rc::Rc;\n+\n pub fn expand_syntax_ext(cx: &mut base::ExtCtxt,\n                          sp: codemap::Span,\n                          tt: &[ast::TokenTree])\n                       -> base::MacResult {\n \n     cx.print_backtrace();\n     println!(\"{}\", print::pprust::tt_to_str(&ast::TTDelim(\n-                @tt.iter().map(|x| (*x).clone()).collect())));\n+                Rc::new(tt.iter().map(|x| (*x).clone()).collect()))));\n \n     // any so that `log_syntax` can be invoked as an expression and item.\n     base::MacResult::dummy_any(sp)"}, {"sha": "ae537cc47826e90f48e4db867437c5316dbb9fdb", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 25, "deletions": 24, "changes": 49, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -21,6 +21,7 @@ use parse::parser::{LifetimeAndTypesWithoutColons, Parser};\n use parse::token::{Token, EOF, Nonterminal};\n use parse::token;\n \n+use std::rc::Rc;\n use collections::HashMap;\n \n /* This is an Earley-like parser, without support for in-grammar nonterminals,\n@@ -102,7 +103,7 @@ pub struct MatcherPos {\n     sep: Option<Token>,\n     idx: uint,\n     up: Option<~MatcherPos>,\n-    matches: Vec<Vec<@NamedMatch>>,\n+    matches: Vec<Vec<Rc<NamedMatch>>>,\n     match_lo: uint, match_hi: uint,\n     sp_lo: BytePos,\n }\n@@ -165,14 +166,14 @@ pub fn initial_matcher_pos(ms: Vec<Matcher> , sep: Option<Token>, lo: BytePos)\n // ast::Matcher it was derived from.\n \n pub enum NamedMatch {\n-    MatchedSeq(Vec<@NamedMatch> , codemap::Span),\n+    MatchedSeq(Vec<Rc<NamedMatch>>, codemap::Span),\n     MatchedNonterminal(Nonterminal)\n }\n \n-pub fn nameize(p_s: &ParseSess, ms: &[Matcher], res: &[@NamedMatch])\n-            -> HashMap<Ident, @NamedMatch> {\n-    fn n_rec(p_s: &ParseSess, m: &Matcher, res: &[@NamedMatch],\n-             ret_val: &mut HashMap<Ident, @NamedMatch>) {\n+pub fn nameize(p_s: &ParseSess, ms: &[Matcher], res: &[Rc<NamedMatch>])\n+            -> HashMap<Ident, Rc<NamedMatch>> {\n+    fn n_rec(p_s: &ParseSess, m: &Matcher, res: &[Rc<NamedMatch>],\n+             ret_val: &mut HashMap<Ident, Rc<NamedMatch>>) {\n         match *m {\n           codemap::Spanned {node: MatchTok(_), .. } => (),\n           codemap::Spanned {node: MatchSeq(ref more_ms, _, _, _, _), .. } => {\n@@ -189,7 +190,7 @@ pub fn nameize(p_s: &ParseSess, ms: &[Matcher], res: &[@NamedMatch])\n                 p_s.span_diagnostic\n                    .span_fatal(span, \"duplicated bind name: \" + string.get())\n             }\n-            ret_val.insert(bind_name, res[idx]);\n+            ret_val.insert(bind_name, res[idx].clone());\n           }\n         }\n     }\n@@ -199,16 +200,16 @@ pub fn nameize(p_s: &ParseSess, ms: &[Matcher], res: &[@NamedMatch])\n }\n \n pub enum ParseResult {\n-    Success(HashMap<Ident, @NamedMatch>),\n+    Success(HashMap<Ident, Rc<NamedMatch>>),\n     Failure(codemap::Span, ~str),\n     Error(codemap::Span, ~str)\n }\n \n-pub fn parse_or_else<R: Reader>(sess: &ParseSess,\n-                                cfg: ast::CrateConfig,\n-                                rdr: R,\n-                                ms: Vec<Matcher> )\n-                                -> HashMap<Ident, @NamedMatch> {\n+pub fn parse_or_else(sess: &ParseSess,\n+                     cfg: ast::CrateConfig,\n+                     rdr: TtReader,\n+                     ms: Vec<Matcher> )\n+                     -> HashMap<Ident, Rc<NamedMatch>> {\n     match parse(sess, cfg, rdr, ms.as_slice()) {\n         Success(m) => m,\n         Failure(sp, str) => sess.span_diagnostic.span_fatal(sp, str),\n@@ -226,11 +227,11 @@ pub fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n     }\n }\n \n-pub fn parse<R: Reader>(sess: &ParseSess,\n-                        cfg: ast::CrateConfig,\n-                        rdr: R,\n-                        ms: &[Matcher])\n-                        -> ParseResult {\n+pub fn parse(sess: &ParseSess,\n+             cfg: ast::CrateConfig,\n+             mut rdr: TtReader,\n+             ms: &[Matcher])\n+             -> ParseResult {\n     let mut cur_eis = Vec::new();\n     cur_eis.push(initial_matcher_pos(ms.iter()\n                                        .map(|x| (*x).clone())\n@@ -282,8 +283,8 @@ pub fn parse<R: Reader>(sess: &ParseSess,\n                             let sub = (*ei.matches.get(idx)).clone();\n                             new_pos.matches\n                                    .get_mut(idx)\n-                                   .push(@MatchedSeq(sub, mk_sp(ei.sp_lo,\n-                                                                sp.hi)));\n+                                   .push(Rc::new(MatchedSeq(sub, mk_sp(ei.sp_lo,\n+                                                                       sp.hi))));\n                         }\n \n                         new_pos.idx += 1;\n@@ -325,7 +326,7 @@ pub fn parse<R: Reader>(sess: &ParseSess,\n                         for idx in range(match_idx_lo, match_idx_hi) {\n                             new_ei.matches\n                                   .get_mut(idx)\n-                                  .push(@MatchedSeq(Vec::new(), sp));\n+                                  .push(Rc::new(MatchedSeq(Vec::new(), sp)));\n                         }\n \n                         cur_eis.push(new_ei);\n@@ -395,14 +396,14 @@ pub fn parse<R: Reader>(sess: &ParseSess,\n                 }\n                 rdr.next_token();\n             } else /* bb_eis.len() == 1 */ {\n-                let mut rust_parser = Parser(sess, cfg.clone(), rdr.dup());\n+                let mut rust_parser = Parser(sess, cfg.clone(), ~rdr.clone());\n \n                 let mut ei = bb_eis.pop().unwrap();\n                 match ei.elts.get(ei.idx).node {\n                   MatchNonterminal(_, name, idx) => {\n                     let name_string = token::get_ident(name);\n-                    ei.matches.get_mut(idx).push(@MatchedNonterminal(\n-                        parse_nt(&mut rust_parser, name_string.get())));\n+                    ei.matches.get_mut(idx).push(Rc::new(MatchedNonterminal(\n+                        parse_nt(&mut rust_parser, name_string.get()))));\n                     ei.idx += 1u;\n                   }\n                   _ => fail!()"}, {"sha": "d4a883a63ebbf9ca4f5e1366955ec0de79adb8e8", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -28,6 +28,7 @@ use print;\n use util::small_vector::SmallVector;\n \n use std::cell::RefCell;\n+use std::rc::Rc;\n \n struct ParserAnyMacro<'a> {\n     parser: RefCell<Parser<'a>>,\n@@ -85,8 +86,8 @@ impl<'a> AnyMacro for ParserAnyMacro<'a> {\n \n struct MacroRulesMacroExpander {\n     name: Ident,\n-    lhses: @Vec<@NamedMatch> ,\n-    rhses: @Vec<@NamedMatch> ,\n+    lhses: Vec<Rc<NamedMatch>>,\n+    rhses: Vec<Rc<NamedMatch>>,\n }\n \n impl MacroExpander for MacroRulesMacroExpander {\n@@ -109,15 +110,15 @@ fn generic_extension(cx: &ExtCtxt,\n                      sp: Span,\n                      name: Ident,\n                      arg: &[ast::TokenTree],\n-                     lhses: &[@NamedMatch],\n-                     rhses: &[@NamedMatch])\n+                     lhses: &[Rc<NamedMatch>],\n+                     rhses: &[Rc<NamedMatch>])\n                      -> MacResult {\n     if cx.trace_macros() {\n         println!(\"{}! \\\\{ {} \\\\}\",\n                  token::get_ident(name),\n-                 print::pprust::tt_to_str(&TTDelim(@arg.iter()\n-                                                       .map(|x| (*x).clone())\n-                                                       .collect())));\n+                 print::pprust::tt_to_str(&TTDelim(Rc::new(arg.iter()\n+                                                              .map(|x| (*x).clone())\n+                                                              .collect()))));\n     }\n \n     // Which arm's failure should we report? (the one furthest along)\n@@ -220,12 +221,12 @@ pub fn add_new_extension(cx: &mut ExtCtxt,\n \n     // Extract the arguments:\n     let lhses = match **argument_map.get(&lhs_nm) {\n-        MatchedSeq(ref s, _) => /* FIXME (#2543) */ @(*s).clone(),\n+        MatchedSeq(ref s, _) => /* FIXME (#2543) */ (*s).clone(),\n         _ => cx.span_bug(sp, \"wrong-structured lhs\")\n     };\n \n     let rhses = match **argument_map.get(&rhs_nm) {\n-        MatchedSeq(ref s, _) => /* FIXME (#2543) */ @(*s).clone(),\n+        MatchedSeq(ref s, _) => /* FIXME (#2543) */ (*s).clone(),\n         _ => cx.span_bug(sp, \"wrong-structured rhs\")\n     };\n "}, {"sha": "bc8709befaee26c18b79d63c619c11390ad08d49", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 147, "deletions": 178, "changes": 325, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -17,107 +17,79 @@ use parse::token::{EOF, INTERPOLATED, IDENT, Token, NtIdent};\n use parse::token;\n use parse::lexer::TokenAndSpan;\n \n-use std::cell::{Cell, RefCell};\n+use std::rc::Rc;\n use collections::HashMap;\n \n ///an unzipping of `TokenTree`s\n+#[deriving(Clone)]\n struct TtFrame {\n-    forest: @Vec<ast::TokenTree> ,\n-    idx: Cell<uint>,\n+    forest: Rc<Vec<ast::TokenTree>>,\n+    idx: uint,\n     dotdotdoted: bool,\n     sep: Option<Token>,\n-    up: Option<@TtFrame>,\n }\n \n+#[deriving(Clone)]\n pub struct TtReader<'a> {\n     sp_diag: &'a SpanHandler,\n     // the unzipped tree:\n-    priv stack: RefCell<@TtFrame>,\n+    priv stack: Vec<TtFrame>,\n     /* for MBE-style macro transcription */\n-    priv interpolations: RefCell<HashMap<Ident, @NamedMatch>>,\n-    priv repeat_idx: RefCell<Vec<uint> >,\n-    priv repeat_len: RefCell<Vec<uint> >,\n+    priv interpolations: HashMap<Ident, Rc<NamedMatch>>,\n+    priv repeat_idx: Vec<uint>,\n+    priv repeat_len: Vec<uint>,\n     /* cached: */\n-    cur_tok: RefCell<Token>,\n-    cur_span: RefCell<Span>,\n+    cur_tok: Token,\n+    cur_span: Span,\n }\n \n /** This can do Macro-By-Example transcription. On the other hand, if\n  *  `src` contains no `TTSeq`s and `TTNonterminal`s, `interp` can (and\n  *  should) be none. */\n pub fn new_tt_reader<'a>(sp_diag: &'a SpanHandler,\n-                         interp: Option<HashMap<Ident, @NamedMatch>>,\n+                         interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n                          src: Vec<ast::TokenTree> )\n                          -> TtReader<'a> {\n-    let r = TtReader {\n+    let mut r = TtReader {\n         sp_diag: sp_diag,\n-        stack: RefCell::new(@TtFrame {\n-            forest: @src,\n-            idx: Cell::new(0u),\n+        stack: vec!(TtFrame {\n+            forest: Rc::new(src),\n+            idx: 0,\n             dotdotdoted: false,\n             sep: None,\n-            up: None\n         }),\n         interpolations: match interp { /* just a convienience */\n-            None => RefCell::new(HashMap::new()),\n-            Some(x) => RefCell::new(x),\n+            None => HashMap::new(),\n+            Some(x) => x,\n         },\n-        repeat_idx: RefCell::new(Vec::new()),\n-        repeat_len: RefCell::new(Vec::new()),\n+        repeat_idx: Vec::new(),\n+        repeat_len: Vec::new(),\n         /* dummy values, never read: */\n-        cur_tok: RefCell::new(EOF),\n-        cur_span: RefCell::new(DUMMY_SP),\n+        cur_tok: EOF,\n+        cur_span: DUMMY_SP,\n     };\n-    tt_next_token(&r); /* get cur_tok and cur_span set up */\n+    tt_next_token(&mut r); /* get cur_tok and cur_span set up */\n     r\n }\n \n-fn dup_tt_frame(f: @TtFrame) -> @TtFrame {\n-    @TtFrame {\n-        forest: @(*f.forest).clone(),\n-        idx: f.idx.clone(),\n-        dotdotdoted: f.dotdotdoted,\n-        sep: f.sep.clone(),\n-        up: match f.up {\n-            Some(up_frame) => Some(dup_tt_frame(up_frame)),\n-            None => None\n-        }\n-    }\n-}\n-\n-pub fn dup_tt_reader<'a>(r: &TtReader<'a>) -> TtReader<'a> {\n-    TtReader {\n-        sp_diag: r.sp_diag,\n-        stack: RefCell::new(dup_tt_frame(r.stack.get())),\n-        repeat_idx: r.repeat_idx.clone(),\n-        repeat_len: r.repeat_len.clone(),\n-        cur_tok: r.cur_tok.clone(),\n-        cur_span: r.cur_span.clone(),\n-        interpolations: r.interpolations.clone(),\n-    }\n-}\n-\n-\n-fn lookup_cur_matched_by_matched(r: &TtReader, start: @NamedMatch)\n-                                 -> @NamedMatch {\n-    fn red(ad: @NamedMatch, idx: &uint) -> @NamedMatch {\n+fn lookup_cur_matched_by_matched(r: &TtReader, start: Rc<NamedMatch>) -> Rc<NamedMatch> {\n+    r.repeat_idx.iter().fold(start, |ad, idx| {\n         match *ad {\n             MatchedNonterminal(_) => {\n                 // end of the line; duplicate henceforth\n-                ad\n+                ad.clone()\n             }\n-            MatchedSeq(ref ads, _) => *ads.get(*idx)\n+            MatchedSeq(ref ads, _) => ads.get(*idx).clone()\n         }\n-    }\n-    r.repeat_idx.borrow().iter().fold(start, red)\n+    })\n }\n \n-fn lookup_cur_matched(r: &TtReader, name: Ident) -> @NamedMatch {\n-    let matched_opt = r.interpolations.borrow().find_copy(&name);\n+fn lookup_cur_matched(r: &TtReader, name: Ident) -> Rc<NamedMatch> {\n+    let matched_opt = r.interpolations.find_copy(&name);\n     match matched_opt {\n         Some(s) => lookup_cur_matched_by_matched(r, s),\n         None => {\n-            r.sp_diag.span_fatal(r.cur_span.get(),\n+            r.sp_diag.span_fatal(r.cur_span,\n                                  format!(\"unknown macro variable `{}`\",\n                                          token::get_ident(name)));\n         }\n@@ -167,143 +139,140 @@ fn lockstep_iter_size(t: &TokenTree, r: &TtReader) -> LockstepIterSize {\n \n // return the next token from the TtReader.\n // EFFECT: advances the reader's token field\n-pub fn tt_next_token(r: &TtReader) -> TokenAndSpan {\n+pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n     // FIXME(pcwalton): Bad copy?\n     let ret_val = TokenAndSpan {\n-        tok: r.cur_tok.get(),\n-        sp: r.cur_span.get(),\n+        tok: r.cur_tok.clone(),\n+        sp: r.cur_span.clone(),\n     };\n     loop {\n-        if r.stack.borrow().idx.get() < r.stack.borrow().forest.len() {\n-            break;\n-        }\n-\n-        /* done with this set; pop or repeat? */\n-        if !r.stack.get().dotdotdoted || {\n-                *r.repeat_idx.borrow().last().unwrap() ==\n-                *r.repeat_len.borrow().last().unwrap() - 1\n-            } {\n-\n-            match r.stack.get().up {\n-              None => {\n-                r.cur_tok.set(EOF);\n+        let should_pop = match r.stack.last() {\n+            None => {\n+                assert_eq!(ret_val.tok, EOF);\n                 return ret_val;\n-              }\n-              Some(tt_f) => {\n-                if r.stack.get().dotdotdoted {\n-                    r.repeat_idx.borrow_mut().pop().unwrap();\n-                    r.repeat_len.borrow_mut().pop().unwrap();\n+            }\n+            Some(frame) => {\n+                if frame.idx < frame.forest.len() {\n+                    break;\n                 }\n-\n-                r.stack.set(tt_f);\n-                r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n-              }\n+                !frame.dotdotdoted ||\n+                    *r.repeat_idx.last().unwrap() == *r.repeat_len.last().unwrap() - 1\n             }\n+        };\n \n-        } else { /* repeat */\n-            r.stack.get().idx.set(0u);\n-            {\n-                let mut repeat_idx = r.repeat_idx.borrow_mut();\n-                let last_repeat_idx = repeat_idx.len() - 1u;\n-                *repeat_idx.get_mut(last_repeat_idx) += 1u;\n+        /* done with this set; pop or repeat? */\n+        if should_pop {\n+            let prev = r.stack.pop().unwrap();\n+            match r.stack.mut_last() {\n+                None => {\n+                    r.cur_tok = EOF;\n+                    return ret_val;\n+                }\n+                Some(frame) => {\n+                    frame.idx += 1;\n+                }\n             }\n-            match r.stack.get().sep.clone() {\n-              Some(tk) => {\n-                r.cur_tok.set(tk); /* repeat same span, I guess */\n-                return ret_val;\n-              }\n-              None => ()\n+            if prev.dotdotdoted {\n+                r.repeat_idx.pop();\n+                r.repeat_len.pop();\n+            }\n+        } else { /* repeat */\n+            *r.repeat_idx.mut_last().unwrap() += 1u;\n+            r.stack.mut_last().unwrap().idx = 0;\n+            match r.stack.last().unwrap().sep.clone() {\n+                Some(tk) => {\n+                    r.cur_tok = tk; /* repeat same span, I guess */\n+                    return ret_val;\n+                }\n+                None => {}\n             }\n         }\n     }\n     loop { /* because it's easiest, this handles `TTDelim` not starting\n-    with a `TTTok`, even though it won't happen */\n-        // FIXME(pcwalton): Bad copy.\n-        match (*r.stack.get().forest.get(r.stack.get().idx.get())).clone() {\n-          TTDelim(tts) => {\n-            r.stack.set(@TtFrame {\n-                forest: tts,\n-                idx: Cell::new(0u),\n-                dotdotdoted: false,\n-                sep: None,\n-                up: Some(r.stack.get())\n-            });\n-            // if this could be 0-length, we'd need to potentially recur here\n-          }\n-          TTTok(sp, tok) => {\n-            r.cur_span.set(sp);\n-            r.cur_tok.set(tok);\n-            r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n-            return ret_val;\n-          }\n-          TTSeq(sp, tts, sep, zerok) => {\n+              with a `TTTok`, even though it won't happen */\n+        let t = {\n+            let frame = r.stack.last().unwrap();\n             // FIXME(pcwalton): Bad copy.\n-            let t = TTSeq(sp, tts, sep.clone(), zerok);\n-            match lockstep_iter_size(&t, r) {\n-              LisUnconstrained => {\n-                r.sp_diag.span_fatal(\n-                    sp, /* blame macro writer */\n-                      \"attempted to repeat an expression \\\n-                       containing no syntax \\\n-                       variables matched as repeating at this depth\");\n-                  }\n-                  LisContradiction(ref msg) => {\n-                      /* FIXME #2887 blame macro invoker instead*/\n-                      r.sp_diag.span_fatal(sp, (*msg));\n-                  }\n-                  LisConstraint(len, _) => {\n-                    if len == 0 {\n-                      if !zerok {\n-                        r.sp_diag.span_fatal(sp, /* FIXME #2887 blame invoker\n-                        */\n-                                             \"this must repeat at least \\\n-                                              once\");\n-                          }\n-\n-                    r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n-                    return tt_next_token(r);\n-                } else {\n-                    r.repeat_len.borrow_mut().push(len);\n-                    r.repeat_idx.borrow_mut().push(0u);\n-                    r.stack.set(@TtFrame {\n-                        forest: tts,\n-                        idx: Cell::new(0u),\n-                        dotdotdoted: true,\n-                        sep: sep,\n-                        up: Some(r.stack.get())\n-                    });\n-                }\n-              }\n+            (*frame.forest.get(frame.idx)).clone()\n+        };\n+        match t {\n+            TTDelim(tts) => {\n+                r.stack.push(TtFrame {\n+                    forest: tts,\n+                    idx: 0,\n+                    dotdotdoted: false,\n+                    sep: None\n+                });\n+                // if this could be 0-length, we'd need to potentially recur here\n             }\n-          }\n-          // FIXME #2887: think about span stuff here\n-          TTNonterminal(sp, ident) => {\n-            match *lookup_cur_matched(r, ident) {\n-              /* sidestep the interpolation tricks for ident because\n-              (a) idents can be in lots of places, so it'd be a pain\n-              (b) we actually can, since it's a token. */\n-              MatchedNonterminal(NtIdent(~sn,b)) => {\n-                r.cur_span.set(sp);\n-                r.cur_tok.set(IDENT(sn,b));\n-                r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n+            TTTok(sp, tok) => {\n+                r.cur_span = sp;\n+                r.cur_tok = tok;\n+                r.stack.mut_last().unwrap().idx += 1;\n                 return ret_val;\n-              }\n-              MatchedNonterminal(ref other_whole_nt) => {\n+            }\n+            TTSeq(sp, tts, sep, zerok) => {\n                 // FIXME(pcwalton): Bad copy.\n-                r.cur_span.set(sp);\n-                r.cur_tok.set(INTERPOLATED((*other_whole_nt).clone()));\n-                r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n-                return ret_val;\n-              }\n-              MatchedSeq(..) => {\n-                r.sp_diag.span_fatal(\n-                    r.cur_span.get(), /* blame the macro writer */\n-                    format!(\"variable '{}' is still repeating at this depth\",\n-                            token::get_ident(ident)));\n-              }\n+                match lockstep_iter_size(&TTSeq(sp, tts.clone(), sep.clone(), zerok), r) {\n+                    LisUnconstrained => {\n+                        r.sp_diag.span_fatal(\n+                            sp.clone(), /* blame macro writer */\n+                            \"attempted to repeat an expression \\\n+                             containing no syntax \\\n+                             variables matched as repeating at this depth\");\n+                        }\n+                        LisContradiction(ref msg) => {\n+                            // FIXME #2887 blame macro invoker instead\n+                            r.sp_diag.span_fatal(sp.clone(), *msg);\n+                        }\n+                    LisConstraint(len, _) => {\n+                        if len == 0 {\n+                            if !zerok {\n+                                // FIXME #2887 blame invoker\n+                                r.sp_diag.span_fatal(sp.clone(),\n+                                                     \"this must repeat at least once\");\n+                            }\n+\n+                            r.stack.mut_last().unwrap().idx += 1;\n+                            return tt_next_token(r);\n+                        }\n+                        r.repeat_len.push(len);\n+                        r.repeat_idx.push(0);\n+                        r.stack.push(TtFrame {\n+                            forest: tts,\n+                            idx: 0,\n+                            dotdotdoted: true,\n+                            sep: sep.clone()\n+                        });\n+                    }\n+                }\n+            }\n+            // FIXME #2887: think about span stuff here\n+            TTNonterminal(sp, ident) => {\n+                r.stack.mut_last().unwrap().idx += 1;\n+                match *lookup_cur_matched(r, ident) {\n+                    /* sidestep the interpolation tricks for ident because\n+                       (a) idents can be in lots of places, so it'd be a pain\n+                       (b) we actually can, since it's a token. */\n+                    MatchedNonterminal(NtIdent(~sn,b)) => {\n+                        r.cur_span = sp;\n+                        r.cur_tok = IDENT(sn,b);\n+                        return ret_val;\n+                    }\n+                    MatchedNonterminal(ref other_whole_nt) => {\n+                        // FIXME(pcwalton): Bad copy.\n+                        r.cur_span = sp;\n+                        r.cur_tok = INTERPOLATED((*other_whole_nt).clone());\n+                        return ret_val;\n+                    }\n+                    MatchedSeq(..) => {\n+                        r.sp_diag.span_fatal(\n+                            r.cur_span, /* blame the macro writer */\n+                            format!(\"variable '{}' is still repeating at this depth\",\n+                                    token::get_ident(ident)));\n+                    }\n+                }\n             }\n-          }\n         }\n     }\n-\n }"}, {"sha": "0f8c74f9ee0716be8d12fd5d9235782e3bbac725", "filename": "src/libsyntax/fold.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Ffold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Ffold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffold.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -16,6 +16,8 @@ use parse::token;\n use owned_slice::OwnedSlice;\n use util::small_vector::SmallVector;\n \n+use std::rc::Rc;\n+\n // We may eventually want to be able to fold over type parameters, too.\n pub trait Folder {\n     fn fold_crate(&mut self, c: Crate) -> Crate {\n@@ -375,10 +377,10 @@ pub fn fold_tts<T: Folder>(tts: &[TokenTree], fld: &mut T) -> Vec<TokenTree> {\n         match *tt {\n             TTTok(span, ref tok) =>\n             TTTok(span,maybe_fold_ident(tok,fld)),\n-            TTDelim(tts) => TTDelim(@fold_tts(tts.as_slice(), fld)),\n-            TTSeq(span, pattern, ref sep, is_optional) =>\n+            TTDelim(ref tts) => TTDelim(Rc::new(fold_tts(tts.as_slice(), fld))),\n+            TTSeq(span, ref pattern, ref sep, is_optional) =>\n             TTSeq(span,\n-                  @fold_tts(pattern.as_slice(), fld),\n+                  Rc::new(fold_tts(pattern.as_slice(), fld)),\n                   sep.as_ref().map(|tok|maybe_fold_ident(tok,fld)),\n                   is_optional),\n             TTNonterminal(sp,ref ident) =>"}, {"sha": "53586a665133cf0de64860e9422c8463251db11f", "filename": "src/libsyntax/parse/comments.rs", "status": "modified", "additions": 30, "deletions": 32, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fcomments.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -133,55 +133,53 @@ pub fn strip_doc_comment_decoration(comment: &str) -> ~str {\n     fail!(\"not a doc-comment: {}\", comment);\n }\n \n-fn read_to_eol(rdr: &StringReader) -> ~str {\n+fn read_to_eol(rdr: &mut StringReader) -> ~str {\n     let mut val = ~\"\";\n     while !rdr.curr_is('\\n') && !is_eof(rdr) {\n-        val.push_char(rdr.curr.get().unwrap());\n+        val.push_char(rdr.curr.unwrap());\n         bump(rdr);\n     }\n     if rdr.curr_is('\\n') { bump(rdr); }\n     return val;\n }\n \n-fn read_one_line_comment(rdr: &StringReader) -> ~str {\n+fn read_one_line_comment(rdr: &mut StringReader) -> ~str {\n     let val = read_to_eol(rdr);\n     assert!((val[0] == '/' as u8 && val[1] == '/' as u8) ||\n                  (val[0] == '#' as u8 && val[1] == '!' as u8));\n     return val;\n }\n \n-fn consume_non_eol_whitespace(rdr: &StringReader) {\n-    while is_whitespace(rdr.curr.get()) && !rdr.curr_is('\\n') &&\n-            !is_eof(rdr) {\n+fn consume_non_eol_whitespace(rdr: &mut StringReader) {\n+    while is_whitespace(rdr.curr) && !rdr.curr_is('\\n') && !is_eof(rdr) {\n         bump(rdr);\n     }\n }\n \n-fn push_blank_line_comment(rdr: &StringReader, comments: &mut Vec<Comment> ) {\n+fn push_blank_line_comment(rdr: &StringReader, comments: &mut Vec<Comment>) {\n     debug!(\">>> blank-line comment\");\n-    let v: Vec<~str> = Vec::new();\n     comments.push(Comment {\n         style: BlankLine,\n-        lines: v,\n-        pos: rdr.last_pos.get(),\n+        lines: Vec::new(),\n+        pos: rdr.last_pos,\n     });\n }\n \n-fn consume_whitespace_counting_blank_lines(rdr: &StringReader,\n-                                           comments: &mut Vec<Comment> ) {\n-    while is_whitespace(rdr.curr.get()) && !is_eof(rdr) {\n-        if rdr.col.get() == CharPos(0u) && rdr.curr_is('\\n') {\n+fn consume_whitespace_counting_blank_lines(rdr: &mut StringReader,\n+                                           comments: &mut Vec<Comment>) {\n+    while is_whitespace(rdr.curr) && !is_eof(rdr) {\n+        if rdr.col == CharPos(0u) && rdr.curr_is('\\n') {\n             push_blank_line_comment(rdr, &mut *comments);\n         }\n         bump(rdr);\n     }\n }\n \n \n-fn read_shebang_comment(rdr: &StringReader, code_to_the_left: bool,\n-                                            comments: &mut Vec<Comment> ) {\n+fn read_shebang_comment(rdr: &mut StringReader, code_to_the_left: bool,\n+                        comments: &mut Vec<Comment>) {\n     debug!(\">>> shebang comment\");\n-    let p = rdr.last_pos.get();\n+    let p = rdr.last_pos;\n     debug!(\"<<< shebang comment\");\n     comments.push(Comment {\n         style: if code_to_the_left { Trailing } else { Isolated },\n@@ -190,10 +188,10 @@ fn read_shebang_comment(rdr: &StringReader, code_to_the_left: bool,\n     });\n }\n \n-fn read_line_comments(rdr: &StringReader, code_to_the_left: bool,\n-                                          comments: &mut Vec<Comment> ) {\n+fn read_line_comments(rdr: &mut StringReader, code_to_the_left: bool,\n+                      comments: &mut Vec<Comment>) {\n     debug!(\">>> line comments\");\n-    let p = rdr.last_pos.get();\n+    let p = rdr.last_pos;\n     let mut lines: Vec<~str> = Vec::new();\n     while rdr.curr_is('/') && nextch_is(rdr, '/') {\n         let line = read_one_line_comment(rdr);\n@@ -247,13 +245,13 @@ fn trim_whitespace_prefix_and_push_line(lines: &mut Vec<~str> ,\n     lines.push(s1);\n }\n \n-fn read_block_comment(rdr: &StringReader,\n+fn read_block_comment(rdr: &mut StringReader,\n                       code_to_the_left: bool,\n                       comments: &mut Vec<Comment> ) {\n     debug!(\">>> block comment\");\n-    let p = rdr.last_pos.get();\n+    let p = rdr.last_pos;\n     let mut lines: Vec<~str> = Vec::new();\n-    let col: CharPos = rdr.col.get();\n+    let col = rdr.col;\n     bump(rdr);\n     bump(rdr);\n \n@@ -262,7 +260,7 @@ fn read_block_comment(rdr: &StringReader,\n     // doc-comments are not really comments, they are attributes\n     if (rdr.curr_is('*') && !nextch_is(rdr, '*')) || rdr.curr_is('!') {\n         while !(rdr.curr_is('*') && nextch_is(rdr, '/')) && !is_eof(rdr) {\n-            curr_line.push_char(rdr.curr.get().unwrap());\n+            curr_line.push_char(rdr.curr.unwrap());\n             bump(rdr);\n         }\n         if !is_eof(rdr) {\n@@ -286,7 +284,7 @@ fn read_block_comment(rdr: &StringReader,\n                 curr_line = ~\"\";\n                 bump(rdr);\n             } else {\n-                curr_line.push_char(rdr.curr.get().unwrap());\n+                curr_line.push_char(rdr.curr.unwrap());\n                 if rdr.curr_is('/') && nextch_is(rdr, '*') {\n                     bump(rdr);\n                     bump(rdr);\n@@ -324,7 +322,7 @@ fn peeking_at_comment(rdr: &StringReader) -> bool {\n           !lexer::nextnextch_is(rdr, '['));\n }\n \n-fn consume_comment(rdr: &StringReader,\n+fn consume_comment(rdr: &mut StringReader,\n                    code_to_the_left: bool,\n                    comments: &mut Vec<Comment> ) {\n     debug!(\">>> consume comment\");\n@@ -355,28 +353,28 @@ pub fn gather_comments_and_literals(span_diagnostic:\n     let src = str::from_utf8_owned(src).unwrap();\n     let cm = CodeMap::new();\n     let filemap = cm.new_filemap(path, src);\n-    let rdr = lexer::new_low_level_string_reader(span_diagnostic, filemap);\n+    let mut rdr = lexer::new_low_level_string_reader(span_diagnostic, filemap);\n \n     let mut comments: Vec<Comment> = Vec::new();\n     let mut literals: Vec<Literal> = Vec::new();\n     let mut first_read: bool = true;\n     while !is_eof(&rdr) {\n         loop {\n             let mut code_to_the_left = !first_read;\n-            consume_non_eol_whitespace(&rdr);\n+            consume_non_eol_whitespace(&mut rdr);\n             if rdr.curr_is('\\n') {\n                 code_to_the_left = false;\n-                consume_whitespace_counting_blank_lines(&rdr, &mut comments);\n+                consume_whitespace_counting_blank_lines(&mut rdr, &mut comments);\n             }\n             while peeking_at_comment(&rdr) {\n-                consume_comment(&rdr, code_to_the_left, &mut comments);\n-                consume_whitespace_counting_blank_lines(&rdr, &mut comments);\n+                consume_comment(&mut rdr, code_to_the_left, &mut comments);\n+                consume_whitespace_counting_blank_lines(&mut rdr, &mut comments);\n             }\n             break;\n         }\n \n \n-        let bstart = rdr.last_pos.get();\n+        let bstart = rdr.last_pos;\n         rdr.next_token();\n         //discard, and look ahead; we're working with internal state\n         let TokenAndSpan {tok: tok, sp: sp} = rdr.peek();"}, {"sha": "c18571deaf5bee85cf445eb99cdc1acb7058d4e8", "filename": "src/libsyntax/parse/lexer.rs", "status": "modified", "additions": 150, "deletions": 171, "changes": 321, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -12,11 +12,10 @@ use ast;\n use codemap::{BytePos, CharPos, CodeMap, Pos, Span};\n use codemap;\n use diagnostic::SpanHandler;\n-use ext::tt::transcribe::{dup_tt_reader, tt_next_token};\n+use ext::tt::transcribe::tt_next_token;\n use parse::token;\n use parse::token::{str_to_ident};\n \n-use std::cell::{Cell, RefCell};\n use std::char;\n use std::mem::replace;\n use std::num::from_str_radix;\n@@ -27,11 +26,10 @@ pub use ext::tt::transcribe::{TtReader, new_tt_reader};\n \n pub trait Reader {\n     fn is_eof(&self) -> bool;\n-    fn next_token(&self) -> TokenAndSpan;\n+    fn next_token(&mut self) -> TokenAndSpan;\n     fn fatal(&self, ~str) -> !;\n     fn span_diag<'a>(&'a self) -> &'a SpanHandler;\n     fn peek(&self) -> TokenAndSpan;\n-    fn dup(&self) -> ~Reader:;\n }\n \n #[deriving(Clone, Eq, Show)]\n@@ -43,30 +41,30 @@ pub struct TokenAndSpan {\n pub struct StringReader<'a> {\n     span_diagnostic: &'a SpanHandler,\n     // The absolute offset within the codemap of the next character to read\n-    pos: Cell<BytePos>,\n+    pos: BytePos,\n     // The absolute offset within the codemap of the last character read(curr)\n-    last_pos: Cell<BytePos>,\n+    last_pos: BytePos,\n     // The column of the next character to read\n-    col: Cell<CharPos>,\n+    col: CharPos,\n     // The last character to be read\n-    curr: Cell<Option<char>>,\n+    curr: Option<char>,\n     filemap: Rc<codemap::FileMap>,\n     /* cached: */\n-    peek_tok: RefCell<token::Token>,\n-    peek_span: RefCell<Span>,\n+    peek_tok: token::Token,\n+    peek_span: Span,\n }\n \n impl<'a> StringReader<'a> {\n     pub fn curr_is(&self, c: char) -> bool {\n-        self.curr.get() == Some(c)\n+        self.curr == Some(c)\n     }\n }\n \n pub fn new_string_reader<'a>(span_diagnostic: &'a SpanHandler,\n                              filemap: Rc<codemap::FileMap>)\n                              -> StringReader<'a> {\n-    let r = new_low_level_string_reader(span_diagnostic, filemap);\n-    string_advance_token(&r); /* fill in peek_* */\n+    let mut r = new_low_level_string_reader(span_diagnostic, filemap);\n+    string_advance_token(&mut r); /* fill in peek_* */\n     r\n }\n \n@@ -76,97 +74,79 @@ pub fn new_low_level_string_reader<'a>(span_diagnostic: &'a SpanHandler,\n                                        -> StringReader<'a> {\n     // Force the initial reader bump to start on a fresh line\n     let initial_char = '\\n';\n-    let r = StringReader {\n+    let mut r = StringReader {\n         span_diagnostic: span_diagnostic,\n-        pos: Cell::new(filemap.start_pos),\n-        last_pos: Cell::new(filemap.start_pos),\n-        col: Cell::new(CharPos(0)),\n-        curr: Cell::new(Some(initial_char)),\n+        pos: filemap.start_pos,\n+        last_pos: filemap.start_pos,\n+        col: CharPos(0),\n+        curr: Some(initial_char),\n         filemap: filemap,\n         /* dummy values; not read */\n-        peek_tok: RefCell::new(token::EOF),\n-        peek_span: RefCell::new(codemap::DUMMY_SP),\n+        peek_tok: token::EOF,\n+        peek_span: codemap::DUMMY_SP,\n     };\n-    bump(&r);\n+    bump(&mut r);\n     r\n }\n \n-// duplicating the string reader is probably a bad idea, in\n-// that using them will cause interleaved pushes of line\n-// offsets to the underlying filemap...\n-fn dup_string_reader<'a>(r: &StringReader<'a>) -> StringReader<'a> {\n-    StringReader {\n-        span_diagnostic: r.span_diagnostic,\n-        pos: Cell::new(r.pos.get()),\n-        last_pos: Cell::new(r.last_pos.get()),\n-        col: Cell::new(r.col.get()),\n-        curr: Cell::new(r.curr.get()),\n-        filemap: r.filemap.clone(),\n-        peek_tok: r.peek_tok.clone(),\n-        peek_span: r.peek_span.clone(),\n-    }\n-}\n-\n impl<'a> Reader for StringReader<'a> {\n     fn is_eof(&self) -> bool { is_eof(self) }\n     // return the next token. EFFECT: advances the string_reader.\n-    fn next_token(&self) -> TokenAndSpan {\n+    fn next_token(&mut self) -> TokenAndSpan {\n         let ret_val = TokenAndSpan {\n-            tok: replace(&mut *self.peek_tok.borrow_mut(), token::UNDERSCORE),\n-            sp: self.peek_span.get(),\n+            tok: replace(&mut self.peek_tok, token::UNDERSCORE),\n+            sp: self.peek_span,\n         };\n         string_advance_token(self);\n         ret_val\n     }\n     fn fatal(&self, m: ~str) -> ! {\n-        self.span_diagnostic.span_fatal(self.peek_span.get(), m)\n+        self.span_diagnostic.span_fatal(self.peek_span, m)\n     }\n     fn span_diag<'a>(&'a self) -> &'a SpanHandler { self.span_diagnostic }\n     fn peek(&self) -> TokenAndSpan {\n         // FIXME(pcwalton): Bad copy!\n         TokenAndSpan {\n-            tok: self.peek_tok.get(),\n-            sp: self.peek_span.get(),\n+            tok: self.peek_tok.clone(),\n+            sp: self.peek_span.clone(),\n         }\n     }\n-    fn dup(&self) -> ~Reader: { ~dup_string_reader(self) as ~Reader: }\n }\n \n impl<'a> Reader for TtReader<'a> {\n     fn is_eof(&self) -> bool {\n-        *self.cur_tok.borrow() == token::EOF\n+        self.cur_tok == token::EOF\n     }\n-    fn next_token(&self) -> TokenAndSpan {\n+    fn next_token(&mut self) -> TokenAndSpan {\n         let r = tt_next_token(self);\n         debug!(\"TtReader: r={:?}\", r);\n-        return r;\n+        r\n     }\n     fn fatal(&self, m: ~str) -> ! {\n-        self.sp_diag.span_fatal(self.cur_span.get(), m);\n+        self.sp_diag.span_fatal(self.cur_span, m);\n     }\n     fn span_diag<'a>(&'a self) -> &'a SpanHandler { self.sp_diag }\n     fn peek(&self) -> TokenAndSpan {\n         TokenAndSpan {\n-            tok: self.cur_tok.get(),\n-            sp: self.cur_span.get(),\n+            tok: self.cur_tok.clone(),\n+            sp: self.cur_span.clone(),\n         }\n     }\n-    fn dup(&self) -> ~Reader: { ~dup_tt_reader(self) as ~Reader: }\n }\n \n // report a lexical error spanning [`from_pos`, `to_pos`)\n-fn fatal_span(rdr: &StringReader,\n+fn fatal_span(rdr: &mut StringReader,\n               from_pos: BytePos,\n               to_pos: BytePos,\n               m: ~str)\n            -> ! {\n-    rdr.peek_span.set(codemap::mk_sp(from_pos, to_pos));\n+    rdr.peek_span = codemap::mk_sp(from_pos, to_pos);\n     rdr.fatal(m);\n }\n \n // report a lexical error spanning [`from_pos`, `to_pos`), appending an\n // escaped character to the error message\n-fn fatal_span_char(rdr: &StringReader,\n+fn fatal_span_char(rdr: &mut StringReader,\n                    from_pos: BytePos,\n                    to_pos: BytePos,\n                    m: ~str,\n@@ -180,36 +160,35 @@ fn fatal_span_char(rdr: &StringReader,\n \n // report a lexical error spanning [`from_pos`, `to_pos`), appending the\n // offending string to the error message\n-fn fatal_span_verbose(rdr: &StringReader,\n+fn fatal_span_verbose(rdr: &mut StringReader,\n                       from_pos: BytePos,\n                       to_pos: BytePos,\n                       m: ~str)\n                    -> ! {\n     let mut m = m;\n     m.push_str(\": \");\n-    let s = rdr.filemap.src.slice(\n-                  byte_offset(rdr, from_pos).to_uint(),\n-                  byte_offset(rdr, to_pos).to_uint());\n-    m.push_str(s);\n+    let from = byte_offset(rdr, from_pos).to_uint();\n+    let to = byte_offset(rdr, to_pos).to_uint();\n+    m.push_str(rdr.filemap.src.slice(from, to));\n     fatal_span(rdr, from_pos, to_pos, m);\n }\n \n // EFFECT: advance peek_tok and peek_span to refer to the next token.\n // EFFECT: update the interner, maybe.\n-fn string_advance_token(r: &StringReader) {\n+fn string_advance_token(r: &mut StringReader) {\n     match consume_whitespace_and_comments(r) {\n         Some(comment) => {\n-            r.peek_span.set(comment.sp);\n-            r.peek_tok.set(comment.tok);\n+            r.peek_span = comment.sp;\n+            r.peek_tok = comment.tok;\n         },\n         None => {\n             if is_eof(r) {\n-                r.peek_tok.set(token::EOF);\n+                r.peek_tok = token::EOF;\n             } else {\n-                let start_bytepos = r.last_pos.get();\n-                r.peek_tok.set(next_token_inner(r));\n-                r.peek_span.set(codemap::mk_sp(start_bytepos,\n-                                               r.last_pos.get()));\n+                let start_bytepos = r.last_pos;\n+                r.peek_tok = next_token_inner(r);\n+                r.peek_span = codemap::mk_sp(start_bytepos,\n+                                             r.last_pos);\n             };\n         }\n     }\n@@ -227,7 +206,7 @@ pub fn with_str_from<T>(\n                      start: BytePos,\n                      f: |s: &str| -> T)\n                      -> T {\n-    with_str_from_to(rdr, start, rdr.last_pos.get(), f)\n+    with_str_from_to(rdr, start, rdr.last_pos, f)\n }\n \n /// Calls `f` with astring slice of the source text spanning from `start`\n@@ -245,36 +224,36 @@ fn with_str_from_to<T>(\n \n // EFFECT: advance the StringReader by one character. If a newline is\n // discovered, add it to the FileMap's list of line start offsets.\n-pub fn bump(rdr: &StringReader) {\n-    rdr.last_pos.set(rdr.pos.get());\n-    let current_byte_offset = byte_offset(rdr, rdr.pos.get()).to_uint();\n+pub fn bump(rdr: &mut StringReader) {\n+    rdr.last_pos = rdr.pos;\n+    let current_byte_offset = byte_offset(rdr, rdr.pos).to_uint();\n     if current_byte_offset < rdr.filemap.src.len() {\n-        assert!(rdr.curr.get().is_some());\n-        let last_char = rdr.curr.get().unwrap();\n+        assert!(rdr.curr.is_some());\n+        let last_char = rdr.curr.unwrap();\n         let next = rdr.filemap.src.char_range_at(current_byte_offset);\n         let byte_offset_diff = next.next - current_byte_offset;\n-        rdr.pos.set(rdr.pos.get() + Pos::from_uint(byte_offset_diff));\n-        rdr.curr.set(Some(next.ch));\n-        rdr.col.set(rdr.col.get() + CharPos(1u));\n+        rdr.pos = rdr.pos + Pos::from_uint(byte_offset_diff);\n+        rdr.curr = Some(next.ch);\n+        rdr.col = rdr.col + CharPos(1u);\n         if last_char == '\\n' {\n-            rdr.filemap.next_line(rdr.last_pos.get());\n-            rdr.col.set(CharPos(0u));\n+            rdr.filemap.next_line(rdr.last_pos);\n+            rdr.col = CharPos(0u);\n         }\n \n         if byte_offset_diff > 1 {\n-            rdr.filemap.record_multibyte_char(rdr.last_pos.get(), byte_offset_diff);\n+            rdr.filemap.record_multibyte_char(rdr.last_pos, byte_offset_diff);\n         }\n     } else {\n-        rdr.curr.set(None);\n+        rdr.curr = None;\n     }\n }\n \n pub fn is_eof(rdr: &StringReader) -> bool {\n-    rdr.curr.get().is_none()\n+    rdr.curr.is_none()\n }\n \n pub fn nextch(rdr: &StringReader) -> Option<char> {\n-    let offset = byte_offset(rdr, rdr.pos.get()).to_uint();\n+    let offset = byte_offset(rdr, rdr.pos).to_uint();\n     if offset < rdr.filemap.src.len() {\n         Some(rdr.filemap.src.char_at(offset))\n     } else {\n@@ -286,7 +265,7 @@ pub fn nextch_is(rdr: &StringReader, c: char) -> bool {\n }\n \n pub fn nextnextch(rdr: &StringReader) -> Option<char> {\n-    let offset = byte_offset(rdr, rdr.pos.get()).to_uint();\n+    let offset = byte_offset(rdr, rdr.pos).to_uint();\n     let s = rdr.filemap.deref().src.as_slice();\n     if offset >= s.len() { return None }\n     let str::CharRange { next, .. } = s.char_range_at(offset);\n@@ -332,9 +311,9 @@ fn is_hex_digit(c: Option<char>) -> bool {\n \n // EFFECT: eats whitespace and comments.\n // returns a Some(sugared-doc-attr) if one exists, None otherwise.\n-fn consume_whitespace_and_comments(rdr: &StringReader)\n+fn consume_whitespace_and_comments(rdr: &mut StringReader)\n                                 -> Option<TokenAndSpan> {\n-    while is_whitespace(rdr.curr.get()) { bump(rdr); }\n+    while is_whitespace(rdr.curr) { bump(rdr); }\n     return consume_any_line_comment(rdr);\n }\n \n@@ -345,7 +324,7 @@ pub fn is_line_non_doc_comment(s: &str) -> bool {\n // PRECONDITION: rdr.curr is not whitespace\n // EFFECT: eats any kind of comment.\n // returns a Some(sugared-doc-attr) if one exists, None otherwise\n-fn consume_any_line_comment(rdr: &StringReader)\n+fn consume_any_line_comment(rdr: &mut StringReader)\n                          -> Option<TokenAndSpan> {\n     if rdr.curr_is('/') {\n         match nextch(rdr) {\n@@ -354,7 +333,7 @@ fn consume_any_line_comment(rdr: &StringReader)\n                 bump(rdr);\n                 // line comments starting with \"///\" or \"//!\" are doc-comments\n                 if rdr.curr_is('/') || rdr.curr_is('!') {\n-                    let start_bpos = rdr.pos.get() - BytePos(3);\n+                    let start_bpos = rdr.pos - BytePos(3);\n                     while !rdr.curr_is('\\n') && !is_eof(rdr) {\n                         bump(rdr);\n                     }\n@@ -363,7 +342,7 @@ fn consume_any_line_comment(rdr: &StringReader)\n                         if !is_line_non_doc_comment(string) {\n                             Some(TokenAndSpan{\n                                 tok: token::DOC_COMMENT(str_to_ident(string)),\n-                                sp: codemap::mk_sp(start_bpos, rdr.pos.get())\n+                                sp: codemap::mk_sp(start_bpos, rdr.pos)\n                             })\n                         } else {\n                             None\n@@ -394,7 +373,7 @@ fn consume_any_line_comment(rdr: &StringReader)\n             // we're at the beginning of the file...\n             let cmap = CodeMap::new();\n             cmap.files.borrow_mut().push(rdr.filemap.clone());\n-            let loc = cmap.lookup_char_pos_adj(rdr.last_pos.get());\n+            let loc = cmap.lookup_char_pos_adj(rdr.last_pos);\n             if loc.line == 1u && loc.col == CharPos(0u) {\n                 while !rdr.curr_is('\\n') && !is_eof(rdr) { bump(rdr); }\n                 return consume_whitespace_and_comments(rdr);\n@@ -411,10 +390,10 @@ pub fn is_block_non_doc_comment(s: &str) -> bool {\n }\n \n // might return a sugared-doc-attr\n-fn consume_block_comment(rdr: &StringReader) -> Option<TokenAndSpan> {\n+fn consume_block_comment(rdr: &mut StringReader) -> Option<TokenAndSpan> {\n     // block comments starting with \"/**\" or \"/*!\" are doc-comments\n     let is_doc_comment = rdr.curr_is('*') || rdr.curr_is('!');\n-    let start_bpos = rdr.pos.get() - BytePos(if is_doc_comment {3} else {2});\n+    let start_bpos = rdr.pos - BytePos(if is_doc_comment {3} else {2});\n \n     let mut level: int = 1;\n     while level > 0 {\n@@ -424,7 +403,7 @@ fn consume_block_comment(rdr: &StringReader) -> Option<TokenAndSpan> {\n             } else {\n                 ~\"unterminated block comment\"\n             };\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(), msg);\n+            fatal_span(rdr, start_bpos, rdr.last_pos, msg);\n         } else if rdr.curr_is('/') && nextch_is(rdr, '*') {\n             level += 1;\n             bump(rdr);\n@@ -444,7 +423,7 @@ fn consume_block_comment(rdr: &StringReader) -> Option<TokenAndSpan> {\n             if !is_block_non_doc_comment(string) {\n                 Some(TokenAndSpan{\n                         tok: token::DOC_COMMENT(str_to_ident(string)),\n-                        sp: codemap::mk_sp(start_bpos, rdr.pos.get())\n+                        sp: codemap::mk_sp(start_bpos, rdr.pos)\n                     })\n             } else {\n                 None\n@@ -458,14 +437,14 @@ fn consume_block_comment(rdr: &StringReader) -> Option<TokenAndSpan> {\n     if res.is_some() { res } else { consume_whitespace_and_comments(rdr) }\n }\n \n-fn scan_exponent(rdr: &StringReader, start_bpos: BytePos) -> Option<~str> {\n+fn scan_exponent(rdr: &mut StringReader, start_bpos: BytePos) -> Option<~str> {\n     // \\x00 hits the `return None` case immediately, so this is fine.\n-    let mut c = rdr.curr.get().unwrap_or('\\x00');\n+    let mut c = rdr.curr.unwrap_or('\\x00');\n     let mut rslt = ~\"\";\n     if c == 'e' || c == 'E' {\n         rslt.push_char(c);\n         bump(rdr);\n-        c = rdr.curr.get().unwrap_or('\\x00');\n+        c = rdr.curr.unwrap_or('\\x00');\n         if c == '-' || c == '+' {\n             rslt.push_char(c);\n             bump(rdr);\n@@ -474,16 +453,16 @@ fn scan_exponent(rdr: &StringReader, start_bpos: BytePos) -> Option<~str> {\n         if exponent.len() > 0u {\n             return Some(rslt + exponent);\n         } else {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"scan_exponent: bad fp literal\");\n         }\n     } else { return None::<~str>; }\n }\n \n-fn scan_digits(rdr: &StringReader, radix: uint) -> ~str {\n+fn scan_digits(rdr: &mut StringReader, radix: uint) -> ~str {\n     let mut rslt = ~\"\";\n     loop {\n-        let c = rdr.curr.get();\n+        let c = rdr.curr;\n         if c == Some('_') { bump(rdr); continue; }\n         match c.and_then(|cc| char::to_digit(cc, radix)) {\n           Some(_) => {\n@@ -495,7 +474,7 @@ fn scan_digits(rdr: &StringReader, radix: uint) -> ~str {\n     };\n }\n \n-fn check_float_base(rdr: &StringReader, start_bpos: BytePos, last_bpos: BytePos,\n+fn check_float_base(rdr: &mut StringReader, start_bpos: BytePos, last_bpos: BytePos,\n                     base: uint) {\n     match base {\n       16u => fatal_span(rdr, start_bpos, last_bpos,\n@@ -508,12 +487,12 @@ fn check_float_base(rdr: &StringReader, start_bpos: BytePos, last_bpos: BytePos,\n     }\n }\n \n-fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n+fn scan_number(c: char, rdr: &mut StringReader) -> token::Token {\n     let mut num_str;\n     let mut base = 10u;\n     let mut c = c;\n     let mut n = nextch(rdr).unwrap_or('\\x00');\n-    let start_bpos = rdr.last_pos.get();\n+    let start_bpos = rdr.last_pos;\n     if c == '0' && n == 'x' {\n         bump(rdr);\n         bump(rdr);\n@@ -528,7 +507,7 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n         base = 2u;\n     }\n     num_str = scan_digits(rdr, base);\n-    c = rdr.curr.get().unwrap_or('\\x00');\n+    c = rdr.curr.unwrap_or('\\x00');\n     nextch(rdr);\n     if c == 'u' || c == 'i' {\n         enum Result { Signed(ast::IntTy), Unsigned(ast::UintTy) }\n@@ -538,7 +517,7 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n             else { Unsigned(ast::TyU) }\n         };\n         bump(rdr);\n-        c = rdr.curr.get().unwrap_or('\\x00');\n+        c = rdr.curr.unwrap_or('\\x00');\n         if c == '8' {\n             bump(rdr);\n             tp = if signed { Signed(ast::TyI8) }\n@@ -562,12 +541,12 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n                       else { Unsigned(ast::TyU64) };\n         }\n         if num_str.len() == 0u {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"no valid digits found for number\");\n         }\n         let parsed = match from_str_radix::<u64>(num_str, base as uint) {\n             Some(p) => p,\n-            None => fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            None => fatal_span(rdr, start_bpos, rdr.last_pos,\n                                ~\"int literal is too large\")\n         };\n \n@@ -594,37 +573,37 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n \n     if rdr.curr_is('f') {\n         bump(rdr);\n-        c = rdr.curr.get().unwrap_or('\\x00');\n+        c = rdr.curr.unwrap_or('\\x00');\n         n = nextch(rdr).unwrap_or('\\x00');\n         if c == '3' && n == '2' {\n             bump(rdr);\n             bump(rdr);\n-            check_float_base(rdr, start_bpos, rdr.last_pos.get(), base);\n+            check_float_base(rdr, start_bpos, rdr.last_pos, base);\n             return token::LIT_FLOAT(str_to_ident(num_str), ast::TyF32);\n         } else if c == '6' && n == '4' {\n             bump(rdr);\n             bump(rdr);\n-            check_float_base(rdr, start_bpos, rdr.last_pos.get(), base);\n+            check_float_base(rdr, start_bpos, rdr.last_pos, base);\n             return token::LIT_FLOAT(str_to_ident(num_str), ast::TyF64);\n             /* FIXME (#2252): if this is out of range for either a\n             32-bit or 64-bit float, it won't be noticed till the\n             back-end.  */\n         } else {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"expected `f32` or `f64` suffix\");\n         }\n     }\n     if is_float {\n-        check_float_base(rdr, start_bpos, rdr.last_pos.get(), base);\n+        check_float_base(rdr, start_bpos, rdr.last_pos, base);\n         return token::LIT_FLOAT_UNSUFFIXED(str_to_ident(num_str));\n     } else {\n         if num_str.len() == 0u {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"no valid digits found for number\");\n         }\n         let parsed = match from_str_radix::<u64>(num_str, base as uint) {\n             Some(p) => p,\n-            None => fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            None => fatal_span(rdr, start_bpos, rdr.last_pos,\n                                ~\"int literal is too large\")\n         };\n \n@@ -633,14 +612,14 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n     }\n }\n \n-fn scan_numeric_escape(rdr: &StringReader, n_hex_digits: uint) -> char {\n+fn scan_numeric_escape(rdr: &mut StringReader, n_hex_digits: uint) -> char {\n     let mut accum_int = 0;\n     let mut i = n_hex_digits;\n-    let start_bpos = rdr.last_pos.get();\n+    let start_bpos = rdr.last_pos;\n     while i != 0u && !is_eof(rdr) {\n-        let n = rdr.curr.get();\n+        let n = rdr.curr;\n         if !is_hex_digit(n) {\n-            fatal_span_char(rdr, rdr.last_pos.get(), rdr.pos.get(),\n+            fatal_span_char(rdr, rdr.last_pos, rdr.pos,\n                             ~\"illegal character in numeric character escape\",\n                             n.unwrap());\n         }\n@@ -650,13 +629,13 @@ fn scan_numeric_escape(rdr: &StringReader, n_hex_digits: uint) -> char {\n         i -= 1u;\n     }\n     if i != 0 && is_eof(rdr) {\n-        fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+        fatal_span(rdr, start_bpos, rdr.last_pos,\n                    ~\"unterminated numeric character escape\");\n     }\n \n     match char::from_u32(accum_int as u32) {\n         Some(x) => x,\n-        None => fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+        None => fatal_span(rdr, start_bpos, rdr.last_pos,\n                            ~\"illegal numeric character escape\")\n     }\n }\n@@ -683,14 +662,14 @@ fn ident_continue(c: Option<char>) -> bool {\n // return the next token from the string\n // EFFECT: advances the input past that token\n // EFFECT: updates the interner\n-fn next_token_inner(rdr: &StringReader) -> token::Token {\n-    let c = rdr.curr.get();\n+fn next_token_inner(rdr: &mut StringReader) -> token::Token {\n+    let c = rdr.curr;\n     if ident_start(c) && !nextch_is(rdr, '\"') && !nextch_is(rdr, '#') {\n         // Note: r as in r\" or r#\" is part of a raw string literal,\n         // not an identifier, and is handled further down.\n \n-        let start = rdr.last_pos.get();\n-        while ident_continue(rdr.curr.get()) {\n+        let start = rdr.last_pos;\n+        while ident_continue(rdr.curr) {\n             bump(rdr);\n         }\n \n@@ -708,7 +687,7 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n     if is_dec_digit(c) {\n         return scan_number(c.unwrap(), rdr);\n     }\n-    fn binop(rdr: &StringReader, op: token::BinOp) -> token::Token {\n+    fn binop(rdr: &mut StringReader, op: token::BinOp) -> token::Token {\n         bump(rdr);\n         if rdr.curr_is('=') {\n             bump(rdr);\n@@ -783,12 +762,12 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n       }\n       '<' => {\n         bump(rdr);\n-        match rdr.curr.get().unwrap_or('\\x00') {\n+        match rdr.curr.unwrap_or('\\x00') {\n           '=' => { bump(rdr); return token::LE; }\n           '<' => { return binop(rdr, token::SHL); }\n           '-' => {\n             bump(rdr);\n-            match rdr.curr.get().unwrap_or('\\x00') {\n+            match rdr.curr.unwrap_or('\\x00') {\n               '>' => { bump(rdr); return token::DARROW; }\n               _ => { return token::LARROW; }\n             }\n@@ -798,7 +777,7 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n       }\n       '>' => {\n         bump(rdr);\n-        match rdr.curr.get().unwrap_or('\\x00') {\n+        match rdr.curr.unwrap_or('\\x00') {\n           '=' => { bump(rdr); return token::GE; }\n           '>' => { return binop(rdr, token::SHR); }\n           _ => { return token::GT; }\n@@ -807,41 +786,41 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n       '\\'' => {\n         // Either a character constant 'a' OR a lifetime name 'abc\n         bump(rdr);\n-        let start = rdr.last_pos.get();\n+        let start = rdr.last_pos;\n \n         // the eof will be picked up by the final `'` check below\n-        let mut c2 = rdr.curr.get().unwrap_or('\\x00');\n+        let mut c2 = rdr.curr.unwrap_or('\\x00');\n         bump(rdr);\n \n         // If the character is an ident start not followed by another single\n         // quote, then this is a lifetime name:\n         if ident_start(Some(c2)) && !rdr.curr_is('\\'') {\n-            while ident_continue(rdr.curr.get()) {\n+            while ident_continue(rdr.curr) {\n                 bump(rdr);\n             }\n-            return with_str_from(rdr, start, |lifetime_name| {\n-                let ident = str_to_ident(lifetime_name);\n-                let tok = &token::IDENT(ident, false);\n-\n-                if token::is_keyword(token::keywords::Self, tok) {\n-                    fatal_span(rdr, start, rdr.last_pos.get(),\n-                               ~\"invalid lifetime name: 'self is no longer a special lifetime\");\n-                } else if token::is_any_keyword(tok) &&\n-                    !token::is_keyword(token::keywords::Static, tok) {\n-                    fatal_span(rdr, start, rdr.last_pos.get(),\n-                               ~\"invalid lifetime name\");\n-                } else {\n-                    token::LIFETIME(ident)\n-                }\n-            })\n+            let ident = with_str_from(rdr, start, |lifetime_name| {\n+                str_to_ident(lifetime_name)\n+            });\n+            let tok = &token::IDENT(ident, false);\n+\n+            if token::is_keyword(token::keywords::Self, tok) {\n+                fatal_span(rdr, start, rdr.last_pos,\n+                           ~\"invalid lifetime name: 'self is no longer a special lifetime\");\n+            } else if token::is_any_keyword(tok) &&\n+                !token::is_keyword(token::keywords::Static, tok) {\n+                fatal_span(rdr, start, rdr.last_pos,\n+                           ~\"invalid lifetime name\");\n+            } else {\n+                return token::LIFETIME(ident);\n+            }\n         }\n \n         // Otherwise it is a character constant:\n         match c2 {\n             '\\\\' => {\n                 // '\\X' for some X must be a character constant:\n-                let escaped = rdr.curr.get();\n-                let escaped_pos = rdr.last_pos.get();\n+                let escaped = rdr.curr;\n+                let escaped_pos = rdr.last_pos;\n                 bump(rdr);\n                 match escaped {\n                     None => {}\n@@ -858,15 +837,15 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n                             'u' => scan_numeric_escape(rdr, 4u),\n                             'U' => scan_numeric_escape(rdr, 8u),\n                             c2 => {\n-                                fatal_span_char(rdr, escaped_pos, rdr.last_pos.get(),\n+                                fatal_span_char(rdr, escaped_pos, rdr.last_pos,\n                                                 ~\"unknown character escape\", c2)\n                             }\n                         }\n                     }\n                 }\n             }\n             '\\t' | '\\n' | '\\r' | '\\'' => {\n-                fatal_span_char(rdr, start, rdr.last_pos.get(),\n+                fatal_span_char(rdr, start, rdr.last_pos,\n                                 ~\"character constant must be escaped\", c2);\n             }\n             _ => {}\n@@ -877,33 +856,33 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n                                // character before position `start` is an\n                                // ascii single quote.\n                                start - BytePos(1),\n-                               rdr.last_pos.get(),\n+                               rdr.last_pos,\n                                ~\"unterminated character constant\");\n         }\n         bump(rdr); // advance curr past token\n         return token::LIT_CHAR(c2 as u32);\n       }\n       '\"' => {\n         let mut accum_str = ~\"\";\n-        let start_bpos = rdr.last_pos.get();\n+        let start_bpos = rdr.last_pos;\n         bump(rdr);\n         while !rdr.curr_is('\"') {\n             if is_eof(rdr) {\n-                fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+                fatal_span(rdr, start_bpos, rdr.last_pos,\n                            ~\"unterminated double quote string\");\n             }\n \n-            let ch = rdr.curr.get().unwrap();\n+            let ch = rdr.curr.unwrap();\n             bump(rdr);\n             match ch {\n               '\\\\' => {\n                 if is_eof(rdr) {\n-                    fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+                    fatal_span(rdr, start_bpos, rdr.last_pos,\n                            ~\"unterminated double quote string\");\n                 }\n \n-                let escaped = rdr.curr.get().unwrap();\n-                let escaped_pos = rdr.last_pos.get();\n+                let escaped = rdr.curr.unwrap();\n+                let escaped_pos = rdr.last_pos;\n                 bump(rdr);\n                 match escaped {\n                   'n' => accum_str.push_char('\\n'),\n@@ -924,7 +903,7 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n                     accum_str.push_char(scan_numeric_escape(rdr, 8u));\n                   }\n                   c2 => {\n-                    fatal_span_char(rdr, escaped_pos, rdr.last_pos.get(),\n+                    fatal_span_char(rdr, escaped_pos, rdr.last_pos,\n                                     ~\"unknown string escape\", c2);\n                   }\n                 }\n@@ -936,7 +915,7 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n         return token::LIT_STR(str_to_ident(accum_str));\n       }\n       'r' => {\n-        let start_bpos = rdr.last_pos.get();\n+        let start_bpos = rdr.last_pos;\n         bump(rdr);\n         let mut hash_count = 0u;\n         while rdr.curr_is('#') {\n@@ -945,24 +924,24 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n         }\n \n         if is_eof(rdr) {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"unterminated raw string\");\n         } else if !rdr.curr_is('\"') {\n-            fatal_span_char(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span_char(rdr, start_bpos, rdr.last_pos,\n                             ~\"only `#` is allowed in raw string delimitation; \\\n                               found illegal character\",\n-                            rdr.curr.get().unwrap());\n+                            rdr.curr.unwrap());\n         }\n         bump(rdr);\n-        let content_start_bpos = rdr.last_pos.get();\n+        let content_start_bpos = rdr.last_pos;\n         let mut content_end_bpos;\n         'outer: loop {\n             if is_eof(rdr) {\n-                fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+                fatal_span(rdr, start_bpos, rdr.last_pos,\n                            ~\"unterminated raw string\");\n             }\n             if rdr.curr_is('\"') {\n-                content_end_bpos = rdr.last_pos.get();\n+                content_end_bpos = rdr.last_pos;\n                 for _ in range(0, hash_count) {\n                     bump(rdr);\n                     if !rdr.curr_is('#') {\n@@ -1006,14 +985,14 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n       '^' => { return binop(rdr, token::CARET); }\n       '%' => { return binop(rdr, token::PERCENT); }\n       c => {\n-          fatal_span_char(rdr, rdr.last_pos.get(), rdr.pos.get(),\n+          fatal_span_char(rdr, rdr.last_pos, rdr.pos,\n                           ~\"unknown start of token\", c);\n       }\n     }\n }\n \n-fn consume_whitespace(rdr: &StringReader) {\n-    while is_whitespace(rdr.curr.get()) && !is_eof(rdr) { bump(rdr); }\n+fn consume_whitespace(rdr: &mut StringReader) {\n+    while is_whitespace(rdr.curr) && !is_eof(rdr) { bump(rdr); }\n }\n \n #[cfg(test)]\n@@ -1041,7 +1020,7 @@ mod test {\n \n     #[test] fn t1 () {\n         let span_handler = mk_sh();\n-        let string_reader = setup(&span_handler,\n+        let mut string_reader = setup(&span_handler,\n             ~\"/* my source file */ \\\n               fn main() { println!(\\\"zebra\\\"); }\\n\");\n         let id = str_to_ident(\"fn\");\n@@ -1051,20 +1030,20 @@ mod test {\n             sp:Span {lo:BytePos(21),hi:BytePos(23),expn_info: None}};\n         assert_eq!(tok1,tok2);\n         // the 'main' id is already read:\n-        assert_eq!(string_reader.last_pos.get().clone(), BytePos(28));\n+        assert_eq!(string_reader.last_pos.clone(), BytePos(28));\n         // read another token:\n         let tok3 = string_reader.next_token();\n         let tok4 = TokenAndSpan{\n             tok:token::IDENT(str_to_ident(\"main\"), false),\n             sp:Span {lo:BytePos(24),hi:BytePos(28),expn_info: None}};\n         assert_eq!(tok3,tok4);\n         // the lparen is already read:\n-        assert_eq!(string_reader.last_pos.get().clone(), BytePos(29))\n+        assert_eq!(string_reader.last_pos.clone(), BytePos(29))\n     }\n \n     // check that the given reader produces the desired stream\n     // of tokens (stop checking after exhausting the expected vec)\n-    fn check_tokenization (string_reader: StringReader, expected: Vec<token::Token> ) {\n+    fn check_tokenization (mut string_reader: StringReader, expected: Vec<token::Token> ) {\n         for expected_tok in expected.iter() {\n             assert_eq!(&string_reader.next_token().tok, expected_tok);\n         }"}, {"sha": "f2a7f543bd642a61aece0a388c45c01c3ea141fd", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -366,13 +366,13 @@ mod test {\n             [ast::TTTok(_,_),\n              ast::TTTok(_,token::NOT),\n              ast::TTTok(_,_),\n-             ast::TTDelim(delim_elts)] => {\n+             ast::TTDelim(ref delim_elts)] => {\n                 let delim_elts: &[ast::TokenTree] = delim_elts.as_slice();\n                 match delim_elts {\n                     [ast::TTTok(_,token::LPAREN),\n-                     ast::TTDelim(first_set),\n+                     ast::TTDelim(ref first_set),\n                      ast::TTTok(_,token::FAT_ARROW),\n-                     ast::TTDelim(second_set),\n+                     ast::TTDelim(ref second_set),\n                      ast::TTTok(_,token::RPAREN)] => {\n                         let first_set: &[ast::TokenTree] =\n                             first_set.as_slice();"}, {"sha": "a20854884b2fab29717c18d272819fd616a20177", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -80,6 +80,7 @@ use owned_slice::OwnedSlice;\n use collections::HashSet;\n use std::kinds::marker;\n use std::mem::replace;\n+use std::rc::Rc;\n use std::vec;\n \n #[allow(non_camel_case_types)]\n@@ -274,7 +275,7 @@ struct ParsedItemsAndViewItems {\n \n /* ident is handled by common.rs */\n \n-pub fn Parser<'a>(sess: &'a ParseSess, cfg: ast::CrateConfig, rdr: ~Reader:)\n+pub fn Parser<'a>(sess: &'a ParseSess, cfg: ast::CrateConfig, mut rdr: ~Reader:)\n               -> Parser<'a> {\n     let tok0 = rdr.next_token();\n     let span = tok0.sp;\n@@ -328,7 +329,7 @@ pub struct Parser<'a> {\n     restriction: restriction,\n     quote_depth: uint, // not (yet) related to the quasiquoter\n     reader: ~Reader:,\n-    interner: @token::IdentInterner,\n+    interner: Rc<token::IdentInterner>,\n     /// The set of seen errors about obsolete syntax. Used to suppress\n     /// extra detail when the same error is seen twice\n     obsolete_set: HashSet<ObsoleteSyntax>,\n@@ -2104,7 +2105,7 @@ impl<'a> Parser<'a> {\n                     let seq = match seq {\n                         Spanned { node, .. } => node,\n                     };\n-                    TTSeq(mk_sp(sp.lo, p.span.hi), @seq, s, z)\n+                    TTSeq(mk_sp(sp.lo, p.span.hi), Rc::new(seq), s, z)\n                 } else {\n                     TTNonterminal(sp, p.parse_ident())\n                 }\n@@ -2147,7 +2148,7 @@ impl<'a> Parser<'a> {\n                 result.push(parse_any_tt_tok(self));\n                 self.open_braces.pop().unwrap();\n \n-                TTDelim(@result)\n+                TTDelim(Rc::new(result))\n             }\n             _ => parse_non_delim_tt_tok(self)\n         }"}, {"sha": "f0b4ebc593a8323bba7d48bd922fdbbe54117653", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -22,6 +22,7 @@ use std::char;\n use std::fmt;\n use std::local_data;\n use std::path::BytesContainer;\n+use std::rc::Rc;\n \n #[allow(non_camel_case_types)]\n #[deriving(Clone, Encodable, Decodable, Eq, TotalEq, Hash, Show)]\n@@ -531,13 +532,14 @@ pub type IdentInterner = StrInterner;\n \n // if an interner exists in TLS, return it. Otherwise, prepare a\n // fresh one.\n-pub fn get_ident_interner() -> @IdentInterner {\n-    local_data_key!(key: @::parse::token::IdentInterner)\n-    match local_data::get(key, |k| k.map(|k| *k)) {\n+// FIXME(eddyb) #8726 This should probably use a task-local reference.\n+pub fn get_ident_interner() -> Rc<IdentInterner> {\n+    local_data_key!(key: Rc<::parse::token::IdentInterner>)\n+    match local_data::get(key, |k| k.map(|k| k.clone())) {\n         Some(interner) => interner,\n         None => {\n-            let interner = @mk_fresh_ident_interner();\n-            local_data::set(key, interner);\n+            let interner = Rc::new(mk_fresh_ident_interner());\n+            local_data::set(key, interner.clone());\n             interner\n         }\n     }"}, {"sha": "b410e0c7169ab49b038f69743c21f63e2a336c38", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 11, "deletions": 12, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6584f3746e552a626003f564e2f83262d93cbd57/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=6584f3746e552a626003f564e2f83262d93cbd57", "patch": "@@ -26,13 +26,12 @@ use print::pp::{Breaks, Consistent, Inconsistent, eof};\n use print::pp;\n \n use std::cast;\n-use std::cell::RefCell;\n use std::char;\n use std::str;\n use std::io;\n use std::io::{IoResult, MemWriter};\n+use std::rc::Rc;\n \n-// The &mut State is stored here to prevent recursive type.\n pub enum AnnNode<'a> {\n     NodeBlock(&'a ast::Block),\n     NodeItem(&'a ast::Item),\n@@ -57,11 +56,11 @@ pub struct CurrentCommentAndLiteral {\n pub struct State<'a> {\n     s: pp::Printer,\n     cm: Option<&'a CodeMap>,\n-    intr: @token::IdentInterner,\n+    intr: Rc<token::IdentInterner>,\n     comments: Option<Vec<comments::Comment> >,\n     literals: Option<Vec<comments::Literal> >,\n     cur_cmnt_and_lit: CurrentCommentAndLiteral,\n-    boxes: RefCell<Vec<pp::Breaks> >,\n+    boxes: Vec<pp::Breaks>,\n     ann: &'a PpAnn\n }\n \n@@ -82,7 +81,7 @@ pub fn rust_printer_annotated<'a>(writer: ~io::Writer,\n             cur_cmnt: 0,\n             cur_lit: 0\n         },\n-        boxes: RefCell::new(Vec::new()),\n+        boxes: Vec::new(),\n         ann: ann\n     }\n }\n@@ -124,7 +123,7 @@ pub fn print_crate<'a>(cm: &'a CodeMap,\n             cur_cmnt: 0,\n             cur_lit: 0\n         },\n-        boxes: RefCell::new(Vec::new()),\n+        boxes: Vec::new(),\n         ann: ann\n     };\n     try!(s.print_mod(&krate.module, krate.attrs.as_slice()));\n@@ -238,23 +237,23 @@ pub fn visibility_qualified(vis: ast::Visibility, s: &str) -> ~str {\n \n impl<'a> State<'a> {\n     pub fn ibox(&mut self, u: uint) -> IoResult<()> {\n-        self.boxes.borrow_mut().push(pp::Inconsistent);\n+        self.boxes.push(pp::Inconsistent);\n         pp::ibox(&mut self.s, u)\n     }\n \n     pub fn end(&mut self) -> IoResult<()> {\n-        self.boxes.borrow_mut().pop().unwrap();\n+        self.boxes.pop().unwrap();\n         pp::end(&mut self.s)\n     }\n \n     pub fn cbox(&mut self, u: uint) -> IoResult<()> {\n-        self.boxes.borrow_mut().push(pp::Consistent);\n+        self.boxes.push(pp::Consistent);\n         pp::cbox(&mut self.s, u)\n     }\n \n     // \"raw box\"\n     pub fn rbox(&mut self, u: uint, b: pp::Breaks) -> IoResult<()> {\n-        self.boxes.borrow_mut().push(b);\n+        self.boxes.push(b);\n         pp::rbox(&mut self.s, u, b)\n     }\n \n@@ -321,8 +320,8 @@ impl<'a> State<'a> {\n         self.s.last_token().is_eof() || self.s.last_token().is_hardbreak_tok()\n     }\n \n-    pub fn in_cbox(&mut self) -> bool {\n-        match self.boxes.borrow().last() {\n+    pub fn in_cbox(&self) -> bool {\n+        match self.boxes.last() {\n             Some(&last_box) => last_box == pp::Consistent,\n             None => false\n         }"}]}