{"sha": "f2775ac2e955218321e9dce2ca0580e3e6e505bb", "node_id": "C_kwDOAAsO6NoAKGYyNzc1YWMyZTk1NTIxODMyMWU5ZGNlMmNhMDU4MGUzZTZlNTA1YmI", "commit": {"author": {"name": "hamidreza kalbasi", "email": "hamidrezakalbasi@protonmail.com", "date": "2021-09-18T17:44:47Z"}, "committer": {"name": "hamidreza kalbasi", "email": "hamidrezakalbasi@protonmail.com", "date": "2021-09-26T06:34:02Z"}, "message": "reuse hover results with resultset", "tree": {"sha": "04a396a2939097707e2aa953f92ee22d206f35ac", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/04a396a2939097707e2aa953f92ee22d206f35ac"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f2775ac2e955218321e9dce2ca0580e3e6e505bb", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f2775ac2e955218321e9dce2ca0580e3e6e505bb", "html_url": "https://github.com/rust-lang/rust/commit/f2775ac2e955218321e9dce2ca0580e3e6e505bb", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f2775ac2e955218321e9dce2ca0580e3e6e505bb/comments", "author": {"login": "HKalbasi", "id": 45197576, "node_id": "MDQ6VXNlcjQ1MTk3NTc2", "avatar_url": "https://avatars.githubusercontent.com/u/45197576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HKalbasi", "html_url": "https://github.com/HKalbasi", "followers_url": "https://api.github.com/users/HKalbasi/followers", "following_url": "https://api.github.com/users/HKalbasi/following{/other_user}", "gists_url": "https://api.github.com/users/HKalbasi/gists{/gist_id}", "starred_url": "https://api.github.com/users/HKalbasi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HKalbasi/subscriptions", "organizations_url": "https://api.github.com/users/HKalbasi/orgs", "repos_url": "https://api.github.com/users/HKalbasi/repos", "events_url": "https://api.github.com/users/HKalbasi/events{/privacy}", "received_events_url": "https://api.github.com/users/HKalbasi/received_events", "type": "User", "site_admin": false}, "committer": {"login": "HKalbasi", "id": 45197576, "node_id": "MDQ6VXNlcjQ1MTk3NTc2", "avatar_url": "https://avatars.githubusercontent.com/u/45197576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HKalbasi", "html_url": "https://github.com/HKalbasi", "followers_url": "https://api.github.com/users/HKalbasi/followers", "following_url": "https://api.github.com/users/HKalbasi/following{/other_user}", "gists_url": "https://api.github.com/users/HKalbasi/gists{/gist_id}", "starred_url": "https://api.github.com/users/HKalbasi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HKalbasi/subscriptions", "organizations_url": "https://api.github.com/users/HKalbasi/orgs", "repos_url": "https://api.github.com/users/HKalbasi/repos", "events_url": "https://api.github.com/users/HKalbasi/events{/privacy}", "received_events_url": "https://api.github.com/users/HKalbasi/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "70061d2b7e7e2ec9d7b0704f18c488985dcd7417", "url": "https://api.github.com/repos/rust-lang/rust/commits/70061d2b7e7e2ec9d7b0704f18c488985dcd7417", "html_url": "https://github.com/rust-lang/rust/commit/70061d2b7e7e2ec9d7b0704f18c488985dcd7417"}], "stats": {"total": 267, "additions": 173, "deletions": 94}, "files": [{"sha": "d50680ce14e130ee3029a10fc0ef7b38615945d9", "filename": "crates/ide/src/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/f2775ac2e955218321e9dce2ca0580e3e6e505bb/crates%2Fide%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f2775ac2e955218321e9dce2ca0580e3e6e505bb/crates%2Fide%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide%2Fsrc%2Flib.rs?ref=f2775ac2e955218321e9dce2ca0580e3e6e505bb", "patch": "@@ -87,7 +87,7 @@ pub use crate::{\n     references::ReferenceSearchResult,\n     rename::RenameError,\n     runnables::{Runnable, RunnableKind, TestId},\n-    static_index::{StaticIndex, StaticIndexedFile, TokenStaticData},\n+    static_index::{StaticIndex, StaticIndexedFile, TokenStaticData, TokenId},\n     syntax_highlighting::{\n         tags::{Highlight, HlMod, HlMods, HlOperator, HlPunct, HlTag},\n         HlRange,"}, {"sha": "bd71177990bf0b5d4ba4583b980e39be48bae6aa", "filename": "crates/ide/src/static_index.rs", "status": "modified", "additions": 96, "deletions": 46, "changes": 142, "blob_url": "https://github.com/rust-lang/rust/blob/f2775ac2e955218321e9dce2ca0580e3e6e505bb/crates%2Fide%2Fsrc%2Fstatic_index.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f2775ac2e955218321e9dce2ca0580e3e6e505bb/crates%2Fide%2Fsrc%2Fstatic_index.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide%2Fsrc%2Fstatic_index.rs?ref=f2775ac2e955218321e9dce2ca0580e3e6e505bb", "patch": "@@ -1,31 +1,62 @@\n //! This module provides `StaticIndex` which is used for powering\n //! read-only code browsers and emitting LSIF\n \n+use std::collections::HashMap;\n+\n use hir::{db::HirDatabase, Crate, Module};\n-use ide_db::base_db::{FileId, FileRange, SourceDatabaseExt};\n+use ide_db::base_db::{FileId, SourceDatabaseExt};\n use ide_db::RootDatabase;\n+use ide_db::defs::Definition;\n use rustc_hash::FxHashSet;\n use syntax::TextRange;\n use syntax::{AstNode, SyntaxKind::*, T};\n \n+use crate::hover::{get_definition_of_token, hover_for_definition};\n use crate::{Analysis, Cancellable, Fold, HoverConfig, HoverDocFormat, HoverResult};\n \n /// A static representation of fully analyzed source code.\n ///\n /// The intended use-case is powering read-only code browsers and emitting LSIF\n-pub struct StaticIndex {\n+pub struct StaticIndex<'a> {\n     pub files: Vec<StaticIndexedFile>,\n+    pub tokens: TokenStore,\n+    analysis: &'a Analysis,\n+    db: &'a RootDatabase,\n+    def_map: HashMap<Definition, TokenId>,\n }\n \n pub struct TokenStaticData {\n-    pub range: TextRange,\n     pub hover: Option<HoverResult>,\n }\n \n+#[derive(Clone, Copy, PartialEq, Eq, Hash)]\n+pub struct TokenId(usize);\n+\n+#[derive(Default)]\n+pub struct TokenStore(Vec<TokenStaticData>);\n+\n+impl TokenStore {\n+    pub fn insert(&mut self, data: TokenStaticData) -> TokenId {\n+        let id = TokenId(self.0.len());\n+        self.0.push(data);\n+        id\n+    }\n+\n+    pub fn get(&self, id: TokenId) -> Option<&TokenStaticData> {\n+        self.0.get(id.0)\n+    }\n+    \n+    pub fn iter(self) -> impl Iterator<Item=(TokenId, TokenStaticData)> {\n+        self.0.into_iter().enumerate().map(|(i, x)| {\n+            (TokenId(i), x)\n+        })\n+    }\n+}\n+\n pub struct StaticIndexedFile {\n     pub file_id: FileId,\n     pub folds: Vec<Fold>,\n-    pub tokens: Vec<TokenStaticData>,\n+    pub tokens: Vec<(TextRange, TokenId)>,\n }\n \n fn all_modules(db: &dyn HirDatabase) -> Vec<Module> {\n@@ -41,62 +72,81 @@ fn all_modules(db: &dyn HirDatabase) -> Vec<Module> {\n     modules\n }\n \n-impl StaticIndex {\n-    pub fn compute(db: &RootDatabase, analysis: &Analysis) -> Cancellable<StaticIndex> {\n+impl StaticIndex<'_> {\n+    fn add_file(&mut self, file_id: FileId) -> Cancellable<()> {\n+        let folds = self.analysis.folding_ranges(file_id)?;\n+        // hovers\n+        let sema = hir::Semantics::new(self.db);\n+        let tokens_or_nodes = sema.parse(file_id).syntax().clone();\n+        let tokens = tokens_or_nodes.descendants_with_tokens().filter_map(|x| match x {\n+            syntax::NodeOrToken::Node(_) => None,\n+            syntax::NodeOrToken::Token(x) => Some(x),\n+        });\n+        let hover_config =\n+            HoverConfig { links_in_hover: true, documentation: Some(HoverDocFormat::Markdown) };\n+        let tokens = tokens\n+            .filter(|token| match token.kind() {\n+                IDENT\n+                | INT_NUMBER\n+                | LIFETIME_IDENT\n+                | T![self]\n+                | T![super]\n+                | T![crate] => true,\n+                _ => false,\n+            });\n+        let mut result = StaticIndexedFile {\n+            file_id,\n+            folds,\n+            tokens: vec![],\n+        };\n+        for token in tokens {\n+            let range = token.text_range();\n+            let node = token.parent().unwrap();\n+            let def = get_definition_of_token(self.db, &sema, &sema.descend_into_macros(token), file_id, range.start(), &mut None);\n+            let def = if let Some(x) = def {\n+                x\n+            } else {\n+                continue;\n+            };\n+            let id = if let Some(x) = self.def_map.get(&def) {\n+                *x\n+            } else {\n+                let x = self.tokens.insert(TokenStaticData {\n+                    hover: hover_for_definition(self.db, file_id, &sema, def, node, &hover_config),\n+                });\n+                self.def_map.insert(def, x);\n+                x\n+            };\n+            result.tokens.push((range, id));\n+        }\n+        self.files.push(result);\n+        Ok(())\n+    }\n+    \n+    pub fn compute<'a>(db: &'a RootDatabase, analysis: &'a Analysis) -> Cancellable<StaticIndex<'a>> {\n         let work = all_modules(db).into_iter().filter(|module| {\n             let file_id = module.definition_source(db).file_id.original_file(db);\n             let source_root = db.file_source_root(file_id);\n             let source_root = db.source_root(source_root);\n             !source_root.is_library\n         });\n-\n+        let mut this = StaticIndex {\n+            files: vec![],\n+            tokens: Default::default(),\n+            analysis, db,\n+            def_map: Default::default(),\n+        };\n         let mut visited_files = FxHashSet::default();\n-        let mut result_files = Vec::<StaticIndexedFile>::new();\n         for module in work {\n             let file_id = module.definition_source(db).file_id.original_file(db);\n             if visited_files.contains(&file_id) {\n                 continue;\n             }\n-            let folds = analysis.folding_ranges(file_id)?;\n-            // hovers\n-            let sema = hir::Semantics::new(db);\n-            let tokens_or_nodes = sema.parse(file_id).syntax().clone();\n-            let tokens = tokens_or_nodes.descendants_with_tokens().filter_map(|x| match x {\n-                syntax::NodeOrToken::Node(_) => None,\n-                syntax::NodeOrToken::Token(x) => Some(x),\n-            });\n-            let hover_config =\n-                HoverConfig { links_in_hover: true, documentation: Some(HoverDocFormat::Markdown) };\n-            let tokens = tokens\n-                .filter(|token| match token.kind() {\n-                    IDENT\n-                    | INT_NUMBER\n-                    | LIFETIME_IDENT\n-                    | T![self]\n-                    | T![super]\n-                    | T![crate]\n-                    | T!['(']\n-                    | T![')'] => true,\n-                    _ => false,\n-                })\n-                .map(|token| {\n-                    let range = token.text_range();\n-                    let hover = analysis\n-                        .hover(\n-                            &hover_config,\n-                            FileRange {\n-                                file_id,\n-                                range: TextRange::new(range.start(), range.start()),\n-                            },\n-                        )?\n-                        .map(|x| x.info);\n-                    Ok(TokenStaticData { range, hover })\n-                })\n-                .collect::<Result<Vec<_>, _>>()?;\n-            result_files.push(StaticIndexedFile { file_id, folds, tokens });\n+            this.add_file(file_id)?;\n             // mark the file\n             visited_files.insert(file_id);\n         }\n-        Ok(StaticIndex { files: result_files })\n+        //eprintln!(\"{:#?}\", token_map);\n+        Ok(this)\n     }\n }"}, {"sha": "509842516a825d6e0d6824ee2476684f25fa6c98", "filename": "crates/rust-analyzer/src/cli/lsif.rs", "status": "modified", "additions": 76, "deletions": 47, "changes": 123, "blob_url": "https://github.com/rust-lang/rust/blob/f2775ac2e955218321e9dce2ca0580e3e6e505bb/crates%2Frust-analyzer%2Fsrc%2Fcli%2Flsif.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f2775ac2e955218321e9dce2ca0580e3e6e505bb/crates%2Frust-analyzer%2Fsrc%2Fcli%2Flsif.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fcli%2Flsif.rs?ref=f2775ac2e955218321e9dce2ca0580e3e6e505bb", "patch": "@@ -1,15 +1,16 @@\n //! Lsif generator\n \n+use std::collections::HashMap;\n use std::env;\n use std::time::Instant;\n \n-use ide::{StaticIndex, StaticIndexedFile, TokenStaticData};\n+use ide::{Analysis, Cancellable, RootDatabase, StaticIndex, StaticIndexedFile, TokenId, TokenStaticData};\n use ide_db::LineIndexDatabase;\n \n use ide_db::base_db::salsa::{self, ParallelDatabase};\n use lsp_types::{lsif::*, Hover, HoverContents, NumberOrString};\n use project_model::{CargoConfig, ProjectManifest, ProjectWorkspace};\n-use vfs::AbsPathBuf;\n+use vfs::{AbsPathBuf, Vfs};\n \n use crate::cli::{\n     flags,\n@@ -27,9 +28,12 @@ impl<DB: ParallelDatabase> Clone for Snap<salsa::Snapshot<DB>> {\n     }\n }\n \n-#[derive(Default)]\n-struct LsifManager {\n+struct LsifManager<'a> {\n     count: i32,\n+    token_map: HashMap<TokenId, Id>,\n+    analysis: &'a Analysis,\n+    db: &'a RootDatabase,\n+    vfs: &'a Vfs,\n }\n \n #[derive(Clone, Copy)]\n@@ -41,7 +45,17 @@ impl From<Id> for NumberOrString {\n     }\n }\n \n-impl LsifManager {\n+impl LsifManager<'_> {\n+    fn new<'a>(analysis: &'a Analysis, db: &'a RootDatabase, vfs: &'a Vfs) -> LsifManager<'a> {\n+        LsifManager {\n+            count: 0,\n+            token_map: HashMap::default(),\n+            analysis,\n+            db,\n+            vfs,\n+        }\n+    }\n+    \n     fn add(&mut self, data: Element) -> Id {\n         let id = Id(self.count);\n         self.emit(&serde_json::to_string(&Entry { id: id.into(), data }).unwrap());\n@@ -54,33 +68,67 @@ impl LsifManager {\n         println!(\"{}\", data);\n     }\n \n-    fn add_tokens(&mut self, line_index: &LineIndex, doc_id: Id, tokens: Vec<TokenStaticData>) {\n+    fn add_token(&mut self, id: TokenId, token: TokenStaticData) {\n+        let result_set_id = self.add(Element::Vertex(Vertex::ResultSet(ResultSet { key: None })));\n+        self.token_map.insert(id, result_set_id);\n+        if let Some(hover) = token.hover {\n+            let hover_id = self.add(Element::Vertex(Vertex::HoverResult {\n+                result: Hover {\n+                    contents: HoverContents::Markup(to_proto::markup_content(hover.markup)),\n+                    range: None,\n+                },\n+            }));\n+            self.add(Element::Edge(Edge::Hover(EdgeData {\n+                in_v: hover_id.into(),\n+                out_v: result_set_id.into(),\n+            })));\n+        }\n+    }\n+\n+    fn add_file(&mut self, file: StaticIndexedFile) -> Cancellable<()> {\n+        let StaticIndexedFile { file_id, tokens, folds} = file;\n+        let path = self.vfs.file_path(file_id);\n+        let path = path.as_path().unwrap();\n+        let doc_id = self.add(Element::Vertex(Vertex::Document(Document {\n+            language_id: \"rust\".to_string(),\n+            uri: lsp_types::Url::from_file_path(path).unwrap(),\n+        })));\n+        let text = self.analysis.file_text(file_id)?;\n+        let line_index = self.db.line_index(file_id);\n+        let line_index = LineIndex {\n+            index: line_index.clone(),\n+            encoding: OffsetEncoding::Utf16,\n+            endings: LineEndings::Unix,\n+        };\n+        let result = folds\n+            .into_iter()\n+            .map(|it| to_proto::folding_range(&*text, &line_index, false, it))\n+            .collect();\n+        let folding_id = self.add(Element::Vertex(Vertex::FoldingRangeResult { result }));\n+        self.add(Element::Edge(Edge::FoldingRange(EdgeData {\n+            in_v: folding_id.into(),\n+            out_v: doc_id.into(),\n+        })));\n         let tokens_id = tokens\n             .into_iter()\n-            .map(|token| {\n-                let token_id = self.add(Element::Vertex(Vertex::Range {\n-                    range: to_proto::range(line_index, token.range),\n+            .map(|(range, id)| {\n+                let range_id = self.add(Element::Vertex(Vertex::Range {\n+                    range: to_proto::range(&line_index, range),\n                     tag: None,\n                 }));\n-                if let Some(hover) = token.hover {\n-                    let hover_id = self.add(Element::Vertex(Vertex::HoverResult {\n-                        result: Hover {\n-                            contents: HoverContents::Markup(to_proto::markup_content(hover.markup)),\n-                            range: None,\n-                        },\n-                    }));\n-                    self.add(Element::Edge(Edge::Hover(EdgeData {\n-                        in_v: hover_id.into(),\n-                        out_v: token_id.into(),\n-                    })));\n-                }\n-                token_id.into()\n+                let result_set_id = *self.token_map.get(&id).expect(\"token map doesn't contain id\");\n+                self.add(Element::Edge(Edge::Next(EdgeData {\n+                    in_v: result_set_id.into(),\n+                    out_v: range_id.into(),\n+                })));\n+                range_id.into()\n             })\n             .collect();\n         self.add(Element::Edge(Edge::Contains(EdgeDataMultiIn {\n             in_vs: tokens_id,\n             out_v: doc_id.into(),\n         })));\n+        Ok(())\n     }\n }\n \n@@ -106,37 +154,18 @@ impl flags::Lsif {\n \n         let si = StaticIndex::compute(db, &analysis)?;\n \n-        let mut lsif = LsifManager::default();\n+        let mut lsif = LsifManager::new(&analysis, db, &vfs);\n         lsif.add(Element::Vertex(Vertex::MetaData(MetaData {\n             version: String::from(\"0.5.0\"),\n             project_root: lsp_types::Url::from_file_path(path).unwrap(),\n             position_encoding: Encoding::Utf16,\n             tool_info: None,\n         })));\n-        for StaticIndexedFile { file_id, folds, tokens } in si.files {\n-            let path = vfs.file_path(file_id);\n-            let path = path.as_path().unwrap();\n-            let doc_id = lsif.add(Element::Vertex(Vertex::Document(Document {\n-                language_id: \"rust\".to_string(),\n-                uri: lsp_types::Url::from_file_path(path).unwrap(),\n-            })));\n-            let text = analysis.file_text(file_id)?;\n-            let line_index = db.line_index(file_id);\n-            let line_index = LineIndex {\n-                index: line_index.clone(),\n-                encoding: OffsetEncoding::Utf16,\n-                endings: LineEndings::Unix,\n-            };\n-            let result = folds\n-                .into_iter()\n-                .map(|it| to_proto::folding_range(&*text, &line_index, false, it))\n-                .collect();\n-            let folding_id = lsif.add(Element::Vertex(Vertex::FoldingRangeResult { result }));\n-            lsif.add(Element::Edge(Edge::FoldingRange(EdgeData {\n-                in_v: folding_id.into(),\n-                out_v: doc_id.into(),\n-            })));\n-            lsif.add_tokens(&line_index, doc_id, tokens);\n+        for (id, token) in si.tokens.iter() {\n+            lsif.add_token(id, token);\n+        }\n+        for file in si.files {\n+            lsif.add_file(file)?;\n         }\n         eprintln!(\"Generating LSIF finished in {:?}\", now.elapsed());\n         Ok(())"}]}