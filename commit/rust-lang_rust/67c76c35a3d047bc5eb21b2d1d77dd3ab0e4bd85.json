{"sha": "67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85", "node_id": "MDY6Q29tbWl0NzI0NzEyOjY3Yzc2YzM1YTNkMDQ3YmM1ZWIyMWIyZDFkNzdkZDNhYjBlNGJkODU=", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2020-10-07T09:55:44Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-10-07T09:55:44Z"}, "message": "Merge #6165\n\n6165: Cleanup r=matklad a=matklad\n\nbors r+\n\ud83e\udd16\n\nCo-authored-by: Aleksey Kladov <aleksey.kladov@gmail.com>", "tree": {"sha": "bdc3ea8635a014692a38aae8d70542c782766c63", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/bdc3ea8635a014692a38aae8d70542c782766c63"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJffZCgCRBK7hj4Ov3rIwAAdHIIAAFwfCFHZq6dVj/beyl+hKRO\nDhzZluAer9PSXzSpefyrXTrUHx74HJoS7paNFaQqhkN4aqak0mKl9uY0mOTj2mbj\n9h8TlbXvtHt6/k5BPiTs3DCHfInrPbVRWPXHBdbE1iXMfaMhUmeh7V+h4+iT1yj5\nbl67Vdr9sQdUULYScRQvy7Byahx3Bm9yemsaN/W3gVmavWqXZZTjY6Hy+c2EsDEr\nrbArl5chtaUWLIjMroLJDUWg0VcMIsVZHk8++MM4Uk4dfAFE94kG8df8skMUdxzq\nfAFHImlcYMgcnC24bcT6AH/GcfCMC6dpiXa4XaD+589s/v4bImevQh8nc3Lw1SM=\n=SohO\n-----END PGP SIGNATURE-----\n", "payload": "tree bdc3ea8635a014692a38aae8d70542c782766c63\nparent faddea935332de3156a5462baa07136bf2e98bf9\nparent fd8622e1ec6371b7cef6fef50f312bc767b317df\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1602064544 +0000\ncommitter GitHub <noreply@github.com> 1602064544 +0000\n\nMerge #6165\n\n6165: Cleanup r=matklad a=matklad\n\nbors r+\n\ud83e\udd16\n\nCo-authored-by: Aleksey Kladov <aleksey.kladov@gmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85", "html_url": "https://github.com/rust-lang/rust/commit/67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "faddea935332de3156a5462baa07136bf2e98bf9", "url": "https://api.github.com/repos/rust-lang/rust/commits/faddea935332de3156a5462baa07136bf2e98bf9", "html_url": "https://github.com/rust-lang/rust/commit/faddea935332de3156a5462baa07136bf2e98bf9"}, {"sha": "fd8622e1ec6371b7cef6fef50f312bc767b317df", "url": "https://api.github.com/repos/rust-lang/rust/commits/fd8622e1ec6371b7cef6fef50f312bc767b317df", "html_url": "https://github.com/rust-lang/rust/commit/fd8622e1ec6371b7cef6fef50f312bc767b317df"}], "stats": {"total": 45, "additions": 23, "deletions": 22}, "files": [{"sha": "396ce8b162f79db5efcfc10568f41cf7767b5a45", "filename": "crates/mbe/src/subtree_source.rs", "status": "modified", "additions": 10, "deletions": 12, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs?ref=67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85", "patch": "@@ -2,7 +2,7 @@\n \n use parser::{Token, TokenSource};\n use std::cell::{Cell, Ref, RefCell};\n-use syntax::{tokenize, SmolStr, SyntaxKind, SyntaxKind::*, T};\n+use syntax::{lex_single_syntax_kind, SmolStr, SyntaxKind, SyntaxKind::*, T};\n use tt::buffer::{Cursor, TokenBuffer};\n \n #[derive(Debug, Clone, Eq, PartialEq)]\n@@ -155,17 +155,15 @@ fn convert_delim(d: Option<tt::DelimiterKind>, closing: bool) -> TtToken {\n }\n \n fn convert_literal(l: &tt::Literal) -> TtToken {\n-    let mut kinds = tokenize(&l.text).0.into_iter().map(|token| token.kind);\n-\n-    let kind = match kinds.next() {\n-        Some(kind) if kind.is_literal() => Some(kind),\n-        Some(SyntaxKind::MINUS) => match kinds.next() {\n-            Some(kind) if kind.is_literal() => Some(kind),\n-            _ => None,\n-        },\n-        _ => None,\n-    }\n-    .unwrap_or_else(|| panic!(\"Fail to convert given literal {:#?}\", &l));\n+    let is_negated = l.text.starts_with('-');\n+    let inner_text = &l.text[if is_negated { 1 } else { 0 }..];\n+\n+    let kind = lex_single_syntax_kind(inner_text)\n+        .map(|(kind, _error)| kind)\n+        .filter(|kind| {\n+            kind.is_literal() && (!is_negated || matches!(kind, FLOAT_NUMBER | INT_NUMBER))\n+        })\n+        .unwrap_or_else(|| panic!(\"Fail to convert given literal {:#?}\", &l));\n \n     TtToken { kind, is_joint_to_next: false, text: l.text.clone() }\n }"}, {"sha": "7e38c32cc5e4e33d0ed56add320a7a56c9ba0945", "filename": "crates/syntax/src/parsing/lexer.rs", "status": "modified", "additions": 13, "deletions": 10, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85/crates%2Fsyntax%2Fsrc%2Fparsing%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85/crates%2Fsyntax%2Fsrc%2Fparsing%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Flexer.rs?ref=67c76c35a3d047bc5eb21b2d1d77dd3ab0e4bd85", "patch": "@@ -1,10 +1,10 @@\n //! Lexer analyzes raw input string and produces lexemes (tokens).\n //! It is just a bridge to `rustc_lexer`.\n \n-use rustc_lexer::{LiteralKind as LK, RawStrError};\n-\n use std::convert::TryInto;\n \n+use rustc_lexer::{LiteralKind as LK, RawStrError};\n+\n use crate::{\n     SyntaxError,\n     SyntaxKind::{self, *},\n@@ -61,27 +61,30 @@ pub fn tokenize(text: &str) -> (Vec<Token>, Vec<SyntaxError>) {\n     (tokens, errors)\n }\n \n-/// Returns `SyntaxKind` and `Option<SyntaxError>` of the first token\n-/// encountered at the beginning of the string.\n+/// Returns `SyntaxKind` and `Option<SyntaxError>` if `text` parses as a single token.\n ///\n /// Returns `None` if the string contains zero *or two or more* tokens.\n /// The token is malformed if the returned error is not `None`.\n ///\n /// Beware that unescape errors are not checked at tokenization time.\n pub fn lex_single_syntax_kind(text: &str) -> Option<(SyntaxKind, Option<SyntaxError>)> {\n-    lex_first_token(text)\n-        .filter(|(token, _)| token.len == TextSize::of(text))\n-        .map(|(token, error)| (token.kind, error))\n+    let (first_token, err) = lex_first_token(text)?;\n+    if first_token.len != TextSize::of(text) {\n+        return None;\n+    }\n+    Some((first_token.kind, err))\n }\n \n /// The same as `lex_single_syntax_kind()` but returns only `SyntaxKind` and\n /// returns `None` if any tokenization error occured.\n ///\n /// Beware that unescape errors are not checked at tokenization time.\n pub fn lex_single_valid_syntax_kind(text: &str) -> Option<SyntaxKind> {\n-    lex_first_token(text)\n-        .filter(|(token, error)| !error.is_some() && token.len == TextSize::of(text))\n-        .map(|(token, _error)| token.kind)\n+    let (single_token, err) = lex_single_syntax_kind(text)?;\n+    if err.is_some() {\n+        return None;\n+    }\n+    Some(single_token)\n }\n \n /// Returns `SyntaxKind` and `Option<SyntaxError>` of the first token"}]}