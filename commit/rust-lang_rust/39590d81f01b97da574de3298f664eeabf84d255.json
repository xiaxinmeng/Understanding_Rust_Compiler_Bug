{"sha": "39590d81f01b97da574de3298f664eeabf84d255", "node_id": "MDY6Q29tbWl0NzI0NzEyOjM5NTkwZDgxZjAxYjk3ZGE1NzRkZTMyOThmNjY0ZWVhYmY4NGQyNTU=", "commit": {"author": {"name": "Paul Stansifer", "email": "paul.stansifer@gmail.com", "date": "2012-06-27T22:29:35Z"}, "committer": {"name": "Paul Stansifer", "email": "paul.stansifer@gmail.com", "date": "2012-07-06T01:09:31Z"}, "message": "Some rearranging in perparation for MBE-style TT transcription.", "tree": {"sha": "5601ee33036ef90887e2776fe9c6ba9fbcc4da56", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/5601ee33036ef90887e2776fe9c6ba9fbcc4da56"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/39590d81f01b97da574de3298f664eeabf84d255", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/39590d81f01b97da574de3298f664eeabf84d255", "html_url": "https://github.com/rust-lang/rust/commit/39590d81f01b97da574de3298f664eeabf84d255", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/39590d81f01b97da574de3298f664eeabf84d255/comments", "author": {"login": "paulstansifer", "id": 1431, "node_id": "MDQ6VXNlcjE0MzE=", "avatar_url": "https://avatars.githubusercontent.com/u/1431?v=4", "gravatar_id": "", "url": "https://api.github.com/users/paulstansifer", "html_url": "https://github.com/paulstansifer", "followers_url": "https://api.github.com/users/paulstansifer/followers", "following_url": "https://api.github.com/users/paulstansifer/following{/other_user}", "gists_url": "https://api.github.com/users/paulstansifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/paulstansifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/paulstansifer/subscriptions", "organizations_url": "https://api.github.com/users/paulstansifer/orgs", "repos_url": "https://api.github.com/users/paulstansifer/repos", "events_url": "https://api.github.com/users/paulstansifer/events{/privacy}", "received_events_url": "https://api.github.com/users/paulstansifer/received_events", "type": "User", "site_admin": false}, "committer": {"login": "paulstansifer", "id": 1431, "node_id": "MDQ6VXNlcjE0MzE=", "avatar_url": "https://avatars.githubusercontent.com/u/1431?v=4", "gravatar_id": "", "url": "https://api.github.com/users/paulstansifer", "html_url": "https://github.com/paulstansifer", "followers_url": "https://api.github.com/users/paulstansifer/followers", "following_url": "https://api.github.com/users/paulstansifer/following{/other_user}", "gists_url": "https://api.github.com/users/paulstansifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/paulstansifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/paulstansifer/subscriptions", "organizations_url": "https://api.github.com/users/paulstansifer/orgs", "repos_url": "https://api.github.com/users/paulstansifer/repos", "events_url": "https://api.github.com/users/paulstansifer/events{/privacy}", "received_events_url": "https://api.github.com/users/paulstansifer/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "74c2266a06d3c1038491fa0aea32be52a47f598d", "url": "https://api.github.com/repos/rust-lang/rust/commits/74c2266a06d3c1038491fa0aea32be52a47f598d", "html_url": "https://github.com/rust-lang/rust/commit/74c2266a06d3c1038491fa0aea32be52a47f598d"}], "stats": {"total": 321, "additions": 198, "deletions": 123}, "files": [{"sha": "46ca8240f6e1a79406693da1a97fd8f0b773a63e", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=39590d81f01b97da574de3298f664eeabf84d255", "patch": "@@ -374,12 +374,17 @@ enum blk_sort {\n */\n \n #[auto_serialize]\n+#[doc=\"For macro invocations; parsing is delegated to the macro\"]\n enum token_tree {\n-    /* for macro invocations; parsing is the macro's job */\n     tt_delim(~[token_tree]),\n-    tt_flat(span, token::token)\n+    tt_flat(span, token::token),\n+    /* These only make sense for right-hand-sides of MBE macros*/\n+    tt_dotdotdot(~[token_tree]),\n+    tt_interpolate(ident)\n }\n \n+\n+\n #[auto_serialize]\n type matcher = spanned<matcher_>;\n "}, {"sha": "89f4fe4d670bf7cf31e51d26dc8a3d81473665f7", "filename": "src/libsyntax/ext/earley_parser.rs", "status": "modified", "additions": 33, "deletions": 7, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fext%2Fearley_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fext%2Fearley_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fearley_parser.rs?ref=39590d81f01b97da574de3298f664eeabf84d255", "patch": "@@ -1,12 +1,15 @@\n // Earley-like parser for macros.\n import parse::token;\n import parse::token::{token, EOF, to_str, whole_nt};\n-import parse::lexer::{reader, tt_reader, tt_reader_as_reader};\n+import parse::lexer::*; //resolve bug?\n+//import parse::lexer::{reader, tt_reader, tt_reader_as_reader};\n import parse::parser::{parser,SOURCE_FILE};\n-import parse::common::parser_common;\n+//import parse::common::parser_common;\n+import parse::common::*; //resolve bug?\n import parse::parse_sess;\n import dvec::{dvec, extensions};\n-import ast::{matcher, mtc_tok, mtc_rep, mtc_bb};\n+import ast::{matcher, mtc_tok, mtc_rep, mtc_bb, ident};\n+import std::map::{hashmap, box_str_hash};\n \n /* This is an Earley-like parser, without support for nonterminals.  This\n means that there are no completer or predictor rules, and therefore no need to\n@@ -66,8 +69,31 @@ enum arb_depth { leaf(whole_nt), seq(~[@arb_depth]) }\n type earley_item = matcher_pos;\n \n \n+fn nameize(&&p_s: parse_sess, ms: ~[matcher], &&res: ~[@arb_depth])\n+    -> hashmap<ident,@arb_depth> {\n+    fn n_rec(&&p_s: parse_sess, &&m: matcher, &&res: ~[@arb_depth],\n+             &&ret_val: hashmap<ident, @arb_depth>) {\n+        alt m {\n+          {node: mtc_tok(_), span: _} { }\n+          {node: mtc_rep(more_ms, _, _), span: _} {\n+            for more_ms.each() |next_m| { n_rec(p_s, next_m, res, ret_val) };\n+          }\n+          {node: mtc_bb(bind_name, _, idx), span: sp} {\n+            if ret_val.contains_key(bind_name) {\n+                p_s.span_diagnostic.span_fatal(sp, \"Duplicated bind name: \"\n+                                               + *bind_name)\n+            }\n+            ret_val.insert(bind_name, res[idx]);\n+          }\n+        }\n+    }\n+    let ret_val = box_str_hash::<@arb_depth>();\n+    for ms.each() |m| { n_rec(p_s, m, res, ret_val) };\n+    ret ret_val;\n+}\n+\n fn parse(sess: parse_sess, cfg: ast::crate_cfg, rdr: reader, ms: ~[matcher])\n-    -> ~[@arb_depth] {\n+    -> hashmap<ident,@arb_depth> {\n     let mut cur_eis = ~[];\n     vec::push(cur_eis, new_matcher_pos(ms, none));\n \n@@ -164,9 +190,9 @@ fn parse(sess: parse_sess, cfg: ast::crate_cfg, rdr: reader, ms: ~[matcher])\n \n         /* error messages here could be improved with links to orig. rules */\n         if tok == EOF {\n-            if eof_eis.len() == 1u {\n-                let ret_val = vec::map(eof_eis[0u].matches, |dv| dv.pop());\n-                ret ret_val; /* success */\n+            if eof_eis.len() == 1u { /* success */\n+                ret nameize(sess, ms,\n+                            vec::map(eof_eis[0u].matches, |dv| dv.pop()));\n             } else if eof_eis.len() > 1u {\n                 rdr.fatal(\"Ambiguity: multiple successful parses\");\n             } else {"}, {"sha": "246e24e617aa31470ebbb2d0d54741b1a6c70d7b", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "added", "additions": 114, "deletions": 0, "changes": 114, "blob_url": "https://github.com/rust-lang/rust/blob/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=39590d81f01b97da574de3298f664eeabf84d255", "patch": "@@ -0,0 +1,114 @@\n+import util::interner::interner;\n+import diagnostic::span_handler;\n+import ast::{tt_delim,tt_flat,tt_dotdotdot,tt_interpolate,ident};\n+import ext::earley_parser::arb_depth;\n+import codemap::span;\n+import parse::token::{EOF,token};\n+\n+export tt_reader,  new_tt_reader, dup_tt_reader, tt_next_token;\n+\n+enum tt_frame_up { /* to break a circularity */\n+    tt_frame_up(option<tt_frame>)\n+}\n+\n+/* TODO: figure out how to have a uniquely linked stack, and change to `~` */\n+///an unzipping of `token_tree`s\n+type tt_frame = @{\n+    readme: [ast::token_tree]/~,\n+    mut idx: uint,\n+    up: tt_frame_up\n+};\n+\n+type tt_reader = @{\n+    span_diagnostic: span_handler,\n+    interner: @interner<@str>,\n+    mut cur: tt_frame,\n+    /* for MBE-style macro transcription */\n+    interpolations: std::map::hashmap<ident, @arb_depth>,\n+    /* cached: */\n+    mut cur_tok: token,\n+    mut cur_span: span\n+};\n+\n+/** This can do Macro-By-Example transcription. On the other hand, if\n+ *  `doc` contains no `tt_dotdotdot`s and `tt_interpolate`s, `interp` can (and\n+ *  should) be none. */\n+fn new_tt_reader(span_diagnostic: span_handler, itr: @interner<@str>,\n+                 interp: option<std::map::hashmap<ident,@arb_depth>>,\n+                 src: [ast::token_tree]/~)\n+    -> tt_reader {\n+    let r = @{span_diagnostic: span_diagnostic, interner: itr,\n+              mut cur: @{readme: src, mut idx: 0u,\n+                         up: tt_frame_up(option::none)},\n+              interpolations: alt interp { /* just a convienience */\n+                none { std::map::box_str_hash::<@arb_depth>() }\n+                some(x) { x }\n+              },\n+              /* dummy values, never read: */\n+              mut cur_tok: EOF,\n+              mut cur_span: ast_util::mk_sp(0u,0u)\n+             };\n+    tt_next_token(r); /* get cur_tok and cur_span set up */\n+    ret r;\n+}\n+\n+pure fn dup_tt_frame(&&f: tt_frame) -> tt_frame {\n+    @{readme: f.readme, mut idx: f.idx,\n+      up: alt f.up {\n+        tt_frame_up(some(up_frame)) {\n+          tt_frame_up(some(dup_tt_frame(up_frame)))\n+        }\n+        tt_frame_up(none) { tt_frame_up(none) }\n+      }\n+     }\n+}\n+\n+pure fn dup_tt_reader(&&r: tt_reader) -> tt_reader {\n+    @{span_diagnostic: r.span_diagnostic, interner: r.interner,\n+      mut cur: dup_tt_frame(r.cur),\n+      interpolations: r.interpolations,\n+      mut cur_tok: r.cur_tok, mut cur_span: r.cur_span}\n+}\n+\n+\n+fn tt_next_token(&&r: tt_reader) -> {tok: token, sp: span} {\n+    let ret_val = { tok: r.cur_tok, sp: r.cur_span };\n+    if r.cur.idx >= vec::len(r.cur.readme) {\n+        /* done with this set; pop */\n+        alt r.cur.up {\n+          tt_frame_up(none) {\n+            r.cur_tok = EOF;\n+            ret ret_val;\n+          }\n+          tt_frame_up(some(tt_f)) {\n+            r.cur = tt_f;\n+            /* the above `if` would need to be a `while` if we didn't know\n+            that the last thing in a `tt_delim` is always a `tt_flat` */\n+            r.cur.idx += 1u;\n+          }\n+        }\n+    }\n+    /* if `tt_delim`s could be 0-length, we'd need to be able to switch\n+    between popping and pushing until we got to an actual `tt_flat` */\n+    loop { /* because it's easiest, this handles `tt_delim` not starting\n+    with a `tt_flat`, even though it won't happen */\n+        alt copy r.cur.readme[r.cur.idx] {\n+          tt_delim(tts) {\n+            r.cur = @{readme: tts, mut idx: 0u,\n+                      up: tt_frame_up(option::some(r.cur)) };\n+          }\n+          tt_flat(sp, tok) {\n+            r.cur_span = sp; r.cur_tok = tok;\n+            r.cur.idx += 1u;\n+            ret ret_val;\n+          }\n+          tt_dotdotdot(tts) {\n+            fail;\n+          }\n+          tt_interpolate(ident) {\n+            fail;\n+          }\n+        }\n+    }\n+\n+}\n\\ No newline at end of file"}, {"sha": "9c143257e9eaead90d81f2325b95a668ae439253", "filename": "src/libsyntax/parse.rs", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fparse.rs", "raw_url": "https://github.com/rust-lang/rust/raw/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fparse.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse.rs?ref=39590d81f01b97da574de3298f664eeabf84d255", "patch": "@@ -13,8 +13,10 @@ export parse_expr_from_source_str, parse_item_from_source_str;\n export parse_from_source_str;\n \n import parser::parser;\n-import attr::parser_attr;\n-import common::parser_common;\n+//import attr::parser_attr;\n+import attr::*; //resolve bug?\n+//import common::parser_common;\n+import common::*; //resolve bug?\n import ast::node_id;\n import util::interner;\n // FIXME (#1935): resolve badness\n@@ -199,6 +201,7 @@ fn new_parser_from_file(sess: parse_sess, cfg: ast::crate_cfg, +path: str,\n \n fn new_parser_from_tt(sess: parse_sess, cfg: ast::crate_cfg,\n                       tt: ~[ast::token_tree]) -> parser {\n-    let trdr = lexer::new_tt_reader(sess.span_diagnostic, sess.interner, tt);\n+    let trdr = lexer::new_tt_reader(sess.span_diagnostic, sess.interner,\n+                                    none, tt);\n     ret parser(sess, cfg, trdr as reader, parser::SOURCE_FILE)\n }"}, {"sha": "d7ae4995520ec6e23404073d2a3f01b61c23a920", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=39590d81f01b97da574de3298f664eeabf84d255", "patch": "@@ -1,6 +1,7 @@\n import either::{either, left, right};\n import ast_util::spanned;\n-import common::{parser_common, seq_sep_trailing_disallowed};\n+import common::*; //resolve bug?\n+//import common::{parser_common, seq_sep_trailing_disallowed};\n \n export attr_or_ext;\n export parser_attr;"}, {"sha": "7742e2dc9a22e2ccad2724b017074067fb6afdac", "filename": "src/libsyntax/parse/lexer.rs", "status": "modified", "additions": 19, "deletions": 109, "changes": 128, "blob_url": "https://github.com/rust-lang/rust/blob/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fparse%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fparse%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer.rs?ref=39590d81f01b97da574de3298f664eeabf84d255", "patch": "@@ -1,103 +1,50 @@\n-import util::interner;\n-import util::interner::intern;\n-import diagnostic;\n-import ast::{tt_delim,tt_flat};\n+import util::interner::{interner,intern};\n+import diagnostic::span_handler;\n import codemap::span;\n+import ext::tt::transcribe::{tt_reader,  new_tt_reader, dup_tt_reader,\n+                             tt_next_token};\n \n export reader, string_reader, new_string_reader, is_whitespace;\n-export tt_reader,  new_tt_reader, dup_tt_reader;\n+export tt_reader,  new_tt_reader;\n export nextch, is_eof, bump, get_str_from, new_low_level_string_reader;\n export string_reader_as_reader, tt_reader_as_reader;\n \n iface reader {\n     fn is_eof() -> bool;\n     fn next_token() -> {tok: token::token, sp: span};\n     fn fatal(str) -> !;\n-    fn span_diag() -> diagnostic::span_handler;\n-    fn interner() -> @interner::interner<@str>;\n+    fn span_diag() -> span_handler;\n+    fn interner() -> @interner<@str>;\n     fn peek() -> {tok: token::token, sp: span};\n     fn dup() -> reader;\n }\n \n-enum tt_frame_up { /* to break a circularity */\n-    tt_frame_up(option<tt_frame>)\n-}\n-\n-/* FIXME (#2811): figure out how to have a uniquely linked stack,\n-   and change to `~` */\n-/// an unzipping of `token_tree`s\n-type tt_frame = @{\n-    readme: ~[ast::token_tree],\n-    mut idx: uint,\n-    up: tt_frame_up\n-};\n-\n-type tt_reader = @{\n-    span_diagnostic: diagnostic::span_handler,\n-    interner: @interner::interner<@str>,\n-    mut cur: tt_frame,\n-    /* cached: */\n-    mut cur_tok: token::token,\n-    mut cur_span: span\n-};\n-\n-fn new_tt_reader(span_diagnostic: diagnostic::span_handler,\n-                 itr: @interner::interner<@str>, src: ~[ast::token_tree])\n-    -> tt_reader {\n-    let r = @{span_diagnostic: span_diagnostic, interner: itr,\n-              mut cur: @{readme: src, mut idx: 0u,\n-                         up: tt_frame_up(option::none)},\n-              /* dummy values, never read: */\n-              mut cur_tok: token::EOF,\n-              mut cur_span: ast_util::mk_sp(0u,0u)\n-             };\n-    tt_next_token(r); /* get cur_tok and cur_span set up */\n-    ret r;\n-}\n-\n-pure fn dup_tt_frame(&&f: tt_frame) -> tt_frame {\n-    @{readme: f.readme, mut idx: f.idx,\n-      up: alt f.up {\n-        tt_frame_up(some(up_frame)) {\n-          tt_frame_up(some(dup_tt_frame(up_frame)))\n-        }\n-        tt_frame_up(none) { tt_frame_up(none) }\n-      }\n-     }\n-}\n-\n-pure fn dup_tt_reader(&&r: tt_reader) -> tt_reader {\n-    @{span_diagnostic: r.span_diagnostic, interner: r.interner,\n-      mut cur: dup_tt_frame(r.cur),\n-      mut cur_tok: r.cur_tok, mut cur_span: r.cur_span}\n-}\n-\n type string_reader = @{\n-    span_diagnostic: diagnostic::span_handler,\n+    span_diagnostic: span_handler,\n     src: @str,\n     mut col: uint,\n     mut pos: uint,\n     mut curr: char,\n     mut chpos: uint,\n     filemap: codemap::filemap,\n-    interner: @interner::interner<@str>,\n+    interner: @interner<@str>,\n     /* cached: */\n     mut peek_tok: token::token,\n     mut peek_span: span\n };\n \n-fn new_string_reader(span_diagnostic: diagnostic::span_handler,\n+fn new_string_reader(span_diagnostic: span_handler,\n                      filemap: codemap::filemap,\n-                     itr: @interner::interner<@str>) -> string_reader {\n+                     itr: @interner<@str>) -> string_reader {\n     let r = new_low_level_string_reader(span_diagnostic, filemap, itr);\n     string_advance_token(r); /* fill in peek_* */\n     ret r;\n }\n \n /* For comments.rs, which hackily pokes into 'pos' and 'curr' */\n-fn new_low_level_string_reader(span_diagnostic: diagnostic::span_handler,\n+fn new_low_level_string_reader(span_diagnostic: span_handler,\n                                filemap: codemap::filemap,\n-                               itr: @interner::interner<@str>)\n+                               itr: @interner<@str>)\n     -> string_reader {\n     let r = @{span_diagnostic: span_diagnostic, src: filemap.src,\n               mut col: 0u, mut pos: 0u, mut curr: -1 as char,\n@@ -131,8 +78,8 @@ impl string_reader_as_reader of reader for string_reader {\n     fn fatal(m: str) -> ! {\n         self.span_diagnostic.span_fatal(copy self.peek_span, m)\n     }\n-    fn span_diag() -> diagnostic::span_handler { self.span_diagnostic }\n-    fn interner() -> @interner::interner<@str> { self.interner }\n+    fn span_diag() -> span_handler { self.span_diagnostic }\n+    fn interner() -> @interner<@str> { self.interner }\n     fn peek() -> {tok: token::token, sp: span} {\n         {tok: self.peek_tok, sp: self.peek_span}\n     }\n@@ -153,8 +100,8 @@ impl tt_reader_as_reader of reader for tt_reader {\n     fn fatal(m: str) -> ! {\n         self.span_diagnostic.span_fatal(copy self.cur_span, m);\n     }\n-    fn span_diag() -> diagnostic::span_handler { self.span_diagnostic }\n-    fn interner() -> @interner::interner<@str> { self.interner }\n+    fn span_diag() -> span_handler { self.span_diagnostic }\n+    fn interner() -> @interner<@str> { self.interner }\n     fn peek() -> {tok: token::token, sp: span} {\n         { tok: self.cur_tok, sp: self.cur_span }\n     }\n@@ -178,42 +125,6 @@ fn string_advance_token(&&r: string_reader) {\n \n }\n \n-fn tt_next_token(&&r: tt_reader) -> {tok: token::token, sp: span} {\n-    let ret_val = { tok: r.cur_tok, sp: r.cur_span };\n-    if r.cur.idx >= vec::len(r.cur.readme) {\n-        /* done with this set; pop */\n-        alt r.cur.up {\n-          tt_frame_up(none) {\n-            r.cur_tok = token::EOF;\n-            ret ret_val;\n-          }\n-          tt_frame_up(some(tt_f)) {\n-            r.cur = tt_f;\n-            /* the above `if` would need to be a `while` if we didn't know\n-            that the last thing in a `tt_delim` is always a `tt_flat` */\n-            r.cur.idx += 1u;\n-          }\n-        }\n-    }\n-    /* if `tt_delim`s could be 0-length, we'd need to be able to switch\n-    between popping and pushing until we got to an actual `tt_flat` */\n-    loop { /* because it's easiest, this handles `tt_delim` not starting\n-    with a `tt_flat`, even though it won't happen */\n-        alt copy r.cur.readme[r.cur.idx] {\n-          tt_delim(tts) {\n-            r.cur = @{readme: tts, mut idx: 0u,\n-                      up: tt_frame_up(option::some(r.cur)) };\n-          }\n-          tt_flat(sp, tok) {\n-            r.cur_span = sp; r.cur_tok = tok;\n-            r.cur.idx += 1u;\n-            ret ret_val;\n-          }\n-        }\n-    }\n-\n-}\n-\n fn get_str_from(rdr: string_reader, start: uint) -> str unsafe {\n     // I'm pretty skeptical about this subtraction. What if there's a\n     // multi-byte character before the mark?\n@@ -548,7 +459,7 @@ fn next_token_inner(rdr: string_reader) -> token::token {\n         let is_mod_name = c == ':' && nextch(rdr) == ':';\n \n         // FIXME: perform NFKC normalization here. (Issue #2253)\n-        ret token::IDENT(interner::intern(*rdr.interner,\n+        ret token::IDENT(intern(*rdr.interner,\n                                           @accum_str), is_mod_name);\n     }\n     if is_dec_digit(c) {\n@@ -713,8 +624,7 @@ fn next_token_inner(rdr: string_reader) -> token::token {\n             }\n         }\n         bump(rdr);\n-        ret token::LIT_STR(interner::intern(*rdr.interner,\n-                                            @accum_str));\n+        ret token::LIT_STR(intern(*rdr.interner, @accum_str));\n       }\n       '-' {\n         if nextch(rdr) == '>' {"}, {"sha": "90c5b3f3720c060b49a44403bb865c17970c9318", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=39590d81f01b97da574de3298f664eeabf84d255", "patch": "@@ -70,6 +70,7 @@ class parser {\n     let mut buffer_start: int;\n     let mut buffer_end: int;\n     let mut restriction: restriction;\n+    let mut quote_depth: uint; // not (yet) related to the quasiquoter\n     let reader: reader;\n     let keywords: hashmap<str, ()>;\n     let restricted_keywords: hashmap<str, ()>;\n@@ -94,6 +95,7 @@ class parser {\n         self.buffer_start = 0;\n         self.buffer_end = 0;\n         self.restriction = UNRESTRICTED;\n+        self.quote_depth = 0u;\n         self.keywords = token::keyword_table();\n         self.restricted_keywords = token::restricted_keyword_table();\n     }\n@@ -1067,6 +1069,11 @@ class parser {\n         }\n \n         fn parse_tt_flat(p: parser, delim_ok: bool) -> token_tree {\n+            if p.eat_keyword(\"many\") && p.quote_depth > 0u {\n+                ret tt_dotdotdot(\n+                    p.parse_seq(token::LPAREN, token::RPAREN, seq_sep_none(),\n+                                |p| p.parse_token_tree()).node);\n+            }\n             alt p.token {\n               token::RPAREN | token::RBRACE | token::RBRACKET\n               if !delim_ok {\n@@ -1076,6 +1083,11 @@ class parser {\n               token::EOF {\n                 p.fatal(\"file ended in the middle of a macro invocation\");\n               }\n+              /* we ought to allow different depths of unquotation */\n+              token::DOLLAR if p.quote_depth > 0u {\n+                p.bump();\n+                ret tt_interpolate(p.parse_ident());\n+              }\n               _ { /* ok */ }\n             }\n             let res = tt_flat(p.span, p.token);\n@@ -1104,10 +1116,11 @@ class parser {\n                                 common::seq_sep_none(),\n                                 |p| p.parse_matcher(@mut 0u)).node;\n         let tt = self.parse_token_tree();\n+        //let tt_rhs = self.parse_token_tree();\n         alt tt {\n           tt_delim(tts) {\n             let rdr = lexer::new_tt_reader(self.reader.span_diag(),\n-                                           self.reader.interner(), tts)\n+                                           self.reader.interner(), none, tts)\n                 as reader;\n             ext::earley_parser::parse(self.sess, self.cfg, rdr, ms);\n           }"}, {"sha": "4e61bf09426c4e11fb0b636f200f6aae4cc05095", "filename": "src/libsyntax/syntax.rc", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fsyntax.rc", "raw_url": "https://github.com/rust-lang/rust/raw/39590d81f01b97da574de3298f664eeabf84d255/src%2Flibsyntax%2Fsyntax.rc", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fsyntax.rc?ref=39590d81f01b97da574de3298f664eeabf84d255", "patch": "@@ -66,6 +66,9 @@ mod ext {\n     mod build;\n \n     mod earley_parser;\n+    mod tt {\n+        mod transcribe;\n+    }\n \n     mod fmt;\n     mod env;"}]}