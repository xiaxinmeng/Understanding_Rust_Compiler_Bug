{"sha": "def3269a71be2e737cad27418a3dad9f5bd6cd32", "node_id": "MDY6Q29tbWl0NzI0NzEyOmRlZjMyNjlhNzFiZTJlNzM3Y2FkMjc0MThhM2RhZDlmNWJkNmNkMzI=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2018-01-30T11:10:06Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2018-01-30T11:10:06Z"}, "message": "Auto merge of #47870 - kennytm:rollup, r=kennytm\n\nRollup of 12 pull requests\n\n- Successful merges: #47515, #47603, #47718, #47732, #47760, #47780, #47822, #47826, #47836, #47839, #47853, #47855\n- Failed merges:", "tree": {"sha": "1b6a3bf9de28edffa1c9ac671aa066d1c079e35d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/1b6a3bf9de28edffa1c9ac671aa066d1c079e35d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/def3269a71be2e737cad27418a3dad9f5bd6cd32", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/def3269a71be2e737cad27418a3dad9f5bd6cd32", "html_url": "https://github.com/rust-lang/rust/commit/def3269a71be2e737cad27418a3dad9f5bd6cd32", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/def3269a71be2e737cad27418a3dad9f5bd6cd32/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fe7e1a45f37f4265434cead827f587e75412f85c", "url": "https://api.github.com/repos/rust-lang/rust/commits/fe7e1a45f37f4265434cead827f587e75412f85c", "html_url": "https://github.com/rust-lang/rust/commit/fe7e1a45f37f4265434cead827f587e75412f85c"}, {"sha": "393a1994af93977fe1e35fb4ec308bc4e5d1a6dd", "url": "https://api.github.com/repos/rust-lang/rust/commits/393a1994af93977fe1e35fb4ec308bc4e5d1a6dd", "html_url": "https://github.com/rust-lang/rust/commit/393a1994af93977fe1e35fb4ec308bc4e5d1a6dd"}], "stats": {"total": 894, "additions": 706, "deletions": 188}, "files": [{"sha": "1272643edd2599d9d5b8a95428eaa0fbb3f10059", "filename": "src/bootstrap/builder.rs", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Fbootstrap%2Fbuilder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Fbootstrap%2Fbuilder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fbootstrap%2Fbuilder.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -469,6 +469,18 @@ impl<'a> Builder<'a> {\n             stage = compiler.stage;\n         }\n \n+        let mut extra_args = env::var(&format!(\"RUSTFLAGS_STAGE_{}\", stage)).unwrap_or_default();\n+        if stage != 0 {\n+            let s = env::var(\"RUSTFLAGS_STAGE_NOT_0\").unwrap_or_default();\n+            extra_args.push_str(\" \");\n+            extra_args.push_str(&s);\n+        }\n+\n+        if !extra_args.is_empty() {\n+            cargo.env(\"RUSTFLAGS\",\n+                format!(\"{} {}\", env::var(\"RUSTFLAGS\").unwrap_or_default(), extra_args));\n+        }\n+\n         // Customize the compiler we're running. Specify the compiler to cargo\n         // as our shim and then pass it some various options used to configure\n         // how the actual compiler itself is called."}, {"sha": "e8e2132dca254e01aae972b8f7c86c51abffd808", "filename": "src/doc/unstable-book/src/language-features/generators.md", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Fdoc%2Funstable-book%2Fsrc%2Flanguage-features%2Fgenerators.md", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Fdoc%2Funstable-book%2Fsrc%2Flanguage-features%2Fgenerators.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdoc%2Funstable-book%2Fsrc%2Flanguage-features%2Fgenerators.md?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -139,11 +139,11 @@ closure-like semantics. Namely:\n   types and such.\n \n * Traits like `Send` and `Sync` are automatically implemented for a `Generator`\n-  depending on the captured variables of the environment. Unlike closures though\n+  depending on the captured variables of the environment. Unlike closures,\n   generators also depend on variables live across suspension points. This means\n   that although the ambient environment may be `Send` or `Sync`, the generator\n   itself may not be due to internal variables live across `yield` points being\n-  not-`Send` or not-`Sync`. Note, though, that generators, like closures, do\n+  not-`Send` or not-`Sync`. Note that generators, like closures, do\n   not implement traits like `Copy` or `Clone` automatically.\n \n * Whenever a generator is dropped it will drop all captured environment\n@@ -155,7 +155,7 @@ lifted at a future date, the design is ongoing!\n \n ### Generators as state machines\n \n-In the compiler generators are currently compiled as state machines. Each\n+In the compiler, generators are currently compiled as state machines. Each\n `yield` expression will correspond to a different state that stores all live\n variables over that suspension point. Resumption of a generator will dispatch on\n the current state and then execute internally until a `yield` is reached, at"}, {"sha": "b320bed54320a11da4581742c50f7e51e04a794d", "filename": "src/liballoc/btree/map.rs", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Fliballoc%2Fbtree%2Fmap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Fliballoc%2Fbtree%2Fmap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Fbtree%2Fmap.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -1748,6 +1748,11 @@ impl<'a, K: Ord, Q: ?Sized, V> Index<&'a Q> for BTreeMap<K, V>\n {\n     type Output = V;\n \n+    /// Returns a reference to the value corresponding to the supplied key.\n+    ///\n+    /// # Panics\n+    ///\n+    /// Panics if the key is not present in the `BTreeMap`.\n     #[inline]\n     fn index(&self, key: &Q) -> &V {\n         self.get(key).expect(\"no entry found for key\")"}, {"sha": "ffb5efd93ed54fad04029b4638b20703e95ab719", "filename": "src/librustc_errors/emitter.rs", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibrustc_errors%2Femitter.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibrustc_errors%2Femitter.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_errors%2Femitter.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -1014,8 +1014,21 @@ impl EmitterWriter {\n \n                 // Then, the secondary file indicator\n                 buffer.prepend(buffer_msg_line_offset + 1, \"::: \", Style::LineNumber);\n+                let loc = if let Some(first_line) = annotated_file.lines.first() {\n+                    let col = if let Some(first_annotation) = first_line.annotations.first() {\n+                        format!(\":{}\", first_annotation.start_col + 1)\n+                    } else {\n+                        \"\".to_string()\n+                    };\n+                    format!(\"{}:{}{}\",\n+                            annotated_file.file.name,\n+                            cm.doctest_offset_line(first_line.line_index),\n+                            col)\n+                } else {\n+                    annotated_file.file.name.to_string()\n+                };\n                 buffer.append(buffer_msg_line_offset + 1,\n-                              &annotated_file.file.name.to_string(),\n+                              &loc,\n                               Style::LineAndColumn);\n                 for _ in 0..max_line_num_len {\n                     buffer.prepend(buffer_msg_line_offset + 1, \" \", Style::NoStyle);"}, {"sha": "6035f33c822cee1eedf6b87ad412c1b4d9f87fb8", "filename": "src/librustc_errors/snippet.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibrustc_errors%2Fsnippet.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibrustc_errors%2Fsnippet.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_errors%2Fsnippet.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -27,7 +27,8 @@ pub struct FileInfo {\n \n     /// The \"primary file\", if any, gets a `-->` marker instead of\n     /// `>>>`, and has a line-number/column printed and not just a\n-    /// filename.  It appears first in the listing. It is known to\n+    /// filename (other files are not guaranteed to have line numbers\n+    /// or columns). It appears first in the listing. It is known to\n     /// contain at least one primary span, though primary spans (which\n     /// are designated with `^^^`) may also occur in other files.\n     primary_span: Option<Span>,"}, {"sha": "843231d376f6c49a82794b68dd447823597f234a", "filename": "src/librustc_trans/llvm_util.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibrustc_trans%2Fllvm_util.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibrustc_trans%2Fllvm_util.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fllvm_util.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -79,16 +79,16 @@ unsafe fn configure_llvm(sess: &Session) {\n // detection code will walk past the end of the feature array,\n // leading to crashes.\n \n-const ARM_WHITELIST: &'static [&'static str] = &[\"neon\\0\", \"vfp2\\0\", \"vfp3\\0\", \"vfp4\\0\"];\n+const ARM_WHITELIST: &'static [&'static str] = &[\"neon\\0\", \"v7\\0\", \"vfp2\\0\", \"vfp3\\0\", \"vfp4\\0\"];\n \n-const AARCH64_WHITELIST: &'static [&'static str] = &[\"neon\\0\"];\n+const AARCH64_WHITELIST: &'static [&'static str] = &[\"neon\\0\", \"v7\\0\"];\n \n const X86_WHITELIST: &'static [&'static str] = &[\"avx\\0\", \"avx2\\0\", \"bmi\\0\", \"bmi2\\0\", \"sse\\0\",\n                                                  \"sse2\\0\", \"sse3\\0\", \"sse4.1\\0\", \"sse4.2\\0\",\n                                                  \"ssse3\\0\", \"tbm\\0\", \"lzcnt\\0\", \"popcnt\\0\",\n                                                  \"sse4a\\0\", \"rdrnd\\0\", \"rdseed\\0\", \"fma\\0\",\n                                                  \"xsave\\0\", \"xsaveopt\\0\", \"xsavec\\0\",\n-                                                 \"xsaves\\0\",\n+                                                 \"xsaves\\0\", \"aes\\0\",\n                                                  \"avx512bw\\0\", \"avx512cd\\0\",\n                                                  \"avx512dq\\0\", \"avx512er\\0\",\n                                                  \"avx512f\\0\", \"avx512ifma\\0\","}, {"sha": "82ced00644da8ea09418af69b07c3fd81c12c9ab", "filename": "src/librustdoc/html/markdown.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibrustdoc%2Fhtml%2Fmarkdown.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibrustdoc%2Fhtml%2Fmarkdown.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fmarkdown.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -872,7 +872,7 @@ pub fn render(w: &mut fmt::Formatter,\n         let link_out = format!(\"<a href=\\\"{link}\\\"{title}>{content}</a>\",\n                                link = link_buf,\n                                title = title.map_or(String::new(),\n-                                                    |t| format!(\" title=\\\"{}\\\"\", t)),\n+                                                    |t| format!(\" title=\\\"{}\\\"\", Escape(&t))),\n                                content = content.unwrap_or(String::new()));\n \n         unsafe { hoedown_buffer_put(ob, link_out.as_ptr(), link_out.len()); }"}, {"sha": "82a687ae5e4930cd9fa73d6113ba287f5873d0cb", "filename": "src/libstd/collections/hash/map.rs", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibstd%2Fcollections%2Fhash%2Fmap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibstd%2Fcollections%2Fhash%2Fmap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcollections%2Fhash%2Fmap.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -1384,9 +1384,14 @@ impl<'a, K, Q: ?Sized, V, S> Index<&'a Q> for HashMap<K, V, S>\n {\n     type Output = V;\n \n+    /// Returns a reference to the value corresponding to the supplied key.\n+    ///\n+    /// # Panics\n+    ///\n+    /// Panics if the key is not present in the `HashMap`.\n     #[inline]\n-    fn index(&self, index: &Q) -> &V {\n-        self.get(index).expect(\"no entry found for key\")\n+    fn index(&self, key: &Q) -> &V {\n+        self.get(key).expect(\"no entry found for key\")\n     }\n }\n "}, {"sha": "9b2f815b71383b0e6fdca479ecee7ded45264321", "filename": "src/libstd/process.rs", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibstd%2Fprocess.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibstd%2Fprocess.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fprocess.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -1843,4 +1843,10 @@ mod tests {\n         }\n         assert!(events > 0);\n     }\n+\n+    #[test]\n+    fn test_command_implements_send() {\n+        fn take_send_type<T: Send>(_: T) {}\n+        take_send_type(Command::new(\"\"))\n+    }\n }"}, {"sha": "7e057401fab70a2a0d41407677e97c90916e9a2b", "filename": "src/libstd/sys/unix/process/process_common.rs", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibstd%2Fsys%2Funix%2Fprocess%2Fprocess_common.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibstd%2Fsys%2Funix%2Fprocess%2Fprocess_common.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsys%2Funix%2Fprocess%2Fprocess_common.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -45,7 +45,7 @@ pub struct Command {\n     // other keys.\n     program: CString,\n     args: Vec<CString>,\n-    argv: Vec<*const c_char>,\n+    argv: Argv,\n     env: CommandEnv<DefaultEnvKey>,\n \n     cwd: Option<CString>,\n@@ -58,6 +58,12 @@ pub struct Command {\n     stderr: Option<Stdio>,\n }\n \n+// Create a new type for argv, so that we can make it `Send`\n+struct Argv(Vec<*const c_char>);\n+\n+// It is safe to make Argv Send, because it contains pointers to memory owned by `Command.args`\n+unsafe impl Send for Argv {}\n+\n // passed back to std::process with the pipes connected to the child, if any\n // were requested\n pub struct StdioPipes {\n@@ -92,7 +98,7 @@ impl Command {\n         let mut saw_nul = false;\n         let program = os2c(program, &mut saw_nul);\n         Command {\n-            argv: vec![program.as_ptr(), ptr::null()],\n+            argv: Argv(vec![program.as_ptr(), ptr::null()]),\n             program,\n             args: Vec::new(),\n             env: Default::default(),\n@@ -111,8 +117,8 @@ impl Command {\n         // Overwrite the trailing NULL pointer in `argv` and then add a new null\n         // pointer.\n         let arg = os2c(arg, &mut self.saw_nul);\n-        self.argv[self.args.len() + 1] = arg.as_ptr();\n-        self.argv.push(ptr::null());\n+        self.argv.0[self.args.len() + 1] = arg.as_ptr();\n+        self.argv.0.push(ptr::null());\n \n         // Also make sure we keep track of the owned value to schedule a\n         // destructor for this memory.\n@@ -133,7 +139,7 @@ impl Command {\n         self.saw_nul\n     }\n     pub fn get_argv(&self) -> &Vec<*const c_char> {\n-        &self.argv\n+        &self.argv.0\n     }\n \n     #[allow(dead_code)]"}, {"sha": "1a9849ca5307de616f1dd4f662ac94e4457a828f", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 351, "deletions": 126, "changes": 477, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -90,8 +90,8 @@ use codemap::Spanned;\n use errors::FatalError;\n use ext::tt::quoted::{self, TokenTree};\n use parse::{Directory, ParseSess};\n-use parse::parser::{PathStyle, Parser};\n-use parse::token::{self, DocComment, Token, Nonterminal};\n+use parse::parser::{Parser, PathStyle};\n+use parse::token::{self, DocComment, Nonterminal, Token};\n use print::pprust;\n use symbol::keywords;\n use tokenstream::TokenStream;\n@@ -100,25 +100,29 @@ use util::small_vector::SmallVector;\n use std::mem;\n use std::rc::Rc;\n use std::collections::HashMap;\n-use std::collections::hash_map::Entry::{Vacant, Occupied};\n+use std::collections::hash_map::Entry::{Occupied, Vacant};\n \n-// To avoid costly uniqueness checks, we require that `MatchSeq` always has\n-// a nonempty body.\n+// To avoid costly uniqueness checks, we require that `MatchSeq` always has a nonempty body.\n \n+/// Either a sequence of token trees or a single one. This is used as the representation of the\n+/// sequence of tokens that make up a matcher.\n #[derive(Clone)]\n enum TokenTreeOrTokenTreeVec {\n     Tt(TokenTree),\n     TtSeq(Vec<TokenTree>),\n }\n \n impl TokenTreeOrTokenTreeVec {\n+    /// Returns the number of constituent top-level token trees of `self` (top-level in that it\n+    /// will not recursively descend into subtrees).\n     fn len(&self) -> usize {\n         match *self {\n             TtSeq(ref v) => v.len(),\n             Tt(ref tt) => tt.len(),\n         }\n     }\n \n+    /// The the `index`-th token tree of `self`.\n     fn get_tt(&self, index: usize) -> TokenTree {\n         match *self {\n             TtSeq(ref v) => v[index].clone(),\n@@ -127,36 +131,96 @@ impl TokenTreeOrTokenTreeVec {\n     }\n }\n \n-/// an unzipping of `TokenTree`s\n+/// An unzipping of `TokenTree`s... see the `stack` field of `MatcherPos`.\n+///\n+/// This is used by `inner_parse_loop` to keep track of delimited submatchers that we have\n+/// descended into.\n #[derive(Clone)]\n struct MatcherTtFrame {\n+    /// The \"parent\" matcher that we are descending into.\n     elts: TokenTreeOrTokenTreeVec,\n+    /// The position of the \"dot\" in `elts` at the time we descended.\n     idx: usize,\n }\n \n+/// Represents a single \"position\" (aka \"matcher position\", aka \"item\"), as described in the module\n+/// documentation.\n #[derive(Clone)]\n struct MatcherPos {\n-    stack: Vec<MatcherTtFrame>,\n+    /// The token or sequence of tokens that make up the matcher\n     top_elts: TokenTreeOrTokenTreeVec,\n-    sep: Option<Token>,\n+    /// The position of the \"dot\" in this matcher\n     idx: usize,\n-    up: Option<Box<MatcherPos>>,\n+    /// The beginning position in the source that the beginning of this matcher corresponds to. In\n+    /// other words, the token in the source at `sp_lo` is matched against the first token of the\n+    /// matcher.\n+    sp_lo: BytePos,\n+\n+    /// For each named metavar in the matcher, we keep track of token trees matched against the\n+    /// metavar by the black box parser. In particular, there may be more than one match per\n+    /// metavar if we are in a repetition (each repetition matches each of the variables).\n+    /// Moreover, matchers and repetitions can be nested; the `matches` field is shared (hence the\n+    /// `Rc`) among all \"nested\" matchers. `match_lo`, `match_cur`, and `match_hi` keep track of\n+    /// the current position of the `self` matcher position in the shared `matches` list.\n+    ///\n+    /// Also, note that while we are descending into a sequence, matchers are given their own\n+    /// `matches` vector. Only once we reach the end of a full repetition of the sequence do we add\n+    /// all bound matches from the submatcher into the shared top-level `matches` vector. If `sep`\n+    /// and `up` are `Some`, then `matches` is _not_ the shared top-level list. Instead, if one\n+    /// wants the shared `matches`, one should use `up.matches`.\n     matches: Vec<Rc<Vec<NamedMatch>>>,\n+    /// The position in `matches` corresponding to the first metavar in this matcher's sequence of\n+    /// token trees. In other words, the first metavar in the first token of `top_elts` corresponds\n+    /// to `matches[match_lo]`.\n     match_lo: usize,\n+    /// The position in `matches` corresponding to the metavar we are currently trying to match\n+    /// against the source token stream. `match_lo <= match_cur <= match_hi`.\n     match_cur: usize,\n+    /// Similar to `match_lo` except `match_hi` is the position in `matches` of the _last_ metavar\n+    /// in this matcher.\n     match_hi: usize,\n-    sp_lo: BytePos,\n+\n+    // Specifically used if we are matching a repetition. If we aren't both should be `None`.\n+    /// The separator if we are in a repetition\n+    sep: Option<Token>,\n+    /// The \"parent\" matcher position if we are in a repetition. That is, the matcher position just\n+    /// before we enter the sequence.\n+    up: Option<Box<MatcherPos>>,\n+\n+    // Specifically used to \"unzip\" token trees. By \"unzip\", we mean to unwrap the delimiters from\n+    // a delimited token tree (e.g. something wrapped in `(` `)`) or to get the contents of a doc\n+    // comment...\n+    /// When matching against matchers with nested delimited submatchers (e.g. `pat ( pat ( .. )\n+    /// pat ) pat`), we need to keep track of the matchers we are descending into. This stack does\n+    /// that where the bottom of the stack is the outermost matcher.\n+    // Also, throughout the comments, this \"descent\" is often referred to as \"unzipping\"...\n+    stack: Vec<MatcherTtFrame>,\n }\n \n impl MatcherPos {\n+    /// Add `m` as a named match for the `idx`-th metavar.\n     fn push_match(&mut self, idx: usize, m: NamedMatch) {\n         let matches = Rc::make_mut(&mut self.matches[idx]);\n         matches.push(m);\n     }\n }\n \n+/// Represents the possible results of an attempted parse.\n+pub enum ParseResult<T> {\n+    /// Parsed successfully.\n+    Success(T),\n+    /// Arm failed to match. If the second parameter is `token::Eof`, it indicates an unexpected\n+    /// end of macro invocation. Otherwise, it indicates that no rules expected the given token.\n+    Failure(syntax_pos::Span, Token),\n+    /// Fatal error (malformed macro?). Abort compilation.\n+    Error(syntax_pos::Span, String),\n+}\n+\n+/// A `ParseResult` where the `Success` variant contains a mapping of `Ident`s to `NamedMatch`es.\n+/// This represents the mapping of metavars to the token trees they bind to.\n pub type NamedParseResult = ParseResult<HashMap<Ident, Rc<NamedMatch>>>;\n \n+/// Count how many metavars are named in the given matcher `ms`.\n pub fn count_names(ms: &[TokenTree]) -> usize {\n     ms.iter().fold(0, |count, elt| {\n         count + match *elt {\n@@ -169,20 +233,38 @@ pub fn count_names(ms: &[TokenTree]) -> usize {\n     })\n }\n \n+/// Initialize `len` empty shared `Vec`s to be used to store matches of metavars.\n+fn create_matches(len: usize) -> Vec<Rc<Vec<NamedMatch>>> {\n+    (0..len).into_iter().map(|_| Rc::new(Vec::new())).collect()\n+}\n+\n+/// Generate the top-level matcher position in which the \"dot\" is before the first token of the\n+/// matcher `ms` and we are going to start matching at position `lo` in the source.\n fn initial_matcher_pos(ms: Vec<TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n     let match_idx_hi = count_names(&ms[..]);\n     let matches = create_matches(match_idx_hi);\n     Box::new(MatcherPos {\n-        stack: vec![],\n-        top_elts: TtSeq(ms),\n-        sep: None,\n+        // Start with the top level matcher given to us\n+        top_elts: TtSeq(ms), // \"elts\" is an abbr. for \"elements\"\n+        // The \"dot\" is before the first token of the matcher\n         idx: 0,\n-        up: None,\n+        // We start matching with byte `lo` in the source code\n+        sp_lo: lo,\n+\n+        // Initialize `matches` to a bunch of empty `Vec`s -- one for each metavar in `top_elts`.\n+        // `match_lo` for `top_elts` is 0 and `match_hi` is `matches.len()`. `match_cur` is 0 since\n+        // we haven't actually matched anything yet.\n         matches,\n         match_lo: 0,\n         match_cur: 0,\n         match_hi: match_idx_hi,\n-        sp_lo: lo\n+\n+        // Haven't descended into any delimiters, so empty stack\n+        stack: vec![],\n+\n+        // Haven't descended into any sequences, so both of these are `None`.\n+        sep: None,\n+        up: None,\n     })\n }\n \n@@ -202,29 +284,36 @@ fn initial_matcher_pos(ms: Vec<TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n /// token tree. The depth of the `NamedMatch` structure will therefore depend\n /// only on the nesting depth of `ast::TTSeq`s in the originating\n /// token tree it was derived from.\n-\n #[derive(Debug, Clone)]\n pub enum NamedMatch {\n     MatchedSeq(Rc<Vec<NamedMatch>>, syntax_pos::Span),\n-    MatchedNonterminal(Rc<Nonterminal>)\n+    MatchedNonterminal(Rc<Nonterminal>),\n }\n \n-fn nameize<I: Iterator<Item=NamedMatch>>(sess: &ParseSess, ms: &[TokenTree], mut res: I)\n-                                             -> NamedParseResult {\n-    fn n_rec<I: Iterator<Item=NamedMatch>>(sess: &ParseSess, m: &TokenTree, res: &mut I,\n-             ret_val: &mut HashMap<Ident, Rc<NamedMatch>>)\n-             -> Result<(), (syntax_pos::Span, String)> {\n+/// Takes a sequence of token trees `ms` representing a matcher which successfully matched input\n+/// and an iterator of items that matched input and produces a `NamedParseResult`.\n+fn nameize<I: Iterator<Item = NamedMatch>>(\n+    sess: &ParseSess,\n+    ms: &[TokenTree],\n+    mut res: I,\n+) -> NamedParseResult {\n+    // Recursively descend into each type of matcher (e.g. sequences, delimited, metavars) and make\n+    // sure that each metavar has _exactly one_ binding. If a metavar does not have exactly one\n+    // binding, then there is an error. If it does, then we insert the binding into the\n+    // `NamedParseResult`.\n+    fn n_rec<I: Iterator<Item = NamedMatch>>(\n+        sess: &ParseSess,\n+        m: &TokenTree,\n+        res: &mut I,\n+        ret_val: &mut HashMap<Ident, Rc<NamedMatch>>,\n+    ) -> Result<(), (syntax_pos::Span, String)> {\n         match *m {\n-            TokenTree::Sequence(_, ref seq) => {\n-                for next_m in &seq.tts {\n-                    n_rec(sess, next_m, res.by_ref(), ret_val)?\n-                }\n-            }\n-            TokenTree::Delimited(_, ref delim) => {\n-                for next_m in &delim.tts {\n-                    n_rec(sess, next_m, res.by_ref(), ret_val)?;\n-                }\n-            }\n+            TokenTree::Sequence(_, ref seq) => for next_m in &seq.tts {\n+                n_rec(sess, next_m, res.by_ref(), ret_val)?\n+            },\n+            TokenTree::Delimited(_, ref delim) => for next_m in &delim.tts {\n+                n_rec(sess, next_m, res.by_ref(), ret_val)?;\n+            },\n             TokenTree::MetaVarDecl(span, _, id) if id.name == keywords::Invalid.name() => {\n                 if sess.missing_fragment_specifiers.borrow_mut().remove(&span) {\n                     return Err((span, \"missing fragment specifier\".to_string()));\n@@ -250,33 +339,28 @@ fn nameize<I: Iterator<Item=NamedMatch>>(sess: &ParseSess, ms: &[TokenTree], mut\n     let mut ret_val = HashMap::new();\n     for m in ms {\n         match n_rec(sess, m, res.by_ref(), &mut ret_val) {\n-            Ok(_) => {},\n+            Ok(_) => {}\n             Err((sp, msg)) => return Error(sp, msg),\n         }\n     }\n \n     Success(ret_val)\n }\n \n-pub enum ParseResult<T> {\n-    Success(T),\n-    /// Arm failed to match. If the second parameter is `token::Eof`, it\n-    /// indicates an unexpected end of macro invocation. Otherwise, it\n-    /// indicates that no rules expected the given token.\n-    Failure(syntax_pos::Span, Token),\n-    /// Fatal error (malformed macro?). Abort compilation.\n-    Error(syntax_pos::Span, String)\n-}\n-\n+/// Generate an appropriate parsing failure message. For EOF, this is \"unexpected end...\". For\n+/// other tokens, this is \"unexpected token...\".\n pub fn parse_failure_msg(tok: Token) -> String {\n     match tok {\n         token::Eof => \"unexpected end of macro invocation\".to_string(),\n-        _ => format!(\"no rules expected the token `{}`\", pprust::token_to_string(&tok)),\n+        _ => format!(\n+            \"no rules expected the token `{}`\",\n+            pprust::token_to_string(&tok)\n+        ),\n     }\n }\n \n /// Perform a token equality check, ignoring syntax context (that is, an unhygienic comparison)\n-fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n+fn token_name_eq(t1: &Token, t2: &Token) -> bool {\n     if let (Some(id1), Some(id2)) = (t1.ident(), t2.ident()) {\n         id1.name == id2.name\n     } else if let (&token::Lifetime(id1), &token::Lifetime(id2)) = (t1, t2) {\n@@ -286,77 +370,121 @@ fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n     }\n }\n \n-fn create_matches(len: usize) -> Vec<Rc<Vec<NamedMatch>>> {\n-    (0..len).into_iter().map(|_| Rc::new(Vec::new())).collect()\n-}\n-\n-fn inner_parse_loop(sess: &ParseSess,\n-                    cur_items: &mut SmallVector<Box<MatcherPos>>,\n-                    next_items: &mut Vec<Box<MatcherPos>>,\n-                    eof_items: &mut SmallVector<Box<MatcherPos>>,\n-                    bb_items: &mut SmallVector<Box<MatcherPos>>,\n-                    token: &Token,\n-                    span: syntax_pos::Span)\n-                    -> ParseResult<()> {\n+/// Process the matcher positions of `cur_items` until it is empty. In the process, this will\n+/// produce more items in `next_items`, `eof_items`, and `bb_items`.\n+///\n+/// For more info about the how this happens, see the module-level doc comments and the inline\n+/// comments of this function.\n+///\n+/// # Parameters\n+///\n+/// - `sess`: the parsing session into which errors are emitted.\n+/// - `cur_items`: the set of current items to be processed. This should be empty by the end of a\n+///   successful execution of this function.\n+/// - `next_items`: the set of newly generated items. These are used to replenish `cur_items` in\n+///   the function `parse`.\n+/// - `eof_items`: the set of items that would be valid if this was the EOF.\n+/// - `bb_items`: the set of items that are waiting for the black-box parser.\n+/// - `token`: the current token of the parser.\n+/// - `span`: the `Span` in the source code corresponding to the token trees we are trying to match\n+///   against the matcher positions in `cur_items`.\n+///\n+/// # Returns\n+///\n+/// A `ParseResult`. Note that matches are kept track of through the items generated.\n+fn inner_parse_loop(\n+    sess: &ParseSess,\n+    cur_items: &mut SmallVector<Box<MatcherPos>>,\n+    next_items: &mut Vec<Box<MatcherPos>>,\n+    eof_items: &mut SmallVector<Box<MatcherPos>>,\n+    bb_items: &mut SmallVector<Box<MatcherPos>>,\n+    token: &Token,\n+    span: syntax_pos::Span,\n+) -> ParseResult<()> {\n+    // Pop items from `cur_items` until it is empty.\n     while let Some(mut item) = cur_items.pop() {\n-        // When unzipped trees end, remove them\n+        // When unzipped trees end, remove them. This corresponds to backtracking out of a\n+        // delimited submatcher into which we already descended. In backtracking out again, we need\n+        // to advance the \"dot\" past the delimiters in the outer matcher.\n         while item.idx >= item.top_elts.len() {\n             match item.stack.pop() {\n                 Some(MatcherTtFrame { elts, idx }) => {\n                     item.top_elts = elts;\n                     item.idx = idx + 1;\n                 }\n-                None => break\n+                None => break,\n             }\n         }\n \n+        // Get the current position of the \"dot\" (`idx`) in `item` and the number of token trees in\n+        // the matcher (`len`).\n         let idx = item.idx;\n         let len = item.top_elts.len();\n \n-        // at end of sequence\n+        // If `idx >= len`, then we are at or past the end of the matcher of `item`.\n         if idx >= len {\n-            // We are repeating iff there is a parent\n+            // We are repeating iff there is a parent. If the matcher is inside of a repetition,\n+            // then we could be at the end of a sequence or at the beginning of the next\n+            // repetition.\n             if item.up.is_some() {\n-                // Disregarding the separator, add the \"up\" case to the tokens that should be\n-                // examined.\n-                // (remove this condition to make trailing seps ok)\n+                // At this point, regardless of whether there is a separator, we should add all\n+                // matches from the complete repetition of the sequence to the shared, top-level\n+                // `matches` list (actually, `up.matches`, which could itself not be the top-level,\n+                // but anyway...). Moreover, we add another item to `cur_items` in which the \"dot\"\n+                // is at the end of the `up` matcher. This ensures that the \"dot\" in the `up`\n+                // matcher is also advanced sufficiently.\n+                //\n+                // NOTE: removing the condition `idx == len` allows trailing separators.\n                 if idx == len {\n+                    // Get the `up` matcher\n                     let mut new_pos = item.up.clone().unwrap();\n \n-                    // update matches (the MBE \"parse tree\") by appending\n-                    // each tree as a subtree.\n-\n-                    // Only touch the binders we have actually bound\n+                    // Add matches from this repetition to the `matches` of `up`\n                     for idx in item.match_lo..item.match_hi {\n                         let sub = item.matches[idx].clone();\n                         let span = span.with_lo(item.sp_lo);\n                         new_pos.push_match(idx, MatchedSeq(sub, span));\n                     }\n \n+                    // Move the \"dot\" past the repetition in `up`\n                     new_pos.match_cur = item.match_hi;\n                     new_pos.idx += 1;\n                     cur_items.push(new_pos);\n                 }\n \n-                // Check if we need a separator\n+                // Check if we need a separator.\n                 if idx == len && item.sep.is_some() {\n-                    // We have a separator, and it is the current token.\n-                    if item.sep.as_ref().map(|sep| token_name_eq(token, sep)).unwrap_or(false) {\n+                    // We have a separator, and it is the current token. We can advance past the\n+                    // separator token.\n+                    if item.sep\n+                        .as_ref()\n+                        .map(|sep| token_name_eq(token, sep))\n+                        .unwrap_or(false)\n+                    {\n                         item.idx += 1;\n                         next_items.push(item);\n                     }\n-                } else { // we don't need a separator\n+                }\n+                // We don't need a separator. Move the \"dot\" back to the beginning of the matcher\n+                // and try to match again.\n+                else {\n                     item.match_cur = item.match_lo;\n                     item.idx = 0;\n                     cur_items.push(item);\n                 }\n-            } else {\n-                // We aren't repeating, so we must be potentially at the end of the input.\n+            }\n+            // If we are not in a repetition, then being at the end of a matcher means that we have\n+            // reached the potential end of the input.\n+            else {\n                 eof_items.push(item);\n             }\n-        } else {\n+        }\n+        // We are in the middle of a matcher.\n+        else {\n+            // Look at what token in the matcher we are trying to match the current token (`token`)\n+            // against. Depending on that, we may generate new items.\n             match item.top_elts.get_tt(idx) {\n-                /* need to descend into sequence */\n+                // Need to descend into a sequence\n                 TokenTree::Sequence(sp, seq) => {\n                     if seq.op == quoted::KleeneOp::ZeroOrMore {\n                         // Examine the case where there are 0 matches of this sequence\n@@ -384,18 +512,30 @@ fn inner_parse_loop(sess: &ParseSess,\n                         top_elts: Tt(TokenTree::Sequence(sp, seq)),\n                     }));\n                 }\n+\n+                // We need to match a metavar (but the identifier is invalid)... this is an error\n                 TokenTree::MetaVarDecl(span, _, id) if id.name == keywords::Invalid.name() => {\n                     if sess.missing_fragment_specifiers.borrow_mut().remove(&span) {\n                         return Error(span, \"missing fragment specifier\".to_string());\n                     }\n                 }\n+\n+                // We need to match a metavar with a valid ident... call out to the black-box\n+                // parser by adding an item to `bb_items`.\n                 TokenTree::MetaVarDecl(_, _, id) => {\n                     // Built-in nonterminals never start with these tokens,\n                     // so we can eliminate them from consideration.\n                     if may_begin_with(&*id.name.as_str(), token) {\n                         bb_items.push(item);\n                     }\n                 }\n+\n+                // We need to descend into a delimited submatcher or a doc comment. To do this, we\n+                // push the current matcher onto a stack and push a new item containing the\n+                // submatcher onto `cur_items`.\n+                //\n+                // At the beginning of the loop, if we reach the end of the delimited submatcher,\n+                // we pop the stack to backtrack out of the descent.\n                 seq @ TokenTree::Delimited(..) | seq @ TokenTree::Token(_, DocComment(..)) => {\n                     let lower_elts = mem::replace(&mut item.top_elts, Tt(seq));\n                     let idx = item.idx;\n@@ -406,83 +546,152 @@ fn inner_parse_loop(sess: &ParseSess,\n                     item.idx = 0;\n                     cur_items.push(item);\n                 }\n+\n+                // We just matched a normal token. We can just advance the parser.\n                 TokenTree::Token(_, ref t) if token_name_eq(t, token) => {\n                     item.idx += 1;\n                     next_items.push(item);\n                 }\n+\n+                // There was another token that was not `token`... This means we can't add any\n+                // rules. NOTE that this is not necessarily an error unless _all_ items in\n+                // `cur_items` end up doing this. There may still be some other matchers that do\n+                // end up working out.\n                 TokenTree::Token(..) | TokenTree::MetaVar(..) => {}\n             }\n         }\n     }\n \n+    // Yay a successful parse (so far)!\n     Success(())\n }\n \n-pub fn parse(sess: &ParseSess,\n-             tts: TokenStream,\n-             ms: &[TokenTree],\n-             directory: Option<Directory>,\n-             recurse_into_modules: bool)\n-             -> NamedParseResult {\n+/// Use the given sequence of token trees (`ms`) as a matcher. Match the given token stream `tts`\n+/// against it and return the match.\n+///\n+/// # Parameters\n+///\n+/// - `sess`: The session into which errors are emitted\n+/// - `tts`: The tokenstream we are matching against the pattern `ms`\n+/// - `ms`: A sequence of token trees representing a pattern against which we are matching\n+/// - `directory`: Information about the file locations (needed for the black-box parser)\n+/// - `recurse_into_modules`: Whether or not to recurse into modules (needed for the black-box\n+///   parser)\n+pub fn parse(\n+    sess: &ParseSess,\n+    tts: TokenStream,\n+    ms: &[TokenTree],\n+    directory: Option<Directory>,\n+    recurse_into_modules: bool,\n+) -> NamedParseResult {\n+    // Create a parser that can be used for the \"black box\" parts.\n     let mut parser = Parser::new(sess, tts, directory, recurse_into_modules, true);\n+\n+    // A queue of possible matcher positions. We initialize it with the matcher position in which\n+    // the \"dot\" is before the first token of the first token tree in `ms`. `inner_parse_loop` then\n+    // processes all of these possible matcher positions and produces posible next positions into\n+    // `next_items`. After some post-processing, the contents of `next_items` replenish `cur_items`\n+    // and we start over again.\n     let mut cur_items = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo()));\n-    let mut next_items = Vec::new(); // or proceed normally\n+    let mut next_items = Vec::new();\n \n     loop {\n-        let mut bb_items = SmallVector::new(); // black-box parsed by parser.rs\n+        // Matcher positions black-box parsed by parser.rs (`parser`)\n+        let mut bb_items = SmallVector::new();\n+\n+        // Matcher positions that would be valid if the macro invocation was over now\n         let mut eof_items = SmallVector::new();\n         assert!(next_items.is_empty());\n \n-        match inner_parse_loop(sess, &mut cur_items, &mut next_items, &mut eof_items, &mut bb_items,\n-                               &parser.token, parser.span) {\n-            Success(_) => {},\n+        // Process `cur_items` until either we have finished the input or we need to get some\n+        // parsing from the black-box parser done. The result is that `next_items` will contain a\n+        // bunch of possible next matcher positions in `next_items`.\n+        match inner_parse_loop(\n+            sess,\n+            &mut cur_items,\n+            &mut next_items,\n+            &mut eof_items,\n+            &mut bb_items,\n+            &parser.token,\n+            parser.span,\n+        ) {\n+            Success(_) => {}\n             Failure(sp, tok) => return Failure(sp, tok),\n             Error(sp, msg) => return Error(sp, msg),\n         }\n \n         // inner parse loop handled all cur_items, so it's empty\n         assert!(cur_items.is_empty());\n \n-        /* error messages here could be improved with links to orig. rules */\n+        // We need to do some post processing after the `inner_parser_loop`.\n+        //\n+        // Error messages here could be improved with links to original rules.\n+\n+        // If we reached the EOF, check that there is EXACTLY ONE possible matcher. Otherwise,\n+        // either the parse is ambiguous (which should never happen) or their is a syntax error.\n         if token_name_eq(&parser.token, &token::Eof) {\n             if eof_items.len() == 1 {\n-                let matches = eof_items[0].matches.iter_mut().map(|dv| {\n-                    Rc::make_mut(dv).pop().unwrap()\n-                });\n+                let matches = eof_items[0]\n+                    .matches\n+                    .iter_mut()\n+                    .map(|dv| Rc::make_mut(dv).pop().unwrap());\n                 return nameize(sess, ms, matches);\n             } else if eof_items.len() > 1 {\n-                return Error(parser.span, \"ambiguity: multiple successful parses\".to_string());\n+                return Error(\n+                    parser.span,\n+                    \"ambiguity: multiple successful parses\".to_string(),\n+                );\n             } else {\n                 return Failure(parser.span, token::Eof);\n             }\n-        } else if (!bb_items.is_empty() && !next_items.is_empty()) || bb_items.len() > 1 {\n-            let nts = bb_items.iter().map(|item| match item.top_elts.get_tt(item.idx) {\n-                TokenTree::MetaVarDecl(_, bind, name) => {\n-                    format!(\"{} ('{}')\", name, bind)\n-                }\n-                _ => panic!()\n-            }).collect::<Vec<String>>().join(\" or \");\n-\n-            return Error(parser.span, format!(\n-                \"local ambiguity: multiple parsing options: {}\",\n-                match next_items.len() {\n-                    0 => format!(\"built-in NTs {}.\", nts),\n-                    1 => format!(\"built-in NTs {} or 1 other option.\", nts),\n-                    n => format!(\"built-in NTs {} or {} other options.\", nts, n),\n-                }\n-            ));\n-        } else if bb_items.is_empty() && next_items.is_empty() {\n+        }\n+        // Another possibility is that we need to call out to parse some rust nonterminal\n+        // (black-box) parser. However, if there is not EXACTLY ONE of these, something is wrong.\n+        else if (!bb_items.is_empty() && !next_items.is_empty()) || bb_items.len() > 1 {\n+            let nts = bb_items\n+                .iter()\n+                .map(|item| match item.top_elts.get_tt(item.idx) {\n+                    TokenTree::MetaVarDecl(_, bind, name) => format!(\"{} ('{}')\", name, bind),\n+                    _ => panic!(),\n+                })\n+                .collect::<Vec<String>>()\n+                .join(\" or \");\n+\n+            return Error(\n+                parser.span,\n+                format!(\n+                    \"local ambiguity: multiple parsing options: {}\",\n+                    match next_items.len() {\n+                        0 => format!(\"built-in NTs {}.\", nts),\n+                        1 => format!(\"built-in NTs {} or 1 other option.\", nts),\n+                        n => format!(\"built-in NTs {} or {} other options.\", nts, n),\n+                    }\n+                ),\n+            );\n+        }\n+        // If there are no posible next positions AND we aren't waiting for the black-box parser,\n+        // then their is a syntax error.\n+        else if bb_items.is_empty() && next_items.is_empty() {\n             return Failure(parser.span, parser.token);\n-        } else if !next_items.is_empty() {\n-            /* Now process the next token */\n+        }\n+        // Dump all possible `next_items` into `cur_items` for the next iteration.\n+        else if !next_items.is_empty() {\n+            // Now process the next token\n             cur_items.extend(next_items.drain(..));\n             parser.bump();\n-        } else /* bb_items.len() == 1 */ {\n+        }\n+        // Finally, we have the case where we need to call the black-box parser to get some\n+        // nonterminal.\n+        else {\n+            assert_eq!(bb_items.len(), 1);\n+\n             let mut item = bb_items.pop().unwrap();\n             if let TokenTree::MetaVarDecl(span, _, ident) = item.top_elts.get_tt(item.idx) {\n                 let match_cur = item.match_cur;\n-                item.push_match(match_cur,\n-                    MatchedNonterminal(Rc::new(parse_nt(&mut parser, span, &ident.name.as_str()))));\n+                item.push_match(\n+                    match_cur,\n+                    MatchedNonterminal(Rc::new(parse_nt(&mut parser, span, &ident.name.as_str()))),\n+                );\n                 item.idx += 1;\n                 item.match_cur += 1;\n             } else {\n@@ -512,20 +721,21 @@ fn may_begin_with(name: &str, token: &Token) -> bool {\n         \"expr\" => token.can_begin_expr(),\n         \"ty\" => token.can_begin_type(),\n         \"ident\" => token.is_ident(),\n-        \"vis\" => match *token { // The follow-set of :vis + \"priv\" keyword + interpolated\n+        \"vis\" => match *token {\n+            // The follow-set of :vis + \"priv\" keyword + interpolated\n             Token::Comma | Token::Ident(_) | Token::Interpolated(_) => true,\n             _ => token.can_begin_type(),\n         },\n         \"block\" => match *token {\n             Token::OpenDelim(token::Brace) => true,\n             Token::Interpolated(ref nt) => match nt.0 {\n-                token::NtItem(_) |\n-                token::NtPat(_) |\n-                token::NtTy(_) |\n-                token::NtIdent(_) |\n-                token::NtMeta(_) |\n-                token::NtPath(_) |\n-                token::NtVis(_) => false, // none of these may start with '{'.\n+                token::NtItem(_)\n+                | token::NtPat(_)\n+                | token::NtTy(_)\n+                | token::NtIdent(_)\n+                | token::NtMeta(_)\n+                | token::NtPath(_)\n+                | token::NtVis(_) => false, // none of these may start with '{'.\n                 _ => true,\n             },\n             _ => false,\n@@ -562,6 +772,18 @@ fn may_begin_with(name: &str, token: &Token) -> bool {\n     }\n }\n \n+/// A call to the \"black-box\" parser to parse some rust nonterminal.\n+///\n+/// # Parameters\n+///\n+/// - `p`: the \"black-box\" parser to use\n+/// - `sp`: the `Span` we want to parse\n+/// - `name`: the name of the metavar _matcher_ we want to match (e.g. `tt`, `ident`, `block`,\n+///   etc...)\n+///\n+/// # Returns\n+///\n+/// The parsed nonterminal.\n fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n     if name == \"tt\" {\n         return token::NtTT(p.parse_token_tree());\n@@ -591,12 +813,15 @@ fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n         \"ident\" => match p.token {\n             token::Ident(sn) => {\n                 p.bump();\n-                token::NtIdent(Spanned::<Ident>{node: sn, span: p.prev_span})\n+                token::NtIdent(Spanned::<Ident> {\n+                    node: sn,\n+                    span: p.prev_span,\n+                })\n             }\n             _ => {\n                 let token_str = pprust::token_to_string(&p.token);\n-                p.fatal(&format!(\"expected ident, found {}\",\n-                                 &token_str[..])).emit();\n+                p.fatal(&format!(\"expected ident, found {}\", &token_str[..]))\n+                    .emit();\n                 FatalError.raise()\n             }\n         },\n@@ -606,6 +831,6 @@ fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n         \"lifetime\" => token::NtLifetime(p.expect_lifetime()),\n         // this is not supposed to happen, since it has been checked\n         // when compiling the macro.\n-        _ => p.span_bug(sp, \"invalid fragment specifier\")\n+        _ => p.span_bug(sp, \"invalid fragment specifier\"),\n     }\n }"}, {"sha": "c55dfaba8f6b26bec457ea005cc78aa0d465d95b", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "modified", "additions": 173, "deletions": 39, "changes": 212, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -10,29 +10,34 @@\n \n use ast;\n use ext::tt::macro_parser;\n-use parse::{ParseSess, token};\n+use parse::{token, ParseSess};\n use print::pprust;\n use symbol::keywords;\n-use syntax_pos::{DUMMY_SP, Span, BytePos};\n+use syntax_pos::{BytePos, Span, DUMMY_SP};\n use tokenstream;\n \n use std::rc::Rc;\n \n+/// Contains the sub-token-trees of a \"delimited\" token tree, such as the contents of `(`. Note\n+/// that the delimiter itself might be `NoDelim`.\n #[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n pub struct Delimited {\n     pub delim: token::DelimToken,\n     pub tts: Vec<TokenTree>,\n }\n \n impl Delimited {\n+    /// Return the opening delimiter (possibly `NoDelim`).\n     pub fn open_token(&self) -> token::Token {\n         token::OpenDelim(self.delim)\n     }\n \n+    /// Return the closing delimiter (possibly `NoDelim`).\n     pub fn close_token(&self) -> token::Token {\n         token::CloseDelim(self.delim)\n     }\n \n+    /// Return a `self::TokenTree` with a `Span` corresponding to the opening delimiter.\n     pub fn open_tt(&self, span: Span) -> TokenTree {\n         let open_span = if span == DUMMY_SP {\n             DUMMY_SP\n@@ -42,6 +47,7 @@ impl Delimited {\n         TokenTree::Token(open_span, self.open_token())\n     }\n \n+    /// Return a `self::TokenTree` with a `Span` corresponding to the closing delimiter.\n     pub fn close_tt(&self, span: Span) -> TokenTree {\n         let close_span = if span == DUMMY_SP {\n             DUMMY_SP\n@@ -68,12 +74,14 @@ pub struct SequenceRepetition {\n /// for token sequences.\n #[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug, Copy)]\n pub enum KleeneOp {\n+    /// Kleene star (`*`) for zero or more repetitions\n     ZeroOrMore,\n+    /// Kleene plus (`+`) for one or more repetitions\n     OneOrMore,\n }\n \n /// Similar to `tokenstream::TokenTree`, except that `$i`, `$i:ident`, and `$(...)`\n-/// are \"first-class\" token trees.\n+/// are \"first-class\" token trees. Useful for parsing macros.\n #[derive(Debug, Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash)]\n pub enum TokenTree {\n     Token(Span, token::Token),\n@@ -83,10 +91,15 @@ pub enum TokenTree {\n     /// E.g. `$var`\n     MetaVar(Span, ast::Ident),\n     /// E.g. `$var:expr`. This is only used in the left hand side of MBE macros.\n-    MetaVarDecl(Span, ast::Ident /* name to bind */, ast::Ident /* kind of nonterminal */),\n+    MetaVarDecl(\n+        Span,\n+        ast::Ident, /* name to bind */\n+        ast::Ident, /* kind of nonterminal */\n+    ),\n }\n \n impl TokenTree {\n+    /// Return the number of tokens in the tree.\n     pub fn len(&self) -> usize {\n         match *self {\n             TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n@@ -98,6 +111,8 @@ impl TokenTree {\n         }\n     }\n \n+    /// Returns true if the given token tree contains no other tokens. This is vacuously true for\n+    /// single tokens or metavar/decls, but may be false for delimited trees or sequences.\n     pub fn is_empty(&self) -> bool {\n         match *self {\n             TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n@@ -109,6 +124,7 @@ impl TokenTree {\n         }\n     }\n \n+    /// Get the `index`-th sub-token-tree. This only makes sense for delimited trees and sequences.\n     pub fn get_tt(&self, index: usize) -> TokenTree {\n         match (self, index) {\n             (&TokenTree::Delimited(_, ref delimed), _) if delimed.delim == token::NoDelim => {\n@@ -131,21 +147,48 @@ impl TokenTree {\n     /// Retrieve the `TokenTree`'s span.\n     pub fn span(&self) -> Span {\n         match *self {\n-            TokenTree::Token(sp, _) |\n-            TokenTree::MetaVar(sp, _) |\n-            TokenTree::MetaVarDecl(sp, _, _) |\n-            TokenTree::Delimited(sp, _) |\n-            TokenTree::Sequence(sp, _) => sp,\n+            TokenTree::Token(sp, _)\n+            | TokenTree::MetaVar(sp, _)\n+            | TokenTree::MetaVarDecl(sp, _, _)\n+            | TokenTree::Delimited(sp, _)\n+            | TokenTree::Sequence(sp, _) => sp,\n         }\n     }\n }\n \n-pub fn parse(input: tokenstream::TokenStream, expect_matchers: bool, sess: &ParseSess)\n-             -> Vec<TokenTree> {\n+/// Takes a `tokenstream::TokenStream` and returns a `Vec<self::TokenTree>`. Specifically, this\n+/// takes a generic `TokenStream`, such as is used in the rest of the compiler, and returns a\n+/// collection of `TokenTree` for use in parsing a macro.\n+///\n+/// # Parameters\n+///\n+/// - `input`: a token stream to read from, the contents of which we are parsing.\n+/// - `expect_matchers`: `parse` can be used to parse either the \"patterns\" or the \"body\" of a\n+///   macro. Both take roughly the same form _except_ that in a pattern, metavars are declared with\n+///   their \"matcher\" type. For example `$var:expr` or `$id:ident`. In this example, `expr` and\n+///   `ident` are \"matchers\". They are not present in the body of a macro rule -- just in the\n+///   pattern, so we pass a parameter to indicate whether to expect them or not.\n+/// - `sess`: the parsing session. Any errors will be emitted to this session.\n+///\n+/// # Returns\n+///\n+/// A collection of `self::TokenTree`. There may also be some errors emitted to `sess`.\n+pub fn parse(\n+    input: tokenstream::TokenStream,\n+    expect_matchers: bool,\n+    sess: &ParseSess,\n+) -> Vec<TokenTree> {\n+    // Will contain the final collection of `self::TokenTree`\n     let mut result = Vec::new();\n+\n+    // For each token tree in `input`, parse the token into a `self::TokenTree`, consuming\n+    // additional trees if need be.\n     let mut trees = input.trees();\n     while let Some(tree) = trees.next() {\n         let tree = parse_tree(tree, &mut trees, expect_matchers, sess);\n+\n+        // Given the parsed tree, if there is a metavar and we are expecting matchers, actually\n+        // parse out the matcher (i.e. in `$id:ident` this would parse the `:` and `ident`).\n         match tree {\n             TokenTree::MetaVar(start_sp, ident) if expect_matchers => {\n                 let span = match trees.next() {\n@@ -154,78 +197,149 @@ pub fn parse(input: tokenstream::TokenStream, expect_matchers: bool, sess: &Pars\n                             Some(kind) => {\n                                 let span = end_sp.with_lo(start_sp.lo());\n                                 result.push(TokenTree::MetaVarDecl(span, ident, kind));\n-                                continue\n+                                continue;\n                             }\n                             _ => end_sp,\n                         },\n-                        tree => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n+                        tree => tree.as_ref()\n+                            .map(tokenstream::TokenTree::span)\n+                            .unwrap_or(span),\n                     },\n-                    tree => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(start_sp),\n+                    tree => tree.as_ref()\n+                        .map(tokenstream::TokenTree::span)\n+                        .unwrap_or(start_sp),\n                 };\n                 sess.missing_fragment_specifiers.borrow_mut().insert(span);\n-                result.push(TokenTree::MetaVarDecl(span, ident, keywords::Invalid.ident()));\n+                result.push(TokenTree::MetaVarDecl(\n+                    span,\n+                    ident,\n+                    keywords::Invalid.ident(),\n+                ));\n             }\n+\n+            // Not a metavar or no matchers allowed, so just return the tree\n             _ => result.push(tree),\n         }\n     }\n     result\n }\n \n-fn parse_tree<I>(tree: tokenstream::TokenTree,\n-                 trees: &mut I,\n-                 expect_matchers: bool,\n-                 sess: &ParseSess)\n-                 -> TokenTree\n-    where I: Iterator<Item = tokenstream::TokenTree>,\n+/// Takes a `tokenstream::TokenTree` and returns a `self::TokenTree`. Specifically, this takes a\n+/// generic `TokenTree`, such as is used in the rest of the compiler, and returns a `TokenTree`\n+/// for use in parsing a macro.\n+///\n+/// Converting the given tree may involve reading more tokens.\n+///\n+/// # Parameters\n+///\n+/// - `tree`: the tree we wish to convert.\n+/// - `trees`: an iterator over trees. We may need to read more tokens from it in order to finish\n+///   converting `tree`\n+/// - `expect_matchers`: same as for `parse` (see above).\n+/// - `sess`: the parsing session. Any errors will be emitted to this session.\n+fn parse_tree<I>(\n+    tree: tokenstream::TokenTree,\n+    trees: &mut I,\n+    expect_matchers: bool,\n+    sess: &ParseSess,\n+) -> TokenTree\n+where\n+    I: Iterator<Item = tokenstream::TokenTree>,\n {\n+    // Depending on what `tree` is, we could be parsing different parts of a macro\n     match tree {\n+        // `tree` is a `$` token. Look at the next token in `trees`\n         tokenstream::TokenTree::Token(span, token::Dollar) => match trees.next() {\n+            // `tree` is followed by a delimited set of token trees. This indicates the beginning\n+            // of a repetition sequence in the macro (e.g. `$(pat)*`).\n             Some(tokenstream::TokenTree::Delimited(span, delimited)) => {\n+                // Must have `(` not `{` or `[`\n                 if delimited.delim != token::Paren {\n                     let tok = pprust::token_to_string(&token::OpenDelim(delimited.delim));\n                     let msg = format!(\"expected `(`, found `{}`\", tok);\n                     sess.span_diagnostic.span_err(span, &msg);\n                 }\n+                // Parse the contents of the sequence itself\n                 let sequence = parse(delimited.tts.into(), expect_matchers, sess);\n+                // Get the Kleene operator and optional separator\n                 let (separator, op) = parse_sep_and_kleene_op(trees, span, sess);\n+                // Count the number of captured \"names\" (i.e. named metavars)\n                 let name_captures = macro_parser::count_names(&sequence);\n-                TokenTree::Sequence(span, Rc::new(SequenceRepetition {\n-                    tts: sequence,\n-                    separator,\n-                    op,\n-                    num_captures: name_captures,\n-                }))\n+                TokenTree::Sequence(\n+                    span,\n+                    Rc::new(SequenceRepetition {\n+                        tts: sequence,\n+                        separator,\n+                        op,\n+                        num_captures: name_captures,\n+                    }),\n+                )\n             }\n+\n+            // `tree` is followed by an `ident`. This could be `$meta_var` or the `$crate` special\n+            // metavariable that names the crate of the invokation.\n             Some(tokenstream::TokenTree::Token(ident_span, ref token)) if token.is_ident() => {\n                 let ident = token.ident().unwrap();\n                 let span = ident_span.with_lo(span.lo());\n                 if ident.name == keywords::Crate.name() {\n-                    let ident = ast::Ident { name: keywords::DollarCrate.name(), ..ident };\n+                    let ident = ast::Ident {\n+                        name: keywords::DollarCrate.name(),\n+                        ..ident\n+                    };\n                     TokenTree::Token(span, token::Ident(ident))\n                 } else {\n                     TokenTree::MetaVar(span, ident)\n                 }\n             }\n+\n+            // `tree` is followed by a random token. This is an error.\n             Some(tokenstream::TokenTree::Token(span, tok)) => {\n-                let msg = format!(\"expected identifier, found `{}`\", pprust::token_to_string(&tok));\n+                let msg = format!(\n+                    \"expected identifier, found `{}`\",\n+                    pprust::token_to_string(&tok)\n+                );\n                 sess.span_diagnostic.span_err(span, &msg);\n                 TokenTree::MetaVar(span, keywords::Invalid.ident())\n             }\n+\n+            // There are no more tokens. Just return the `$` we already have.\n             None => TokenTree::Token(span, token::Dollar),\n         },\n+\n+        // `tree` is an arbitrary token. Keep it.\n         tokenstream::TokenTree::Token(span, tok) => TokenTree::Token(span, tok),\n-        tokenstream::TokenTree::Delimited(span, delimited) => {\n-            TokenTree::Delimited(span, Rc::new(Delimited {\n+\n+        // `tree` is the beginning of a delimited set of tokens (e.g. `(` or `{`). We need to\n+        // descend into the delimited set and further parse it.\n+        tokenstream::TokenTree::Delimited(span, delimited) => TokenTree::Delimited(\n+            span,\n+            Rc::new(Delimited {\n                 delim: delimited.delim,\n                 tts: parse(delimited.tts.into(), expect_matchers, sess),\n-            }))\n-        }\n+            }),\n+        ),\n     }\n }\n \n-fn parse_sep_and_kleene_op<I>(input: &mut I, span: Span, sess: &ParseSess)\n-                              -> (Option<token::Token>, KleeneOp)\n-    where I: Iterator<Item = tokenstream::TokenTree>,\n+/// Attempt to parse a single Kleene star, possibly with a separator.\n+///\n+/// For example, in a pattern such as `$(a),*`, `a` is the pattern to be repeated, `,` is the\n+/// separator, and `*` is the Kleene operator. This function is specifically concerned with parsing\n+/// the last two tokens of such a pattern: namely, the optional separator and the Kleene operator\n+/// itself. Note that here we are parsing the _macro_ itself, rather than trying to match some\n+/// stream of tokens in an invocation of a macro.\n+///\n+/// This function will take some input iterator `input` corresponding to `span` and a parsing\n+/// session `sess`. If the next one (or possibly two) tokens in `input` correspond to a Kleene\n+/// operator and separator, then a tuple with `(separator, KleeneOp)` is returned. Otherwise, an\n+/// error with the appropriate span is emitted to `sess` and a dummy value is returned.\n+fn parse_sep_and_kleene_op<I>(\n+    input: &mut I,\n+    span: Span,\n+    sess: &ParseSess,\n+) -> (Option<token::Token>, KleeneOp)\n+where\n+    I: Iterator<Item = tokenstream::TokenTree>,\n {\n     fn kleene_op(token: &token::Token) -> Option<KleeneOp> {\n         match *token {\n@@ -235,20 +349,40 @@ fn parse_sep_and_kleene_op<I>(input: &mut I, span: Span, sess: &ParseSess)\n         }\n     }\n \n+    // We attempt to look at the next two token trees in `input`. I will call the first #1 and the\n+    // second #2. If #1 and #2 don't match a valid KleeneOp with/without separator, that is an\n+    // error, and we should emit an error on the most specific span possible.\n     let span = match input.next() {\n+        // #1 is a token\n         Some(tokenstream::TokenTree::Token(span, tok)) => match kleene_op(&tok) {\n+            // #1 is a KleeneOp with no separator\n             Some(op) => return (None, op),\n+\n+            // #1 is not a KleeneOp, but may be a separator... need to look at #2\n             None => match input.next() {\n+                // #2 is a token\n                 Some(tokenstream::TokenTree::Token(span, tok2)) => match kleene_op(&tok2) {\n+                    // #2 is a KleeneOp, so #1 must be a separator\n                     Some(op) => return (Some(tok), op),\n+\n+                    // #2 is not a KleeneOp... error\n                     None => span,\n                 },\n-                tree => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n-            }\n+\n+                // #2 is not a token at all... error\n+                tree => tree.as_ref()\n+                    .map(tokenstream::TokenTree::span)\n+                    .unwrap_or(span),\n+            },\n         },\n-        tree => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n+\n+        // #1 is not a token at all... error\n+        tree => tree.as_ref()\n+            .map(tokenstream::TokenTree::span)\n+            .unwrap_or(span),\n     };\n \n+    // Error...\n     sess.span_diagnostic.span_err(span, \"expected `*` or `+`\");\n     (None, KleeneOp::ZeroOrMore)\n }"}, {"sha": "9bbff1eeb81f3870004c02c6fe90cfce2577661b", "filename": "src/test/run-pass/env-home-dir.rs", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Frun-pass%2Fenv-home-dir.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Frun-pass%2Fenv-home-dir.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Fenv-home-dir.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -27,7 +27,10 @@ fn main() {\n     if cfg!(target_os = \"android\") {\n         assert!(home_dir().is_none());\n     } else {\n-        assert!(home_dir().is_some());\n+        // When HOME is not set, some platforms return `None`,\n+        // but others return `Some` with a default.\n+        // Just check that it is not \"/home/MountainView\".\n+        assert_ne!(home_dir(), Some(PathBuf::from(\"/home/MountainView\")));\n     }\n }\n "}, {"sha": "a28f8da9ff882f31167faa7d0aebbcc4f6e9da60", "filename": "src/test/run-pass/use-nested-groups.rs", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Frun-pass%2Fuse-nested-groups.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Frun-pass%2Fuse-nested-groups.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Fuse-nested-groups.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -24,12 +24,19 @@ mod a {\n     }\n }\n \n+// Test every possible part of the syntax\n use a::{B, d::{self, *, g::H}};\n \n+// Test a more common use case\n+use std::sync::{Arc, atomic::{AtomicBool, Ordering}};\n+\n fn main() {\n     let _: B;\n     let _: E;\n     let _: F;\n     let _: H;\n     let _: d::g::I;\n+\n+    let _: Arc<AtomicBool>;\n+    let _: Ordering;\n }"}, {"sha": "eb53c3c2cb52dbb0c7bc6425615f0d3ef4e5100c", "filename": "src/test/rustdoc/link-title-escape.rs", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Frustdoc%2Flink-title-escape.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Frustdoc%2Flink-title-escape.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frustdoc%2Flink-title-escape.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -0,0 +1,19 @@\n+// Copyright 2018 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// compile-flags: -Z unstable-options --disable-commonmark\n+\n+#![crate_name = \"foo\"]\n+\n+//! hello [foo]\n+//!\n+//! [foo]: url 'title & <stuff> & \"things\"'\n+\n+// @has 'foo/index.html' 'title &amp; &lt;stuff&gt; &amp; &quot;things&quot;'"}, {"sha": "8eae79a21a9832c2946df3deea3cf662c7f6904a", "filename": "src/test/ui/cross-file-errors/main.rs", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fcross-file-errors%2Fmain.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fcross-file-errors%2Fmain.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fcross-file-errors%2Fmain.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -0,0 +1,16 @@\n+// Copyright 2018 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+#[macro_use]\n+mod underscore;\n+\n+fn main() {\n+    underscore!();\n+}"}, {"sha": "a1cdae10edfcdf631b98b172c665d6395f20d28d", "filename": "src/test/ui/cross-file-errors/main.stderr", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fcross-file-errors%2Fmain.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fcross-file-errors%2Fmain.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fcross-file-errors%2Fmain.stderr?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -0,0 +1,11 @@\n+error: expected expression, found `_`\n+  --> $DIR/underscore.rs:18:9\n+   |\n+18 |         _\n+   |         ^\n+   | \n+  ::: $DIR/main.rs:15:5\n+   |\n+15 |     underscore!();\n+   |     -------------- in this macro invocation\n+"}, {"sha": "312b3b8f4ddd5b8823248cd9a5cf7ee248533a8a", "filename": "src/test/ui/cross-file-errors/underscore.rs", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fcross-file-errors%2Funderscore.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fcross-file-errors%2Funderscore.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fcross-file-errors%2Funderscore.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -0,0 +1,20 @@\n+// Copyright 2018 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// We want this file only so we can test cross-file error\n+// messages, but we don't want it in an external crate.\n+// ignore-test\n+#![crate_type = \"lib\"]\n+\n+macro_rules! underscore {\n+    () => (\n+        _\n+    )\n+}"}, {"sha": "48138ee711b3fbb01d0a4854e0b2544ffbcdfbd8", "filename": "src/test/ui/macro_backtrace/main.stderr", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fmacro_backtrace%2Fmain.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fmacro_backtrace%2Fmain.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fmacro_backtrace%2Fmain.stderr?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -22,7 +22,7 @@ error: expected one of `!`, `.`, `::`, `;`, `?`, `{`, `}`, or an operator, found\n 27 |       ping!();\n    |       -------- in this macro invocation\n    | \n-  ::: <ping macros>\n+  ::: <ping macros>:1:1\n    |\n 1  |   (  ) => { pong ! (  ) ; }\n    |   -------------------------\n@@ -42,31 +42,31 @@ error: expected one of `!`, `.`, `::`, `;`, `?`, `{`, `}`, or an operator, found\n 28 |       deep!();\n    |       -------- in this macro invocation (#1)\n    | \n-  ::: <deep macros>\n+  ::: <deep macros>:1:1\n    |\n 1  |   (  ) => { foo ! (  ) ; }\n    |   ------------------------\n    |   |         |\n    |   |         in this macro invocation (#2)\n    |   in this expansion of `deep!` (#1)\n    | \n-  ::: <foo macros>\n+  ::: <foo macros>:1:1\n    |\n 1  |   (  ) => { bar ! (  ) ; }\n    |   ------------------------\n    |   |         |\n    |   |         in this macro invocation (#3)\n    |   in this expansion of `foo!` (#2)\n    | \n-  ::: <bar macros>\n+  ::: <bar macros>:1:1\n    |\n 1  |   (  ) => { ping ! (  ) ; }\n    |   -------------------------\n    |   |         |\n    |   |         in this macro invocation (#4)\n    |   in this expansion of `bar!` (#3)\n    | \n-  ::: <ping macros>\n+  ::: <ping macros>:1:1\n    |\n 1  |   (  ) => { pong ! (  ) ; }\n    |   -------------------------"}, {"sha": "a9b6b3ee70d57cb4bddbdc5e07c41aee5286922a", "filename": "src/test/ui/use-nested-groups-error.rs", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fuse-nested-groups-error.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fuse-nested-groups-error.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fuse-nested-groups-error.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -0,0 +1,27 @@\n+// Copyright 2018 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+#![feature(use_nested_groups)]\n+\n+mod a {\n+    pub mod b1 {\n+        pub enum C2 {}\n+    }\n+\n+    pub enum B2 {}\n+}\n+\n+use a::{b1::{C1, C2}, B2};\n+//~^ ERROR unresolved import `a::b1::C1`\n+\n+fn main() {\n+    let _: C2;\n+    let _: B2;\n+}"}, {"sha": "cae34684c8e38dbed173beffa83a32590740e9d7", "filename": "src/test/ui/use-nested-groups-error.stderr", "status": "added", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fuse-nested-groups-error.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftest%2Fui%2Fuse-nested-groups-error.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fuse-nested-groups-error.stderr?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -0,0 +1,8 @@\n+error[E0432]: unresolved import `a::b1::C1`\n+  --> $DIR/use-nested-groups-error.rs:21:14\n+   |\n+21 | use a::{b1::{C1, C2}, B2};\n+   |              ^^ no `C1` in `a::b1`. Did you mean to use `C2`?\n+\n+error: aborting due to previous error\n+"}, {"sha": "abf62a060b83b388f77c00a8c6f3d355891ebb27", "filename": "src/tools/compiletest/src/runtest.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftools%2Fcompiletest%2Fsrc%2Fruntest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/def3269a71be2e737cad27418a3dad9f5bd6cd32/src%2Ftools%2Fcompiletest%2Fsrc%2Fruntest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fcompiletest%2Fsrc%2Fruntest.rs?ref=def3269a71be2e737cad27418a3dad9f5bd6cd32", "patch": "@@ -1402,7 +1402,7 @@ impl<'test> TestCx<'test> {\n     }\n \n     /// For each `aux-build: foo/bar` annotation, we check to find the\n-    /// file in a `aux` directory relative to the test itself.\n+    /// file in a `auxiliary` directory relative to the test itself.\n     fn compute_aux_test_paths(&self, rel_ab: &str) -> TestPaths {\n         let test_ab = self.testpaths\n             .file"}]}