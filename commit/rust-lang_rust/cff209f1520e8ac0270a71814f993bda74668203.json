{"sha": "cff209f1520e8ac0270a71814f993bda74668203", "node_id": "C_kwDOAAsO6NoAKGNmZjIwOWYxNTIwZThhYzAyNzBhNzE4MTRmOTkzYmRhNzQ2NjgyMDM", "commit": {"author": {"name": "Florian Diebold", "email": "flodiebold@gmail.com", "date": "2022-02-07T17:08:31Z"}, "committer": {"name": "Florian Diebold", "email": "flodiebold@gmail.com", "date": "2022-02-07T17:12:51Z"}, "message": "WIP: Actually fix up syntax errors in attribute macro input", "tree": {"sha": "e3d0c16bc7fc4b0e76e87ee6a7fee9c22b79f0ac", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/e3d0c16bc7fc4b0e76e87ee6a7fee9c22b79f0ac"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/cff209f1520e8ac0270a71814f993bda74668203", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/cff209f1520e8ac0270a71814f993bda74668203", "html_url": "https://github.com/rust-lang/rust/commit/cff209f1520e8ac0270a71814f993bda74668203", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/cff209f1520e8ac0270a71814f993bda74668203/comments", "author": {"login": "flodiebold", "id": 906069, "node_id": "MDQ6VXNlcjkwNjA2OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/906069?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flodiebold", "html_url": "https://github.com/flodiebold", "followers_url": "https://api.github.com/users/flodiebold/followers", "following_url": "https://api.github.com/users/flodiebold/following{/other_user}", "gists_url": "https://api.github.com/users/flodiebold/gists{/gist_id}", "starred_url": "https://api.github.com/users/flodiebold/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flodiebold/subscriptions", "organizations_url": "https://api.github.com/users/flodiebold/orgs", "repos_url": "https://api.github.com/users/flodiebold/repos", "events_url": "https://api.github.com/users/flodiebold/events{/privacy}", "received_events_url": "https://api.github.com/users/flodiebold/received_events", "type": "User", "site_admin": false}, "committer": {"login": "flodiebold", "id": 906069, "node_id": "MDQ6VXNlcjkwNjA2OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/906069?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flodiebold", "html_url": "https://github.com/flodiebold", "followers_url": "https://api.github.com/users/flodiebold/followers", "following_url": "https://api.github.com/users/flodiebold/following{/other_user}", "gists_url": "https://api.github.com/users/flodiebold/gists{/gist_id}", "starred_url": "https://api.github.com/users/flodiebold/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flodiebold/subscriptions", "organizations_url": "https://api.github.com/users/flodiebold/orgs", "repos_url": "https://api.github.com/users/flodiebold/repos", "events_url": "https://api.github.com/users/flodiebold/events{/privacy}", "received_events_url": "https://api.github.com/users/flodiebold/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "212e82fd4183f17670a343a8fabf220ab4b6f7a2", "url": "https://api.github.com/repos/rust-lang/rust/commits/212e82fd4183f17670a343a8fabf220ab4b6f7a2", "html_url": "https://github.com/rust-lang/rust/commit/212e82fd4183f17670a343a8fabf220ab4b6f7a2"}], "stats": {"total": 146, "additions": 112, "deletions": 34}, "files": [{"sha": "16df7ce4cf58bb8d0c20bb77fd3ca9d73e40a11a", "filename": "crates/hir_def/src/macro_expansion_tests.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests.rs?ref=cff209f1520e8ac0270a71814f993bda74668203", "patch": "@@ -345,6 +345,7 @@ impl base_db::ProcMacroExpander for IdentityWhenValidProcMacroExpander {\n         if parse.errors().is_empty() {\n             Ok(subtree.clone())\n         } else {\n+            eprintln!(\"parse errors: {:?}\", parse.errors());\n             use tt::{Delimiter, DelimiterKind, Ident, Leaf, Literal, Punct, TokenTree};\n             let mut subtree = Subtree::default();\n             subtree.token_trees.push(TokenTree::Leaf("}, {"sha": "e0c5367cf36d2ea99bfffd4dcb765384c6936dc7", "filename": "crates/hir_def/src/macro_expansion_tests/proc_macros.rs", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs?ref=cff209f1520e8ac0270a71814f993bda74668203", "patch": "@@ -86,6 +86,10 @@ fn foo() { bar.; blub }\n         expect![[r##\"\n #[proc_macros::identity_when_valid]\n fn foo() { bar.; blub }\n-\"##]],\n+\n+fn foo() {\n+    bar.;\n+    blub\n+}\"##]],\n     );\n }"}, {"sha": "6576701817162966e82411c15e258df4db9cffda", "filename": "crates/hir_expand/src/db.rs", "status": "modified", "additions": 16, "deletions": 5, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fdb.rs?ref=cff209f1520e8ac0270a71814f993bda74668203", "patch": "@@ -5,16 +5,16 @@ use std::sync::Arc;\n use base_db::{salsa, SourceDatabase};\n use either::Either;\n use limit::Limit;\n-use mbe::{syntax_node_to_token_tree, ExpandError, ExpandResult};\n-use rustc_hash::FxHashSet;\n+use mbe::{syntax_node_to_token_tree, ExpandError, ExpandResult, SyntheticToken};\n+use rustc_hash::{FxHashMap, FxHashSet};\n use syntax::{\n     algo::diff,\n     ast::{self, HasAttrs, HasDocComments},\n     AstNode, GreenNode, Parse, SyntaxNode, SyntaxToken, T,\n };\n \n use crate::{\n-    ast_id_map::AstIdMap, hygiene::HygieneFrame, BuiltinAttrExpander, BuiltinDeriveExpander,\n+    ast_id_map::AstIdMap, fixup, hygiene::HygieneFrame, BuiltinAttrExpander, BuiltinDeriveExpander,\n     BuiltinFnLikeExpander, ExpandTo, HirFileId, HirFileIdRepr, MacroCallId, MacroCallKind,\n     MacroCallLoc, MacroDefId, MacroDefKind, MacroFile, ProcMacroExpander,\n };\n@@ -146,8 +146,10 @@ pub fn expand_speculative(\n \n     // Build the subtree and token mapping for the speculative args\n     let censor = censor_for_macro_input(&loc, &speculative_args);\n+    let mut fixups = fixup::fixup_syntax(&speculative_args);\n+    fixups.replace.extend(censor.into_iter().map(|node| (node, Vec::new())));\n     let (mut tt, spec_args_tmap) =\n-        mbe::syntax_node_to_token_tree_censored(&speculative_args, &censor);\n+        mbe::syntax_node_to_token_tree_censored(&speculative_args, fixups.replace, fixups.append);\n \n     let (attr_arg, token_id) = match loc.kind {\n         MacroCallKind::Attr { invoc_attr_index, .. } => {\n@@ -294,8 +296,17 @@ fn macro_arg(db: &dyn AstDatabase, id: MacroCallId) -> Option<Arc<(tt::Subtree,\n     let loc = db.lookup_intern_macro_call(id);\n \n     let node = SyntaxNode::new_root(arg);\n+    eprintln!(\"input text:\\n{node}\");\n+    eprintln!(\"input syntax:\\n{node:#?}\");\n     let censor = censor_for_macro_input(&loc, &node);\n-    let (mut tt, tmap) = mbe::syntax_node_to_token_tree_censored(&node, &censor);\n+    // TODO only fixup for attribute macro input\n+    let mut fixups = fixup::fixup_syntax(&node);\n+    fixups.replace.extend(censor.into_iter().map(|node| (node, Vec::new())));\n+    eprintln!(\"fixups: {fixups:?}\");\n+    let (mut tt, tmap) =\n+        mbe::syntax_node_to_token_tree_censored(&node, fixups.replace, fixups.append);\n+\n+    eprintln!(\"fixed-up input: {}\", tt);\n \n     if loc.def.is_proc_macro() {\n         // proc macros expect their inputs without parentheses, MBEs expect it with them included"}, {"sha": "b23d54724266676a6833af76780c806245dd89f6", "filename": "crates/hir_expand/src/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fhir_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fhir_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Flib.rs?ref=cff209f1520e8ac0270a71814f993bda74668203", "patch": "@@ -15,6 +15,7 @@ pub mod proc_macro;\n pub mod quote;\n pub mod eager;\n pub mod mod_path;\n+mod fixup;\n \n pub use mbe::{ExpandError, ExpandResult, Origin};\n "}, {"sha": "3633624c6415cc2f652b1ac47e9fcc3e46481669", "filename": "crates/mbe/src/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fmbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fmbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Flib.rs?ref=cff209f1520e8ac0270a71814f993bda74668203", "patch": "@@ -30,7 +30,7 @@ pub use tt::{Delimiter, DelimiterKind, Punct};\n pub use crate::{\n     syntax_bridge::{\n         parse_exprs_with_sep, parse_to_token_tree, syntax_node_to_token_tree,\n-        syntax_node_to_token_tree_censored, token_tree_to_syntax_node,\n+        syntax_node_to_token_tree_censored, token_tree_to_syntax_node, SyntheticToken,\n     },\n     token_map::TokenMap,\n };"}, {"sha": "b46e9594665c2fc11385061420684d5c26fe5bc0", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 88, "deletions": 27, "changes": 115, "blob_url": "https://github.com/rust-lang/rust/blob/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cff209f1520e8ac0270a71814f993bda74668203/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=cff209f1520e8ac0270a71814f993bda74668203", "patch": "@@ -15,22 +15,26 @@ use crate::{to_parser_input::to_parser_input, tt_iter::TtIter, TokenMap};\n /// Convert the syntax node to a `TokenTree` (what macro\n /// will consume).\n pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> (tt::Subtree, TokenMap) {\n-    syntax_node_to_token_tree_censored(node, &Default::default())\n+    syntax_node_to_token_tree_censored(node, Default::default(), Default::default())\n }\n \n+// TODO rename\n /// Convert the syntax node to a `TokenTree` (what macro will consume)\n /// with the censored range excluded.\n pub fn syntax_node_to_token_tree_censored(\n     node: &SyntaxNode,\n-    censor: &FxHashSet<SyntaxNode>,\n+    replace: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    append: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n ) -> (tt::Subtree, TokenMap) {\n     let global_offset = node.text_range().start();\n-    let mut c = Convertor::new(node, global_offset, censor);\n+    let mut c = Convertor::new(node, global_offset, replace, append);\n     let subtree = convert_tokens(&mut c);\n     c.id_alloc.map.shrink_to_fit();\n     (subtree, c.id_alloc.map)\n }\n \n+pub type SyntheticToken = (SyntaxKind, SmolStr);\n+\n // The following items are what `rustc` macro can be parsed into :\n // link: https://github.com/rust-lang/rust/blob/9ebf47851a357faa4cd97f4b1dc7835f6376e639/src/libsyntax/ext/expand.rs#L141\n // * Expr(P<ast::Expr>)                     -> token_tree_to_expr\n@@ -465,86 +469,124 @@ impl<'a> TokenConvertor for RawConvertor<'a> {\n     }\n }\n \n-struct Convertor<'c> {\n+struct Convertor {\n     id_alloc: TokenIdAlloc,\n     current: Option<SyntaxToken>,\n+    current_synthetic: Vec<SyntheticToken>,\n     preorder: PreorderWithTokens,\n-    censor: &'c FxHashSet<SyntaxNode>,\n+    replace: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    append: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n     range: TextRange,\n     punct_offset: Option<(SyntaxToken, TextSize)>,\n }\n \n-impl<'c> Convertor<'c> {\n+impl Convertor {\n     fn new(\n         node: &SyntaxNode,\n         global_offset: TextSize,\n-        censor: &'c FxHashSet<SyntaxNode>,\n-    ) -> Convertor<'c> {\n+        replace: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+        append: FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    ) -> Convertor {\n         let range = node.text_range();\n         let mut preorder = node.preorder_with_tokens();\n-        let first = Self::next_token(&mut preorder, censor);\n+        let (first, synthetic) = Self::next_token(&mut preorder, &replace, &append);\n         Convertor {\n             id_alloc: { TokenIdAlloc { map: TokenMap::default(), global_offset, next_id: 0 } },\n             current: first,\n+            current_synthetic: synthetic,\n             preorder,\n             range,\n-            censor,\n+            replace,\n+            append,\n             punct_offset: None,\n         }\n     }\n \n     fn next_token(\n         preorder: &mut PreorderWithTokens,\n-        censor: &FxHashSet<SyntaxNode>,\n-    ) -> Option<SyntaxToken> {\n+        replace: &FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+        append: &FxHashMap<SyntaxNode, Vec<SyntheticToken>>,\n+    ) -> (Option<SyntaxToken>, Vec<SyntheticToken>) {\n         while let Some(ev) = preorder.next() {\n             let ele = match ev {\n                 WalkEvent::Enter(ele) => ele,\n+                WalkEvent::Leave(SyntaxElement::Node(node)) => {\n+                    if let Some(v) = append.get(&node) {\n+                        eprintln!(\"after {:?}, appending {:?}\", node, v);\n+                        if !v.is_empty() {\n+                            let mut reversed = v.clone();\n+                            reversed.reverse();\n+                            return (None, reversed);\n+                        }\n+                    }\n+                    continue;\n+                }\n                 _ => continue,\n             };\n             match ele {\n-                SyntaxElement::Token(t) => return Some(t),\n-                SyntaxElement::Node(node) if censor.contains(&node) => preorder.skip_subtree(),\n-                SyntaxElement::Node(_) => (),\n+                SyntaxElement::Token(t) => return (Some(t), Vec::new()),\n+                SyntaxElement::Node(node) => {\n+                    if let Some(v) = replace.get(&node) {\n+                        preorder.skip_subtree();\n+                        eprintln!(\"replacing {:?} by {:?}\", node, v);\n+                        if !v.is_empty() {\n+                            let mut reversed = v.clone();\n+                            reversed.reverse();\n+                            return (None, reversed);\n+                        }\n+                    }\n+                }\n             }\n         }\n-        None\n+        (None, Vec::new())\n     }\n }\n \n #[derive(Debug)]\n enum SynToken {\n     Ordinary(SyntaxToken),\n+    // FIXME is this supposed to be `Punct`?\n     Punch(SyntaxToken, TextSize),\n+    Synthetic(SyntheticToken),\n }\n \n impl SynToken {\n-    fn token(&self) -> &SyntaxToken {\n+    fn token(&self) -> Option<&SyntaxToken> {\n         match self {\n-            SynToken::Ordinary(it) | SynToken::Punch(it, _) => it,\n+            SynToken::Ordinary(it) | SynToken::Punch(it, _) => Some(it),\n+            SynToken::Synthetic(_) => None,\n         }\n     }\n }\n \n-impl<'a> SrcToken<Convertor<'a>> for SynToken {\n-    fn kind(&self, _ctx: &Convertor<'a>) -> SyntaxKind {\n-        self.token().kind()\n+impl SrcToken<Convertor> for SynToken {\n+    fn kind(&self, _ctx: &Convertor) -> SyntaxKind {\n+        match self {\n+            SynToken::Ordinary(token) => token.kind(),\n+            SynToken::Punch(token, _) => token.kind(),\n+            SynToken::Synthetic((kind, _)) => *kind,\n+        }\n     }\n-    fn to_char(&self, _ctx: &Convertor<'a>) -> Option<char> {\n+    fn to_char(&self, _ctx: &Convertor) -> Option<char> {\n         match self {\n             SynToken::Ordinary(_) => None,\n             SynToken::Punch(it, i) => it.text().chars().nth((*i).into()),\n+            SynToken::Synthetic(_) => None,\n         }\n     }\n-    fn to_text(&self, _ctx: &Convertor<'a>) -> SmolStr {\n-        self.token().text().into()\n+    fn to_text(&self, _ctx: &Convertor) -> SmolStr {\n+        match self {\n+            SynToken::Ordinary(token) => token.text().into(),\n+            SynToken::Punch(token, _) => token.text().into(),\n+            SynToken::Synthetic((_, text)) => text.clone(),\n+        }\n     }\n }\n \n-impl TokenConvertor for Convertor<'_> {\n+impl TokenConvertor for Convertor {\n     type Token = SynToken;\n     fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n-        convert_doc_comment(token.token())\n+        convert_doc_comment(token.token()?)\n     }\n \n     fn bump(&mut self) -> Option<(Self::Token, TextRange)> {\n@@ -558,11 +600,25 @@ impl TokenConvertor for Convertor<'_> {\n             }\n         }\n \n+        if let Some(synth_token) = self.current_synthetic.pop() {\n+            if self.current_synthetic.is_empty() {\n+                let (new_current, new_synth) =\n+                    Self::next_token(&mut self.preorder, &self.replace, &self.append);\n+                self.current = new_current;\n+                self.current_synthetic = new_synth;\n+            }\n+            // TODO fix range?\n+            return Some((SynToken::Synthetic(synth_token), self.range));\n+        }\n+\n         let curr = self.current.clone()?;\n         if !&self.range.contains_range(curr.text_range()) {\n             return None;\n         }\n-        self.current = Self::next_token(&mut self.preorder, self.censor);\n+        let (new_current, new_synth) =\n+            Self::next_token(&mut self.preorder, &self.replace, &self.append);\n+        self.current = new_current;\n+        self.current_synthetic = new_synth;\n         let token = if curr.kind().is_punct() {\n             self.punct_offset = Some((curr.clone(), 0.into()));\n             let range = curr.text_range();\n@@ -585,6 +641,11 @@ impl TokenConvertor for Convertor<'_> {\n             }\n         }\n \n+        if let Some(synth_token) = self.current_synthetic.last() {\n+            // TODO fix range?\n+            return Some(SynToken::Synthetic(synth_token.clone()));\n+        }\n+\n         let curr = self.current.clone()?;\n         if !self.range.contains_range(curr.text_range()) {\n             return None;"}]}