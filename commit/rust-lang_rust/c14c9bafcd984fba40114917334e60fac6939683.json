{"sha": "c14c9bafcd984fba40114917334e60fac6939683", "node_id": "MDY6Q29tbWl0NzI0NzEyOmMxNGM5YmFmY2Q5ODRmYmE0MDExNDkxNzMzNGU2MGZhYzY5Mzk2ODM=", "commit": {"author": {"name": "Yuki Okushi", "email": "huyuumi.dev@gmail.com", "date": "2020-10-10T18:19:07Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-10-10T18:19:07Z"}, "message": "Rollup merge of #77629 - Julian-Wollersberger:recomputeRawStrError, r=varkor\n\nCleanup of `eat_while()` in lexer\n\nThe size of a lexer Token was inflated by the largest `TokenKind` variants `LiteralKind::RawStr` and `RawByteStr`, because\n* it used `usize` although `u32` is sufficient in rustc, since crates must be smaller than 4GB,\n* and it stored the 20 bytes big `RawStrError` enum for error reporting.\n\nIf a raw string is invalid, it now needs to be reparsed to get the `RawStrError` data, but that is a very cold code path.\n\nTechnically this breaks other tools that depend on rustc_lexer because they are now also restricted to a max file size of 4GB. But this shouldn't matter in practice, and rustc_lexer isn't stable anyway.\n\nCan I also get a perf run?\n\nEdit: This makes no difference in performance. The PR now only contains a small cleanup.", "tree": {"sha": "fbfb907aff6d48e0432096d1841fd2e5bcc3c722", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/fbfb907aff6d48e0432096d1841fd2e5bcc3c722"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/c14c9bafcd984fba40114917334e60fac6939683", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJfgfsbCRBK7hj4Ov3rIwAAdHIIAGbGJoZr59AOUbHT47bwekF5\ngYM6hK0/8oBvUj3swwQu4az66koUVCBQ81rwuZ+wblgGQLajMg9LhsRSZ1+z5wn+\nbiQ0bFRfNQ2As+3E/tNhUghQs0fs5nnPRRrYOh8zT0TNPLCYQyPYiHGYGTHmyNyd\nvAvnniQCdww7pgqKJRJXSldUJheqksSEIQJH7pMHX535kvqDjSlDCFd90bpAa4HD\n8Rux588dVExSy8wQGRq30p8KIly+JaY6GWGuZL5hko6DmEl3aTdinlq36f6chcaW\negikZpXtnGr+/vtpgaIyOpuNM2hV2c99DGaQkPizmfaRi/ZnjUcMm8y9R0dMHR8=\n=OKU/\n-----END PGP SIGNATURE-----\n", "payload": "tree fbfb907aff6d48e0432096d1841fd2e5bcc3c722\nparent 1b134430ef110f99f0a280c9cf604dd54275b543\nparent bd49ded308f7243d1ba3170ea1bd0d5855d0544b\nauthor Yuki Okushi <huyuumi.dev@gmail.com> 1602353947 +0900\ncommitter GitHub <noreply@github.com> 1602353947 +0900\n\nRollup merge of #77629 - Julian-Wollersberger:recomputeRawStrError, r=varkor\n\nCleanup of `eat_while()` in lexer\n\nThe size of a lexer Token was inflated by the largest `TokenKind` variants `LiteralKind::RawStr` and `RawByteStr`, because\n* it used `usize` although `u32` is sufficient in rustc, since crates must be smaller than 4GB,\n* and it stored the 20 bytes big `RawStrError` enum for error reporting.\n\nIf a raw string is invalid, it now needs to be reparsed to get the `RawStrError` data, but that is a very cold code path.\n\nTechnically this breaks other tools that depend on rustc_lexer because they are now also restricted to a max file size of 4GB. But this shouldn't matter in practice, and rustc_lexer isn't stable anyway.\n\nCan I also get a perf run?\n\nEdit: This makes no difference in performance. The PR now only contains a small cleanup.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/c14c9bafcd984fba40114917334e60fac6939683", "html_url": "https://github.com/rust-lang/rust/commit/c14c9bafcd984fba40114917334e60fac6939683", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/c14c9bafcd984fba40114917334e60fac6939683/comments", "author": {"login": "JohnTitor", "id": 25030997, "node_id": "MDQ6VXNlcjI1MDMwOTk3", "avatar_url": "https://avatars.githubusercontent.com/u/25030997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JohnTitor", "html_url": "https://github.com/JohnTitor", "followers_url": "https://api.github.com/users/JohnTitor/followers", "following_url": "https://api.github.com/users/JohnTitor/following{/other_user}", "gists_url": "https://api.github.com/users/JohnTitor/gists{/gist_id}", "starred_url": "https://api.github.com/users/JohnTitor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JohnTitor/subscriptions", "organizations_url": "https://api.github.com/users/JohnTitor/orgs", "repos_url": "https://api.github.com/users/JohnTitor/repos", "events_url": "https://api.github.com/users/JohnTitor/events{/privacy}", "received_events_url": "https://api.github.com/users/JohnTitor/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1b134430ef110f99f0a280c9cf604dd54275b543", "url": "https://api.github.com/repos/rust-lang/rust/commits/1b134430ef110f99f0a280c9cf604dd54275b543", "html_url": "https://github.com/rust-lang/rust/commit/1b134430ef110f99f0a280c9cf604dd54275b543"}, {"sha": "bd49ded308f7243d1ba3170ea1bd0d5855d0544b", "url": "https://api.github.com/repos/rust-lang/rust/commits/bd49ded308f7243d1ba3170ea1bd0d5855d0544b", "html_url": "https://github.com/rust-lang/rust/commit/bd49ded308f7243d1ba3170ea1bd0d5855d0544b"}], "stats": {"total": 34, "additions": 14, "deletions": 20}, "files": [{"sha": "c5b59a041abf6a99e620ba33397907eccfa488ee", "filename": "compiler/rustc_lexer/src/lib.rs", "status": "modified", "additions": 14, "deletions": 20, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/c14c9bafcd984fba40114917334e60fac6939683/compiler%2Frustc_lexer%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c14c9bafcd984fba40114917334e60fac6939683/compiler%2Frustc_lexer%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Flib.rs?ref=c14c9bafcd984fba40114917334e60fac6939683", "patch": "@@ -48,6 +48,7 @@ impl Token {\n }\n \n /// Enum representing common lexeme types.\n+// perf note: Changing all `usize` to `u32` doesn't change performance. See #77629\n #[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]\n pub enum TokenKind {\n     // Multi-char tokens:\n@@ -160,6 +161,7 @@ pub enum LiteralKind {\n /// - `r##~\"abcde\"##`: `InvalidStarter`\n /// - `r###\"abcde\"##`: `NoTerminator { expected: 3, found: 2, possible_terminator_offset: Some(11)`\n /// - Too many `#`s (>65535): `TooManyDelimiters`\n+// perf note: It doesn't matter that this makes `Token` 36 bytes bigger. See #77629\n #[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]\n pub enum RawStrError {\n     /// Non `#` characters exist between `r` and `\"` eg. `r#~\"..`\n@@ -689,7 +691,12 @@ impl Cursor<'_> {\n         let mut max_hashes = 0;\n \n         // Count opening '#' symbols.\n-        let n_start_hashes = self.eat_while(|c| c == '#');\n+        let mut eaten = 0;\n+        while self.first() == '#' {\n+            eaten += 1;\n+            self.bump();\n+        }\n+        let n_start_hashes = eaten;\n \n         // Check that string is started.\n         match self.bump() {\n@@ -724,16 +731,11 @@ impl Cursor<'_> {\n             // Note that this will not consume extra trailing `#` characters:\n             // `r###\"abcde\"####` is lexed as a `RawStr { n_hashes: 3 }`\n             // followed by a `#` token.\n-            let mut hashes_left = n_start_hashes;\n-            let is_closing_hash = |c| {\n-                if c == '#' && hashes_left != 0 {\n-                    hashes_left -= 1;\n-                    true\n-                } else {\n-                    false\n-                }\n-            };\n-            let n_end_hashes = self.eat_while(is_closing_hash);\n+            let mut n_end_hashes = 0;\n+            while self.first() == '#' && n_end_hashes < n_start_hashes {\n+                n_end_hashes += 1;\n+                self.bump();\n+            }\n \n             if n_end_hashes == n_start_hashes {\n                 return (n_start_hashes, None);\n@@ -807,17 +809,9 @@ impl Cursor<'_> {\n     }\n \n     /// Eats symbols while predicate returns true or until the end of file is reached.\n-    /// Returns amount of eaten symbols.\n-    fn eat_while<F>(&mut self, mut predicate: F) -> usize\n-    where\n-        F: FnMut(char) -> bool,\n-    {\n-        let mut eaten: usize = 0;\n+    fn eat_while(&mut self, mut predicate: impl FnMut(char) -> bool) {\n         while predicate(self.first()) && !self.is_eof() {\n-            eaten += 1;\n             self.bump();\n         }\n-\n-        eaten\n     }\n }"}]}