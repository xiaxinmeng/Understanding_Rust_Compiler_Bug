{"sha": "4f104954a67ad736244ce212467290c836394fad", "node_id": "MDY6Q29tbWl0NzI0NzEyOjRmMTA0OTU0YTY3YWQ3MzYyNDRjZTIxMjQ2NzI5MGM4MzYzOTRmYWQ=", "commit": {"author": {"name": "Paul Stansifer", "email": "paul.stansifer@gmail.com", "date": "2012-06-12T17:59:50Z"}, "committer": {"name": "Paul Stansifer", "email": "paul.stansifer@gmail.com", "date": "2012-06-26T01:01:37Z"}, "message": "parsing for the macro system", "tree": {"sha": "f7c8af3b048c43d55d8dfbca638faefba9d6105e", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f7c8af3b048c43d55d8dfbca638faefba9d6105e"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/4f104954a67ad736244ce212467290c836394fad", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/4f104954a67ad736244ce212467290c836394fad", "html_url": "https://github.com/rust-lang/rust/commit/4f104954a67ad736244ce212467290c836394fad", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/4f104954a67ad736244ce212467290c836394fad/comments", "author": {"login": "paulstansifer", "id": 1431, "node_id": "MDQ6VXNlcjE0MzE=", "avatar_url": "https://avatars.githubusercontent.com/u/1431?v=4", "gravatar_id": "", "url": "https://api.github.com/users/paulstansifer", "html_url": "https://github.com/paulstansifer", "followers_url": "https://api.github.com/users/paulstansifer/followers", "following_url": "https://api.github.com/users/paulstansifer/following{/other_user}", "gists_url": "https://api.github.com/users/paulstansifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/paulstansifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/paulstansifer/subscriptions", "organizations_url": "https://api.github.com/users/paulstansifer/orgs", "repos_url": "https://api.github.com/users/paulstansifer/repos", "events_url": "https://api.github.com/users/paulstansifer/events{/privacy}", "received_events_url": "https://api.github.com/users/paulstansifer/received_events", "type": "User", "site_admin": false}, "committer": {"login": "paulstansifer", "id": 1431, "node_id": "MDQ6VXNlcjE0MzE=", "avatar_url": "https://avatars.githubusercontent.com/u/1431?v=4", "gravatar_id": "", "url": "https://api.github.com/users/paulstansifer", "html_url": "https://github.com/paulstansifer", "followers_url": "https://api.github.com/users/paulstansifer/followers", "following_url": "https://api.github.com/users/paulstansifer/following{/other_user}", "gists_url": "https://api.github.com/users/paulstansifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/paulstansifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/paulstansifer/subscriptions", "organizations_url": "https://api.github.com/users/paulstansifer/orgs", "repos_url": "https://api.github.com/users/paulstansifer/repos", "events_url": "https://api.github.com/users/paulstansifer/events{/privacy}", "received_events_url": "https://api.github.com/users/paulstansifer/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "650dfe58a3d0b41b561d8a68924f31e93f79d4bc", "url": "https://api.github.com/repos/rust-lang/rust/commits/650dfe58a3d0b41b561d8a68924f31e93f79d4bc", "html_url": "https://github.com/rust-lang/rust/commit/650dfe58a3d0b41b561d8a68924f31e93f79d4bc"}], "stats": {"total": 399, "additions": 370, "deletions": 29}, "files": [{"sha": "d6d2d4f3165e70bf1b7dd1927e6ebd7ef6a990a9", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=4f104954a67ad736244ce212467290c836394fad", "patch": "@@ -377,6 +377,17 @@ enum token_tree {\n     tt_flat(span, token::token)\n }\n \n+#[auto_serialize]\n+type matcher = spanned<matcher_>;\n+\n+#[auto_serialize]\n+enum matcher_ {\n+    mtc_tok(token::token),\n+    /* body, separator, zero ok? : */\n+    mtc_rep([matcher], option<token::token>, bool),\n+    mtc_bb(ident, ident, uint)\n+}\n+\n #[auto_serialize]\n type mac = spanned<mac_>;\n "}, {"sha": "b1a2524ca3518d81f0388917cc3e482595d21b8e", "filename": "src/libsyntax/ext/earley_parser.rs", "status": "added", "additions": 243, "deletions": 0, "changes": 243, "blob_url": "https://github.com/rust-lang/rust/blob/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fext%2Fearley_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fext%2Fearley_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fearley_parser.rs?ref=4f104954a67ad736244ce212467290c836394fad", "patch": "@@ -0,0 +1,243 @@\n+// Earley-like parser for macros.\n+import parse::token;\n+import parse::token::{token, EOF, to_str, whole_nt};\n+import parse::lexer::{reader, tt_reader, tt_reader_as_reader};\n+import parse::parser::{parser,SOURCE_FILE};\n+import parse::common::parser_common;\n+import parse::parse_sess;\n+import dvec::{dvec, extensions};\n+import ast::{matcher, mtc_tok, mtc_rep, mtc_bb};\n+\n+/* This is an Earley-like parser, without support for nonterminals.  This\n+means that there are no completer or predictor rules, and therefore no need to\n+store one column per token: instead, there's a set of current Earley items and\n+a set of next ones. Instead of NTs, we have a special case for Kleene\n+star. The big-O, in pathological cases, is worse than traditional Earley\n+parsing, but it's an easier fit for Macro-by-Example-style rules, and I think\n+the overhead is lower. */\n+\n+\n+/* to avoid costly uniqueness checks, we require that `mtc_rep` always has a\n+nonempty body. */\n+\n+enum matcher_pos_up { /* to break a circularity */\n+    matcher_pos_up(option<matcher_pos>)\n+}\n+\n+fn is_some(&&mpu: matcher_pos_up) -> bool {\n+    alt mpu {\n+      matcher_pos_up(none) { false }\n+      _ { true }\n+    }\n+}\n+\n+type matcher_pos = ~{\n+    elts: [ast::matcher], // maybe should be /& ? Need to understand regions.\n+    sep: option<token>,\n+    mut idx: uint,\n+    mut up: matcher_pos_up, // mutable for swapping only\n+    matches: [dvec<@arb_depth>]\n+};\n+\n+fn copy_up(&& mpu: matcher_pos_up) -> matcher_pos {\n+    alt mpu {\n+      matcher_pos_up(some(mp)) { copy mp }\n+      _ { fail }\n+    }\n+}\n+\n+fn count_names(ms: [matcher]/&) -> uint {\n+    vec::foldl(0u, ms, {|ct, m|\n+        ct + alt m.node {\n+          mtc_tok(_) { 0u }\n+          mtc_rep(more_ms, _, _) { count_names(more_ms) }\n+          mtc_bb(_,_,_) { 1u }\n+        }})\n+}\n+\n+fn new_matcher_pos(ms: [matcher], sep: option<token>) -> matcher_pos {\n+    ~{elts: ms, sep: sep, mut idx: 0u, mut up: matcher_pos_up(none),\n+      matches: copy vec::from_fn(count_names(ms), {|_i| dvec::dvec()}) }\n+}\n+\n+/* logically, an arb_depth should contain only one kind of nonterminal */\n+enum arb_depth { leaf(whole_nt), seq([@arb_depth]) }\n+\n+type earley_item = matcher_pos;\n+\n+\n+fn parse(sess: parse_sess, cfg: ast::crate_cfg, rdr: reader, ms: [matcher])\n+    -> [@arb_depth] {\n+    let mut cur_eis = [];\n+    vec::push(cur_eis, new_matcher_pos(ms, none));\n+\n+    loop {\n+        let mut bb_eis = []; // black-box parsed by parser.rs\n+        let mut next_eis = []; // or proceed normally\n+        let mut eof_eis = [];\n+\n+        let {tok: tok, sp: _} = rdr.peek();\n+\n+        /* we append new items to this while we go */\n+        while cur_eis.len() > 0u { /* for each Earley Item */\n+            let mut ei = vec::pop(cur_eis);\n+\n+            let idx = ei.idx;\n+            let len = ei.elts.len();\n+\n+            /* at end of sequence */\n+            if idx >= len {\n+                // can't move out of `alt`s, so:\n+                if is_some(ei.up) {\n+                    // hack: a matcher sequence is repeating iff it has a\n+                    // parent (the top level is just a container)\n+\n+\n+                    // disregard separator, try to go up\n+                    // (remove this condition to make trailing seps ok)\n+                    if idx == len {\n+                        // pop from the matcher position\n+\n+                        let new_pos = copy_up(ei.up);\n+\n+                        // update matches (the MBE \"parse tree\") by appending\n+                        // each tree as a subtree.\n+\n+                        // I bet this is a perf problem: we're preemptively\n+                        // doing a lot of array work that will get thrown away\n+                        // most of the time.\n+                        for ei.matches.eachi() { |idx, elt|\n+                            new_pos.matches[idx].push(@seq(elt.get()));\n+                        }\n+\n+                        new_pos.idx += 1u;\n+                        vec::push(cur_eis, new_pos);\n+                    }\n+\n+                    // can we go around again?\n+\n+                    // the *_t vars are workarounds for the lack of unary move\n+                    alt copy ei.sep {\n+                      some(t) if idx == len { // we need a separator\n+                        if tok == t { //pass the separator\n+                            let ei_t <- ei;\n+                            ei_t.idx += 1u;\n+                            vec::push(next_eis, ei_t);\n+                        }\n+                      }\n+                      _ { // we don't need a separator\n+                        let ei_t <- ei;\n+                        ei_t.idx = 0u;\n+                        vec::push(cur_eis, ei_t);\n+                      }\n+                    }\n+                } else {\n+                    vec::push(eof_eis, ei);\n+                }\n+            } else {\n+                alt copy ei.elts[idx].node {\n+                  /* need to descend into sequence */\n+                  mtc_rep(matchers, sep, zero_ok) {\n+                    if zero_ok {\n+                        let new_ei = copy ei;\n+                        new_ei.idx += 1u;\n+                        vec::push(cur_eis, new_ei);\n+                    }\n+\n+                    let matches = vec::map(ei.matches, // fresh, same size:\n+                                           {|_m| dvec::<@arb_depth>()});\n+                    let ei_t <- ei;\n+                    vec::push(cur_eis, ~{\n+                        elts: matchers, sep: sep, mut idx: 0u,\n+                        mut up: matcher_pos_up(some(ei_t)),\n+                        matches: matches\n+                    });\n+                  }\n+                  mtc_bb(_,_,_) { vec::push(bb_eis, ei) }\n+                  mtc_tok(t) {\n+                    let ei_t <- ei;\n+                    if t == tok { ei_t.idx += 1u; vec::push(next_eis, ei_t)}\n+                  }\n+                }\n+            }\n+        }\n+\n+        /* error messages here could be improved with links to orig. rules */\n+        if tok == EOF {\n+            if eof_eis.len() == 1u {\n+                let ret_val = vec::map(eof_eis[0u].matches, {|dv| dv.pop()});\n+                ret ret_val; /* success */\n+            } else if eof_eis.len() > 1u {\n+                rdr.fatal(\"Ambiguity: multiple successful parses\");\n+            } else {\n+                rdr.fatal(\"Unexpected end of macro invocation\");\n+            }\n+        } else {\n+            if (bb_eis.len() > 0u && next_eis.len() > 0u)\n+                || bb_eis.len() > 1u {\n+                let nts = str::connect(vec::map(bb_eis, {|ei|\n+                    alt ei.elts[ei.idx].node\n+                        { mtc_bb(_,name,_) { *name } _ { fail; } }\n+                }), \" or \");\n+                rdr.fatal(#fmt[\"Local ambiguity: multiple parsing options: \\\n+                                built-in NTs %s or %u other options.\",\n+                              nts, next_eis.len()]);\n+            } else if (bb_eis.len() == 0u && next_eis.len() == 0u) {\n+                rdr.fatal(\"No rules expected the token \"\n+                          + to_str(*rdr.interner(), tok));\n+            } else if (next_eis.len() > 0u) {\n+                /* Now process the next token */\n+                while(next_eis.len() > 0u) {\n+                    vec::push(cur_eis, vec::pop(next_eis));\n+                }\n+                rdr.next_token();\n+            } else /* bb_eis.len() == 1 */ {\n+                let rust_parser = parser(sess, cfg, rdr.dup(), SOURCE_FILE);\n+\n+                let ei = vec::pop(bb_eis);\n+                alt ei.elts[ei.idx].node {\n+                  mtc_bb(_, name, idx) {\n+                    ei.matches[idx].push(@leaf(\n+                        parse_nt(rust_parser, *name)));\n+                    ei.idx += 1u;\n+                  }\n+                  _ { fail; }\n+                }\n+                vec::push(cur_eis,ei);\n+\n+                /* this would fail if zero-length tokens existed */\n+                while rdr.peek().sp.lo < rust_parser.span.lo {\n+                    rdr.next_token();\n+                }\n+            }\n+        }\n+\n+        assert cur_eis.len() > 0u;\n+    }\n+}\n+\n+fn parse_nt(p: parser, name: str) -> whole_nt {\n+    alt name {\n+      \"item\" { alt p.parse_item([], ast::public) {\n+        some(i) { token::w_item(i) }\n+        none { p.fatal(\"expected an item keyword\") }\n+      }}\n+      \"block\" { token::w_block(p.parse_block()) }\n+      \"stmt\" { token::w_stmt(p.parse_stmt([])) }\n+      \"pat\" { token::w_pat(p.parse_pat()) }\n+      \"expr\" { token::w_expr(p.parse_expr()) }\n+      \"ty\" { token::w_ty(p.parse_ty(false /* no need to disambiguate*/)) }\n+      // this could be handled like a token, since it is one\n+      \"ident\" { token::w_ident(p.parse_ident()) }\n+      \"path\" { token::w_path(p.parse_path_with_tps(false)) }\n+      _ { p.fatal(\"Unsupported builtin nonterminal parser: \" + name)}\n+    }\n+}\n+\n+// Local Variables:\n+// mode: rust;\n+// fill-column: 78;\n+// indent-tabs-mode: nil\n+// c-basic-offset: 4\n+// buffer-file-coding-system: utf-8-unix\n+// End:"}, {"sha": "54d14f2eaf4f9a1d054c902193e86b2c6f5e587c", "filename": "src/libsyntax/parse/comments.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fcomments.rs?ref=4f104954a67ad736244ce212467290c836394fad", "patch": "@@ -199,8 +199,9 @@ fn gather_comments_and_literals(span_diagnostic: diagnostic::span_handler,\n \n \n         let bstart = rdr.pos;\n+        rdr.next_token();\n         //discard, and look ahead; we're working with internal state\n-        let {tok: tok, sp: sp} = rdr.next_token();\n+        let {tok: tok, sp: sp} = rdr.peek();\n         if token::is_lit(tok) {\n             let s = get_str_from(rdr, bstart);\n             vec::push(literals, {lit: s, pos: sp.lo});"}, {"sha": "1d92561a108f4834c1c1ce1d9be7954d225cbffd", "filename": "src/libsyntax/parse/common.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fcommon.rs?ref=4f104954a67ad736244ce212467290c836394fad", "patch": "@@ -111,8 +111,8 @@ impl parser_common for parser {\n         if !self.eat_keyword(word) {\n             self.fatal(\"expecting \" + word + \", found \" +\n                     token_to_str(self.reader, self.token));\n+        }\n     }\n-}\n \n     fn is_restricted_keyword(word: str) -> bool {\n         self.restricted_keywords.contains_key(word)"}, {"sha": "5a3dceace8dc31d77be3e2e4c5b1a6eeeaa156c2", "filename": "src/libsyntax/parse/lexer.rs", "status": "modified", "additions": 49, "deletions": 21, "changes": 70, "blob_url": "https://github.com/rust-lang/rust/blob/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer.rs?ref=4f104954a67ad736244ce212467290c836394fad", "patch": "@@ -13,21 +13,25 @@ iface reader {\n     fn is_eof() -> bool;\n     fn next_token() -> {tok: token::token, sp: span};\n     fn fatal(str) -> !;\n+    fn span_diag() -> diagnostic::span_handler;\n     fn interner() -> @interner::interner<@str>;\n+    fn peek() -> {tok: token::token, sp: span};\n+    fn dup() -> reader;\n }\n \n enum tt_frame_up { /* to break a circularity */\n     tt_frame_up(option<tt_frame>)\n }\n \n+/* TODO: figure out how to have a uniquely linked stack, and change to `~` */\n #[doc = \"an unzipping of `token_tree`s\"]\n type tt_frame = @{\n     readme: [ast::token_tree],\n     mut idx: uint,\n     up: tt_frame_up\n };\n \n-type tt_reader = ~{\n+type tt_reader = @{\n     span_diagnostic: diagnostic::span_handler,\n     interner: @interner::interner<@str>,\n     mut cur: tt_frame,\n@@ -39,7 +43,7 @@ type tt_reader = ~{\n fn new_tt_reader(span_diagnostic: diagnostic::span_handler,\n                  itr: @interner::interner<@str>, src: [ast::token_tree])\n     -> tt_reader {\n-    let r = ~{span_diagnostic: span_diagnostic, interner: itr,\n+    let r = @{span_diagnostic: span_diagnostic, interner: itr,\n               mut cur: @{readme: src, mut idx: 0u,\n                          up: tt_frame_up(option::none)},\n               /* dummy values, never read: */\n@@ -62,7 +66,7 @@ pure fn dup_tt_frame(&&f: tt_frame) -> tt_frame {\n }\n \n pure fn dup_tt_reader(&&r: tt_reader) -> tt_reader {\n-    ~{span_diagnostic: r.span_diagnostic, interner: r.interner,\n+    @{span_diagnostic: r.span_diagnostic, interner: r.interner,\n       mut cur: dup_tt_frame(r.cur),\n       mut cur_tok: r.cur_tok, mut cur_span: r.cur_span}\n }\n@@ -75,13 +79,17 @@ type string_reader = @{\n     mut curr: char,\n     mut chpos: uint,\n     filemap: codemap::filemap,\n-    interner: @interner::interner<@str>\n+    interner: @interner::interner<@str>,\n+    /* cached: */\n+    mut peek_tok: token::token,\n+    mut peek_span: span\n };\n \n fn new_string_reader(span_diagnostic: diagnostic::span_handler,\n                      filemap: codemap::filemap,\n                      itr: @interner::interner<@str>) -> string_reader {\n     let r = new_low_level_string_reader(span_diagnostic, filemap, itr);\n+    string_advance_token(r); /* fill in peek_* */\n     ret r;\n }\n \n@@ -93,7 +101,10 @@ fn new_low_level_string_reader(span_diagnostic: diagnostic::span_handler,\n     let r = @{span_diagnostic: span_diagnostic, src: filemap.src,\n               mut col: 0u, mut pos: 0u, mut curr: -1 as char,\n               mut chpos: filemap.start_pos.ch,\n-              filemap: filemap, interner: itr};\n+              filemap: filemap, interner: itr,\n+              /* dummy values; not read */\n+              mut peek_tok: token::EOF,\n+              mut peek_span: ast_util::mk_sp(0u,0u)};\n     if r.pos < (*filemap.src).len() {\n         let next = str::char_range_at(*r.src, r.pos);\n         r.pos = next.next;\n@@ -102,23 +113,29 @@ fn new_low_level_string_reader(span_diagnostic: diagnostic::span_handler,\n     ret r;\n }\n \n+fn dup_string_reader(&&r: string_reader) -> string_reader {\n+    @{span_diagnostic: r.span_diagnostic, src: r.src,\n+      mut col: r.col, mut pos: r.pos, mut curr: r.curr, mut chpos: r.chpos,\n+      filemap: r.filemap, interner: r.interner,\n+      mut peek_tok: r.peek_tok, mut peek_span: r.peek_span}\n+}\n+\n impl string_reader_as_reader of reader for string_reader {\n     fn is_eof() -> bool { is_eof(self) }\n     fn next_token() -> {tok: token::token, sp: span} {\n-        consume_whitespace_and_comments(self);\n-        let start_chpos = self.chpos;\n-        let tok = if is_eof(self) {\n-            token::EOF\n-        } else {\n-            next_token_inner(self)\n-        };\n-        ret {tok: tok, sp: ast_util::mk_sp(start_chpos, self.chpos)};\n+        let ret_val = {tok: self.peek_tok, sp: self.peek_span};\n+        string_advance_token(self);\n+        ret ret_val;\n     }\n     fn fatal(m: str) -> ! {\n-        self.span_diagnostic.span_fatal(\n-            ast_util::mk_sp(self.chpos, self.chpos), m)\n+        self.span_diagnostic.span_fatal(copy self.peek_span, m)\n     }\n+    fn span_diag() -> diagnostic::span_handler { self.span_diagnostic }\n     fn interner() -> @interner::interner<@str> { self.interner }\n+    fn peek() -> {tok: token::token, sp: span} {\n+        {tok: self.peek_tok, sp: self.peek_span}\n+    }\n+    fn dup() -> reader { dup_string_reader(self) as reader }\n }\n \n impl tt_reader_as_reader of reader for tt_reader {\n@@ -135,25 +152,37 @@ impl tt_reader_as_reader of reader for tt_reader {\n     fn fatal(m: str) -> ! {\n         self.span_diagnostic.span_fatal(copy self.cur_span, m);\n     }\n+    fn span_diag() -> diagnostic::span_handler { self.span_diagnostic }\n     fn interner() -> @interner::interner<@str> { self.interner }\n+    fn peek() -> {tok: token::token, sp: span} {\n+        { tok: self.cur_tok, sp: self.cur_span }\n+    }\n+    fn dup() -> reader { dup_tt_reader(self) as reader }\n }\n \n fn string_advance_token(&&r: string_reader) {\n     consume_whitespace_and_comments(r);\n \n-    next_token_inner(r);\n+    if is_eof(r) {\n+        r.peek_tok = token::EOF;\n+    } else {\n+        let start_chpos = r.chpos;\n+        r.peek_tok = next_token_inner(r);\n+        r.peek_span = ast_util::mk_sp(start_chpos, r.chpos);\n+    };\n+\n }\n \n fn tt_next_token(&&r: tt_reader) -> {tok: token::token, sp: span} {\n     let ret_val = { tok: r.cur_tok, sp: r.cur_span };\n     if r.cur.idx >= vec::len(r.cur.readme) {\n         /* done with this set; pop */\n         alt r.cur.up {\n-          tt_frame_up(option::none) {\n+          tt_frame_up(none) {\n             r.cur_tok = token::EOF;\n             ret ret_val;\n           }\n-          tt_frame_up(option::some(tt_f)) {\n+          tt_frame_up(some(tt_f)) {\n             r.cur = tt_f;\n             /* the above `if` would need to be a `while` if we didn't know\n             that the last thing in a `tt_delim` is always a `tt_flat` */\n@@ -165,11 +194,10 @@ fn tt_next_token(&&r: tt_reader) -> {tok: token::token, sp: span} {\n     between popping and pushing until we got to an actual `tt_flat` */\n     loop { /* because it's easiest, this handles `tt_delim` not starting\n     with a `tt_flat`, even though it won't happen */\n-        alt r.cur.readme[r.cur.idx] {\n+        alt copy r.cur.readme[r.cur.idx] {\n           tt_delim(tts) {\n-            /* TODO: this copy should be a unary move, once they exist */\n             r.cur = @{readme: tts, mut idx: 0u,\n-                      up: tt_frame_up(option::some(copy r.cur)) };\n+                      up: tt_frame_up(option::some(r.cur)) };\n           }\n           tt_flat(sp, tok) {\n             r.cur_span = sp; r.cur_tok = tok;"}, {"sha": "02d08ab5ae8b9018f5c9f48d8fc80480effdf9ec", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 57, "deletions": 4, "changes": 61, "blob_url": "https://github.com/rust-lang/rust/blob/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=4f104954a67ad736244ce212467290c836394fad", "patch": "@@ -803,10 +803,11 @@ class parser {\n         } else if self.token == token::POUND\n             && self.look_ahead(1u) == token::POUND {\n             self.bump(); self.bump();\n-            let macname = self.parse_path_without_tps();\n-            let macbody = self.parse_token_tree();\n-            ret pexpr(self.mk_mac_expr(lo, self.span.hi,\n-                                       mac_invoc_tt(macname, macbody)));\n+            //let macname = self.parse_path_without_tps();\n+            //let macbody = self.parse_token_tree();\n+            //ret pexpr(self.mk_mac_expr(lo, self.span.hi,\n+            //                           mac_invoc_tt(macname, macbody)));\n+            ret pexpr(self.parse_tt_mac_demo());\n         } else if self.token == token::POUND\n             && self.look_ahead(1u) == token::LT {\n             self.bump(); self.bump();\n@@ -1084,6 +1085,58 @@ class parser {\n         };\n     }\n \n+    /* temporary */\n+    fn parse_tt_mac_demo() -> @expr {\n+        let ms = self.parse_seq(token::LBRACE, token::RBRACE,\n+                                common::seq_sep_none(),\n+                                {|p| p.parse_matcher(@mut 0u)}).node;\n+        let tt = self.parse_token_tree();\n+        alt tt {\n+          tt_delim(tts) {\n+            let rdr = lexer::new_tt_reader(self.reader.span_diag(),\n+                                           self.reader.interner(), tts)\n+                as reader;\n+            ext::earley_parser::parse(self.sess, self.cfg, rdr, ms);\n+          }\n+          _ { fail; }\n+        }\n+\n+        ret self.mk_expr(0u, 0u, expr_break);\n+    }\n+\n+    fn parse_matcher(name_idx: @mut uint) -> matcher {\n+        let lo = self.span.lo;\n+        let mut sep = none;\n+        if self.eat_keyword(\"sep\") { sep = some(self.token); self.bump(); }\n+\n+        let m = if self.is_keyword(\"many\")||self.is_keyword(\"at_least_one\") {\n+            let zero_ok = self.is_keyword(\"many\");\n+            self.bump();\n+            let ms = (self.parse_seq(token::LPAREN, token::RPAREN,\n+                                     common::seq_sep_none(),\n+                                     {|p| p.parse_matcher(name_idx)}).node);\n+            if ms.len() == 0u {\n+                self.fatal(\"repetition body must be nonempty\");\n+            }\n+            mtc_rep(ms, sep, zero_ok)\n+        } else if option::is_some(sep) {\n+            self.fatal(\"`sep <tok>` must preceed `many` or `at_least_one`\");\n+        } else if self.eat_keyword(\"parse\") {\n+            let bound_to = self.parse_ident();\n+            self.expect(token::EQ);\n+            let nt_name = self.parse_ident();\n+\n+            let m = mtc_bb(bound_to, nt_name, *name_idx);\n+            *name_idx += 1u;\n+            m\n+        } else {\n+            let m = mtc_tok(self.token);\n+            self.bump();\n+            m\n+        };\n+        ret spanned(lo, self.span.hi, m);\n+    }\n+\n \n     fn parse_prefix_expr() -> pexpr {\n         let lo = self.span.lo;"}, {"sha": "9d6427912df123976c3744b47b0de39d666b9b3a", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=4f104954a67ad736244ce212467290c836394fad", "patch": "@@ -85,7 +85,7 @@ enum token {\n \n #[auto_serialize]\n #[doc = \"For interpolation during macro expansion.\"]\n-enum whole_nonterminal {\n+enum whole_nt {\n     w_item(@ast::item),\n     w_block(ast::blk),\n     w_stmt(@ast::stmt),\n@@ -257,7 +257,9 @@ fn contextual_keyword_table() -> hashmap<str, ()> {\n         \"self\", \"send\", \"static\",\n         \"to\",\n         \"use\",\n-        \"with\"\n+        \"with\",\n+        /* temp */\n+        \"sep\", \"many\", \"at_least_one\", \"parse\"\n     ];\n     for keys.each {|word|\n         words.insert(word, ());"}, {"sha": "cf4d24b8599cca4161e06873ff17fd07e9b67658", "filename": "src/libsyntax/syntax.rc", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fsyntax.rc", "raw_url": "https://github.com/rust-lang/rust/raw/4f104954a67ad736244ce212467290c836394fad/src%2Flibsyntax%2Fsyntax.rc", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fsyntax.rc?ref=4f104954a67ad736244ce212467290c836394fad", "patch": "@@ -29,6 +29,7 @@ mod util {\n \n mod parse {\n     export parser;\n+    export common;\n     export lexer;\n     export token;\n     export comments;\n@@ -64,6 +65,8 @@ mod ext {\n     mod qquote;\n     mod build;\n \n+    mod earley_parser;\n+\n     mod fmt;\n     mod env;\n     mod simplext;"}]}