{"sha": "e0127dbf8135b766a332ce21c4eee48998b59bef", "node_id": "MDY6Q29tbWl0NzI0NzEyOmUwMTI3ZGJmODEzNWI3NjZhMzMyY2UyMWM0ZWVlNDg5OThiNTliZWY=", "commit": {"author": {"name": "Vadim Petrochenkov", "email": "vadim.petrochenkov@gmail.com", "date": "2019-06-04T17:42:43Z"}, "committer": {"name": "Vadim Petrochenkov", "email": "vadim.petrochenkov@gmail.com", "date": "2019-06-06T11:03:15Z"}, "message": "syntax: Use `Token` in `TokenTree::Token`", "tree": {"sha": "4a30906f1c8058e13fd426a56967e7cba9408bf7", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/4a30906f1c8058e13fd426a56967e7cba9408bf7"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/e0127dbf8135b766a332ce21c4eee48998b59bef", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/e0127dbf8135b766a332ce21c4eee48998b59bef", "html_url": "https://github.com/rust-lang/rust/commit/e0127dbf8135b766a332ce21c4eee48998b59bef", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/e0127dbf8135b766a332ce21c4eee48998b59bef/comments", "author": {"login": "petrochenkov", "id": 5751617, "node_id": "MDQ6VXNlcjU3NTE2MTc=", "avatar_url": "https://avatars.githubusercontent.com/u/5751617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrochenkov", "html_url": "https://github.com/petrochenkov", "followers_url": "https://api.github.com/users/petrochenkov/followers", "following_url": "https://api.github.com/users/petrochenkov/following{/other_user}", "gists_url": "https://api.github.com/users/petrochenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrochenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrochenkov/subscriptions", "organizations_url": "https://api.github.com/users/petrochenkov/orgs", "repos_url": "https://api.github.com/users/petrochenkov/repos", "events_url": "https://api.github.com/users/petrochenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrochenkov/received_events", "type": "User", "site_admin": false}, "committer": {"login": "petrochenkov", "id": 5751617, "node_id": "MDQ6VXNlcjU3NTE2MTc=", "avatar_url": "https://avatars.githubusercontent.com/u/5751617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrochenkov", "html_url": "https://github.com/petrochenkov", "followers_url": "https://api.github.com/users/petrochenkov/followers", "following_url": "https://api.github.com/users/petrochenkov/following{/other_user}", "gists_url": "https://api.github.com/users/petrochenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrochenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrochenkov/subscriptions", "organizations_url": "https://api.github.com/users/petrochenkov/orgs", "repos_url": "https://api.github.com/users/petrochenkov/repos", "events_url": "https://api.github.com/users/petrochenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrochenkov/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "url": "https://api.github.com/repos/rust-lang/rust/commits/a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "html_url": "https://github.com/rust-lang/rust/commit/a3425edb46dfcc7031068b8bdda868e5a3b16ae1"}], "stats": {"total": 639, "additions": 327, "deletions": 312}, "files": [{"sha": "e7f52b48cb9ede5018ad0860123c722138c1fb58", "filename": "src/librustc/hir/lowering.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibrustc%2Fhir%2Flowering.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibrustc%2Fhir%2Flowering.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Flowering.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -67,7 +67,7 @@ use syntax::source_map::CompilerDesugaringKind::IfTemporary;\n use syntax::std_inject;\n use syntax::symbol::{kw, sym, Symbol};\n use syntax::tokenstream::{TokenStream, TokenTree};\n-use syntax::parse::token::{self, TokenKind};\n+use syntax::parse::token::{self, Token};\n use syntax::visit::{self, Visitor};\n use syntax_pos::{DUMMY_SP, edition, Span};\n \n@@ -1328,7 +1328,7 @@ impl<'a> LoweringContext<'a> {\n \n     fn lower_token_tree(&mut self, tree: TokenTree) -> TokenStream {\n         match tree {\n-            TokenTree::Token(span, token) => self.lower_token(token, span),\n+            TokenTree::Token(token) => self.lower_token(token),\n             TokenTree::Delimited(span, delim, tts) => TokenTree::Delimited(\n                 span,\n                 delim,\n@@ -1337,13 +1337,13 @@ impl<'a> LoweringContext<'a> {\n         }\n     }\n \n-    fn lower_token(&mut self, token: TokenKind, span: Span) -> TokenStream {\n-        match token {\n+    fn lower_token(&mut self, token: Token) -> TokenStream {\n+        match token.kind {\n             token::Interpolated(nt) => {\n-                let tts = nt.to_tokenstream(&self.sess.parse_sess, span);\n+                let tts = nt.to_tokenstream(&self.sess.parse_sess, token.span);\n                 self.lower_token_stream(tts)\n             }\n-            other => TokenTree::Token(span, other).into(),\n+            _ => TokenTree::Token(token).into(),\n         }\n     }\n "}, {"sha": "a373f434bf71e90cccb6cd9e3456f9065887f2dc", "filename": "src/librustc/ich/impls_syntax.rs", "status": "modified", "additions": 64, "deletions": 60, "changes": 124, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibrustc%2Fich%2Fimpls_syntax.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibrustc%2Fich%2Fimpls_syntax.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fich%2Fimpls_syntax.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -261,9 +261,8 @@ for tokenstream::TokenTree {\n                                           hasher: &mut StableHasher<W>) {\n         mem::discriminant(self).hash_stable(hcx, hasher);\n         match *self {\n-            tokenstream::TokenTree::Token(span, ref token) => {\n-                span.hash_stable(hcx, hasher);\n-                hash_token(token, hcx, hasher);\n+            tokenstream::TokenTree::Token(ref token) => {\n+                token.hash_stable(hcx, hasher);\n             }\n             tokenstream::TokenTree::Delimited(span, delim, ref tts) => {\n                 span.hash_stable(hcx, hasher);\n@@ -306,70 +305,75 @@ impl_stable_hash_for!(struct token::Lit {\n     suffix\n });\n \n-fn hash_token<'a, 'gcx, W: StableHasherResult>(\n-    token: &token::TokenKind,\n-    hcx: &mut StableHashingContext<'a>,\n-    hasher: &mut StableHasher<W>,\n-) {\n-    mem::discriminant(token).hash_stable(hcx, hasher);\n-    match *token {\n-        token::Eq |\n-        token::Lt |\n-        token::Le |\n-        token::EqEq |\n-        token::Ne |\n-        token::Ge |\n-        token::Gt |\n-        token::AndAnd |\n-        token::OrOr |\n-        token::Not |\n-        token::Tilde |\n-        token::At |\n-        token::Dot |\n-        token::DotDot |\n-        token::DotDotDot |\n-        token::DotDotEq |\n-        token::Comma |\n-        token::Semi |\n-        token::Colon |\n-        token::ModSep |\n-        token::RArrow |\n-        token::LArrow |\n-        token::FatArrow |\n-        token::Pound |\n-        token::Dollar |\n-        token::Question |\n-        token::SingleQuote |\n-        token::Whitespace |\n-        token::Comment |\n-        token::Eof => {}\n-\n-        token::BinOp(bin_op_token) |\n-        token::BinOpEq(bin_op_token) => {\n-            std_hash::Hash::hash(&bin_op_token, hasher);\n-        }\n+impl<'a> HashStable<StableHashingContext<'a>> for token::TokenKind {\n+    fn hash_stable<W: StableHasherResult>(&self,\n+                                          hcx: &mut StableHashingContext<'a>,\n+                                          hasher: &mut StableHasher<W>) {\n+        mem::discriminant(self).hash_stable(hcx, hasher);\n+        match *self {\n+            token::Eq |\n+            token::Lt |\n+            token::Le |\n+            token::EqEq |\n+            token::Ne |\n+            token::Ge |\n+            token::Gt |\n+            token::AndAnd |\n+            token::OrOr |\n+            token::Not |\n+            token::Tilde |\n+            token::At |\n+            token::Dot |\n+            token::DotDot |\n+            token::DotDotDot |\n+            token::DotDotEq |\n+            token::Comma |\n+            token::Semi |\n+            token::Colon |\n+            token::ModSep |\n+            token::RArrow |\n+            token::LArrow |\n+            token::FatArrow |\n+            token::Pound |\n+            token::Dollar |\n+            token::Question |\n+            token::SingleQuote |\n+            token::Whitespace |\n+            token::Comment |\n+            token::Eof => {}\n+\n+            token::BinOp(bin_op_token) |\n+            token::BinOpEq(bin_op_token) => {\n+                std_hash::Hash::hash(&bin_op_token, hasher);\n+            }\n \n-        token::OpenDelim(delim_token) |\n-        token::CloseDelim(delim_token) => {\n-            std_hash::Hash::hash(&delim_token, hasher);\n-        }\n-        token::Literal(lit) => lit.hash_stable(hcx, hasher),\n+            token::OpenDelim(delim_token) |\n+            token::CloseDelim(delim_token) => {\n+                std_hash::Hash::hash(&delim_token, hasher);\n+            }\n+            token::Literal(lit) => lit.hash_stable(hcx, hasher),\n \n-        token::Ident(ident, is_raw) => {\n-            ident.name.hash_stable(hcx, hasher);\n-            is_raw.hash_stable(hcx, hasher);\n-        }\n-        token::Lifetime(ident) => ident.name.hash_stable(hcx, hasher),\n+            token::Ident(ident, is_raw) => {\n+                ident.name.hash_stable(hcx, hasher);\n+                is_raw.hash_stable(hcx, hasher);\n+            }\n+            token::Lifetime(ident) => ident.name.hash_stable(hcx, hasher),\n \n-        token::Interpolated(_) => {\n-            bug!(\"interpolated tokens should not be present in the HIR\")\n-        }\n+            token::Interpolated(_) => {\n+                bug!(\"interpolated tokens should not be present in the HIR\")\n+            }\n \n-        token::DocComment(val) |\n-        token::Shebang(val) => val.hash_stable(hcx, hasher),\n+            token::DocComment(val) |\n+            token::Shebang(val) => val.hash_stable(hcx, hasher),\n+        }\n     }\n }\n \n+impl_stable_hash_for!(struct token::Token {\n+    kind,\n+    span\n+});\n+\n impl_stable_hash_for!(enum ::syntax::ast::NestedMetaItem {\n     MetaItem(meta_item),\n     Literal(lit)"}, {"sha": "a3da97bd5db1ec23678f94a62fe983c47ab5c1c7", "filename": "src/librustc_lint/builtin.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibrustc_lint%2Fbuiltin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibrustc_lint%2Fbuiltin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_lint%2Fbuiltin.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -1414,11 +1414,11 @@ impl KeywordIdents {\n     fn check_tokens(&mut self, cx: &EarlyContext<'_>, tokens: TokenStream) {\n         for tt in tokens.into_trees() {\n             match tt {\n-                TokenTree::Token(span, tok) => match tok.ident() {\n+                TokenTree::Token(token) => match token.ident() {\n                     // only report non-raw idents\n                     Some((ident, false)) => {\n                         self.check_ident_token(cx, UnderMacro(true), ast::Ident {\n-                            span: span.substitute_dummy(ident.span),\n+                            span: token.span.substitute_dummy(ident.span),\n                             ..ident\n                         });\n                     }"}, {"sha": "d68741233754bbb60efe23acfedfd6a1f328cfd2", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -234,7 +234,7 @@ impl<'a> Classifier<'a> {\n             // reference or dereference operator or a reference or pointer type, instead of the\n             // bit-and or multiplication operator.\n             token::BinOp(token::And) | token::BinOp(token::Star)\n-                if self.lexer.peek().kind != token::Whitespace => Class::RefKeyWord,\n+                if self.lexer.peek() != token::Whitespace => Class::RefKeyWord,\n \n             // Consider this as part of a macro invocation if there was a\n             // leading identifier.\n@@ -335,7 +335,7 @@ impl<'a> Classifier<'a> {\n                     sym::Option | sym::Result => Class::PreludeTy,\n                     sym::Some | sym::None | sym::Ok | sym::Err => Class::PreludeVal,\n \n-                    _ if token.kind.is_reserved_ident() => Class::KeyWord,\n+                    _ if token.is_reserved_ident() => Class::KeyWord,\n \n                     _ => {\n                         if self.in_macro_nonterminal {"}, {"sha": "448061395afdc91ddbf44c192470cd3912d51068", "filename": "src/libsyntax/attr/mod.rs", "status": "modified", "additions": 18, "deletions": 19, "changes": 37, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fattr%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fattr%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr%2Fmod.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -20,7 +20,7 @@ use crate::source_map::{BytePos, Spanned, dummy_spanned};\n use crate::parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n use crate::parse::parser::Parser;\n use crate::parse::{self, ParseSess, PResult};\n-use crate::parse::token::{self, TokenKind};\n+use crate::parse::token::{self, Token, TokenKind};\n use crate::ptr::P;\n use crate::symbol::{sym, Symbol};\n use crate::ThinVec;\n@@ -465,9 +465,9 @@ impl MetaItem {\n                 let mod_sep_span = Span::new(last_pos,\n                                              segment.ident.span.lo(),\n                                              segment.ident.span.ctxt());\n-                idents.push(TokenTree::Token(mod_sep_span, token::ModSep).into());\n+                idents.push(TokenTree::token(mod_sep_span, token::ModSep).into());\n             }\n-            idents.push(TokenTree::Token(segment.ident.span,\n+            idents.push(TokenTree::token(segment.ident.span,\n                                          TokenKind::from_ast_ident(segment.ident)).into());\n             last_pos = segment.ident.span.hi();\n         }\n@@ -480,10 +480,10 @@ impl MetaItem {\n     {\n         // FIXME: Share code with `parse_path`.\n         let path = match tokens.next() {\n-            Some(TokenTree::Token(span, token @ token::Ident(..))) |\n-            Some(TokenTree::Token(span, token @ token::ModSep)) => 'arm: {\n-                let mut segments = if let token::Ident(ident, _) = token {\n-                    if let Some(TokenTree::Token(_, token::ModSep)) = tokens.peek() {\n+            Some(TokenTree::Token(Token { kind: kind @ token::Ident(..), span })) |\n+            Some(TokenTree::Token(Token { kind: kind @ token::ModSep, span })) => 'arm: {\n+                let mut segments = if let token::Ident(ident, _) = kind {\n+                    if let Some(TokenTree::Token(Token { kind: token::ModSep, .. })) = tokens.peek() {\n                         tokens.next();\n                         vec![PathSegment::from_ident(ident.with_span_pos(span))]\n                     } else {\n@@ -493,13 +493,12 @@ impl MetaItem {\n                     vec![PathSegment::path_root(span)]\n                 };\n                 loop {\n-                    if let Some(TokenTree::Token(span,\n-                                                    token::Ident(ident, _))) = tokens.next() {\n+                    if let Some(TokenTree::Token(Token { kind: token::Ident(ident, _), span })) = tokens.next() {\n                         segments.push(PathSegment::from_ident(ident.with_span_pos(span)));\n                     } else {\n                         return None;\n                     }\n-                    if let Some(TokenTree::Token(_, token::ModSep)) = tokens.peek() {\n+                    if let Some(TokenTree::Token(Token { kind: token::ModSep, .. })) = tokens.peek() {\n                         tokens.next();\n                     } else {\n                         break;\n@@ -508,7 +507,7 @@ impl MetaItem {\n                 let span = span.with_hi(segments.last().unwrap().ident.span.hi());\n                 Path { span, segments }\n             }\n-            Some(TokenTree::Token(_, token::Interpolated(nt))) => match *nt {\n+            Some(TokenTree::Token(Token { kind: token::Interpolated(nt), .. })) => match *nt {\n                 token::Nonterminal::NtIdent(ident, _) => Path::from_ident(ident),\n                 token::Nonterminal::NtMeta(ref meta) => return Some(meta.clone()),\n                 token::Nonterminal::NtPath(ref path) => path.clone(),\n@@ -533,15 +532,15 @@ impl MetaItemKind {\n         match *self {\n             MetaItemKind::Word => TokenStream::empty(),\n             MetaItemKind::NameValue(ref lit) => {\n-                let mut vec = vec![TokenTree::Token(span, token::Eq).into()];\n+                let mut vec = vec![TokenTree::token(span, token::Eq).into()];\n                 lit.tokens().append_to_tree_and_joint_vec(&mut vec);\n                 TokenStream::new(vec)\n             }\n             MetaItemKind::List(ref list) => {\n                 let mut tokens = Vec::new();\n                 for (i, item) in list.iter().enumerate() {\n                     if i > 0 {\n-                        tokens.push(TokenTree::Token(span, token::Comma).into());\n+                        tokens.push(TokenTree::token(span, token::Comma).into());\n                     }\n                     item.tokens().append_to_tree_and_joint_vec(&mut tokens);\n                 }\n@@ -558,10 +557,10 @@ impl MetaItemKind {\n         where I: Iterator<Item = TokenTree>,\n     {\n         let delimited = match tokens.peek().cloned() {\n-            Some(TokenTree::Token(_, token::Eq)) => {\n+            Some(TokenTree::Token(token)) if token == token::Eq => {\n                 tokens.next();\n-                return if let Some(TokenTree::Token(span, token)) = tokens.next() {\n-                    Lit::from_token(&token, span).ok().map(MetaItemKind::NameValue)\n+                return if let Some(TokenTree::Token(token)) = tokens.next() {\n+                    Lit::from_token(&token, token.span).ok().map(MetaItemKind::NameValue)\n                 } else {\n                     None\n                 };\n@@ -579,7 +578,7 @@ impl MetaItemKind {\n             let item = NestedMetaItem::from_tokens(&mut tokens)?;\n             result.push(item);\n             match tokens.next() {\n-                None | Some(TokenTree::Token(_, token::Comma)) => {}\n+                None | Some(TokenTree::Token(Token { kind: token::Comma, .. })) => {}\n                 _ => return None,\n             }\n         }\n@@ -605,8 +604,8 @@ impl NestedMetaItem {\n     fn from_tokens<I>(tokens: &mut iter::Peekable<I>) -> Option<NestedMetaItem>\n         where I: Iterator<Item = TokenTree>,\n     {\n-        if let Some(TokenTree::Token(span, token)) = tokens.peek().cloned() {\n-            if let Ok(lit) = Lit::from_token(&token, span) {\n+        if let Some(TokenTree::Token(token)) = tokens.peek().cloned() {\n+            if let Ok(lit) = Lit::from_token(&token, token.span) {\n                 tokens.next();\n                 return Some(NestedMetaItem::Literal(lit));\n             }"}, {"sha": "b342e4bc47274505b7b0c006019217b2dcde4983", "filename": "src/libsyntax/diagnostics/plugin.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -5,7 +5,7 @@ use crate::ast::{self, Ident, Name};\n use crate::source_map;\n use crate::ext::base::{ExtCtxt, MacEager, MacResult};\n use crate::ext::build::AstBuilder;\n-use crate::parse::token;\n+use crate::parse::token::{self, Token};\n use crate::ptr::P;\n use crate::symbol::kw;\n use crate::tokenstream::{TokenTree};\n@@ -34,7 +34,7 @@ pub fn expand_diagnostic_used<'cx>(ecx: &'cx mut ExtCtxt<'_>,\n                                    token_tree: &[TokenTree])\n                                    -> Box<dyn MacResult+'cx> {\n     let code = match (token_tree.len(), token_tree.get(0)) {\n-        (1, Some(&TokenTree::Token(_, token::Ident(code, _)))) => code,\n+        (1, Some(&TokenTree::Token(Token { kind: token::Ident(code, _), .. }))) => code,\n         _ => unreachable!()\n     };\n \n@@ -72,12 +72,12 @@ pub fn expand_register_diagnostic<'cx>(ecx: &'cx mut ExtCtxt<'_>,\n         token_tree.get(1),\n         token_tree.get(2)\n     ) {\n-        (1, Some(&TokenTree::Token(_, token::Ident(ref code, _))), None, None) => {\n+        (1, Some(&TokenTree::Token(Token { kind: token::Ident(ref code, _), .. })), None, None) => {\n             (code, None)\n         },\n-        (3, Some(&TokenTree::Token(_, token::Ident(ref code, _))),\n-            Some(&TokenTree::Token(_, token::Comma)),\n-            Some(&TokenTree::Token(_, token::Literal(token::Lit { symbol, .. })))) => {\n+        (3, Some(&TokenTree::Token(Token { kind: token::Ident(ref code, _), .. })),\n+            Some(&TokenTree::Token(Token { kind: token::Comma, .. })),\n+            Some(&TokenTree::Token(Token { kind: token::Literal(token::Lit { symbol, .. }), .. }))) => {\n             (code, Some(symbol))\n         }\n         _ => unreachable!()\n@@ -143,9 +143,9 @@ pub fn expand_build_diagnostic_array<'cx>(ecx: &'cx mut ExtCtxt<'_>,\n     let (crate_name, name) = match (&token_tree[0], &token_tree[2]) {\n         (\n             // Crate name.\n-            &TokenTree::Token(_, token::Ident(ref crate_name, _)),\n+            &TokenTree::Token(Token { kind: token::Ident(ref crate_name, _), .. }),\n             // DIAGNOSTICS ident.\n-            &TokenTree::Token(_, token::Ident(ref name, _))\n+            &TokenTree::Token(Token { kind: token::Ident(ref name, _), .. })\n         ) => (*&crate_name, name),\n         _ => unreachable!()\n     };"}, {"sha": "0c2ab67240741a809b861d9acf87a323e44274f4", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -265,10 +265,12 @@ impl<F> TTMacroExpander for F\n \n         impl MutVisitor for AvoidInterpolatedIdents {\n             fn visit_tt(&mut self, tt: &mut tokenstream::TokenTree) {\n-                if let tokenstream::TokenTree::Token(_, token::Interpolated(nt)) = tt {\n-                    if let token::NtIdent(ident, is_raw) = **nt {\n-                        *tt = tokenstream::TokenTree::Token(ident.span,\n-                                                            token::Ident(ident, is_raw));\n+                if let tokenstream::TokenTree::Token(token) = tt {\n+                    if let token::Interpolated(nt) = &token.kind {\n+                        if let token::NtIdent(ident, is_raw) = **nt {\n+                            *tt = tokenstream::TokenTree::token(ident.span,\n+                                                                token::Ident(ident, is_raw));\n+                        }\n                     }\n                 }\n                 mut_visit::noop_visit_tt(tt, self)"}, {"sha": "4396b9be9bbb096b59bd812cfad01426c4e54607", "filename": "src/libsyntax/ext/expand.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fexpand.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -585,7 +585,7 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n             }\n             AttrProcMacro(ref mac, ..) => {\n                 self.gate_proc_macro_attr_item(attr.span, &item);\n-                let item_tok = TokenTree::Token(DUMMY_SP, token::Interpolated(Lrc::new(match item {\n+                let item_tok = TokenTree::token(DUMMY_SP, token::Interpolated(Lrc::new(match item {\n                     Annotatable::Item(item) => token::NtItem(item),\n                     Annotatable::TraitItem(item) => token::NtTraitItem(item.into_inner()),\n                     Annotatable::ImplItem(item) => token::NtImplItem(item.into_inner()),"}, {"sha": "6acdffedd6b1a6e3cfe5f21818394d473187299a", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -78,7 +78,7 @@ use crate::ast::Ident;\n use crate::ext::tt::quoted::{self, TokenTree};\n use crate::parse::{Directory, ParseSess};\n use crate::parse::parser::{Parser, PathStyle};\n-use crate::parse::token::{self, DocComment, Nonterminal, TokenKind};\n+use crate::parse::token::{self, DocComment, Nonterminal, Token, TokenKind};\n use crate::print::pprust;\n use crate::symbol::{kw, sym, Symbol};\n use crate::tokenstream::{DelimSpan, TokenStream};\n@@ -609,7 +609,8 @@ fn inner_parse_loop<'root, 'tt>(\n                 //\n                 // At the beginning of the loop, if we reach the end of the delimited submatcher,\n                 // we pop the stack to backtrack out of the descent.\n-                seq @ TokenTree::Delimited(..) | seq @ TokenTree::Token(_, DocComment(..)) => {\n+                seq @ TokenTree::Delimited(..) |\n+                seq @ TokenTree::Token(Token { kind: DocComment(..), .. }) => {\n                     let lower_elts = mem::replace(&mut item.top_elts, Tt(seq));\n                     let idx = item.idx;\n                     item.stack.push(MatcherTtFrame {\n@@ -621,7 +622,7 @@ fn inner_parse_loop<'root, 'tt>(\n                 }\n \n                 // We just matched a normal token. We can just advance the parser.\n-                TokenTree::Token(_, ref t) if token_name_eq(t, token) => {\n+                TokenTree::Token(t) if token_name_eq(&t, token) => {\n                     item.idx += 1;\n                     next_items.push(item);\n                 }"}, {"sha": "703ad0053a0ef34e3b62b1d4d46671a0fa01c01c", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 16, "deletions": 16, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -11,7 +11,7 @@ use crate::ext::tt::transcribe::transcribe;\n use crate::feature_gate::Features;\n use crate::parse::{Directory, ParseSess};\n use crate::parse::parser::Parser;\n-use crate::parse::token::{self, NtTT};\n+use crate::parse::token::{self, Token, NtTT};\n use crate::parse::token::TokenKind::*;\n use crate::symbol::{Symbol, kw, sym};\n use crate::tokenstream::{DelimSpan, TokenStream, TokenTree};\n@@ -270,7 +270,7 @@ pub fn compile(\n         quoted::TokenTree::Sequence(DelimSpan::dummy(), Lrc::new(quoted::SequenceRepetition {\n             tts: vec![\n                 quoted::TokenTree::MetaVarDecl(DUMMY_SP, lhs_nm, ast::Ident::from_str(\"tt\")),\n-                quoted::TokenTree::Token(DUMMY_SP, token::FatArrow),\n+                quoted::TokenTree::token(DUMMY_SP, token::FatArrow),\n                 quoted::TokenTree::MetaVarDecl(DUMMY_SP, rhs_nm, ast::Ident::from_str(\"tt\")),\n             ],\n             separator: Some(if body.legacy { token::Semi } else { token::Comma }),\n@@ -279,7 +279,7 @@ pub fn compile(\n         })),\n         // to phase into semicolon-termination instead of semicolon-separation\n         quoted::TokenTree::Sequence(DelimSpan::dummy(), Lrc::new(quoted::SequenceRepetition {\n-            tts: vec![quoted::TokenTree::Token(DUMMY_SP, token::Semi)],\n+            tts: vec![quoted::TokenTree::token(DUMMY_SP, token::Semi)],\n             separator: None,\n             op: quoted::KleeneOp::ZeroOrMore,\n             num_captures: 0\n@@ -613,7 +613,7 @@ impl FirstSets {\n \n                         if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n                                                         subfirst.maybe_empty) {\n-                            first.add_one_maybe(TokenTree::Token(sp.entire(), sep.clone()));\n+                            first.add_one_maybe(TokenTree::token(sp.entire(), sep.clone()));\n                         }\n \n                         // Reverse scan: Sequence comes before `first`.\n@@ -663,7 +663,7 @@ impl FirstSets {\n \n                             if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n                                                             subfirst.maybe_empty) {\n-                                first.add_one_maybe(TokenTree::Token(sp.entire(), sep.clone()));\n+                                first.add_one_maybe(TokenTree::token(sp.entire(), sep.clone()));\n                             }\n \n                             assert!(first.maybe_empty);\n@@ -869,7 +869,7 @@ fn check_matcher_core(sess: &ParseSess,\n                 let mut new;\n                 let my_suffix = if let Some(ref u) = seq_rep.separator {\n                     new = suffix_first.clone();\n-                    new.add_one_maybe(TokenTree::Token(sp.entire(), u.clone()));\n+                    new.add_one_maybe(TokenTree::token(sp.entire(), u.clone()));\n                     &new\n                 } else {\n                     &suffix_first\n@@ -1015,7 +1015,7 @@ enum IsInFollow {\n fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n     use quoted::TokenTree;\n \n-    if let TokenTree::Token(_, token::CloseDelim(_)) = *tok {\n+    if let TokenTree::Token(Token { kind: token::CloseDelim(_), .. }) = *tok {\n         // closing a token tree can never be matched by any fragment;\n         // iow, we always require that `(` and `)` match, etc.\n         IsInFollow::Yes\n@@ -1033,8 +1033,8 @@ fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n             },\n             \"stmt\" | \"expr\"  => {\n                 let tokens = vec![\"`=>`\", \"`,`\", \"`;`\"];\n-                match *tok {\n-                    TokenTree::Token(_, ref tok) => match *tok {\n+                match tok {\n+                    TokenTree::Token(token) => match token.kind {\n                         FatArrow | Comma | Semi => IsInFollow::Yes,\n                         _ => IsInFollow::No(tokens),\n                     },\n@@ -1043,8 +1043,8 @@ fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n             },\n             \"pat\" => {\n                 let tokens = vec![\"`=>`\", \"`,`\", \"`=`\", \"`|`\", \"`if`\", \"`in`\"];\n-                match *tok {\n-                    TokenTree::Token(_, ref tok) => match *tok {\n+                match tok {\n+                    TokenTree::Token(token) => match token.kind {\n                         FatArrow | Comma | Eq | BinOp(token::Or) => IsInFollow::Yes,\n                         Ident(i, false) if i.name == kw::If ||\n                                            i.name == kw::In => IsInFollow::Yes,\n@@ -1058,8 +1058,8 @@ fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n                     \"`{`\", \"`[`\", \"`=>`\", \"`,`\", \"`>`\",\"`=`\", \"`:`\", \"`;`\", \"`|`\", \"`as`\",\n                     \"`where`\",\n                 ];\n-                match *tok {\n-                    TokenTree::Token(_, ref tok) => match *tok {\n+                match tok {\n+                    TokenTree::Token(token) => match token.kind {\n                         OpenDelim(token::DelimToken::Brace) |\n                         OpenDelim(token::DelimToken::Bracket) |\n                         Comma | FatArrow | Colon | Eq | Gt | BinOp(token::Shr) | Semi |\n@@ -1089,8 +1089,8 @@ fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n             \"vis\" => {\n                 // Explicitly disallow `priv`, on the off chance it comes back.\n                 let tokens = vec![\"`,`\", \"an ident\", \"a type\"];\n-                match *tok {\n-                    TokenTree::Token(_, ref tok) => match *tok {\n+                match tok {\n+                    TokenTree::Token(token) => match token.kind {\n                         Comma => IsInFollow::Yes,\n                         Ident(i, is_raw) if is_raw || i.name != kw::Priv =>\n                             IsInFollow::Yes,\n@@ -1150,7 +1150,7 @@ fn is_legal_fragment_specifier(_sess: &ParseSess,\n \n fn quoted_tt_to_string(tt: &quoted::TokenTree) -> String {\n     match *tt {\n-        quoted::TokenTree::Token(_, ref tok) => crate::print::pprust::token_to_string(tok),\n+        quoted::TokenTree::Token(ref token) => crate::print::pprust::token_to_string(&token),\n         quoted::TokenTree::MetaVar(_, name) => format!(\"${}\", name),\n         quoted::TokenTree::MetaVarDecl(_, name, kind) => format!(\"${}:{}\", name, kind),\n         _ => panic!(\"unexpected quoted::TokenTree::{{Sequence or Delimited}} \\"}, {"sha": "9f4e35ad3d7795f6899ba97c568c050a04d8f6ad", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "modified", "additions": 41, "deletions": 39, "changes": 80, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -2,7 +2,8 @@ use crate::ast::NodeId;\n use crate::early_buffered_lints::BufferedEarlyLintId;\n use crate::ext::tt::macro_parser;\n use crate::feature_gate::Features;\n-use crate::parse::{token, ParseSess};\n+use crate::parse::token::{self, Token, TokenKind};\n+use crate::parse::ParseSess;\n use crate::print::pprust;\n use crate::tokenstream::{self, DelimSpan};\n use crate::ast;\n@@ -39,7 +40,7 @@ impl Delimited {\n         } else {\n             span.with_lo(span.lo() + BytePos(self.delim.len() as u32))\n         };\n-        TokenTree::Token(open_span, self.open_token())\n+        TokenTree::token(open_span, self.open_token())\n     }\n \n     /// Returns a `self::TokenTree` with a `Span` corresponding to the closing delimiter.\n@@ -49,7 +50,7 @@ impl Delimited {\n         } else {\n             span.with_lo(span.hi() - BytePos(self.delim.len() as u32))\n         };\n-        TokenTree::Token(close_span, self.close_token())\n+        TokenTree::token(close_span, self.close_token())\n     }\n }\n \n@@ -81,7 +82,7 @@ pub enum KleeneOp {\n /// are \"first-class\" token trees. Useful for parsing macros.\n #[derive(Debug, Clone, PartialEq, RustcEncodable, RustcDecodable)]\n pub enum TokenTree {\n-    Token(Span, token::TokenKind),\n+    Token(Token),\n     Delimited(DelimSpan, Lrc<Delimited>),\n     /// A kleene-style repetition sequence\n     Sequence(DelimSpan, Lrc<SequenceRepetition>),\n@@ -144,13 +145,17 @@ impl TokenTree {\n     /// Retrieves the `TokenTree`'s span.\n     pub fn span(&self) -> Span {\n         match *self {\n-            TokenTree::Token(sp, _)\n-            | TokenTree::MetaVar(sp, _)\n-            | TokenTree::MetaVarDecl(sp, _, _) => sp,\n-            TokenTree::Delimited(sp, _)\n-            | TokenTree::Sequence(sp, _) => sp.entire(),\n+            TokenTree::Token(Token { span, .. })\n+            | TokenTree::MetaVar(span, _)\n+            | TokenTree::MetaVarDecl(span, _, _) => span,\n+            TokenTree::Delimited(span, _)\n+            | TokenTree::Sequence(span, _) => span.entire(),\n         }\n     }\n+\n+    crate fn token(span: Span, kind: TokenKind) -> TokenTree {\n+        TokenTree::Token(Token { kind, span })\n+    }\n }\n \n /// Takes a `tokenstream::TokenStream` and returns a `Vec<self::TokenTree>`. Specifically, this\n@@ -205,14 +210,14 @@ pub fn parse(\n         match tree {\n             TokenTree::MetaVar(start_sp, ident) if expect_matchers => {\n                 let span = match trees.next() {\n-                    Some(tokenstream::TokenTree::Token(span, token::Colon)) => match trees.next() {\n-                        Some(tokenstream::TokenTree::Token(end_sp, ref tok)) => match tok.ident() {\n+                    Some(tokenstream::TokenTree::Token(Token { kind: token::Colon, span })) => match trees.next() {\n+                        Some(tokenstream::TokenTree::Token(token)) => match token.ident() {\n                             Some((kind, _)) => {\n-                                let span = end_sp.with_lo(start_sp.lo());\n+                                let span = token.span.with_lo(start_sp.lo());\n                                 result.push(TokenTree::MetaVarDecl(span, ident, kind));\n                                 continue;\n                             }\n-                            _ => end_sp,\n+                            _ => token.span,\n                         },\n                         tree => tree\n                             .as_ref()\n@@ -270,7 +275,7 @@ where\n     // Depending on what `tree` is, we could be parsing different parts of a macro\n     match tree {\n         // `tree` is a `$` token. Look at the next token in `trees`\n-        tokenstream::TokenTree::Token(span, token::Dollar) => match trees.next() {\n+        tokenstream::TokenTree::Token(Token { kind: token::Dollar, span }) => match trees.next() {\n             // `tree` is followed by a delimited set of token trees. This indicates the beginning\n             // of a repetition sequence in the macro (e.g. `$(pat)*`).\n             Some(tokenstream::TokenTree::Delimited(span, delim, tts)) => {\n@@ -316,33 +321,33 @@ where\n \n             // `tree` is followed by an `ident`. This could be `$meta_var` or the `$crate` special\n             // metavariable that names the crate of the invocation.\n-            Some(tokenstream::TokenTree::Token(ident_span, ref token)) if token.is_ident() => {\n+            Some(tokenstream::TokenTree::Token(token)) if token.is_ident() => {\n                 let (ident, is_raw) = token.ident().unwrap();\n-                let span = ident_span.with_lo(span.lo());\n+                let span = token.span.with_lo(span.lo());\n                 if ident.name == kw::Crate && !is_raw {\n                     let ident = ast::Ident::new(kw::DollarCrate, ident.span);\n-                    TokenTree::Token(span, token::Ident(ident, is_raw))\n+                    TokenTree::token(span, token::Ident(ident, is_raw))\n                 } else {\n                     TokenTree::MetaVar(span, ident)\n                 }\n             }\n \n             // `tree` is followed by a random token. This is an error.\n-            Some(tokenstream::TokenTree::Token(span, tok)) => {\n+            Some(tokenstream::TokenTree::Token(token)) => {\n                 let msg = format!(\n                     \"expected identifier, found `{}`\",\n-                    pprust::token_to_string(&tok)\n+                    pprust::token_to_string(&token),\n                 );\n-                sess.span_diagnostic.span_err(span, &msg);\n-                TokenTree::MetaVar(span, ast::Ident::invalid())\n+                sess.span_diagnostic.span_err(token.span, &msg);\n+                TokenTree::MetaVar(token.span, ast::Ident::invalid())\n             }\n \n             // There are no more tokens. Just return the `$` we already have.\n-            None => TokenTree::Token(span, token::Dollar),\n+            None => TokenTree::token(span, token::Dollar),\n         },\n \n         // `tree` is an arbitrary token. Keep it.\n-        tokenstream::TokenTree::Token(span, tok) => TokenTree::Token(span, tok),\n+        tokenstream::TokenTree::Token(token) => TokenTree::Token(token),\n \n         // `tree` is the beginning of a delimited set of tokens (e.g., `(` or `{`). We need to\n         // descend into the delimited set and further parse it.\n@@ -380,17 +385,14 @@ fn kleene_op(token: &token::TokenKind) -> Option<KleeneOp> {\n /// - Ok(Ok((op, span))) if the next token tree is a KleeneOp\n /// - Ok(Err(tok, span)) if the next token tree is a token but not a KleeneOp\n /// - Err(span) if the next token tree is not a token\n-fn parse_kleene_op<I>(\n-    input: &mut I,\n-    span: Span,\n-) -> Result<Result<(KleeneOp, Span), (token::TokenKind, Span)>, Span>\n+fn parse_kleene_op<I>(input: &mut I, span: Span) -> Result<Result<(KleeneOp, Span), Token>, Span>\n where\n     I: Iterator<Item = tokenstream::TokenTree>,\n {\n     match input.next() {\n-        Some(tokenstream::TokenTree::Token(span, tok)) => match kleene_op(&tok) {\n-            Some(op) => Ok(Ok((op, span))),\n-            None => Ok(Err((tok, span))),\n+        Some(tokenstream::TokenTree::Token(token)) => match kleene_op(&token) {\n+            Some(op) => Ok(Ok((op, token.span))),\n+            None => Ok(Err(token)),\n         },\n         tree => Err(tree\n             .as_ref()\n@@ -466,7 +468,7 @@ where\n             assert_eq!(op, KleeneOp::ZeroOrOne);\n \n             // Lookahead at #2. If it is a KleenOp, then #1 is a separator.\n-            let is_1_sep = if let Some(&tokenstream::TokenTree::Token(_, ref tok2)) = input.peek() {\n+            let is_1_sep = if let Some(tokenstream::TokenTree::Token(tok2)) = input.peek() {\n                 kleene_op(tok2).is_some()\n             } else {\n                 false\n@@ -504,7 +506,7 @@ where\n                     }\n \n                     // #2 is a random token (this is an error) :(\n-                    Ok(Err((_, _))) => op1_span,\n+                    Ok(Err(_)) => op1_span,\n \n                     // #2 is not even a token at all :(\n                     Err(_) => op1_span,\n@@ -524,7 +526,7 @@ where\n         }\n \n         // #1 is a separator followed by #2, a KleeneOp\n-        Ok(Err((tok, span))) => match parse_kleene_op(input, span) {\n+        Ok(Err(token)) => match parse_kleene_op(input, token.span) {\n             // #2 is a `?`, which is not allowed as a Kleene op in 2015 edition,\n             // but is allowed in the 2018 edition\n             Ok(Ok((op, op2_span))) if op == KleeneOp::ZeroOrOne => {\n@@ -539,10 +541,10 @@ where\n             }\n \n             // #2 is a KleeneOp :D\n-            Ok(Ok((op, _))) => return (Some(tok), op),\n+            Ok(Ok((op, _))) => return (Some(token.kind), op),\n \n             // #2 is a random token :(\n-            Ok(Err((_, span))) => span,\n+            Ok(Err(token)) => token.span,\n \n             // #2 is not a token at all :(\n             Err(span) => span,\n@@ -580,12 +582,12 @@ where\n         Ok(Ok((op, _))) => return (None, op),\n \n         // #1 is a separator followed by #2, a KleeneOp\n-        Ok(Err((tok, span))) => match parse_kleene_op(input, span) {\n+        Ok(Err(token)) => match parse_kleene_op(input, token.span) {\n             // #2 is the `?` Kleene op, which does not take a separator (error)\n             Ok(Ok((op, _op2_span))) if op == KleeneOp::ZeroOrOne => {\n                 // Error!\n                 sess.span_diagnostic.span_err(\n-                    span,\n+                    token.span,\n                     \"the `?` macro repetition operator does not take a separator\",\n                 );\n \n@@ -594,10 +596,10 @@ where\n             }\n \n             // #2 is a KleeneOp :D\n-            Ok(Ok((op, _))) => return (Some(tok), op),\n+            Ok(Ok((op, _))) => return (Some(token.kind), op),\n \n             // #2 is a random token :(\n-            Ok(Err((_, span))) => span,\n+            Ok(Err(token)) => token.span,\n \n             // #2 is not a token at all :(\n             Err(span) => span,"}, {"sha": "1dbb0638df195968f7045218697c8a38d35abc41", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -119,7 +119,7 @@ pub fn transcribe(\n                             Some((tt, _)) => tt.span(),\n                             None => DUMMY_SP,\n                         };\n-                        result.push(TokenTree::Token(prev_span, sep).into());\n+                        result.push(TokenTree::token(prev_span, sep).into());\n                     }\n                     continue;\n                 }\n@@ -225,7 +225,7 @@ pub fn transcribe(\n                             result.push(tt.clone().into());\n                         } else {\n                             sp = sp.apply_mark(cx.current_expansion.mark);\n-                            let token = TokenTree::Token(sp, token::Interpolated(nt.clone()));\n+                            let token = TokenTree::token(sp, token::Interpolated(nt.clone()));\n                             result.push(token.into());\n                         }\n                     } else {\n@@ -241,8 +241,8 @@ pub fn transcribe(\n                     let ident =\n                         Ident::new(ident.name, ident.span.apply_mark(cx.current_expansion.mark));\n                     sp = sp.apply_mark(cx.current_expansion.mark);\n-                    result.push(TokenTree::Token(sp, token::Dollar).into());\n-                    result.push(TokenTree::Token(sp, token::TokenKind::from_ast_ident(ident)).into());\n+                    result.push(TokenTree::token(sp, token::Dollar).into());\n+                    result.push(TokenTree::token(sp, token::TokenKind::from_ast_ident(ident)).into());\n                 }\n             }\n \n@@ -259,9 +259,9 @@ pub fn transcribe(\n \n             // Nothing much to do here. Just push the token to the result, being careful to\n             // preserve syntax context.\n-            quoted::TokenTree::Token(sp, tok) => {\n+            quoted::TokenTree::Token(token) => {\n                 let mut marker = Marker(cx.current_expansion.mark);\n-                let mut tt = TokenTree::Token(sp, tok);\n+                let mut tt = TokenTree::Token(token);\n                 noop_visit_tt(&mut tt, &mut marker);\n                 result.push(tt.into());\n             }"}, {"sha": "64415204047ba275fd3ba3e7ba2be5f1cea06360", "filename": "src/libsyntax/feature_gate.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Ffeature_gate.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Ffeature_gate.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffeature_gate.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -1958,9 +1958,11 @@ impl<'a> Visitor<'a> for PostExpansionVisitor<'a> {\n                 name,\n                 template\n             ),\n-            None => if let Some(TokenTree::Token(_, token::Eq)) = attr.tokens.trees().next() {\n-                // All key-value attributes are restricted to meta-item syntax.\n-                attr.parse_meta(self.context.parse_sess).map_err(|mut err| err.emit()).ok();\n+            None => if let Some(TokenTree::Token(token)) = attr.tokens.trees().next() {\n+                if token == token::Eq {\n+                    // All key-value attributes are restricted to meta-item syntax.\n+                    attr.parse_meta(self.context.parse_sess).map_err(|mut err| err.emit()).ok();\n+                }\n             }\n         }\n     }"}, {"sha": "6882586ed2cd2992df42ac33b0ea1dda8f063bff", "filename": "src/libsyntax/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Flib.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -10,6 +10,7 @@\n #![deny(rust_2018_idioms)]\n #![deny(internal)]\n \n+#![feature(bind_by_move_pattern_guards)]\n #![feature(crate_visibility_modifier)]\n #![feature(label_break_value)]\n #![feature(nll)]"}, {"sha": "ad6d3f71c652ea8df05aa5640d0935a3eccf0c5e", "filename": "src/libsyntax/mut_visit.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fmut_visit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fmut_visit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fmut_visit.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -9,7 +9,7 @@\n \n use crate::ast::*;\n use crate::source_map::{Spanned, respan};\n-use crate::parse::token::{self, TokenKind};\n+use crate::parse::token::{self, Token, TokenKind};\n use crate::ptr::P;\n use crate::ThinVec;\n use crate::tokenstream::*;\n@@ -576,9 +576,9 @@ pub fn noop_visit_arg<T: MutVisitor>(Arg { id, pat, ty }: &mut Arg, vis: &mut T)\n \n pub fn noop_visit_tt<T: MutVisitor>(tt: &mut TokenTree, vis: &mut T) {\n     match tt {\n-        TokenTree::Token(span, tok) => {\n+        TokenTree::Token(Token { kind, span }) => {\n+            vis.visit_token(kind);\n             vis.visit_span(span);\n-            vis.visit_token(tok);\n         }\n         TokenTree::Delimited(DelimSpan { open, close }, _delim, tts) => {\n             vis.visit_span(open);"}, {"sha": "9b78b56041f21b0c2eda478bd91e703a39651646", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -157,7 +157,7 @@ impl<'a> Parser<'a> {\n                self.check(&token::OpenDelim(DelimToken::Brace)) {\n                    self.parse_token_tree().into()\n             } else if self.eat(&token::Eq) {\n-                let eq = TokenTree::Token(self.prev_span, token::Eq);\n+                let eq = TokenTree::token(self.prev_span, token::Eq);\n                 let mut is_interpolated_expr = false;\n                 if let token::Interpolated(nt) = &self.token {\n                     if let token::NtExpr(..) = **nt {"}, {"sha": "225db0164fe657b96dbcad189cafaf3bf8b5e61f", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 18, "deletions": 22, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -1596,16 +1596,16 @@ mod tests {\n                                         \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\"\n                                             .to_string());\n             let id = Ident::from_str(\"fn\");\n-            assert_eq!(string_reader.next_token().kind, token::Comment);\n-            assert_eq!(string_reader.next_token().kind, token::Whitespace);\n+            assert_eq!(string_reader.next_token(), token::Comment);\n+            assert_eq!(string_reader.next_token(), token::Whitespace);\n             let tok1 = string_reader.next_token();\n             let tok2 = Token {\n                 kind: token::Ident(id, false),\n                 span: Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n             };\n             assert_eq!(tok1.kind, tok2.kind);\n             assert_eq!(tok1.span, tok2.span);\n-            assert_eq!(string_reader.next_token().kind, token::Whitespace);\n+            assert_eq!(string_reader.next_token(), token::Whitespace);\n             // the 'main' id is already read:\n             assert_eq!(string_reader.pos.clone(), BytePos(28));\n             // read another token:\n@@ -1625,7 +1625,7 @@ mod tests {\n     // of tokens (stop checking after exhausting the expected vec)\n     fn check_tokenization(mut string_reader: StringReader<'_>, expected: Vec<TokenKind>) {\n         for expected_tok in &expected {\n-            assert_eq!(&string_reader.next_token().kind, expected_tok);\n+            assert_eq!(&string_reader.next_token(), expected_tok);\n         }\n     }\n \n@@ -1683,7 +1683,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token().kind,\n+            assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token(),\n                        mk_lit(token::Char, \"a\", None));\n         })\n     }\n@@ -1693,7 +1693,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token().kind,\n+            assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token(),\n                        mk_lit(token::Char, \" \", None));\n         })\n     }\n@@ -1703,7 +1703,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token().kind,\n+            assert_eq!(setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token(),\n                        mk_lit(token::Char, \"\\\\n\", None));\n         })\n     }\n@@ -1713,7 +1713,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'abc\".to_string()).next_token().kind,\n+            assert_eq!(setup(&sm, &sh, \"'abc\".to_string()).next_token(),\n                        token::Lifetime(Ident::from_str(\"'abc\")));\n         })\n     }\n@@ -1723,7 +1723,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token().kind,\n+            assert_eq!(setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token(),\n                        mk_lit(token::StrRaw(3), \"\\\"#a\\\\b\\x00c\\\"\", None));\n         })\n     }\n@@ -1735,10 +1735,10 @@ mod tests {\n             let sh = mk_sess(sm.clone());\n             macro_rules! test {\n                 ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n-                    assert_eq!(setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token().kind,\n+                    assert_eq!(setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token(),\n                                mk_lit(token::$tok_type, $tok_contents, Some(\"suffix\")));\n                     // with a whitespace separator:\n-                    assert_eq!(setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token().kind,\n+                    assert_eq!(setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token(),\n                                mk_lit(token::$tok_type, $tok_contents, None));\n                 }}\n             }\n@@ -1753,11 +1753,11 @@ mod tests {\n             test!(\"1.0\", Float, \"1.0\");\n             test!(\"1.0e10\", Float, \"1.0e10\");\n \n-            assert_eq!(setup(&sm, &sh, \"2us\".to_string()).next_token().kind,\n+            assert_eq!(setup(&sm, &sh, \"2us\".to_string()).next_token(),\n                        mk_lit(token::Integer, \"2\", Some(\"us\")));\n-            assert_eq!(setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token().kind,\n+            assert_eq!(setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token(),\n                        mk_lit(token::StrRaw(3), \"raw\", Some(\"suffix\")));\n-            assert_eq!(setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token().kind,\n+            assert_eq!(setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token(),\n                        mk_lit(token::ByteStrRaw(3), \"raw\", Some(\"suffix\")));\n         })\n     }\n@@ -1775,11 +1775,8 @@ mod tests {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n             let mut lexer = setup(&sm, &sh, \"/* /* */ */'a'\".to_string());\n-            match lexer.next_token().kind {\n-                token::Comment => {}\n-                _ => panic!(\"expected a comment!\"),\n-            }\n-            assert_eq!(lexer.next_token().kind, mk_lit(token::Char, \"a\", None));\n+            assert_eq!(lexer.next_token(), token::Comment);\n+            assert_eq!(lexer.next_token(), mk_lit(token::Char, \"a\", None));\n         })\n     }\n \n@@ -1792,9 +1789,8 @@ mod tests {\n             let comment = lexer.next_token();\n             assert_eq!(comment.kind, token::Comment);\n             assert_eq!((comment.span.lo(), comment.span.hi()), (BytePos(0), BytePos(7)));\n-            assert_eq!(lexer.next_token().kind, token::Whitespace);\n-            assert_eq!(lexer.next_token().kind,\n-                    token::DocComment(Symbol::intern(\"/// test\")));\n+            assert_eq!(lexer.next_token(), token::Whitespace);\n+            assert_eq!(lexer.next_token(), token::DocComment(Symbol::intern(\"/// test\")));\n         })\n     }\n }"}, {"sha": "abff7177abd13dd3627b593d79ea6a1d5f0dafcf", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -203,7 +203,7 @@ impl<'a> TokenTreesReader<'a> {\n                 Err(err)\n             },\n             _ => {\n-                let tt = TokenTree::Token(self.span, self.token.clone());\n+                let tt = TokenTree::token(self.span, self.token.clone());\n                 // Note that testing for joint-ness here is done via the raw\n                 // source span as the joint-ness is a property of the raw source\n                 // rather than wanting to take `override_span` into account."}, {"sha": "4b8ef20180f63583b4f03e11a2d76605d81333fa", "filename": "src/libsyntax/parse/literal.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Fliteral.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Fliteral.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fliteral.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -261,7 +261,7 @@ impl Lit {\n             token::Bool => token::Ident(Ident::new(self.token.symbol, self.span), false),\n             _ => token::Literal(self.token),\n         };\n-        TokenTree::Token(self.span, token).into()\n+        TokenTree::token(self.span, token).into()\n     }\n }\n "}, {"sha": "398b4b1da17b0ef74ab0c666b9d63fc311f09c0a", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 16, "deletions": 15, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -385,6 +385,7 @@ mod tests {\n     use crate::ast::{self, Ident, PatKind};\n     use crate::attr::first_attr_value_str_by_name;\n     use crate::ptr::P;\n+    use crate::parse::token::Token;\n     use crate::print::pprust::item_to_string;\n     use crate::tokenstream::{DelimSpan, TokenTree};\n     use crate::util::parser_testing::string_to_stream;\n@@ -426,9 +427,9 @@ mod tests {\n             match (tts.len(), tts.get(0), tts.get(1), tts.get(2), tts.get(3)) {\n                 (\n                     4,\n-                    Some(&TokenTree::Token(_, token::Ident(name_macro_rules, false))),\n-                    Some(&TokenTree::Token(_, token::Not)),\n-                    Some(&TokenTree::Token(_, token::Ident(name_zip, false))),\n+                    Some(&TokenTree::Token(Token { kind: token::Ident(name_macro_rules, false), .. })),\n+                    Some(&TokenTree::Token(Token { kind: token::Not, .. })),\n+                    Some(&TokenTree::Token(Token { kind: token::Ident(name_zip, false), .. })),\n                     Some(&TokenTree::Delimited(_, macro_delim, ref macro_tts)),\n                 )\n                 if name_macro_rules.name == sym::macro_rules\n@@ -438,16 +439,16 @@ mod tests {\n                         (\n                             3,\n                             Some(&TokenTree::Delimited(_, first_delim, ref first_tts)),\n-                            Some(&TokenTree::Token(_, token::FatArrow)),\n+                            Some(&TokenTree::Token(Token { kind: token::FatArrow, .. })),\n                             Some(&TokenTree::Delimited(_, second_delim, ref second_tts)),\n                         )\n                         if macro_delim == token::Paren => {\n                             let tts = &first_tts.trees().collect::<Vec<_>>();\n                             match (tts.len(), tts.get(0), tts.get(1)) {\n                                 (\n                                     2,\n-                                    Some(&TokenTree::Token(_, token::Dollar)),\n-                                    Some(&TokenTree::Token(_, token::Ident(ident, false))),\n+                                    Some(&TokenTree::Token(Token { kind: token::Dollar, .. })),\n+                                    Some(&TokenTree::Token(Token { kind: token::Ident(ident, false), .. })),\n                                 )\n                                 if first_delim == token::Paren && ident.name.as_str() == \"a\" => {},\n                                 _ => panic!(\"value 3: {:?} {:?}\", first_delim, first_tts),\n@@ -456,8 +457,8 @@ mod tests {\n                             match (tts.len(), tts.get(0), tts.get(1)) {\n                                 (\n                                     2,\n-                                    Some(&TokenTree::Token(_, token::Dollar)),\n-                                    Some(&TokenTree::Token(_, token::Ident(ident, false))),\n+                                    Some(&TokenTree::Token(Token { kind: token::Dollar, .. })),\n+                                    Some(&TokenTree::Token(Token { kind: token::Ident(ident, false), .. })),\n                                 )\n                                 if second_delim == token::Paren && ident.name.as_str() == \"a\" => {},\n                                 _ => panic!(\"value 4: {:?} {:?}\", second_delim, second_tts),\n@@ -477,26 +478,26 @@ mod tests {\n             let tts = string_to_stream(\"fn a (b : i32) { b; }\".to_string());\n \n             let expected = TokenStream::new(vec![\n-                TokenTree::Token(sp(0, 2), token::Ident(Ident::from_str(\"fn\"), false)).into(),\n-                TokenTree::Token(sp(3, 4), token::Ident(Ident::from_str(\"a\"), false)).into(),\n+                TokenTree::token(sp(0, 2), token::Ident(Ident::from_str(\"fn\"), false)).into(),\n+                TokenTree::token(sp(3, 4), token::Ident(Ident::from_str(\"a\"), false)).into(),\n                 TokenTree::Delimited(\n                     DelimSpan::from_pair(sp(5, 6), sp(13, 14)),\n                     token::DelimToken::Paren,\n                     TokenStream::new(vec![\n-                        TokenTree::Token(sp(6, 7),\n+                        TokenTree::token(sp(6, 7),\n                                          token::Ident(Ident::from_str(\"b\"), false)).into(),\n-                        TokenTree::Token(sp(8, 9), token::Colon).into(),\n-                        TokenTree::Token(sp(10, 13),\n+                        TokenTree::token(sp(8, 9), token::Colon).into(),\n+                        TokenTree::token(sp(10, 13),\n                                          token::Ident(Ident::from_str(\"i32\"), false)).into(),\n                     ]).into(),\n                 ).into(),\n                 TokenTree::Delimited(\n                     DelimSpan::from_pair(sp(15, 16), sp(20, 21)),\n                     token::DelimToken::Brace,\n                     TokenStream::new(vec![\n-                        TokenTree::Token(sp(17, 18),\n+                        TokenTree::token(sp(17, 18),\n                                          token::Ident(Ident::from_str(\"b\"), false)).into(),\n-                        TokenTree::Token(sp(18, 19), token::Semi).into(),\n+                        TokenTree::token(sp(18, 19), token::Semi).into(),\n                     ]).into(),\n                 ).into()\n             ]);"}, {"sha": "eda67b3a93d8e1e237a5d29045c0f45f22f82afd", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -318,7 +318,7 @@ impl TokenCursor {\n             }\n \n             match tree {\n-                TokenTree::Token(span, kind) => return Token { kind, span },\n+                TokenTree::Token(token) => return token,\n                 TokenTree::Delimited(sp, delim, tts) => {\n                     let frame = TokenCursorFrame::new(sp, delim, &tts);\n                     self.stack.push(mem::replace(&mut self.frame, frame));\n@@ -353,9 +353,9 @@ impl TokenCursor {\n             delim_span,\n             token::Bracket,\n             [\n-                TokenTree::Token(sp, token::Ident(ast::Ident::with_empty_ctxt(sym::doc), false)),\n-                TokenTree::Token(sp, token::Eq),\n-                TokenTree::Token(sp, token::TokenKind::lit(\n+                TokenTree::token(sp, token::Ident(ast::Ident::with_empty_ctxt(sym::doc), false)),\n+                TokenTree::token(sp, token::Eq),\n+                TokenTree::token(sp, token::TokenKind::lit(\n                     token::StrRaw(num_of_hashes), Symbol::intern(&stripped), None\n                 )),\n             ]\n@@ -366,10 +366,10 @@ impl TokenCursor {\n             delim_span,\n             token::NoDelim,\n             &if doc_comment_style(&name.as_str()) == AttrStyle::Inner {\n-                [TokenTree::Token(sp, token::Pound), TokenTree::Token(sp, token::Not), body]\n+                [TokenTree::token(sp, token::Pound), TokenTree::token(sp, token::Not), body]\n                     .iter().cloned().collect::<TokenStream>().into()\n             } else {\n-                [TokenTree::Token(sp, token::Pound), body]\n+                [TokenTree::token(sp, token::Pound), body]\n                     .iter().cloned().collect::<TokenStream>().into()\n             },\n         )));\n@@ -1052,7 +1052,7 @@ impl<'a> Parser<'a> {\n \n         f(&match self.token_cursor.frame.tree_cursor.look_ahead(dist - 1) {\n             Some(tree) => match tree {\n-                TokenTree::Token(_, tok) => tok,\n+                TokenTree::Token(token) => token.kind,\n                 TokenTree::Delimited(_, delim, _) => token::OpenDelim(delim),\n             },\n             None => token::CloseDelim(self.token_cursor.frame.delim),\n@@ -1065,7 +1065,7 @@ impl<'a> Parser<'a> {\n         }\n \n         match self.token_cursor.frame.tree_cursor.look_ahead(dist - 1) {\n-            Some(TokenTree::Token(span, _)) => span,\n+            Some(TokenTree::Token(token)) => token.span,\n             Some(TokenTree::Delimited(span, ..)) => span.entire(),\n             None => self.look_ahead_span(dist - 1),\n         }\n@@ -2675,7 +2675,7 @@ impl<'a> Parser<'a> {\n             _ => {\n                 let (token, span) = (mem::replace(&mut self.token, token::Whitespace), self.span);\n                 self.bump();\n-                TokenTree::Token(span, token)\n+                TokenTree::token(span, token)\n             }\n         }\n     }\n@@ -4344,7 +4344,7 @@ impl<'a> Parser<'a> {\n                     };\n                     TokenStream::new(vec![\n                         args.into(),\n-                        TokenTree::Token(token_lo.to(self.prev_span), token::FatArrow).into(),\n+                        TokenTree::token(token_lo.to(self.prev_span), token::FatArrow).into(),\n                         body.into(),\n                     ])\n                 } else {"}, {"sha": "a06bf9fae7c2958e2132e9533cca171d40913866", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 15, "deletions": 6, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -18,6 +18,7 @@ use log::info;\n \n use std::fmt;\n use std::mem;\n+use std::ops::Deref;\n #[cfg(target_arch = \"x86_64\")]\n use rustc_data_structures::static_assert_size;\n use rustc_data_structures::sync::Lrc;\n@@ -165,7 +166,7 @@ fn ident_can_begin_type(ident: ast::Ident, is_raw: bool) -> bool {\n     ].contains(&ident.name)\n }\n \n-#[derive(Clone, RustcEncodable, RustcDecodable, PartialEq, Debug)]\n+#[derive(Clone, PartialEq, RustcEncodable, RustcDecodable, Debug)]\n pub enum TokenKind {\n     /* Expression-operator symbols. */\n     Eq,\n@@ -235,7 +236,7 @@ pub enum TokenKind {\n #[cfg(target_arch = \"x86_64\")]\n static_assert_size!(TokenKind, 16);\n \n-#[derive(Clone, Debug)]\n+#[derive(Clone, PartialEq, RustcEncodable, RustcDecodable, Debug)]\n pub struct Token {\n     pub kind: TokenKind,\n     pub span: Span,\n@@ -614,6 +615,14 @@ impl PartialEq<TokenKind> for Token {\n     }\n }\n \n+// FIXME: Remove this after all necessary methods are moved from `TokenKind` to `Token`.\n+impl Deref for Token {\n+    type Target = TokenKind;\n+    fn deref(&self) -> &Self::Target {\n+        &self.kind\n+    }\n+}\n+\n #[derive(Clone, RustcEncodable, RustcDecodable)]\n /// For interpolation during macro expansion.\n pub enum Nonterminal {\n@@ -704,11 +713,11 @@ impl Nonterminal {\n             }\n             Nonterminal::NtIdent(ident, is_raw) => {\n                 let token = Ident(ident, is_raw);\n-                Some(TokenTree::Token(ident.span, token).into())\n+                Some(TokenTree::token(ident.span, token).into())\n             }\n             Nonterminal::NtLifetime(ident) => {\n                 let token = Lifetime(ident);\n-                Some(TokenTree::Token(ident.span, token).into())\n+                Some(TokenTree::token(ident.span, token).into())\n             }\n             Nonterminal::NtTT(ref tt) => {\n                 Some(tt.clone().into())\n@@ -794,7 +803,7 @@ fn prepend_attrs(sess: &ParseSess,\n         if attr.path.segments.len() == 1 && attr.path.segments[0].args.is_none() {\n             let ident = attr.path.segments[0].ident;\n             let token = Ident(ident, ident.as_str().starts_with(\"r#\"));\n-            brackets.push(tokenstream::TokenTree::Token(ident.span, token));\n+            brackets.push(tokenstream::TokenTree::token(ident.span, token));\n \n         // ... and for more complicated paths, fall back to a reparse hack that\n         // should eventually be removed.\n@@ -808,7 +817,7 @@ fn prepend_attrs(sess: &ParseSess,\n         // The span we list here for `#` and for `[ ... ]` are both wrong in\n         // that it encompasses more than each token, but it hopefully is \"good\n         // enough\" for now at least.\n-        builder.push(tokenstream::TokenTree::Token(attr.span, Pound));\n+        builder.push(tokenstream::TokenTree::token(attr.span, Pound));\n         let delim_span = DelimSpan::from_single(attr.span);\n         builder.push(tokenstream::TokenTree::Delimited(\n             delim_span, DelimToken::Bracket, brackets.build().into()));"}, {"sha": "07acfb5dc86c3f7feb152d8f98cacf4e83ff8024", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -724,10 +724,10 @@ pub trait PrintState<'a> {\n     /// expression arguments as expressions). It can be done! I think.\n     fn print_tt(&mut self, tt: tokenstream::TokenTree) -> io::Result<()> {\n         match tt {\n-            TokenTree::Token(_, ref tk) => {\n-                self.writer().word(token_to_string(tk))?;\n-                match *tk {\n-                    parse::token::DocComment(..) => {\n+            TokenTree::Token(ref token) => {\n+                self.writer().word(token_to_string(&token))?;\n+                match token.kind {\n+                    token::DocComment(..) => {\n                         self.writer().hardbreak()\n                     }\n                     _ => Ok(())"}, {"sha": "e6fe33d6ccf26afa9d9fb25c51564b93ff81ea33", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 54, "deletions": 57, "changes": 111, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -16,7 +16,7 @@\n use crate::ext::base;\n use crate::ext::tt::{macro_parser, quoted};\n use crate::parse::Directory;\n-use crate::parse::token::{self, DelimToken, TokenKind};\n+use crate::parse::token::{self, DelimToken, Token, TokenKind};\n use crate::print::pprust;\n \n use syntax_pos::{BytePos, Mark, Span, DUMMY_SP};\n@@ -44,7 +44,7 @@ use std::{fmt, iter, mem};\n #[derive(Debug, Clone, PartialEq, RustcEncodable, RustcDecodable)]\n pub enum TokenTree {\n     /// A single token\n-    Token(Span, token::TokenKind),\n+    Token(Token),\n     /// A delimited sequence of token trees\n     Delimited(DelimSpan, DelimToken, TokenStream),\n }\n@@ -53,8 +53,7 @@ pub enum TokenTree {\n #[cfg(parallel_compiler)]\n fn _dummy()\n where\n-    Span: Send + Sync,\n-    token::TokenKind: Send + Sync,\n+    Token: Send + Sync,\n     DelimSpan: Send + Sync,\n     DelimToken: Send + Sync,\n     TokenStream: Send + Sync,\n@@ -86,12 +85,11 @@ impl TokenTree {\n     /// Checks if this TokenTree is equal to the other, regardless of span information.\n     pub fn eq_unspanned(&self, other: &TokenTree) -> bool {\n         match (self, other) {\n-            (&TokenTree::Token(_, ref tk), &TokenTree::Token(_, ref tk2)) => tk == tk2,\n-            (&TokenTree::Delimited(_, delim, ref tts),\n-             &TokenTree::Delimited(_, delim2, ref tts2)) => {\n+            (TokenTree::Token(token), TokenTree::Token(token2)) => token.kind == token2.kind,\n+            (TokenTree::Delimited(_, delim, tts), TokenTree::Delimited(_, delim2, tts2)) => {\n                 delim == delim2 && tts.eq_unspanned(&tts2)\n             }\n-            (_, _) => false,\n+            _ => false,\n         }\n     }\n \n@@ -102,37 +100,36 @@ impl TokenTree {\n     // different method.\n     pub fn probably_equal_for_proc_macro(&self, other: &TokenTree) -> bool {\n         match (self, other) {\n-            (&TokenTree::Token(_, ref tk), &TokenTree::Token(_, ref tk2)) => {\n-                tk.probably_equal_for_proc_macro(tk2)\n+            (TokenTree::Token(token), TokenTree::Token(token2)) => {\n+                token.probably_equal_for_proc_macro(token2)\n             }\n-            (&TokenTree::Delimited(_, delim, ref tts),\n-             &TokenTree::Delimited(_, delim2, ref tts2)) => {\n+            (TokenTree::Delimited(_, delim, tts), TokenTree::Delimited(_, delim2, tts2)) => {\n                 delim == delim2 && tts.probably_equal_for_proc_macro(&tts2)\n             }\n-            (_, _) => false,\n+            _ => false,\n         }\n     }\n \n     /// Retrieves the TokenTree's span.\n     pub fn span(&self) -> Span {\n-        match *self {\n-            TokenTree::Token(sp, _) => sp,\n+        match self {\n+            TokenTree::Token(token) => token.span,\n             TokenTree::Delimited(sp, ..) => sp.entire(),\n         }\n     }\n \n     /// Modify the `TokenTree`'s span in-place.\n     pub fn set_span(&mut self, span: Span) {\n-        match *self {\n-            TokenTree::Token(ref mut sp, _) => *sp = span,\n-            TokenTree::Delimited(ref mut sp, ..) => *sp = DelimSpan::from_single(span),\n+        match self {\n+            TokenTree::Token(token) => token.span = span,\n+            TokenTree::Delimited(dspan, ..) => *dspan = DelimSpan::from_single(span),\n         }\n     }\n \n     /// Indicates if the stream is a token that is equal to the provided token.\n     pub fn eq_token(&self, t: TokenKind) -> bool {\n-        match *self {\n-            TokenTree::Token(_, ref tk) => *tk == t,\n+        match self {\n+            TokenTree::Token(token) => *token == t,\n             _ => false,\n         }\n     }\n@@ -141,14 +138,18 @@ impl TokenTree {\n         TokenStream::new(vec![(self, Joint)])\n     }\n \n+    pub fn token(span: Span, kind: TokenKind) -> TokenTree {\n+        TokenTree::Token(Token { kind, span })\n+    }\n+\n     /// Returns the opening delimiter as a token tree.\n     pub fn open_tt(span: Span, delim: DelimToken) -> TokenTree {\n         let open_span = if span.is_dummy() {\n             span\n         } else {\n             span.with_hi(span.lo() + BytePos(delim.len() as u32))\n         };\n-        TokenTree::Token(open_span, token::OpenDelim(delim))\n+        TokenTree::token(open_span, token::OpenDelim(delim))\n     }\n \n     /// Returns the closing delimiter as a token tree.\n@@ -158,7 +159,7 @@ impl TokenTree {\n         } else {\n             span.with_lo(span.hi() - BytePos(delim.len() as u32))\n         };\n-        TokenTree::Token(close_span, token::CloseDelim(delim))\n+        TokenTree::token(close_span, token::CloseDelim(delim))\n     }\n }\n \n@@ -201,18 +202,17 @@ impl TokenStream {\n             while let Some((pos, ts)) = iter.next() {\n                 if let Some((_, next)) = iter.peek() {\n                     let sp = match (&ts, &next) {\n-                        (_, (TokenTree::Token(_, token::Comma), _)) => continue,\n-                        ((TokenTree::Token(sp, token_left), NonJoint),\n-                         (TokenTree::Token(_, token_right), _))\n+                        (_, (TokenTree::Token(Token { kind: token::Comma, .. }), _)) => continue,\n+                        ((TokenTree::Token(token_left), NonJoint), (TokenTree::Token(token_right), _))\n                         if ((token_left.is_ident() && !token_left.is_reserved_ident())\n                             || token_left.is_lit()) &&\n                             ((token_right.is_ident() && !token_right.is_reserved_ident())\n-                            || token_right.is_lit()) => *sp,\n+                            || token_right.is_lit()) => token_left.span,\n                         ((TokenTree::Delimited(sp, ..), NonJoint), _) => sp.entire(),\n                         _ => continue,\n                     };\n                     let sp = sp.shrink_to_hi();\n-                    let comma = (TokenTree::Token(sp, token::Comma), NonJoint);\n+                    let comma = (TokenTree::token(sp, token::Comma), NonJoint);\n                     suggestion = Some((pos, comma, sp));\n                 }\n             }\n@@ -241,12 +241,6 @@ impl From<TokenTree> for TreeAndJoint {\n     }\n }\n \n-impl From<TokenKind> for TokenStream {\n-    fn from(token: TokenKind) -> TokenStream {\n-        TokenTree::Token(DUMMY_SP, token).into()\n-    }\n-}\n-\n impl<T: Into<TokenStream>> iter::FromIterator<T> for TokenStream {\n     fn from_iter<I: IntoIterator<Item = T>>(iter: I) -> Self {\n         TokenStream::from_streams(iter.into_iter().map(Into::into).collect::<SmallVec<_>>())\n@@ -349,22 +343,25 @@ impl TokenStream {\n         // streams, making a comparison between a token stream generated from an\n         // AST and a token stream which was parsed into an AST more reliable.\n         fn semantic_tree(tree: &TokenTree) -> bool {\n-            match tree {\n-                // The pretty printer tends to add trailing commas to\n-                // everything, and in particular, after struct fields.\n-                | TokenTree::Token(_, token::Comma)\n-                // The pretty printer emits `NoDelim` as whitespace.\n-                | TokenTree::Token(_, token::OpenDelim(DelimToken::NoDelim))\n-                | TokenTree::Token(_, token::CloseDelim(DelimToken::NoDelim))\n-                // The pretty printer collapses many semicolons into one.\n-                | TokenTree::Token(_, token::Semi)\n-                // The pretty printer collapses whitespace arbitrarily and can\n-                // introduce whitespace from `NoDelim`.\n-                | TokenTree::Token(_, token::Whitespace)\n-                // The pretty printer can turn `$crate` into `::crate_name`\n-                | TokenTree::Token(_, token::ModSep) => false,\n-                _ => true\n+            if let TokenTree::Token(token) = tree {\n+                if let\n+                    // The pretty printer tends to add trailing commas to\n+                    // everything, and in particular, after struct fields.\n+                    | token::Comma\n+                    // The pretty printer emits `NoDelim` as whitespace.\n+                    | token::OpenDelim(DelimToken::NoDelim)\n+                    | token::CloseDelim(DelimToken::NoDelim)\n+                    // The pretty printer collapses many semicolons into one.\n+                    | token::Semi\n+                    // The pretty printer collapses whitespace arbitrarily and can\n+                    // introduce whitespace from `NoDelim`.\n+                    | token::Whitespace\n+                    // The pretty printer can turn `$crate` into `::crate_name`\n+                    | token::ModSep = token.kind {\n+                    return false;\n+                }\n             }\n+            true\n         }\n \n         let mut t1 = self.trees().filter(semantic_tree);\n@@ -430,13 +427,13 @@ impl TokenStreamBuilder {\n     pub fn push<T: Into<TokenStream>>(&mut self, stream: T) {\n         let stream = stream.into();\n         let last_tree_if_joint = self.0.last().and_then(TokenStream::last_tree_if_joint);\n-        if let Some(TokenTree::Token(last_span, last_tok)) = last_tree_if_joint {\n-            if let Some((TokenTree::Token(span, tok), is_joint)) = stream.first_tree_and_joint() {\n-                if let Some(glued_tok) = last_tok.glue(tok) {\n+        if let Some(TokenTree::Token(last_token)) = last_tree_if_joint {\n+            if let Some((TokenTree::Token(token), is_joint)) = stream.first_tree_and_joint() {\n+                if let Some(glued_tok) = last_token.kind.glue(token.kind) {\n                     let last_stream = self.0.pop().unwrap();\n                     self.push_all_but_last_tree(&last_stream);\n-                    let glued_span = last_span.to(span);\n-                    let glued_tt = TokenTree::Token(glued_span, glued_tok);\n+                    let glued_span = last_token.span.to(token.span);\n+                    let glued_tt = TokenTree::token(glued_span, glued_tok);\n                     let glued_tokenstream = TokenStream::new(vec![(glued_tt, is_joint)]);\n                     self.0.push(glued_tokenstream);\n                     self.push_all_but_first_tree(&stream);\n@@ -663,7 +660,7 @@ mod tests {\n         with_default_globals(|| {\n             let test0: TokenStream = Vec::<TokenTree>::new().into_iter().collect();\n             let test1: TokenStream =\n-                TokenTree::Token(sp(0, 1), token::Ident(Ident::from_str(\"a\"), false)).into();\n+                TokenTree::token(sp(0, 1), token::Ident(Ident::from_str(\"a\"), false)).into();\n             let test2 = string_to_ts(\"foo(bar::baz)\");\n \n             assert_eq!(test0.is_empty(), true);\n@@ -676,9 +673,9 @@ mod tests {\n     fn test_dotdotdot() {\n         with_default_globals(|| {\n             let mut builder = TokenStreamBuilder::new();\n-            builder.push(TokenTree::Token(sp(0, 1), token::Dot).joint());\n-            builder.push(TokenTree::Token(sp(1, 2), token::Dot).joint());\n-            builder.push(TokenTree::Token(sp(2, 3), token::Dot));\n+            builder.push(TokenTree::token(sp(0, 1), token::Dot).joint());\n+            builder.push(TokenTree::token(sp(1, 2), token::Dot).joint());\n+            builder.push(TokenTree::token(sp(2, 3), token::Dot));\n             let stream = builder.build();\n             assert!(stream.eq_unspanned(&string_to_ts(\"...\")));\n             assert_eq!(stream.trees().count(), 1);"}, {"sha": "e32c5f3f3ecad948f7c7c6895ff70200743feeca", "filename": "src/libsyntax/visit.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fvisit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax%2Fvisit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fvisit.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -855,7 +855,7 @@ pub fn walk_attribute<'a, V: Visitor<'a>>(visitor: &mut V, attr: &'a Attribute)\n \n pub fn walk_tt<'a, V: Visitor<'a>>(visitor: &mut V, tt: TokenTree) {\n     match tt {\n-        TokenTree::Token(_, tok) => visitor.visit_token(tok),\n+        TokenTree::Token(token) => visitor.visit_token(token.kind),\n         TokenTree::Delimited(_, _, tts) => visitor.visit_tts(tts),\n     }\n }"}, {"sha": "83c4c809de372f48fd9e0044b2e5ff62e872964f", "filename": "src/libsyntax_ext/asm.rs", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fasm.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -9,7 +9,8 @@ use errors::DiagnosticBuilder;\n use syntax::ast;\n use syntax::ext::base::{self, *};\n use syntax::feature_gate;\n-use syntax::parse::{self, token};\n+use syntax::parse;\n+use syntax::parse::token::{self, Token};\n use syntax::ptr::P;\n use syntax::symbol::{kw, sym, Symbol};\n use syntax::ast::AsmDialect;\n@@ -86,8 +87,8 @@ fn parse_inline_asm<'a>(\n     let first_colon = tts.iter()\n         .position(|tt| {\n             match *tt {\n-                tokenstream::TokenTree::Token(_, token::Colon) |\n-                tokenstream::TokenTree::Token(_, token::ModSep) => true,\n+                tokenstream::TokenTree::Token(Token { kind: token::Colon, .. }) |\n+                tokenstream::TokenTree::Token(Token { kind: token::ModSep, .. }) => true,\n                 _ => false,\n             }\n         })"}, {"sha": "8a297a5c9bc19996d34d91483b85c9585a5aa50f", "filename": "src/libsyntax_ext/assert.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fassert.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fassert.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fassert.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -29,7 +29,7 @@ pub fn expand_assert<'cx>(\n     let panic_call = Mac_ {\n         path: Path::from_ident(Ident::new(sym::panic, sp)),\n         tts: custom_message.unwrap_or_else(|| {\n-            TokenStream::from(TokenTree::Token(\n+            TokenStream::from(TokenTree::token(\n                 DUMMY_SP,\n                 TokenKind::lit(token::Str, Symbol::intern(&format!(\n                     \"assertion failed: {}\","}, {"sha": "59f25af37427641e6e9bec150420a0266dc48a4c", "filename": "src/libsyntax_ext/concat_idents.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fconcat_idents.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fconcat_idents.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fconcat_idents.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -3,7 +3,7 @@ use rustc_data_structures::thin_vec::ThinVec;\n use syntax::ast;\n use syntax::ext::base::{self, *};\n use syntax::feature_gate;\n-use syntax::parse::token;\n+use syntax::parse::token::{self, Token};\n use syntax::ptr::P;\n use syntax_pos::Span;\n use syntax_pos::symbol::{Symbol, sym};\n@@ -30,15 +30,15 @@ pub fn expand_syntax_ext<'cx>(cx: &'cx mut ExtCtxt<'_>,\n     for (i, e) in tts.iter().enumerate() {\n         if i & 1 == 1 {\n             match *e {\n-                TokenTree::Token(_, token::Comma) => {}\n+                TokenTree::Token(Token { kind: token::Comma, .. }) => {}\n                 _ => {\n                     cx.span_err(sp, \"concat_idents! expecting comma.\");\n                     return DummyResult::any(sp);\n                 }\n             }\n         } else {\n             match *e {\n-                TokenTree::Token(_, token::Ident(ident, _)) =>\n+                TokenTree::Token(Token { kind: token::Ident(ident, _), .. }) =>\n                     res_str.push_str(&ident.as_str()),\n                 _ => {\n                     cx.span_err(sp, \"concat_idents! requires ident args.\");"}, {"sha": "3deab97db88c0fe28e255e59412f3724c90e1d7e", "filename": "src/libsyntax_ext/deriving/custom.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -69,7 +69,7 @@ impl MultiItemModifier for ProcMacroDerive {\n         MarkAttrs(&self.attrs).visit_item(&item);\n \n         let token = token::Interpolated(Lrc::new(token::NtItem(item)));\n-        let input = tokenstream::TokenTree::Token(DUMMY_SP, token).into();\n+        let input = tokenstream::TokenTree::token(DUMMY_SP, token).into();\n \n         let server = proc_macro_server::Rustc::new(ecx);\n         let stream = match self.client.run(&EXEC_STRATEGY, server, input) {"}, {"sha": "26eb9e9d4fc1fdad7ba86baa958d8eee446e476f", "filename": "src/libsyntax_ext/proc_macro_server.rs", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fproc_macro_server.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -55,7 +55,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n         use syntax::parse::token::*;\n \n         let joint = is_joint == Joint;\n-        let (span, token) = match tree {\n+        let Token { kind, span } = match tree {\n             tokenstream::TokenTree::Delimited(span, delim, tts) => {\n                 let delimiter = Delimiter::from_internal(delim);\n                 return TokenTree::Group(Group {\n@@ -64,7 +64,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n                     span,\n                 });\n             }\n-            tokenstream::TokenTree::Token(span, token) => (span, token),\n+            tokenstream::TokenTree::Token(token) => token,\n         };\n \n         macro_rules! tt {\n@@ -93,7 +93,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n             }};\n         }\n \n-        match token {\n+        match kind {\n             Eq => op!('='),\n             Lt => op!('<'),\n             Le => op!('<', '='),\n@@ -164,7 +164,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n                     TokenKind::lit(token::Str, Symbol::intern(&escaped), None),\n                 ]\n                 .into_iter()\n-                .map(|token| tokenstream::TokenTree::Token(span, token))\n+                .map(|kind| tokenstream::TokenTree::token(span, kind))\n                 .collect();\n                 stack.push(TokenTree::Group(Group {\n                     delimiter: Delimiter::Bracket,\n@@ -212,7 +212,7 @@ impl ToInternal<TokenStream> for TokenTree<Group, Punct, Ident, Literal> {\n             }\n             TokenTree::Ident(self::Ident { sym, is_raw, span }) => {\n                 let token = Ident(ast::Ident::new(sym, span), is_raw);\n-                return tokenstream::TokenTree::Token(span, token).into();\n+                return tokenstream::TokenTree::token(span, token).into();\n             }\n             TokenTree::Literal(self::Literal {\n                 lit: token::Lit { kind: token::Integer, symbol, suffix },\n@@ -221,8 +221,8 @@ impl ToInternal<TokenStream> for TokenTree<Group, Punct, Ident, Literal> {\n                 let minus = BinOp(BinOpToken::Minus);\n                 let symbol = Symbol::intern(&symbol.as_str()[1..]);\n                 let integer = TokenKind::lit(token::Integer, symbol, suffix);\n-                let a = tokenstream::TokenTree::Token(span, minus);\n-                let b = tokenstream::TokenTree::Token(span, integer);\n+                let a = tokenstream::TokenTree::token(span, minus);\n+                let b = tokenstream::TokenTree::token(span, integer);\n                 return vec![a, b].into_iter().collect();\n             }\n             TokenTree::Literal(self::Literal {\n@@ -232,16 +232,16 @@ impl ToInternal<TokenStream> for TokenTree<Group, Punct, Ident, Literal> {\n                 let minus = BinOp(BinOpToken::Minus);\n                 let symbol = Symbol::intern(&symbol.as_str()[1..]);\n                 let float = TokenKind::lit(token::Float, symbol, suffix);\n-                let a = tokenstream::TokenTree::Token(span, minus);\n-                let b = tokenstream::TokenTree::Token(span, float);\n+                let a = tokenstream::TokenTree::token(span, minus);\n+                let b = tokenstream::TokenTree::token(span, float);\n                 return vec![a, b].into_iter().collect();\n             }\n             TokenTree::Literal(self::Literal { lit, span }) => {\n-                return tokenstream::TokenTree::Token(span, Literal(lit)).into()\n+                return tokenstream::TokenTree::token(span, Literal(lit)).into()\n             }\n         };\n \n-        let token = match ch {\n+        let kind = match ch {\n             '=' => Eq,\n             '<' => Lt,\n             '>' => Gt,\n@@ -267,7 +267,7 @@ impl ToInternal<TokenStream> for TokenTree<Group, Punct, Ident, Literal> {\n             _ => unreachable!(),\n         };\n \n-        let tree = tokenstream::TokenTree::Token(span, token);\n+        let tree = tokenstream::TokenTree::token(span, kind);\n         TokenStream::new(vec![(tree, if joint { Joint } else { NonJoint })])\n     }\n }"}, {"sha": "6c74f77ff1fb5c4268c1248591c22e21bbd41f29", "filename": "src/libsyntax_ext/trace_macros.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Ftrace_macros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e0127dbf8135b766a332ce21c4eee48998b59bef/src%2Flibsyntax_ext%2Ftrace_macros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Ftrace_macros.rs?ref=e0127dbf8135b766a332ce21c4eee48998b59bef", "patch": "@@ -17,10 +17,10 @@ pub fn expand_trace_macros(cx: &mut ExtCtxt<'_>,\n     }\n \n     match (tt.len(), tt.first()) {\n-        (1, Some(&TokenTree::Token(_, ref tok))) if tok.is_keyword(kw::True) => {\n+        (1, Some(TokenTree::Token(token))) if token.is_keyword(kw::True) => {\n             cx.set_trace_macros(true);\n         }\n-        (1, Some(&TokenTree::Token(_, ref tok))) if tok.is_keyword(kw::False) => {\n+        (1, Some(TokenTree::Token(token))) if token.is_keyword(kw::False) => {\n             cx.set_trace_macros(false);\n         }\n         _ => cx.span_err(sp, \"trace_macros! accepts only `true` or `false`\"),"}]}