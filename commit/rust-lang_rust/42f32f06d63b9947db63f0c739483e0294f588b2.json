{"sha": "42f32f06d63b9947db63f0c739483e0294f588b2", "node_id": "MDY6Q29tbWl0NzI0NzEyOjQyZjMyZjA2ZDYzYjk5NDdkYjYzZjBjNzM5NDgzZTAyOTRmNTg4YjI=", "commit": {"author": {"name": "Mazdak Farrokhzad", "email": "twingoow@gmail.com", "date": "2019-10-09T00:34:22Z"}, "committer": {"name": "Mazdak Farrokhzad", "email": "twingoow@gmail.com", "date": "2019-10-13T18:13:18Z"}, "message": "token: extract Nonterminal::to_tokenstream to parser.", "tree": {"sha": "01fce347eecc2b7d0e4f5e0cea380866b277f638", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/01fce347eecc2b7d0e4f5e0cea380866b277f638"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/42f32f06d63b9947db63f0c739483e0294f588b2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/42f32f06d63b9947db63f0c739483e0294f588b2", "html_url": "https://github.com/rust-lang/rust/commit/42f32f06d63b9947db63f0c739483e0294f588b2", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/42f32f06d63b9947db63f0c739483e0294f588b2/comments", "author": {"login": "Centril", "id": 855702, "node_id": "MDQ6VXNlcjg1NTcwMg==", "avatar_url": "https://avatars.githubusercontent.com/u/855702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Centril", "html_url": "https://github.com/Centril", "followers_url": "https://api.github.com/users/Centril/followers", "following_url": "https://api.github.com/users/Centril/following{/other_user}", "gists_url": "https://api.github.com/users/Centril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Centril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Centril/subscriptions", "organizations_url": "https://api.github.com/users/Centril/orgs", "repos_url": "https://api.github.com/users/Centril/repos", "events_url": "https://api.github.com/users/Centril/events{/privacy}", "received_events_url": "https://api.github.com/users/Centril/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Centril", "id": 855702, "node_id": "MDQ6VXNlcjg1NTcwMg==", "avatar_url": "https://avatars.githubusercontent.com/u/855702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Centril", "html_url": "https://github.com/Centril", "followers_url": "https://api.github.com/users/Centril/followers", "following_url": "https://api.github.com/users/Centril/following{/other_user}", "gists_url": "https://api.github.com/users/Centril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Centril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Centril/subscriptions", "organizations_url": "https://api.github.com/users/Centril/orgs", "repos_url": "https://api.github.com/users/Centril/repos", "events_url": "https://api.github.com/users/Centril/events{/privacy}", "received_events_url": "https://api.github.com/users/Centril/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "aa2ae564d391a3da10bca2a79ab529a9925fbe58", "url": "https://api.github.com/repos/rust-lang/rust/commits/aa2ae564d391a3da10bca2a79ab529a9925fbe58", "html_url": "https://github.com/rust-lang/rust/commit/aa2ae564d391a3da10bca2a79ab529a9925fbe58"}], "stats": {"total": 283, "additions": 143, "deletions": 140}, "files": [{"sha": "513512b9f6b2c2aacabb36e41b13b4640b3fb8a0", "filename": "src/librustc/hir/lowering.rs", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/42f32f06d63b9947db63f0c739483e0294f588b2/src%2Flibrustc%2Fhir%2Flowering.rs", "raw_url": "https://github.com/rust-lang/rust/raw/42f32f06d63b9947db63f0c739483e0294f588b2/src%2Flibrustc%2Fhir%2Flowering.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Flowering.rs?ref=42f32f06d63b9947db63f0c739483e0294f588b2", "patch": "@@ -70,6 +70,7 @@ use syntax::print::pprust;\n use syntax::source_map::{respan, ExpnData, ExpnKind, DesugaringKind, Spanned};\n use syntax::symbol::{kw, sym, Symbol};\n use syntax::tokenstream::{TokenStream, TokenTree};\n+use syntax::parse;\n use syntax::parse::token::{self, Token};\n use syntax::visit::{self, Visitor};\n use syntax_pos::Span;\n@@ -1022,7 +1023,10 @@ impl<'a> LoweringContext<'a> {\n     fn lower_token(&mut self, token: Token) -> TokenStream {\n         match token.kind {\n             token::Interpolated(nt) => {\n-                let tts = nt.to_tokenstream(&self.sess.parse_sess, token.span);\n+                // FIXME(Centril): Consider indirection `(parse_sess.nt_to_tokenstream)(...)`\n+                // to hack around the current hack that requires `nt_to_tokenstream` to live\n+                // in the parser.\n+                let tts = parse::nt_to_tokenstream(&nt, &self.sess.parse_sess, token.span);\n                 self.lower_token_stream(tts)\n             }\n             _ => TokenTree::Token(token).into(),"}, {"sha": "0647c2fa313b59b40892656e92fa4d9ded51bc29", "filename": "src/libsyntax/ext/proc_macro_server.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/42f32f06d63b9947db63f0c739483e0294f588b2/src%2Flibsyntax%2Fext%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/42f32f06d63b9947db63f0c739483e0294f588b2/src%2Flibsyntax%2Fext%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fproc_macro_server.rs?ref=42f32f06d63b9947db63f0c739483e0294f588b2", "patch": "@@ -174,7 +174,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n             }\n \n             Interpolated(nt) => {\n-                let stream = nt.to_tokenstream(sess, span);\n+                let stream = parse::nt_to_tokenstream(&nt, sess, span);\n                 TokenTree::Group(Group {\n                     delimiter: Delimiter::None,\n                     stream,"}, {"sha": "0df695d37b94f10810164165c67ed80797d1d17b", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 134, "deletions": 4, "changes": 138, "blob_url": "https://github.com/rust-lang/rust/blob/42f32f06d63b9947db63f0c739483e0294f588b2/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/42f32f06d63b9947db63f0c739483e0294f588b2/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=42f32f06d63b9947db63f0c739483e0294f588b2", "patch": "@@ -4,10 +4,9 @@ use crate::ast::{self, CrateConfig, NodeId};\n use crate::early_buffered_lints::{BufferedEarlyLint, BufferedEarlyLintId};\n use crate::source_map::{SourceMap, FilePathMapping};\n use crate::feature_gate::UnstableFeatures;\n-use crate::parse::parser::Parser;\n-use crate::parse::parser::emit_unclosed_delims;\n-use crate::parse::token::TokenKind;\n-use crate::tokenstream::{TokenStream, TokenTree};\n+use crate::parse::parser::{Parser, emit_unclosed_delims};\n+use crate::parse::token::{Nonterminal, TokenKind};\n+use crate::tokenstream::{self, TokenStream, TokenTree};\n use crate::print::pprust;\n use crate::symbol::Symbol;\n \n@@ -24,6 +23,8 @@ use std::borrow::Cow;\n use std::path::{Path, PathBuf};\n use std::str;\n \n+use log::info;\n+\n #[cfg(test)]\n mod tests;\n \n@@ -407,3 +408,132 @@ impl SeqSep {\n         }\n     }\n }\n+\n+// NOTE(Centril): The following probably shouldn't be here but it acknowledges the\n+// fact that architecturally, we are using parsing (read on below to understand why).\n+\n+pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> TokenStream {\n+    // A `Nonterminal` is often a parsed AST item. At this point we now\n+    // need to convert the parsed AST to an actual token stream, e.g.\n+    // un-parse it basically.\n+    //\n+    // Unfortunately there's not really a great way to do that in a\n+    // guaranteed lossless fashion right now. The fallback here is to just\n+    // stringify the AST node and reparse it, but this loses all span\n+    // information.\n+    //\n+    // As a result, some AST nodes are annotated with the token stream they\n+    // came from. Here we attempt to extract these lossless token streams\n+    // before we fall back to the stringification.\n+    let tokens = match *nt {\n+        Nonterminal::NtItem(ref item) => {\n+            prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+        }\n+        Nonterminal::NtTraitItem(ref item) => {\n+            prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+        }\n+        Nonterminal::NtImplItem(ref item) => {\n+            prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+        }\n+        Nonterminal::NtIdent(ident, is_raw) => {\n+            Some(tokenstream::TokenTree::token(token::Ident(ident.name, is_raw), ident.span).into())\n+        }\n+        Nonterminal::NtLifetime(ident) => {\n+            Some(tokenstream::TokenTree::token(token::Lifetime(ident.name), ident.span).into())\n+        }\n+        Nonterminal::NtTT(ref tt) => {\n+            Some(tt.clone().into())\n+        }\n+        _ => None,\n+    };\n+\n+    // FIXME(#43081): Avoid this pretty-print + reparse hack\n+    let source = pprust::nonterminal_to_string(nt);\n+    let filename = FileName::macro_expansion_source_code(&source);\n+    let tokens_for_real = parse_stream_from_source_str(filename, source, sess, Some(span));\n+\n+    // During early phases of the compiler the AST could get modified\n+    // directly (e.g., attributes added or removed) and the internal cache\n+    // of tokens my not be invalidated or updated. Consequently if the\n+    // \"lossless\" token stream disagrees with our actual stringification\n+    // (which has historically been much more battle-tested) then we go\n+    // with the lossy stream anyway (losing span information).\n+    //\n+    // Note that the comparison isn't `==` here to avoid comparing spans,\n+    // but it *also* is a \"probable\" equality which is a pretty weird\n+    // definition. We mostly want to catch actual changes to the AST\n+    // like a `#[cfg]` being processed or some weird `macro_rules!`\n+    // expansion.\n+    //\n+    // What we *don't* want to catch is the fact that a user-defined\n+    // literal like `0xf` is stringified as `15`, causing the cached token\n+    // stream to not be literal `==` token-wise (ignoring spans) to the\n+    // token stream we got from stringification.\n+    //\n+    // Instead the \"probably equal\" check here is \"does each token\n+    // recursively have the same discriminant?\" We basically don't look at\n+    // the token values here and assume that such fine grained token stream\n+    // modifications, including adding/removing typically non-semantic\n+    // tokens such as extra braces and commas, don't happen.\n+    if let Some(tokens) = tokens {\n+        if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n+            return tokens\n+        }\n+        info!(\"cached tokens found, but they're not \\\"probably equal\\\", \\\n+                going with stringified version\");\n+    }\n+    return tokens_for_real\n+}\n+\n+fn prepend_attrs(\n+    sess: &ParseSess,\n+    attrs: &[ast::Attribute],\n+    tokens: Option<&tokenstream::TokenStream>,\n+    span: syntax_pos::Span\n+) -> Option<tokenstream::TokenStream> {\n+    let tokens = tokens?;\n+    if attrs.len() == 0 {\n+        return Some(tokens.clone())\n+    }\n+    let mut builder = tokenstream::TokenStreamBuilder::new();\n+    for attr in attrs {\n+        assert_eq!(attr.style, ast::AttrStyle::Outer,\n+                   \"inner attributes should prevent cached tokens from existing\");\n+\n+        let source = pprust::attribute_to_string(attr);\n+        let macro_filename = FileName::macro_expansion_source_code(&source);\n+        if attr.is_sugared_doc {\n+            let stream = parse_stream_from_source_str(macro_filename, source, sess, Some(span));\n+            builder.push(stream);\n+            continue\n+        }\n+\n+        // synthesize # [ $path $tokens ] manually here\n+        let mut brackets = tokenstream::TokenStreamBuilder::new();\n+\n+        // For simple paths, push the identifier directly\n+        if attr.path.segments.len() == 1 && attr.path.segments[0].args.is_none() {\n+            let ident = attr.path.segments[0].ident;\n+            let token = token::Ident(ident.name, ident.as_str().starts_with(\"r#\"));\n+            brackets.push(tokenstream::TokenTree::token(token, ident.span));\n+\n+        // ... and for more complicated paths, fall back to a reparse hack that\n+        // should eventually be removed.\n+        } else {\n+            let stream = parse_stream_from_source_str(macro_filename, source, sess, Some(span));\n+            brackets.push(stream);\n+        }\n+\n+        brackets.push(attr.tokens.clone());\n+\n+        // The span we list here for `#` and for `[ ... ]` are both wrong in\n+        // that it encompasses more than each token, but it hopefully is \"good\n+        // enough\" for now at least.\n+        builder.push(tokenstream::TokenTree::token(token::Pound, attr.span));\n+        let delim_span = tokenstream::DelimSpan::from_single(attr.span);\n+        builder.push(tokenstream::TokenTree::Delimited(\n+            delim_span, token::DelimToken::Bracket, brackets.build().into()));\n+    }\n+    builder.push(tokens.clone());\n+    Some(builder.build())\n+}"}, {"sha": "eb74ab2b9192d8b73af9edae549d368f90dbbf09", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 3, "deletions": 134, "changes": 137, "blob_url": "https://github.com/rust-lang/rust/blob/42f32f06d63b9947db63f0c739483e0294f588b2/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/42f32f06d63b9947db63f0c739483e0294f588b2/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=42f32f06d63b9947db63f0c739483e0294f588b2", "patch": "@@ -4,16 +4,13 @@ pub use DelimToken::*;\n pub use LitKind::*;\n pub use TokenKind::*;\n \n-use crate::ast::{self};\n-use crate::parse::{parse_stream_from_source_str, ParseSess};\n-use crate::print::pprust;\n+use crate::ast;\n use crate::ptr::P;\n use crate::symbol::kw;\n-use crate::tokenstream::{self, DelimSpan, TokenStream, TokenTree};\n+use crate::tokenstream::TokenTree;\n \n use syntax_pos::symbol::Symbol;\n-use syntax_pos::{self, Span, FileName, DUMMY_SP};\n-use log::info;\n+use syntax_pos::{self, Span, DUMMY_SP};\n \n use std::fmt;\n use std::mem;\n@@ -737,131 +734,3 @@ impl fmt::Debug for Nonterminal {\n         }\n     }\n }\n-\n-impl Nonterminal {\n-    pub fn to_tokenstream(&self, sess: &ParseSess, span: Span) -> TokenStream {\n-        // A `Nonterminal` is often a parsed AST item. At this point we now\n-        // need to convert the parsed AST to an actual token stream, e.g.\n-        // un-parse it basically.\n-        //\n-        // Unfortunately there's not really a great way to do that in a\n-        // guaranteed lossless fashion right now. The fallback here is to just\n-        // stringify the AST node and reparse it, but this loses all span\n-        // information.\n-        //\n-        // As a result, some AST nodes are annotated with the token stream they\n-        // came from. Here we attempt to extract these lossless token streams\n-        // before we fall back to the stringification.\n-        let tokens = match *self {\n-            Nonterminal::NtItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtTraitItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtImplItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtIdent(ident, is_raw) => {\n-                Some(TokenTree::token(Ident(ident.name, is_raw), ident.span).into())\n-            }\n-            Nonterminal::NtLifetime(ident) => {\n-                Some(TokenTree::token(Lifetime(ident.name), ident.span).into())\n-            }\n-            Nonterminal::NtTT(ref tt) => {\n-                Some(tt.clone().into())\n-            }\n-            _ => None,\n-        };\n-\n-        // FIXME(#43081): Avoid this pretty-print + reparse hack\n-        let source = pprust::nonterminal_to_string(self);\n-        let filename = FileName::macro_expansion_source_code(&source);\n-        let tokens_for_real = parse_stream_from_source_str(filename, source, sess, Some(span));\n-\n-        // During early phases of the compiler the AST could get modified\n-        // directly (e.g., attributes added or removed) and the internal cache\n-        // of tokens my not be invalidated or updated. Consequently if the\n-        // \"lossless\" token stream disagrees with our actual stringification\n-        // (which has historically been much more battle-tested) then we go\n-        // with the lossy stream anyway (losing span information).\n-        //\n-        // Note that the comparison isn't `==` here to avoid comparing spans,\n-        // but it *also* is a \"probable\" equality which is a pretty weird\n-        // definition. We mostly want to catch actual changes to the AST\n-        // like a `#[cfg]` being processed or some weird `macro_rules!`\n-        // expansion.\n-        //\n-        // What we *don't* want to catch is the fact that a user-defined\n-        // literal like `0xf` is stringified as `15`, causing the cached token\n-        // stream to not be literal `==` token-wise (ignoring spans) to the\n-        // token stream we got from stringification.\n-        //\n-        // Instead the \"probably equal\" check here is \"does each token\n-        // recursively have the same discriminant?\" We basically don't look at\n-        // the token values here and assume that such fine grained token stream\n-        // modifications, including adding/removing typically non-semantic\n-        // tokens such as extra braces and commas, don't happen.\n-        if let Some(tokens) = tokens {\n-            if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n-                return tokens\n-            }\n-            info!(\"cached tokens found, but they're not \\\"probably equal\\\", \\\n-                   going with stringified version\");\n-        }\n-        return tokens_for_real\n-    }\n-}\n-\n-fn prepend_attrs(sess: &ParseSess,\n-                 attrs: &[ast::Attribute],\n-                 tokens: Option<&tokenstream::TokenStream>,\n-                 span: syntax_pos::Span)\n-    -> Option<tokenstream::TokenStream>\n-{\n-    let tokens = tokens?;\n-    if attrs.len() == 0 {\n-        return Some(tokens.clone())\n-    }\n-    let mut builder = tokenstream::TokenStreamBuilder::new();\n-    for attr in attrs {\n-        assert_eq!(attr.style, ast::AttrStyle::Outer,\n-                   \"inner attributes should prevent cached tokens from existing\");\n-\n-        let source = pprust::attribute_to_string(attr);\n-        let macro_filename = FileName::macro_expansion_source_code(&source);\n-        if attr.is_sugared_doc {\n-            let stream = parse_stream_from_source_str(macro_filename, source, sess, Some(span));\n-            builder.push(stream);\n-            continue\n-        }\n-\n-        // synthesize # [ $path $tokens ] manually here\n-        let mut brackets = tokenstream::TokenStreamBuilder::new();\n-\n-        // For simple paths, push the identifier directly\n-        if attr.path.segments.len() == 1 && attr.path.segments[0].args.is_none() {\n-            let ident = attr.path.segments[0].ident;\n-            let token = Ident(ident.name, ident.as_str().starts_with(\"r#\"));\n-            brackets.push(tokenstream::TokenTree::token(token, ident.span));\n-\n-        // ... and for more complicated paths, fall back to a reparse hack that\n-        // should eventually be removed.\n-        } else {\n-            let stream = parse_stream_from_source_str(macro_filename, source, sess, Some(span));\n-            brackets.push(stream);\n-        }\n-\n-        brackets.push(attr.tokens.clone());\n-\n-        // The span we list here for `#` and for `[ ... ]` are both wrong in\n-        // that it encompasses more than each token, but it hopefully is \"good\n-        // enough\" for now at least.\n-        builder.push(tokenstream::TokenTree::token(Pound, attr.span));\n-        let delim_span = DelimSpan::from_single(attr.span);\n-        builder.push(tokenstream::TokenTree::Delimited(\n-            delim_span, DelimToken::Bracket, brackets.build().into()));\n-    }\n-    builder.push(tokens.clone());\n-    Some(builder.build())\n-}"}]}