{"sha": "7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "node_id": "MDY6Q29tbWl0NzI0NzEyOjdjZjRkOGJjNDQ2MTc3MjA0ZTllMTJiMWVmYjE5OWE1ZGJjOTU2YjU=", "commit": {"author": {"name": "Eduard Burtescu", "email": "edy.burt@gmail.com", "date": "2014-03-27T13:14:58Z"}, "committer": {"name": "Eduard Burtescu", "email": "edy.burt@gmail.com", "date": "2014-03-28T16:28:03Z"}, "message": "Used inherited mutability in lexer::Reader.", "tree": {"sha": "4a9a3168062524f2d36875ee2964d7c203d3b55e", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/4a9a3168062524f2d36875ee2964d7c203d3b55e"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "html_url": "https://github.com/rust-lang/rust/commit/7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/comments", "author": {"login": "eddyb", "id": 77424, "node_id": "MDQ6VXNlcjc3NDI0", "avatar_url": "https://avatars.githubusercontent.com/u/77424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eddyb", "html_url": "https://github.com/eddyb", "followers_url": "https://api.github.com/users/eddyb/followers", "following_url": "https://api.github.com/users/eddyb/following{/other_user}", "gists_url": "https://api.github.com/users/eddyb/gists{/gist_id}", "starred_url": "https://api.github.com/users/eddyb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eddyb/subscriptions", "organizations_url": "https://api.github.com/users/eddyb/orgs", "repos_url": "https://api.github.com/users/eddyb/repos", "events_url": "https://api.github.com/users/eddyb/events{/privacy}", "received_events_url": "https://api.github.com/users/eddyb/received_events", "type": "User", "site_admin": false}, "committer": {"login": "eddyb", "id": 77424, "node_id": "MDQ6VXNlcjc3NDI0", "avatar_url": "https://avatars.githubusercontent.com/u/77424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eddyb", "html_url": "https://github.com/eddyb", "followers_url": "https://api.github.com/users/eddyb/followers", "following_url": "https://api.github.com/users/eddyb/following{/other_user}", "gists_url": "https://api.github.com/users/eddyb/gists{/gist_id}", "starred_url": "https://api.github.com/users/eddyb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eddyb/subscriptions", "organizations_url": "https://api.github.com/users/eddyb/orgs", "repos_url": "https://api.github.com/users/eddyb/repos", "events_url": "https://api.github.com/users/eddyb/events{/privacy}", "received_events_url": "https://api.github.com/users/eddyb/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b8601a3d8b91ad3b653d143307611f2f5c75617e", "url": "https://api.github.com/repos/rust-lang/rust/commits/b8601a3d8b91ad3b653d143307611f2f5c75617e", "html_url": "https://github.com/rust-lang/rust/commit/b8601a3d8b91ad3b653d143307611f2f5c75617e"}], "stats": {"total": 725, "additions": 335, "deletions": 390}, "files": [{"sha": "58fc92bf345af3cc7d350426d0c48f16c60a569d", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "patch": "@@ -44,7 +44,7 @@ pub fn highlight(src: &str, class: Option<&str>) -> ~str {\n /// it's used. All source code emission is done as slices from the source map,\n /// not from the tokens themselves, in order to stay true to the original\n /// source.\n-fn doit(sess: &parse::ParseSess, lexer: lexer::StringReader, class: Option<&str>,\n+fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader, class: Option<&str>,\n         out: &mut Writer) -> io::IoResult<()> {\n     use syntax::parse::lexer::Reader;\n \n@@ -55,7 +55,7 @@ fn doit(sess: &parse::ParseSess, lexer: lexer::StringReader, class: Option<&str>\n     let mut is_macro_nonterminal = false;\n     loop {\n         let next = lexer.next_token();\n-        let test = if next.tok == t::EOF {lexer.pos.get()} else {next.sp.lo};\n+        let test = if next.tok == t::EOF {lexer.pos} else {next.sp.lo};\n \n         // The lexer consumes all whitespace and non-doc-comments when iterating\n         // between tokens. If this token isn't directly adjacent to our last"}, {"sha": "d730f50c7bca82b5f181513dd92c793a6ea3c764", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "patch": "@@ -204,11 +204,11 @@ pub enum ParseResult {\n     Error(codemap::Span, ~str)\n }\n \n-pub fn parse_or_else<R: Reader>(sess: &ParseSess,\n-                                cfg: ast::CrateConfig,\n-                                rdr: R,\n-                                ms: Vec<Matcher> )\n-                                -> HashMap<Ident, @NamedMatch> {\n+pub fn parse_or_else(sess: &ParseSess,\n+                     cfg: ast::CrateConfig,\n+                     rdr: TtReader,\n+                     ms: Vec<Matcher> )\n+                     -> HashMap<Ident, @NamedMatch> {\n     match parse(sess, cfg, rdr, ms.as_slice()) {\n         Success(m) => m,\n         Failure(sp, str) => sess.span_diagnostic.span_fatal(sp, str),\n@@ -226,11 +226,11 @@ pub fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n     }\n }\n \n-pub fn parse<R: Reader>(sess: &ParseSess,\n-                        cfg: ast::CrateConfig,\n-                        rdr: R,\n-                        ms: &[Matcher])\n-                        -> ParseResult {\n+pub fn parse(sess: &ParseSess,\n+             cfg: ast::CrateConfig,\n+             mut rdr: TtReader,\n+             ms: &[Matcher])\n+             -> ParseResult {\n     let mut cur_eis = Vec::new();\n     cur_eis.push(initial_matcher_pos(ms.iter()\n                                        .map(|x| (*x).clone())\n@@ -395,7 +395,7 @@ pub fn parse<R: Reader>(sess: &ParseSess,\n                 }\n                 rdr.next_token();\n             } else /* bb_eis.len() == 1 */ {\n-                let mut rust_parser = Parser(sess, cfg.clone(), rdr.dup());\n+                let mut rust_parser = Parser(sess, cfg.clone(), ~rdr.clone());\n \n                 let mut ei = bb_eis.pop().unwrap();\n                 match ei.elts.get(ei.idx).node {"}, {"sha": "1a32332bee61f737abef4d84c57ad4c2087aaa9e", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 141, "deletions": 173, "changes": 314, "blob_url": "https://github.com/rust-lang/rust/blob/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "patch": "@@ -17,29 +17,29 @@ use parse::token::{EOF, INTERPOLATED, IDENT, Token, NtIdent};\n use parse::token;\n use parse::lexer::TokenAndSpan;\n \n-use std::cell::{Cell, RefCell};\n use collections::HashMap;\n \n ///an unzipping of `TokenTree`s\n+#[deriving(Clone)]\n struct TtFrame {\n-    forest: @Vec<ast::TokenTree> ,\n-    idx: Cell<uint>,\n+    forest: @Vec<ast::TokenTree>,\n+    idx: uint,\n     dotdotdoted: bool,\n     sep: Option<Token>,\n-    up: Option<@TtFrame>,\n }\n \n+#[deriving(Clone)]\n pub struct TtReader<'a> {\n     sp_diag: &'a SpanHandler,\n     // the unzipped tree:\n-    priv stack: RefCell<@TtFrame>,\n+    priv stack: Vec<TtFrame>,\n     /* for MBE-style macro transcription */\n-    priv interpolations: RefCell<HashMap<Ident, @NamedMatch>>,\n-    priv repeat_idx: RefCell<Vec<uint> >,\n-    priv repeat_len: RefCell<Vec<uint> >,\n+    priv interpolations: HashMap<Ident, @NamedMatch>,\n+    priv repeat_idx: Vec<uint>,\n+    priv repeat_len: Vec<uint>,\n     /* cached: */\n-    cur_tok: RefCell<Token>,\n-    cur_span: RefCell<Span>,\n+    cur_tok: Token,\n+    cur_span: Span,\n }\n \n /** This can do Macro-By-Example transcription. On the other hand, if\n@@ -49,75 +49,46 @@ pub fn new_tt_reader<'a>(sp_diag: &'a SpanHandler,\n                          interp: Option<HashMap<Ident, @NamedMatch>>,\n                          src: Vec<ast::TokenTree> )\n                          -> TtReader<'a> {\n-    let r = TtReader {\n+    let mut r = TtReader {\n         sp_diag: sp_diag,\n-        stack: RefCell::new(@TtFrame {\n+        stack: vec!(TtFrame {\n             forest: @src,\n-            idx: Cell::new(0u),\n+            idx: 0,\n             dotdotdoted: false,\n             sep: None,\n-            up: None\n         }),\n         interpolations: match interp { /* just a convienience */\n-            None => RefCell::new(HashMap::new()),\n-            Some(x) => RefCell::new(x),\n+            None => HashMap::new(),\n+            Some(x) => x,\n         },\n-        repeat_idx: RefCell::new(Vec::new()),\n-        repeat_len: RefCell::new(Vec::new()),\n+        repeat_idx: Vec::new(),\n+        repeat_len: Vec::new(),\n         /* dummy values, never read: */\n-        cur_tok: RefCell::new(EOF),\n-        cur_span: RefCell::new(DUMMY_SP),\n+        cur_tok: EOF,\n+        cur_span: DUMMY_SP,\n     };\n-    tt_next_token(&r); /* get cur_tok and cur_span set up */\n+    tt_next_token(&mut r); /* get cur_tok and cur_span set up */\n     r\n }\n \n-fn dup_tt_frame(f: @TtFrame) -> @TtFrame {\n-    @TtFrame {\n-        forest: @(*f.forest).clone(),\n-        idx: f.idx.clone(),\n-        dotdotdoted: f.dotdotdoted,\n-        sep: f.sep.clone(),\n-        up: match f.up {\n-            Some(up_frame) => Some(dup_tt_frame(up_frame)),\n-            None => None\n-        }\n-    }\n-}\n-\n-pub fn dup_tt_reader<'a>(r: &TtReader<'a>) -> TtReader<'a> {\n-    TtReader {\n-        sp_diag: r.sp_diag,\n-        stack: RefCell::new(dup_tt_frame(r.stack.get())),\n-        repeat_idx: r.repeat_idx.clone(),\n-        repeat_len: r.repeat_len.clone(),\n-        cur_tok: r.cur_tok.clone(),\n-        cur_span: r.cur_span.clone(),\n-        interpolations: r.interpolations.clone(),\n-    }\n-}\n-\n-\n-fn lookup_cur_matched_by_matched(r: &TtReader, start: @NamedMatch)\n-                                 -> @NamedMatch {\n-    fn red(ad: @NamedMatch, idx: &uint) -> @NamedMatch {\n+fn lookup_cur_matched_by_matched(r: &TtReader, start: @NamedMatch) -> @NamedMatch {\n+    r.repeat_idx.iter().fold(start, |ad, idx| {\n         match *ad {\n             MatchedNonterminal(_) => {\n                 // end of the line; duplicate henceforth\n                 ad\n             }\n             MatchedSeq(ref ads, _) => *ads.get(*idx)\n         }\n-    }\n-    r.repeat_idx.borrow().iter().fold(start, red)\n+    })\n }\n \n fn lookup_cur_matched(r: &TtReader, name: Ident) -> @NamedMatch {\n-    let matched_opt = r.interpolations.borrow().find_copy(&name);\n+    let matched_opt = r.interpolations.find_copy(&name);\n     match matched_opt {\n         Some(s) => lookup_cur_matched_by_matched(r, s),\n         None => {\n-            r.sp_diag.span_fatal(r.cur_span.get(),\n+            r.sp_diag.span_fatal(r.cur_span,\n                                  format!(\"unknown macro variable `{}`\",\n                                          token::get_ident(name)));\n         }\n@@ -167,143 +138,140 @@ fn lockstep_iter_size(t: &TokenTree, r: &TtReader) -> LockstepIterSize {\n \n // return the next token from the TtReader.\n // EFFECT: advances the reader's token field\n-pub fn tt_next_token(r: &TtReader) -> TokenAndSpan {\n+pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n     // FIXME(pcwalton): Bad copy?\n     let ret_val = TokenAndSpan {\n-        tok: r.cur_tok.get(),\n-        sp: r.cur_span.get(),\n+        tok: r.cur_tok.clone(),\n+        sp: r.cur_span.clone(),\n     };\n     loop {\n-        if r.stack.borrow().idx.get() < r.stack.borrow().forest.len() {\n-            break;\n-        }\n-\n-        /* done with this set; pop or repeat? */\n-        if !r.stack.get().dotdotdoted || {\n-                *r.repeat_idx.borrow().last().unwrap() ==\n-                *r.repeat_len.borrow().last().unwrap() - 1\n-            } {\n-\n-            match r.stack.get().up {\n-              None => {\n-                r.cur_tok.set(EOF);\n+        let should_pop = match r.stack.last() {\n+            None => {\n+                assert_eq!(ret_val.tok, EOF);\n                 return ret_val;\n-              }\n-              Some(tt_f) => {\n-                if r.stack.get().dotdotdoted {\n-                    r.repeat_idx.borrow_mut().pop().unwrap();\n-                    r.repeat_len.borrow_mut().pop().unwrap();\n+            }\n+            Some(frame) => {\n+                if frame.idx < frame.forest.len() {\n+                    break;\n                 }\n-\n-                r.stack.set(tt_f);\n-                r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n-              }\n+                !frame.dotdotdoted ||\n+                    *r.repeat_idx.last().unwrap() == *r.repeat_len.last().unwrap() - 1\n             }\n+        };\n \n-        } else { /* repeat */\n-            r.stack.get().idx.set(0u);\n-            {\n-                let mut repeat_idx = r.repeat_idx.borrow_mut();\n-                let last_repeat_idx = repeat_idx.len() - 1u;\n-                *repeat_idx.get_mut(last_repeat_idx) += 1u;\n+        /* done with this set; pop or repeat? */\n+        if should_pop {\n+            let prev = r.stack.pop().unwrap();\n+            match r.stack.mut_last() {\n+                None => {\n+                    r.cur_tok = EOF;\n+                    return ret_val;\n+                }\n+                Some(frame) => {\n+                    frame.idx += 1;\n+                }\n             }\n-            match r.stack.get().sep.clone() {\n-              Some(tk) => {\n-                r.cur_tok.set(tk); /* repeat same span, I guess */\n-                return ret_val;\n-              }\n-              None => ()\n+            if prev.dotdotdoted {\n+                r.repeat_idx.pop();\n+                r.repeat_len.pop();\n+            }\n+        } else { /* repeat */\n+            *r.repeat_idx.mut_last().unwrap() += 1u;\n+            r.stack.mut_last().unwrap().idx = 0;\n+            match r.stack.last().unwrap().sep.clone() {\n+                Some(tk) => {\n+                    r.cur_tok = tk; /* repeat same span, I guess */\n+                    return ret_val;\n+                }\n+                None => {}\n             }\n         }\n     }\n     loop { /* because it's easiest, this handles `TTDelim` not starting\n-    with a `TTTok`, even though it won't happen */\n-        // FIXME(pcwalton): Bad copy.\n-        match (*r.stack.get().forest.get(r.stack.get().idx.get())).clone() {\n-          TTDelim(tts) => {\n-            r.stack.set(@TtFrame {\n-                forest: tts,\n-                idx: Cell::new(0u),\n-                dotdotdoted: false,\n-                sep: None,\n-                up: Some(r.stack.get())\n-            });\n-            // if this could be 0-length, we'd need to potentially recur here\n-          }\n-          TTTok(sp, tok) => {\n-            r.cur_span.set(sp);\n-            r.cur_tok.set(tok);\n-            r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n-            return ret_val;\n-          }\n-          TTSeq(sp, tts, sep, zerok) => {\n+              with a `TTTok`, even though it won't happen */\n+        let t = {\n+            let frame = r.stack.last().unwrap();\n             // FIXME(pcwalton): Bad copy.\n-            let t = TTSeq(sp, tts, sep.clone(), zerok);\n-            match lockstep_iter_size(&t, r) {\n-              LisUnconstrained => {\n-                r.sp_diag.span_fatal(\n-                    sp, /* blame macro writer */\n-                      \"attempted to repeat an expression \\\n-                       containing no syntax \\\n-                       variables matched as repeating at this depth\");\n-                  }\n-                  LisContradiction(ref msg) => {\n-                      /* FIXME #2887 blame macro invoker instead*/\n-                      r.sp_diag.span_fatal(sp, (*msg));\n-                  }\n-                  LisConstraint(len, _) => {\n-                    if len == 0 {\n-                      if !zerok {\n-                        r.sp_diag.span_fatal(sp, /* FIXME #2887 blame invoker\n-                        */\n-                                             \"this must repeat at least \\\n-                                              once\");\n-                          }\n-\n-                    r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n-                    return tt_next_token(r);\n-                } else {\n-                    r.repeat_len.borrow_mut().push(len);\n-                    r.repeat_idx.borrow_mut().push(0u);\n-                    r.stack.set(@TtFrame {\n-                        forest: tts,\n-                        idx: Cell::new(0u),\n-                        dotdotdoted: true,\n-                        sep: sep,\n-                        up: Some(r.stack.get())\n-                    });\n-                }\n-              }\n+            (*frame.forest.get(frame.idx)).clone()\n+        };\n+        match t {\n+            TTDelim(tts) => {\n+                r.stack.push(TtFrame {\n+                    forest: tts,\n+                    idx: 0,\n+                    dotdotdoted: false,\n+                    sep: None\n+                });\n+                // if this could be 0-length, we'd need to potentially recur here\n             }\n-          }\n-          // FIXME #2887: think about span stuff here\n-          TTNonterminal(sp, ident) => {\n-            match *lookup_cur_matched(r, ident) {\n-              /* sidestep the interpolation tricks for ident because\n-              (a) idents can be in lots of places, so it'd be a pain\n-              (b) we actually can, since it's a token. */\n-              MatchedNonterminal(NtIdent(~sn,b)) => {\n-                r.cur_span.set(sp);\n-                r.cur_tok.set(IDENT(sn,b));\n-                r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n+            TTTok(sp, tok) => {\n+                r.cur_span = sp;\n+                r.cur_tok = tok;\n+                r.stack.mut_last().unwrap().idx += 1;\n                 return ret_val;\n-              }\n-              MatchedNonterminal(ref other_whole_nt) => {\n+            }\n+            TTSeq(sp, tts, sep, zerok) => {\n                 // FIXME(pcwalton): Bad copy.\n-                r.cur_span.set(sp);\n-                r.cur_tok.set(INTERPOLATED((*other_whole_nt).clone()));\n-                r.stack.get().idx.set(r.stack.get().idx.get() + 1u);\n-                return ret_val;\n-              }\n-              MatchedSeq(..) => {\n-                r.sp_diag.span_fatal(\n-                    r.cur_span.get(), /* blame the macro writer */\n-                    format!(\"variable '{}' is still repeating at this depth\",\n-                            token::get_ident(ident)));\n-              }\n+                match lockstep_iter_size(&TTSeq(sp, tts, sep.clone(), zerok), r) {\n+                    LisUnconstrained => {\n+                        r.sp_diag.span_fatal(\n+                            sp.clone(), /* blame macro writer */\n+                            \"attempted to repeat an expression \\\n+                             containing no syntax \\\n+                             variables matched as repeating at this depth\");\n+                        }\n+                        LisContradiction(ref msg) => {\n+                            // FIXME #2887 blame macro invoker instead\n+                            r.sp_diag.span_fatal(sp.clone(), *msg);\n+                        }\n+                    LisConstraint(len, _) => {\n+                        if len == 0 {\n+                            if !zerok {\n+                                // FIXME #2887 blame invoker\n+                                r.sp_diag.span_fatal(sp.clone(),\n+                                                     \"this must repeat at least once\");\n+                            }\n+\n+                            r.stack.mut_last().unwrap().idx += 1;\n+                            return tt_next_token(r);\n+                        }\n+                        r.repeat_len.push(len);\n+                        r.repeat_idx.push(0);\n+                        r.stack.push(TtFrame {\n+                            forest: tts,\n+                            idx: 0,\n+                            dotdotdoted: true,\n+                            sep: sep.clone()\n+                        });\n+                    }\n+                }\n+            }\n+            // FIXME #2887: think about span stuff here\n+            TTNonterminal(sp, ident) => {\n+                r.stack.mut_last().unwrap().idx += 1;\n+                match *lookup_cur_matched(r, ident) {\n+                    /* sidestep the interpolation tricks for ident because\n+                       (a) idents can be in lots of places, so it'd be a pain\n+                       (b) we actually can, since it's a token. */\n+                    MatchedNonterminal(NtIdent(~sn,b)) => {\n+                        r.cur_span = sp;\n+                        r.cur_tok = IDENT(sn,b);\n+                        return ret_val;\n+                    }\n+                    MatchedNonterminal(ref other_whole_nt) => {\n+                        // FIXME(pcwalton): Bad copy.\n+                        r.cur_span = sp;\n+                        r.cur_tok = INTERPOLATED((*other_whole_nt).clone());\n+                        return ret_val;\n+                    }\n+                    MatchedSeq(..) => {\n+                        r.sp_diag.span_fatal(\n+                            r.cur_span, /* blame the macro writer */\n+                            format!(\"variable '{}' is still repeating at this depth\",\n+                                    token::get_ident(ident)));\n+                    }\n+                }\n             }\n-          }\n         }\n     }\n-\n }"}, {"sha": "53586a665133cf0de64860e9422c8463251db11f", "filename": "src/libsyntax/parse/comments.rs", "status": "modified", "additions": 30, "deletions": 32, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fparse%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fparse%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fcomments.rs?ref=7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "patch": "@@ -133,55 +133,53 @@ pub fn strip_doc_comment_decoration(comment: &str) -> ~str {\n     fail!(\"not a doc-comment: {}\", comment);\n }\n \n-fn read_to_eol(rdr: &StringReader) -> ~str {\n+fn read_to_eol(rdr: &mut StringReader) -> ~str {\n     let mut val = ~\"\";\n     while !rdr.curr_is('\\n') && !is_eof(rdr) {\n-        val.push_char(rdr.curr.get().unwrap());\n+        val.push_char(rdr.curr.unwrap());\n         bump(rdr);\n     }\n     if rdr.curr_is('\\n') { bump(rdr); }\n     return val;\n }\n \n-fn read_one_line_comment(rdr: &StringReader) -> ~str {\n+fn read_one_line_comment(rdr: &mut StringReader) -> ~str {\n     let val = read_to_eol(rdr);\n     assert!((val[0] == '/' as u8 && val[1] == '/' as u8) ||\n                  (val[0] == '#' as u8 && val[1] == '!' as u8));\n     return val;\n }\n \n-fn consume_non_eol_whitespace(rdr: &StringReader) {\n-    while is_whitespace(rdr.curr.get()) && !rdr.curr_is('\\n') &&\n-            !is_eof(rdr) {\n+fn consume_non_eol_whitespace(rdr: &mut StringReader) {\n+    while is_whitespace(rdr.curr) && !rdr.curr_is('\\n') && !is_eof(rdr) {\n         bump(rdr);\n     }\n }\n \n-fn push_blank_line_comment(rdr: &StringReader, comments: &mut Vec<Comment> ) {\n+fn push_blank_line_comment(rdr: &StringReader, comments: &mut Vec<Comment>) {\n     debug!(\">>> blank-line comment\");\n-    let v: Vec<~str> = Vec::new();\n     comments.push(Comment {\n         style: BlankLine,\n-        lines: v,\n-        pos: rdr.last_pos.get(),\n+        lines: Vec::new(),\n+        pos: rdr.last_pos,\n     });\n }\n \n-fn consume_whitespace_counting_blank_lines(rdr: &StringReader,\n-                                           comments: &mut Vec<Comment> ) {\n-    while is_whitespace(rdr.curr.get()) && !is_eof(rdr) {\n-        if rdr.col.get() == CharPos(0u) && rdr.curr_is('\\n') {\n+fn consume_whitespace_counting_blank_lines(rdr: &mut StringReader,\n+                                           comments: &mut Vec<Comment>) {\n+    while is_whitespace(rdr.curr) && !is_eof(rdr) {\n+        if rdr.col == CharPos(0u) && rdr.curr_is('\\n') {\n             push_blank_line_comment(rdr, &mut *comments);\n         }\n         bump(rdr);\n     }\n }\n \n \n-fn read_shebang_comment(rdr: &StringReader, code_to_the_left: bool,\n-                                            comments: &mut Vec<Comment> ) {\n+fn read_shebang_comment(rdr: &mut StringReader, code_to_the_left: bool,\n+                        comments: &mut Vec<Comment>) {\n     debug!(\">>> shebang comment\");\n-    let p = rdr.last_pos.get();\n+    let p = rdr.last_pos;\n     debug!(\"<<< shebang comment\");\n     comments.push(Comment {\n         style: if code_to_the_left { Trailing } else { Isolated },\n@@ -190,10 +188,10 @@ fn read_shebang_comment(rdr: &StringReader, code_to_the_left: bool,\n     });\n }\n \n-fn read_line_comments(rdr: &StringReader, code_to_the_left: bool,\n-                                          comments: &mut Vec<Comment> ) {\n+fn read_line_comments(rdr: &mut StringReader, code_to_the_left: bool,\n+                      comments: &mut Vec<Comment>) {\n     debug!(\">>> line comments\");\n-    let p = rdr.last_pos.get();\n+    let p = rdr.last_pos;\n     let mut lines: Vec<~str> = Vec::new();\n     while rdr.curr_is('/') && nextch_is(rdr, '/') {\n         let line = read_one_line_comment(rdr);\n@@ -247,13 +245,13 @@ fn trim_whitespace_prefix_and_push_line(lines: &mut Vec<~str> ,\n     lines.push(s1);\n }\n \n-fn read_block_comment(rdr: &StringReader,\n+fn read_block_comment(rdr: &mut StringReader,\n                       code_to_the_left: bool,\n                       comments: &mut Vec<Comment> ) {\n     debug!(\">>> block comment\");\n-    let p = rdr.last_pos.get();\n+    let p = rdr.last_pos;\n     let mut lines: Vec<~str> = Vec::new();\n-    let col: CharPos = rdr.col.get();\n+    let col = rdr.col;\n     bump(rdr);\n     bump(rdr);\n \n@@ -262,7 +260,7 @@ fn read_block_comment(rdr: &StringReader,\n     // doc-comments are not really comments, they are attributes\n     if (rdr.curr_is('*') && !nextch_is(rdr, '*')) || rdr.curr_is('!') {\n         while !(rdr.curr_is('*') && nextch_is(rdr, '/')) && !is_eof(rdr) {\n-            curr_line.push_char(rdr.curr.get().unwrap());\n+            curr_line.push_char(rdr.curr.unwrap());\n             bump(rdr);\n         }\n         if !is_eof(rdr) {\n@@ -286,7 +284,7 @@ fn read_block_comment(rdr: &StringReader,\n                 curr_line = ~\"\";\n                 bump(rdr);\n             } else {\n-                curr_line.push_char(rdr.curr.get().unwrap());\n+                curr_line.push_char(rdr.curr.unwrap());\n                 if rdr.curr_is('/') && nextch_is(rdr, '*') {\n                     bump(rdr);\n                     bump(rdr);\n@@ -324,7 +322,7 @@ fn peeking_at_comment(rdr: &StringReader) -> bool {\n           !lexer::nextnextch_is(rdr, '['));\n }\n \n-fn consume_comment(rdr: &StringReader,\n+fn consume_comment(rdr: &mut StringReader,\n                    code_to_the_left: bool,\n                    comments: &mut Vec<Comment> ) {\n     debug!(\">>> consume comment\");\n@@ -355,28 +353,28 @@ pub fn gather_comments_and_literals(span_diagnostic:\n     let src = str::from_utf8_owned(src).unwrap();\n     let cm = CodeMap::new();\n     let filemap = cm.new_filemap(path, src);\n-    let rdr = lexer::new_low_level_string_reader(span_diagnostic, filemap);\n+    let mut rdr = lexer::new_low_level_string_reader(span_diagnostic, filemap);\n \n     let mut comments: Vec<Comment> = Vec::new();\n     let mut literals: Vec<Literal> = Vec::new();\n     let mut first_read: bool = true;\n     while !is_eof(&rdr) {\n         loop {\n             let mut code_to_the_left = !first_read;\n-            consume_non_eol_whitespace(&rdr);\n+            consume_non_eol_whitespace(&mut rdr);\n             if rdr.curr_is('\\n') {\n                 code_to_the_left = false;\n-                consume_whitespace_counting_blank_lines(&rdr, &mut comments);\n+                consume_whitespace_counting_blank_lines(&mut rdr, &mut comments);\n             }\n             while peeking_at_comment(&rdr) {\n-                consume_comment(&rdr, code_to_the_left, &mut comments);\n-                consume_whitespace_counting_blank_lines(&rdr, &mut comments);\n+                consume_comment(&mut rdr, code_to_the_left, &mut comments);\n+                consume_whitespace_counting_blank_lines(&mut rdr, &mut comments);\n             }\n             break;\n         }\n \n \n-        let bstart = rdr.last_pos.get();\n+        let bstart = rdr.last_pos;\n         rdr.next_token();\n         //discard, and look ahead; we're working with internal state\n         let TokenAndSpan {tok: tok, sp: sp} = rdr.peek();"}, {"sha": "c18571deaf5bee85cf445eb99cdc1acb7058d4e8", "filename": "src/libsyntax/parse/lexer.rs", "status": "modified", "additions": 150, "deletions": 171, "changes": 321, "blob_url": "https://github.com/rust-lang/rust/blob/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fparse%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fparse%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer.rs?ref=7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "patch": "@@ -12,11 +12,10 @@ use ast;\n use codemap::{BytePos, CharPos, CodeMap, Pos, Span};\n use codemap;\n use diagnostic::SpanHandler;\n-use ext::tt::transcribe::{dup_tt_reader, tt_next_token};\n+use ext::tt::transcribe::tt_next_token;\n use parse::token;\n use parse::token::{str_to_ident};\n \n-use std::cell::{Cell, RefCell};\n use std::char;\n use std::mem::replace;\n use std::num::from_str_radix;\n@@ -27,11 +26,10 @@ pub use ext::tt::transcribe::{TtReader, new_tt_reader};\n \n pub trait Reader {\n     fn is_eof(&self) -> bool;\n-    fn next_token(&self) -> TokenAndSpan;\n+    fn next_token(&mut self) -> TokenAndSpan;\n     fn fatal(&self, ~str) -> !;\n     fn span_diag<'a>(&'a self) -> &'a SpanHandler;\n     fn peek(&self) -> TokenAndSpan;\n-    fn dup(&self) -> ~Reader:;\n }\n \n #[deriving(Clone, Eq, Show)]\n@@ -43,30 +41,30 @@ pub struct TokenAndSpan {\n pub struct StringReader<'a> {\n     span_diagnostic: &'a SpanHandler,\n     // The absolute offset within the codemap of the next character to read\n-    pos: Cell<BytePos>,\n+    pos: BytePos,\n     // The absolute offset within the codemap of the last character read(curr)\n-    last_pos: Cell<BytePos>,\n+    last_pos: BytePos,\n     // The column of the next character to read\n-    col: Cell<CharPos>,\n+    col: CharPos,\n     // The last character to be read\n-    curr: Cell<Option<char>>,\n+    curr: Option<char>,\n     filemap: Rc<codemap::FileMap>,\n     /* cached: */\n-    peek_tok: RefCell<token::Token>,\n-    peek_span: RefCell<Span>,\n+    peek_tok: token::Token,\n+    peek_span: Span,\n }\n \n impl<'a> StringReader<'a> {\n     pub fn curr_is(&self, c: char) -> bool {\n-        self.curr.get() == Some(c)\n+        self.curr == Some(c)\n     }\n }\n \n pub fn new_string_reader<'a>(span_diagnostic: &'a SpanHandler,\n                              filemap: Rc<codemap::FileMap>)\n                              -> StringReader<'a> {\n-    let r = new_low_level_string_reader(span_diagnostic, filemap);\n-    string_advance_token(&r); /* fill in peek_* */\n+    let mut r = new_low_level_string_reader(span_diagnostic, filemap);\n+    string_advance_token(&mut r); /* fill in peek_* */\n     r\n }\n \n@@ -76,97 +74,79 @@ pub fn new_low_level_string_reader<'a>(span_diagnostic: &'a SpanHandler,\n                                        -> StringReader<'a> {\n     // Force the initial reader bump to start on a fresh line\n     let initial_char = '\\n';\n-    let r = StringReader {\n+    let mut r = StringReader {\n         span_diagnostic: span_diagnostic,\n-        pos: Cell::new(filemap.start_pos),\n-        last_pos: Cell::new(filemap.start_pos),\n-        col: Cell::new(CharPos(0)),\n-        curr: Cell::new(Some(initial_char)),\n+        pos: filemap.start_pos,\n+        last_pos: filemap.start_pos,\n+        col: CharPos(0),\n+        curr: Some(initial_char),\n         filemap: filemap,\n         /* dummy values; not read */\n-        peek_tok: RefCell::new(token::EOF),\n-        peek_span: RefCell::new(codemap::DUMMY_SP),\n+        peek_tok: token::EOF,\n+        peek_span: codemap::DUMMY_SP,\n     };\n-    bump(&r);\n+    bump(&mut r);\n     r\n }\n \n-// duplicating the string reader is probably a bad idea, in\n-// that using them will cause interleaved pushes of line\n-// offsets to the underlying filemap...\n-fn dup_string_reader<'a>(r: &StringReader<'a>) -> StringReader<'a> {\n-    StringReader {\n-        span_diagnostic: r.span_diagnostic,\n-        pos: Cell::new(r.pos.get()),\n-        last_pos: Cell::new(r.last_pos.get()),\n-        col: Cell::new(r.col.get()),\n-        curr: Cell::new(r.curr.get()),\n-        filemap: r.filemap.clone(),\n-        peek_tok: r.peek_tok.clone(),\n-        peek_span: r.peek_span.clone(),\n-    }\n-}\n-\n impl<'a> Reader for StringReader<'a> {\n     fn is_eof(&self) -> bool { is_eof(self) }\n     // return the next token. EFFECT: advances the string_reader.\n-    fn next_token(&self) -> TokenAndSpan {\n+    fn next_token(&mut self) -> TokenAndSpan {\n         let ret_val = TokenAndSpan {\n-            tok: replace(&mut *self.peek_tok.borrow_mut(), token::UNDERSCORE),\n-            sp: self.peek_span.get(),\n+            tok: replace(&mut self.peek_tok, token::UNDERSCORE),\n+            sp: self.peek_span,\n         };\n         string_advance_token(self);\n         ret_val\n     }\n     fn fatal(&self, m: ~str) -> ! {\n-        self.span_diagnostic.span_fatal(self.peek_span.get(), m)\n+        self.span_diagnostic.span_fatal(self.peek_span, m)\n     }\n     fn span_diag<'a>(&'a self) -> &'a SpanHandler { self.span_diagnostic }\n     fn peek(&self) -> TokenAndSpan {\n         // FIXME(pcwalton): Bad copy!\n         TokenAndSpan {\n-            tok: self.peek_tok.get(),\n-            sp: self.peek_span.get(),\n+            tok: self.peek_tok.clone(),\n+            sp: self.peek_span.clone(),\n         }\n     }\n-    fn dup(&self) -> ~Reader: { ~dup_string_reader(self) as ~Reader: }\n }\n \n impl<'a> Reader for TtReader<'a> {\n     fn is_eof(&self) -> bool {\n-        *self.cur_tok.borrow() == token::EOF\n+        self.cur_tok == token::EOF\n     }\n-    fn next_token(&self) -> TokenAndSpan {\n+    fn next_token(&mut self) -> TokenAndSpan {\n         let r = tt_next_token(self);\n         debug!(\"TtReader: r={:?}\", r);\n-        return r;\n+        r\n     }\n     fn fatal(&self, m: ~str) -> ! {\n-        self.sp_diag.span_fatal(self.cur_span.get(), m);\n+        self.sp_diag.span_fatal(self.cur_span, m);\n     }\n     fn span_diag<'a>(&'a self) -> &'a SpanHandler { self.sp_diag }\n     fn peek(&self) -> TokenAndSpan {\n         TokenAndSpan {\n-            tok: self.cur_tok.get(),\n-            sp: self.cur_span.get(),\n+            tok: self.cur_tok.clone(),\n+            sp: self.cur_span.clone(),\n         }\n     }\n-    fn dup(&self) -> ~Reader: { ~dup_tt_reader(self) as ~Reader: }\n }\n \n // report a lexical error spanning [`from_pos`, `to_pos`)\n-fn fatal_span(rdr: &StringReader,\n+fn fatal_span(rdr: &mut StringReader,\n               from_pos: BytePos,\n               to_pos: BytePos,\n               m: ~str)\n            -> ! {\n-    rdr.peek_span.set(codemap::mk_sp(from_pos, to_pos));\n+    rdr.peek_span = codemap::mk_sp(from_pos, to_pos);\n     rdr.fatal(m);\n }\n \n // report a lexical error spanning [`from_pos`, `to_pos`), appending an\n // escaped character to the error message\n-fn fatal_span_char(rdr: &StringReader,\n+fn fatal_span_char(rdr: &mut StringReader,\n                    from_pos: BytePos,\n                    to_pos: BytePos,\n                    m: ~str,\n@@ -180,36 +160,35 @@ fn fatal_span_char(rdr: &StringReader,\n \n // report a lexical error spanning [`from_pos`, `to_pos`), appending the\n // offending string to the error message\n-fn fatal_span_verbose(rdr: &StringReader,\n+fn fatal_span_verbose(rdr: &mut StringReader,\n                       from_pos: BytePos,\n                       to_pos: BytePos,\n                       m: ~str)\n                    -> ! {\n     let mut m = m;\n     m.push_str(\": \");\n-    let s = rdr.filemap.src.slice(\n-                  byte_offset(rdr, from_pos).to_uint(),\n-                  byte_offset(rdr, to_pos).to_uint());\n-    m.push_str(s);\n+    let from = byte_offset(rdr, from_pos).to_uint();\n+    let to = byte_offset(rdr, to_pos).to_uint();\n+    m.push_str(rdr.filemap.src.slice(from, to));\n     fatal_span(rdr, from_pos, to_pos, m);\n }\n \n // EFFECT: advance peek_tok and peek_span to refer to the next token.\n // EFFECT: update the interner, maybe.\n-fn string_advance_token(r: &StringReader) {\n+fn string_advance_token(r: &mut StringReader) {\n     match consume_whitespace_and_comments(r) {\n         Some(comment) => {\n-            r.peek_span.set(comment.sp);\n-            r.peek_tok.set(comment.tok);\n+            r.peek_span = comment.sp;\n+            r.peek_tok = comment.tok;\n         },\n         None => {\n             if is_eof(r) {\n-                r.peek_tok.set(token::EOF);\n+                r.peek_tok = token::EOF;\n             } else {\n-                let start_bytepos = r.last_pos.get();\n-                r.peek_tok.set(next_token_inner(r));\n-                r.peek_span.set(codemap::mk_sp(start_bytepos,\n-                                               r.last_pos.get()));\n+                let start_bytepos = r.last_pos;\n+                r.peek_tok = next_token_inner(r);\n+                r.peek_span = codemap::mk_sp(start_bytepos,\n+                                             r.last_pos);\n             };\n         }\n     }\n@@ -227,7 +206,7 @@ pub fn with_str_from<T>(\n                      start: BytePos,\n                      f: |s: &str| -> T)\n                      -> T {\n-    with_str_from_to(rdr, start, rdr.last_pos.get(), f)\n+    with_str_from_to(rdr, start, rdr.last_pos, f)\n }\n \n /// Calls `f` with astring slice of the source text spanning from `start`\n@@ -245,36 +224,36 @@ fn with_str_from_to<T>(\n \n // EFFECT: advance the StringReader by one character. If a newline is\n // discovered, add it to the FileMap's list of line start offsets.\n-pub fn bump(rdr: &StringReader) {\n-    rdr.last_pos.set(rdr.pos.get());\n-    let current_byte_offset = byte_offset(rdr, rdr.pos.get()).to_uint();\n+pub fn bump(rdr: &mut StringReader) {\n+    rdr.last_pos = rdr.pos;\n+    let current_byte_offset = byte_offset(rdr, rdr.pos).to_uint();\n     if current_byte_offset < rdr.filemap.src.len() {\n-        assert!(rdr.curr.get().is_some());\n-        let last_char = rdr.curr.get().unwrap();\n+        assert!(rdr.curr.is_some());\n+        let last_char = rdr.curr.unwrap();\n         let next = rdr.filemap.src.char_range_at(current_byte_offset);\n         let byte_offset_diff = next.next - current_byte_offset;\n-        rdr.pos.set(rdr.pos.get() + Pos::from_uint(byte_offset_diff));\n-        rdr.curr.set(Some(next.ch));\n-        rdr.col.set(rdr.col.get() + CharPos(1u));\n+        rdr.pos = rdr.pos + Pos::from_uint(byte_offset_diff);\n+        rdr.curr = Some(next.ch);\n+        rdr.col = rdr.col + CharPos(1u);\n         if last_char == '\\n' {\n-            rdr.filemap.next_line(rdr.last_pos.get());\n-            rdr.col.set(CharPos(0u));\n+            rdr.filemap.next_line(rdr.last_pos);\n+            rdr.col = CharPos(0u);\n         }\n \n         if byte_offset_diff > 1 {\n-            rdr.filemap.record_multibyte_char(rdr.last_pos.get(), byte_offset_diff);\n+            rdr.filemap.record_multibyte_char(rdr.last_pos, byte_offset_diff);\n         }\n     } else {\n-        rdr.curr.set(None);\n+        rdr.curr = None;\n     }\n }\n \n pub fn is_eof(rdr: &StringReader) -> bool {\n-    rdr.curr.get().is_none()\n+    rdr.curr.is_none()\n }\n \n pub fn nextch(rdr: &StringReader) -> Option<char> {\n-    let offset = byte_offset(rdr, rdr.pos.get()).to_uint();\n+    let offset = byte_offset(rdr, rdr.pos).to_uint();\n     if offset < rdr.filemap.src.len() {\n         Some(rdr.filemap.src.char_at(offset))\n     } else {\n@@ -286,7 +265,7 @@ pub fn nextch_is(rdr: &StringReader, c: char) -> bool {\n }\n \n pub fn nextnextch(rdr: &StringReader) -> Option<char> {\n-    let offset = byte_offset(rdr, rdr.pos.get()).to_uint();\n+    let offset = byte_offset(rdr, rdr.pos).to_uint();\n     let s = rdr.filemap.deref().src.as_slice();\n     if offset >= s.len() { return None }\n     let str::CharRange { next, .. } = s.char_range_at(offset);\n@@ -332,9 +311,9 @@ fn is_hex_digit(c: Option<char>) -> bool {\n \n // EFFECT: eats whitespace and comments.\n // returns a Some(sugared-doc-attr) if one exists, None otherwise.\n-fn consume_whitespace_and_comments(rdr: &StringReader)\n+fn consume_whitespace_and_comments(rdr: &mut StringReader)\n                                 -> Option<TokenAndSpan> {\n-    while is_whitespace(rdr.curr.get()) { bump(rdr); }\n+    while is_whitespace(rdr.curr) { bump(rdr); }\n     return consume_any_line_comment(rdr);\n }\n \n@@ -345,7 +324,7 @@ pub fn is_line_non_doc_comment(s: &str) -> bool {\n // PRECONDITION: rdr.curr is not whitespace\n // EFFECT: eats any kind of comment.\n // returns a Some(sugared-doc-attr) if one exists, None otherwise\n-fn consume_any_line_comment(rdr: &StringReader)\n+fn consume_any_line_comment(rdr: &mut StringReader)\n                          -> Option<TokenAndSpan> {\n     if rdr.curr_is('/') {\n         match nextch(rdr) {\n@@ -354,7 +333,7 @@ fn consume_any_line_comment(rdr: &StringReader)\n                 bump(rdr);\n                 // line comments starting with \"///\" or \"//!\" are doc-comments\n                 if rdr.curr_is('/') || rdr.curr_is('!') {\n-                    let start_bpos = rdr.pos.get() - BytePos(3);\n+                    let start_bpos = rdr.pos - BytePos(3);\n                     while !rdr.curr_is('\\n') && !is_eof(rdr) {\n                         bump(rdr);\n                     }\n@@ -363,7 +342,7 @@ fn consume_any_line_comment(rdr: &StringReader)\n                         if !is_line_non_doc_comment(string) {\n                             Some(TokenAndSpan{\n                                 tok: token::DOC_COMMENT(str_to_ident(string)),\n-                                sp: codemap::mk_sp(start_bpos, rdr.pos.get())\n+                                sp: codemap::mk_sp(start_bpos, rdr.pos)\n                             })\n                         } else {\n                             None\n@@ -394,7 +373,7 @@ fn consume_any_line_comment(rdr: &StringReader)\n             // we're at the beginning of the file...\n             let cmap = CodeMap::new();\n             cmap.files.borrow_mut().push(rdr.filemap.clone());\n-            let loc = cmap.lookup_char_pos_adj(rdr.last_pos.get());\n+            let loc = cmap.lookup_char_pos_adj(rdr.last_pos);\n             if loc.line == 1u && loc.col == CharPos(0u) {\n                 while !rdr.curr_is('\\n') && !is_eof(rdr) { bump(rdr); }\n                 return consume_whitespace_and_comments(rdr);\n@@ -411,10 +390,10 @@ pub fn is_block_non_doc_comment(s: &str) -> bool {\n }\n \n // might return a sugared-doc-attr\n-fn consume_block_comment(rdr: &StringReader) -> Option<TokenAndSpan> {\n+fn consume_block_comment(rdr: &mut StringReader) -> Option<TokenAndSpan> {\n     // block comments starting with \"/**\" or \"/*!\" are doc-comments\n     let is_doc_comment = rdr.curr_is('*') || rdr.curr_is('!');\n-    let start_bpos = rdr.pos.get() - BytePos(if is_doc_comment {3} else {2});\n+    let start_bpos = rdr.pos - BytePos(if is_doc_comment {3} else {2});\n \n     let mut level: int = 1;\n     while level > 0 {\n@@ -424,7 +403,7 @@ fn consume_block_comment(rdr: &StringReader) -> Option<TokenAndSpan> {\n             } else {\n                 ~\"unterminated block comment\"\n             };\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(), msg);\n+            fatal_span(rdr, start_bpos, rdr.last_pos, msg);\n         } else if rdr.curr_is('/') && nextch_is(rdr, '*') {\n             level += 1;\n             bump(rdr);\n@@ -444,7 +423,7 @@ fn consume_block_comment(rdr: &StringReader) -> Option<TokenAndSpan> {\n             if !is_block_non_doc_comment(string) {\n                 Some(TokenAndSpan{\n                         tok: token::DOC_COMMENT(str_to_ident(string)),\n-                        sp: codemap::mk_sp(start_bpos, rdr.pos.get())\n+                        sp: codemap::mk_sp(start_bpos, rdr.pos)\n                     })\n             } else {\n                 None\n@@ -458,14 +437,14 @@ fn consume_block_comment(rdr: &StringReader) -> Option<TokenAndSpan> {\n     if res.is_some() { res } else { consume_whitespace_and_comments(rdr) }\n }\n \n-fn scan_exponent(rdr: &StringReader, start_bpos: BytePos) -> Option<~str> {\n+fn scan_exponent(rdr: &mut StringReader, start_bpos: BytePos) -> Option<~str> {\n     // \\x00 hits the `return None` case immediately, so this is fine.\n-    let mut c = rdr.curr.get().unwrap_or('\\x00');\n+    let mut c = rdr.curr.unwrap_or('\\x00');\n     let mut rslt = ~\"\";\n     if c == 'e' || c == 'E' {\n         rslt.push_char(c);\n         bump(rdr);\n-        c = rdr.curr.get().unwrap_or('\\x00');\n+        c = rdr.curr.unwrap_or('\\x00');\n         if c == '-' || c == '+' {\n             rslt.push_char(c);\n             bump(rdr);\n@@ -474,16 +453,16 @@ fn scan_exponent(rdr: &StringReader, start_bpos: BytePos) -> Option<~str> {\n         if exponent.len() > 0u {\n             return Some(rslt + exponent);\n         } else {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"scan_exponent: bad fp literal\");\n         }\n     } else { return None::<~str>; }\n }\n \n-fn scan_digits(rdr: &StringReader, radix: uint) -> ~str {\n+fn scan_digits(rdr: &mut StringReader, radix: uint) -> ~str {\n     let mut rslt = ~\"\";\n     loop {\n-        let c = rdr.curr.get();\n+        let c = rdr.curr;\n         if c == Some('_') { bump(rdr); continue; }\n         match c.and_then(|cc| char::to_digit(cc, radix)) {\n           Some(_) => {\n@@ -495,7 +474,7 @@ fn scan_digits(rdr: &StringReader, radix: uint) -> ~str {\n     };\n }\n \n-fn check_float_base(rdr: &StringReader, start_bpos: BytePos, last_bpos: BytePos,\n+fn check_float_base(rdr: &mut StringReader, start_bpos: BytePos, last_bpos: BytePos,\n                     base: uint) {\n     match base {\n       16u => fatal_span(rdr, start_bpos, last_bpos,\n@@ -508,12 +487,12 @@ fn check_float_base(rdr: &StringReader, start_bpos: BytePos, last_bpos: BytePos,\n     }\n }\n \n-fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n+fn scan_number(c: char, rdr: &mut StringReader) -> token::Token {\n     let mut num_str;\n     let mut base = 10u;\n     let mut c = c;\n     let mut n = nextch(rdr).unwrap_or('\\x00');\n-    let start_bpos = rdr.last_pos.get();\n+    let start_bpos = rdr.last_pos;\n     if c == '0' && n == 'x' {\n         bump(rdr);\n         bump(rdr);\n@@ -528,7 +507,7 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n         base = 2u;\n     }\n     num_str = scan_digits(rdr, base);\n-    c = rdr.curr.get().unwrap_or('\\x00');\n+    c = rdr.curr.unwrap_or('\\x00');\n     nextch(rdr);\n     if c == 'u' || c == 'i' {\n         enum Result { Signed(ast::IntTy), Unsigned(ast::UintTy) }\n@@ -538,7 +517,7 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n             else { Unsigned(ast::TyU) }\n         };\n         bump(rdr);\n-        c = rdr.curr.get().unwrap_or('\\x00');\n+        c = rdr.curr.unwrap_or('\\x00');\n         if c == '8' {\n             bump(rdr);\n             tp = if signed { Signed(ast::TyI8) }\n@@ -562,12 +541,12 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n                       else { Unsigned(ast::TyU64) };\n         }\n         if num_str.len() == 0u {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"no valid digits found for number\");\n         }\n         let parsed = match from_str_radix::<u64>(num_str, base as uint) {\n             Some(p) => p,\n-            None => fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            None => fatal_span(rdr, start_bpos, rdr.last_pos,\n                                ~\"int literal is too large\")\n         };\n \n@@ -594,37 +573,37 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n \n     if rdr.curr_is('f') {\n         bump(rdr);\n-        c = rdr.curr.get().unwrap_or('\\x00');\n+        c = rdr.curr.unwrap_or('\\x00');\n         n = nextch(rdr).unwrap_or('\\x00');\n         if c == '3' && n == '2' {\n             bump(rdr);\n             bump(rdr);\n-            check_float_base(rdr, start_bpos, rdr.last_pos.get(), base);\n+            check_float_base(rdr, start_bpos, rdr.last_pos, base);\n             return token::LIT_FLOAT(str_to_ident(num_str), ast::TyF32);\n         } else if c == '6' && n == '4' {\n             bump(rdr);\n             bump(rdr);\n-            check_float_base(rdr, start_bpos, rdr.last_pos.get(), base);\n+            check_float_base(rdr, start_bpos, rdr.last_pos, base);\n             return token::LIT_FLOAT(str_to_ident(num_str), ast::TyF64);\n             /* FIXME (#2252): if this is out of range for either a\n             32-bit or 64-bit float, it won't be noticed till the\n             back-end.  */\n         } else {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"expected `f32` or `f64` suffix\");\n         }\n     }\n     if is_float {\n-        check_float_base(rdr, start_bpos, rdr.last_pos.get(), base);\n+        check_float_base(rdr, start_bpos, rdr.last_pos, base);\n         return token::LIT_FLOAT_UNSUFFIXED(str_to_ident(num_str));\n     } else {\n         if num_str.len() == 0u {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"no valid digits found for number\");\n         }\n         let parsed = match from_str_radix::<u64>(num_str, base as uint) {\n             Some(p) => p,\n-            None => fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            None => fatal_span(rdr, start_bpos, rdr.last_pos,\n                                ~\"int literal is too large\")\n         };\n \n@@ -633,14 +612,14 @@ fn scan_number(c: char, rdr: &StringReader) -> token::Token {\n     }\n }\n \n-fn scan_numeric_escape(rdr: &StringReader, n_hex_digits: uint) -> char {\n+fn scan_numeric_escape(rdr: &mut StringReader, n_hex_digits: uint) -> char {\n     let mut accum_int = 0;\n     let mut i = n_hex_digits;\n-    let start_bpos = rdr.last_pos.get();\n+    let start_bpos = rdr.last_pos;\n     while i != 0u && !is_eof(rdr) {\n-        let n = rdr.curr.get();\n+        let n = rdr.curr;\n         if !is_hex_digit(n) {\n-            fatal_span_char(rdr, rdr.last_pos.get(), rdr.pos.get(),\n+            fatal_span_char(rdr, rdr.last_pos, rdr.pos,\n                             ~\"illegal character in numeric character escape\",\n                             n.unwrap());\n         }\n@@ -650,13 +629,13 @@ fn scan_numeric_escape(rdr: &StringReader, n_hex_digits: uint) -> char {\n         i -= 1u;\n     }\n     if i != 0 && is_eof(rdr) {\n-        fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+        fatal_span(rdr, start_bpos, rdr.last_pos,\n                    ~\"unterminated numeric character escape\");\n     }\n \n     match char::from_u32(accum_int as u32) {\n         Some(x) => x,\n-        None => fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+        None => fatal_span(rdr, start_bpos, rdr.last_pos,\n                            ~\"illegal numeric character escape\")\n     }\n }\n@@ -683,14 +662,14 @@ fn ident_continue(c: Option<char>) -> bool {\n // return the next token from the string\n // EFFECT: advances the input past that token\n // EFFECT: updates the interner\n-fn next_token_inner(rdr: &StringReader) -> token::Token {\n-    let c = rdr.curr.get();\n+fn next_token_inner(rdr: &mut StringReader) -> token::Token {\n+    let c = rdr.curr;\n     if ident_start(c) && !nextch_is(rdr, '\"') && !nextch_is(rdr, '#') {\n         // Note: r as in r\" or r#\" is part of a raw string literal,\n         // not an identifier, and is handled further down.\n \n-        let start = rdr.last_pos.get();\n-        while ident_continue(rdr.curr.get()) {\n+        let start = rdr.last_pos;\n+        while ident_continue(rdr.curr) {\n             bump(rdr);\n         }\n \n@@ -708,7 +687,7 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n     if is_dec_digit(c) {\n         return scan_number(c.unwrap(), rdr);\n     }\n-    fn binop(rdr: &StringReader, op: token::BinOp) -> token::Token {\n+    fn binop(rdr: &mut StringReader, op: token::BinOp) -> token::Token {\n         bump(rdr);\n         if rdr.curr_is('=') {\n             bump(rdr);\n@@ -783,12 +762,12 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n       }\n       '<' => {\n         bump(rdr);\n-        match rdr.curr.get().unwrap_or('\\x00') {\n+        match rdr.curr.unwrap_or('\\x00') {\n           '=' => { bump(rdr); return token::LE; }\n           '<' => { return binop(rdr, token::SHL); }\n           '-' => {\n             bump(rdr);\n-            match rdr.curr.get().unwrap_or('\\x00') {\n+            match rdr.curr.unwrap_or('\\x00') {\n               '>' => { bump(rdr); return token::DARROW; }\n               _ => { return token::LARROW; }\n             }\n@@ -798,7 +777,7 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n       }\n       '>' => {\n         bump(rdr);\n-        match rdr.curr.get().unwrap_or('\\x00') {\n+        match rdr.curr.unwrap_or('\\x00') {\n           '=' => { bump(rdr); return token::GE; }\n           '>' => { return binop(rdr, token::SHR); }\n           _ => { return token::GT; }\n@@ -807,41 +786,41 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n       '\\'' => {\n         // Either a character constant 'a' OR a lifetime name 'abc\n         bump(rdr);\n-        let start = rdr.last_pos.get();\n+        let start = rdr.last_pos;\n \n         // the eof will be picked up by the final `'` check below\n-        let mut c2 = rdr.curr.get().unwrap_or('\\x00');\n+        let mut c2 = rdr.curr.unwrap_or('\\x00');\n         bump(rdr);\n \n         // If the character is an ident start not followed by another single\n         // quote, then this is a lifetime name:\n         if ident_start(Some(c2)) && !rdr.curr_is('\\'') {\n-            while ident_continue(rdr.curr.get()) {\n+            while ident_continue(rdr.curr) {\n                 bump(rdr);\n             }\n-            return with_str_from(rdr, start, |lifetime_name| {\n-                let ident = str_to_ident(lifetime_name);\n-                let tok = &token::IDENT(ident, false);\n-\n-                if token::is_keyword(token::keywords::Self, tok) {\n-                    fatal_span(rdr, start, rdr.last_pos.get(),\n-                               ~\"invalid lifetime name: 'self is no longer a special lifetime\");\n-                } else if token::is_any_keyword(tok) &&\n-                    !token::is_keyword(token::keywords::Static, tok) {\n-                    fatal_span(rdr, start, rdr.last_pos.get(),\n-                               ~\"invalid lifetime name\");\n-                } else {\n-                    token::LIFETIME(ident)\n-                }\n-            })\n+            let ident = with_str_from(rdr, start, |lifetime_name| {\n+                str_to_ident(lifetime_name)\n+            });\n+            let tok = &token::IDENT(ident, false);\n+\n+            if token::is_keyword(token::keywords::Self, tok) {\n+                fatal_span(rdr, start, rdr.last_pos,\n+                           ~\"invalid lifetime name: 'self is no longer a special lifetime\");\n+            } else if token::is_any_keyword(tok) &&\n+                !token::is_keyword(token::keywords::Static, tok) {\n+                fatal_span(rdr, start, rdr.last_pos,\n+                           ~\"invalid lifetime name\");\n+            } else {\n+                return token::LIFETIME(ident);\n+            }\n         }\n \n         // Otherwise it is a character constant:\n         match c2 {\n             '\\\\' => {\n                 // '\\X' for some X must be a character constant:\n-                let escaped = rdr.curr.get();\n-                let escaped_pos = rdr.last_pos.get();\n+                let escaped = rdr.curr;\n+                let escaped_pos = rdr.last_pos;\n                 bump(rdr);\n                 match escaped {\n                     None => {}\n@@ -858,15 +837,15 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n                             'u' => scan_numeric_escape(rdr, 4u),\n                             'U' => scan_numeric_escape(rdr, 8u),\n                             c2 => {\n-                                fatal_span_char(rdr, escaped_pos, rdr.last_pos.get(),\n+                                fatal_span_char(rdr, escaped_pos, rdr.last_pos,\n                                                 ~\"unknown character escape\", c2)\n                             }\n                         }\n                     }\n                 }\n             }\n             '\\t' | '\\n' | '\\r' | '\\'' => {\n-                fatal_span_char(rdr, start, rdr.last_pos.get(),\n+                fatal_span_char(rdr, start, rdr.last_pos,\n                                 ~\"character constant must be escaped\", c2);\n             }\n             _ => {}\n@@ -877,33 +856,33 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n                                // character before position `start` is an\n                                // ascii single quote.\n                                start - BytePos(1),\n-                               rdr.last_pos.get(),\n+                               rdr.last_pos,\n                                ~\"unterminated character constant\");\n         }\n         bump(rdr); // advance curr past token\n         return token::LIT_CHAR(c2 as u32);\n       }\n       '\"' => {\n         let mut accum_str = ~\"\";\n-        let start_bpos = rdr.last_pos.get();\n+        let start_bpos = rdr.last_pos;\n         bump(rdr);\n         while !rdr.curr_is('\"') {\n             if is_eof(rdr) {\n-                fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+                fatal_span(rdr, start_bpos, rdr.last_pos,\n                            ~\"unterminated double quote string\");\n             }\n \n-            let ch = rdr.curr.get().unwrap();\n+            let ch = rdr.curr.unwrap();\n             bump(rdr);\n             match ch {\n               '\\\\' => {\n                 if is_eof(rdr) {\n-                    fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+                    fatal_span(rdr, start_bpos, rdr.last_pos,\n                            ~\"unterminated double quote string\");\n                 }\n \n-                let escaped = rdr.curr.get().unwrap();\n-                let escaped_pos = rdr.last_pos.get();\n+                let escaped = rdr.curr.unwrap();\n+                let escaped_pos = rdr.last_pos;\n                 bump(rdr);\n                 match escaped {\n                   'n' => accum_str.push_char('\\n'),\n@@ -924,7 +903,7 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n                     accum_str.push_char(scan_numeric_escape(rdr, 8u));\n                   }\n                   c2 => {\n-                    fatal_span_char(rdr, escaped_pos, rdr.last_pos.get(),\n+                    fatal_span_char(rdr, escaped_pos, rdr.last_pos,\n                                     ~\"unknown string escape\", c2);\n                   }\n                 }\n@@ -936,7 +915,7 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n         return token::LIT_STR(str_to_ident(accum_str));\n       }\n       'r' => {\n-        let start_bpos = rdr.last_pos.get();\n+        let start_bpos = rdr.last_pos;\n         bump(rdr);\n         let mut hash_count = 0u;\n         while rdr.curr_is('#') {\n@@ -945,24 +924,24 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n         }\n \n         if is_eof(rdr) {\n-            fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span(rdr, start_bpos, rdr.last_pos,\n                        ~\"unterminated raw string\");\n         } else if !rdr.curr_is('\"') {\n-            fatal_span_char(rdr, start_bpos, rdr.last_pos.get(),\n+            fatal_span_char(rdr, start_bpos, rdr.last_pos,\n                             ~\"only `#` is allowed in raw string delimitation; \\\n                               found illegal character\",\n-                            rdr.curr.get().unwrap());\n+                            rdr.curr.unwrap());\n         }\n         bump(rdr);\n-        let content_start_bpos = rdr.last_pos.get();\n+        let content_start_bpos = rdr.last_pos;\n         let mut content_end_bpos;\n         'outer: loop {\n             if is_eof(rdr) {\n-                fatal_span(rdr, start_bpos, rdr.last_pos.get(),\n+                fatal_span(rdr, start_bpos, rdr.last_pos,\n                            ~\"unterminated raw string\");\n             }\n             if rdr.curr_is('\"') {\n-                content_end_bpos = rdr.last_pos.get();\n+                content_end_bpos = rdr.last_pos;\n                 for _ in range(0, hash_count) {\n                     bump(rdr);\n                     if !rdr.curr_is('#') {\n@@ -1006,14 +985,14 @@ fn next_token_inner(rdr: &StringReader) -> token::Token {\n       '^' => { return binop(rdr, token::CARET); }\n       '%' => { return binop(rdr, token::PERCENT); }\n       c => {\n-          fatal_span_char(rdr, rdr.last_pos.get(), rdr.pos.get(),\n+          fatal_span_char(rdr, rdr.last_pos, rdr.pos,\n                           ~\"unknown start of token\", c);\n       }\n     }\n }\n \n-fn consume_whitespace(rdr: &StringReader) {\n-    while is_whitespace(rdr.curr.get()) && !is_eof(rdr) { bump(rdr); }\n+fn consume_whitespace(rdr: &mut StringReader) {\n+    while is_whitespace(rdr.curr) && !is_eof(rdr) { bump(rdr); }\n }\n \n #[cfg(test)]\n@@ -1041,7 +1020,7 @@ mod test {\n \n     #[test] fn t1 () {\n         let span_handler = mk_sh();\n-        let string_reader = setup(&span_handler,\n+        let mut string_reader = setup(&span_handler,\n             ~\"/* my source file */ \\\n               fn main() { println!(\\\"zebra\\\"); }\\n\");\n         let id = str_to_ident(\"fn\");\n@@ -1051,20 +1030,20 @@ mod test {\n             sp:Span {lo:BytePos(21),hi:BytePos(23),expn_info: None}};\n         assert_eq!(tok1,tok2);\n         // the 'main' id is already read:\n-        assert_eq!(string_reader.last_pos.get().clone(), BytePos(28));\n+        assert_eq!(string_reader.last_pos.clone(), BytePos(28));\n         // read another token:\n         let tok3 = string_reader.next_token();\n         let tok4 = TokenAndSpan{\n             tok:token::IDENT(str_to_ident(\"main\"), false),\n             sp:Span {lo:BytePos(24),hi:BytePos(28),expn_info: None}};\n         assert_eq!(tok3,tok4);\n         // the lparen is already read:\n-        assert_eq!(string_reader.last_pos.get().clone(), BytePos(29))\n+        assert_eq!(string_reader.last_pos.clone(), BytePos(29))\n     }\n \n     // check that the given reader produces the desired stream\n     // of tokens (stop checking after exhausting the expected vec)\n-    fn check_tokenization (string_reader: StringReader, expected: Vec<token::Token> ) {\n+    fn check_tokenization (mut string_reader: StringReader, expected: Vec<token::Token> ) {\n         for expected_tok in expected.iter() {\n             assert_eq!(&string_reader.next_token().tok, expected_tok);\n         }"}, {"sha": "8038baebdcf5a72cadec708161f14f102bb67ed7", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7cf4d8bc446177204e9e12b1efb199a5dbc956b5/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=7cf4d8bc446177204e9e12b1efb199a5dbc956b5", "patch": "@@ -274,7 +274,7 @@ struct ParsedItemsAndViewItems {\n \n /* ident is handled by common.rs */\n \n-pub fn Parser<'a>(sess: &'a ParseSess, cfg: ast::CrateConfig, rdr: ~Reader:)\n+pub fn Parser<'a>(sess: &'a ParseSess, cfg: ast::CrateConfig, mut rdr: ~Reader:)\n               -> Parser<'a> {\n     let tok0 = rdr.next_token();\n     let span = tok0.sp;"}]}