{"sha": "68341a91eecb830c57779cd2423733a7395258ab", "node_id": "MDY6Q29tbWl0NzI0NzEyOjY4MzQxYTkxZWVjYjgzMGM1Nzc3OWNkMjQyMzczM2E3Mzk1MjU4YWI=", "commit": {"author": {"name": "Joshua Lockerman", "email": "j@Js-MacBook-Air.home", "date": "2017-09-29T19:58:11Z"}, "committer": {"name": "Joshua Lockerman", "email": "j@Js-MacBook-Air.home", "date": "2017-10-01T16:15:35Z"}, "message": "Improve performance of spsc_queue and stream.\nThis commit makes two main changes.\n1. It switches the spsc_queue node caching strategy from keeping a shared\ncounter of the number of nodes in the cache to keeping a consumer only counter\nof the number of node eligible to be cached.\n2. It separate the consumer and producers fields of spsc_queue and stream into\na producer cache line and consumer cache line.", "tree": {"sha": "851726d9a606ab4ffd49b719f741dd856e09dfd1", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/851726d9a606ab4ffd49b719f741dd856e09dfd1"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/68341a91eecb830c57779cd2423733a7395258ab", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/68341a91eecb830c57779cd2423733a7395258ab", "html_url": "https://github.com/rust-lang/rust/commit/68341a91eecb830c57779cd2423733a7395258ab", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/68341a91eecb830c57779cd2423733a7395258ab/comments", "author": null, "committer": null, "parents": [{"sha": "0e6f4cf51cd3b799fb057956f8e733d16605d09b", "url": "https://api.github.com/repos/rust-lang/rust/commits/0e6f4cf51cd3b799fb057956f8e733d16605d09b", "html_url": "https://github.com/rust-lang/rust/commit/0e6f4cf51cd3b799fb057956f8e733d16605d09b"}], "stats": {"total": 307, "additions": 208, "deletions": 99}, "files": [{"sha": "83cc9ce582e34b15d3377518b153d6ae343f904d", "filename": "src/libstd/lib.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Flib.rs?ref=68341a91eecb830c57779cd2423733a7395258ab", "patch": "@@ -244,6 +244,7 @@\n #![feature(allow_internal_unstable)]\n #![feature(align_offset)]\n #![feature(asm)]\n+#![feature(attr_literals)]\n #![feature(box_syntax)]\n #![feature(cfg_target_has_atomic)]\n #![feature(cfg_target_thread_local)]\n@@ -290,6 +291,7 @@\n #![feature(prelude_import)]\n #![feature(rand)]\n #![feature(raw)]\n+#![feature(repr_align)]\n #![feature(repr_simd)]\n #![feature(rustc_attrs)]\n #![cfg_attr(not(stage0), feature(rustc_const_unstable))]"}, {"sha": "5af01262573f3b93133abadf5c5293f2d4ffa198", "filename": "src/libstd/sync/mpsc/cache_aligned.rs", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "blob_url": "https://github.com/rust-lang/rust/blob/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Fsync%2Fmpsc%2Fcache_aligned.rs", "raw_url": "https://github.com/rust-lang/rust/raw/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Fsync%2Fmpsc%2Fcache_aligned.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fmpsc%2Fcache_aligned.rs?ref=68341a91eecb830c57779cd2423733a7395258ab", "patch": "@@ -0,0 +1,37 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+use ops::{Deref, DerefMut};\n+\n+#[derive(Copy, Clone, Default, PartialEq, Eq, PartialOrd, Ord, Hash)]\n+#[repr(align(64))]\n+pub(super) struct Aligner;\n+\n+#[derive(Copy, Clone, Default, PartialEq, Eq, PartialOrd, Ord, Hash)]\n+pub(super) struct CacheAligned<T>(pub T, pub Aligner);\n+\n+impl<T> Deref for CacheAligned<T> {\n+     type Target = T;\n+     fn deref(&self) -> &Self::Target {\n+         &self.0\n+     }\n+}\n+\n+impl<T> DerefMut for CacheAligned<T> {\n+     fn deref_mut(&mut self) -> &mut Self::Target {\n+         &mut self.0\n+     }\n+}\n+\n+impl<T> CacheAligned<T> {\n+    pub(super) fn new(t: T) -> Self {\n+        CacheAligned(t, Aligner)\n+    }\n+}"}, {"sha": "0bfbcd2d2cd5943bd2a2ff4352ae219cb8a64e95", "filename": "src/libstd/sync/mpsc/mod.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Fsync%2Fmpsc%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Fsync%2Fmpsc%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fmpsc%2Fmod.rs?ref=68341a91eecb830c57779cd2423733a7395258ab", "patch": "@@ -297,6 +297,8 @@ mod sync;\n mod mpsc_queue;\n mod spsc_queue;\n \n+mod cache_aligned;\n+\n /// The receiving half of Rust's [`channel`][] (or [`sync_channel`]) type.\n /// This half can only be owned by one thread.\n ///"}, {"sha": "3ce592703358192ef48d731a9c751628522d067b", "filename": "src/libstd/sync/mpsc/spsc_queue.rs", "status": "modified", "additions": 106, "deletions": 55, "changes": 161, "blob_url": "https://github.com/rust-lang/rust/blob/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Fsync%2Fmpsc%2Fspsc_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Fsync%2Fmpsc%2Fspsc_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fmpsc%2Fspsc_queue.rs?ref=68341a91eecb830c57779cd2423733a7395258ab", "patch": "@@ -22,50 +22,61 @@ use core::cell::UnsafeCell;\n \n use sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\n \n+use super::cache_aligned::CacheAligned;\n+\n // Node within the linked list queue of messages to send\n struct Node<T> {\n     // FIXME: this could be an uninitialized T if we're careful enough, and\n     //      that would reduce memory usage (and be a bit faster).\n     //      is it worth it?\n     value: Option<T>,           // nullable for re-use of nodes\n+    cached: bool,               // This node goes into the node cache\n     next: AtomicPtr<Node<T>>,   // next node in the queue\n }\n \n /// The single-producer single-consumer queue. This structure is not cloneable,\n /// but it can be safely shared in an Arc if it is guaranteed that there\n /// is only one popper and one pusher touching the queue at any one point in\n /// time.\n-pub struct Queue<T> {\n+pub struct Queue<T, ProducerAddition=(), ConsumerAddition=()> {\n     // consumer fields\n+    consumer: CacheAligned<Consumer<T, ConsumerAddition>>,\n+\n+    // producer fields\n+    producer: CacheAligned<Producer<T, ProducerAddition>>,\n+}\n+\n+struct Consumer<T, Addition> {\n     tail: UnsafeCell<*mut Node<T>>, // where to pop from\n     tail_prev: AtomicPtr<Node<T>>, // where to pop from\n+    cache_bound: usize, // maximum cache size\n+    cached_nodes: AtomicUsize, // number of nodes marked as cachable\n+    addition: Addition,\n+}\n \n-    // producer fields\n+struct Producer<T, Addition> {\n     head: UnsafeCell<*mut Node<T>>,      // where to push to\n     first: UnsafeCell<*mut Node<T>>,     // where to get new nodes from\n     tail_copy: UnsafeCell<*mut Node<T>>, // between first/tail\n-\n-    // Cache maintenance fields. Additions and subtractions are stored\n-    // separately in order to allow them to use nonatomic addition/subtraction.\n-    cache_bound: usize,\n-    cache_additions: AtomicUsize,\n-    cache_subtractions: AtomicUsize,\n+    addition: Addition,\n }\n \n-unsafe impl<T: Send> Send for Queue<T> { }\n+unsafe impl<T: Send, P: Send + Sync, C: Send + Sync> Send for Queue<T, P, C> { }\n \n-unsafe impl<T: Send> Sync for Queue<T> { }\n+unsafe impl<T: Send, P: Send + Sync, C: Send + Sync> Sync for Queue<T, P, C> { }\n \n impl<T> Node<T> {\n     fn new() -> *mut Node<T> {\n         Box::into_raw(box Node {\n             value: None,\n+            cached: false,\n             next: AtomicPtr::new(ptr::null_mut::<Node<T>>()),\n         })\n     }\n }\n \n impl<T> Queue<T> {\n+    #[cfg(test)]\n     /// Creates a new queue.\n     ///\n     /// This is unsafe as the type system doesn't enforce a single\n@@ -84,18 +95,60 @@ impl<T> Queue<T> {\n     ///               no bound. Otherwise, the cache will never grow larger than\n     ///               `bound` (although the queue itself could be much larger.\n     pub unsafe fn new(bound: usize) -> Queue<T> {\n+        Self::with_additions(bound, (), ())\n+    }\n+}\n+\n+impl<T, ProducerAddition, ConsumerAddition> Queue<T, ProducerAddition, ConsumerAddition> {\n+\n+    /// Creates a new queue. With given additional elements in the producer and\n+    /// consumer portions of the queue.\n+    ///\n+    /// Due to the performance implications of cache-contention,\n+    /// we wish to keep fields used mainly by the producer on a separate cache\n+    /// line than those used by the consumer.\n+    /// Since cache lines are usually 64 bytes, it is unreasonably expensive to\n+    /// allocate one for small fields, so we allow users to insert additional\n+    /// fields into the cache lines already allocated by this for the producer\n+    /// and consumer.\n+    ///\n+    /// This is unsafe as the type system doesn't enforce a single\n+    /// consumer-producer relationship. It also allows the consumer to `pop`\n+    /// items while there is a `peek` active due to all methods having a\n+    /// non-mutable receiver.\n+    ///\n+    /// # Arguments\n+    ///\n+    ///   * `bound` - This queue implementation is implemented with a linked\n+    ///               list, and this means that a push is always a malloc. In\n+    ///               order to amortize this cost, an internal cache of nodes is\n+    ///               maintained to prevent a malloc from always being\n+    ///               necessary. This bound is the limit on the size of the\n+    ///               cache (if desired). If the value is 0, then the cache has\n+    ///               no bound. Otherwise, the cache will never grow larger than\n+    ///               `bound` (although the queue itself could be much larger.\n+    pub unsafe fn with_additions(\n+        bound: usize,\n+        producer_addition: ProducerAddition,\n+        consumer_addition: ConsumerAddition,\n+    ) -> Self {\n         let n1 = Node::new();\n         let n2 = Node::new();\n         (*n1).next.store(n2, Ordering::Relaxed);\n         Queue {\n-            tail: UnsafeCell::new(n2),\n-            tail_prev: AtomicPtr::new(n1),\n-            head: UnsafeCell::new(n2),\n-            first: UnsafeCell::new(n1),\n-            tail_copy: UnsafeCell::new(n1),\n-            cache_bound: bound,\n-            cache_additions: AtomicUsize::new(0),\n-            cache_subtractions: AtomicUsize::new(0),\n+            consumer: CacheAligned::new(Consumer {\n+                tail: UnsafeCell::new(n2),\n+                tail_prev: AtomicPtr::new(n1),\n+                cache_bound: bound,\n+                cached_nodes: AtomicUsize::new(0),\n+                addition: consumer_addition\n+            }),\n+            producer: CacheAligned::new(Producer {\n+                head: UnsafeCell::new(n2),\n+                first: UnsafeCell::new(n1),\n+                tail_copy: UnsafeCell::new(n1),\n+                addition: producer_addition\n+            }),\n         }\n     }\n \n@@ -109,35 +162,25 @@ impl<T> Queue<T> {\n             assert!((*n).value.is_none());\n             (*n).value = Some(t);\n             (*n).next.store(ptr::null_mut(), Ordering::Relaxed);\n-            (**self.head.get()).next.store(n, Ordering::Release);\n-            *self.head.get() = n;\n+            (**self.producer.head.get()).next.store(n, Ordering::Release);\n+            *(&self.producer.head).get() = n;\n         }\n     }\n \n     unsafe fn alloc(&self) -> *mut Node<T> {\n         // First try to see if we can consume the 'first' node for our uses.\n-        // We try to avoid as many atomic instructions as possible here, so\n-        // the addition to cache_subtractions is not atomic (plus we're the\n-        // only one subtracting from the cache).\n-        if *self.first.get() != *self.tail_copy.get() {\n-            if self.cache_bound > 0 {\n-                let b = self.cache_subtractions.load(Ordering::Relaxed);\n-                self.cache_subtractions.store(b + 1, Ordering::Relaxed);\n-            }\n-            let ret = *self.first.get();\n-            *self.first.get() = (*ret).next.load(Ordering::Relaxed);\n+        if *self.producer.first.get() != *self.producer.tail_copy.get() {\n+            let ret = *self.producer.first.get();\n+            *self.producer.0.first.get() = (*ret).next.load(Ordering::Relaxed);\n             return ret;\n         }\n         // If the above fails, then update our copy of the tail and try\n         // again.\n-        *self.tail_copy.get() = self.tail_prev.load(Ordering::Acquire);\n-        if *self.first.get() != *self.tail_copy.get() {\n-            if self.cache_bound > 0 {\n-                let b = self.cache_subtractions.load(Ordering::Relaxed);\n-                self.cache_subtractions.store(b + 1, Ordering::Relaxed);\n-            }\n-            let ret = *self.first.get();\n-            *self.first.get() = (*ret).next.load(Ordering::Relaxed);\n+        *self.producer.0.tail_copy.get() =\n+            self.consumer.tail_prev.load(Ordering::Acquire);\n+        if *self.producer.first.get() != *self.producer.tail_copy.get() {\n+            let ret = *self.producer.first.get();\n+            *self.producer.0.first.get() = (*ret).next.load(Ordering::Relaxed);\n             return ret;\n         }\n         // If all of that fails, then we have to allocate a new node\n@@ -153,27 +196,27 @@ impl<T> Queue<T> {\n             // sentinel from where we should start popping from. Hence, look at\n             // tail's next field and see if we can use it. If we do a pop, then\n             // the current tail node is a candidate for going into the cache.\n-            let tail = *self.tail.get();\n+            let tail = *self.consumer.tail.get();\n             let next = (*tail).next.load(Ordering::Acquire);\n             if next.is_null() { return None }\n             assert!((*next).value.is_some());\n             let ret = (*next).value.take();\n \n-            *self.tail.get() = next;\n-            if self.cache_bound == 0 {\n-                self.tail_prev.store(tail, Ordering::Release);\n+            *self.consumer.0.tail.get() = next;\n+            if self.consumer.cache_bound == 0 {\n+                self.consumer.tail_prev.store(tail, Ordering::Release);\n             } else {\n-                // FIXME: this is dubious with overflow.\n-                let additions = self.cache_additions.load(Ordering::Relaxed);\n-                let subtractions = self.cache_subtractions.load(Ordering::Relaxed);\n-                let size = additions - subtractions;\n-\n-                if size < self.cache_bound {\n-                    self.tail_prev.store(tail, Ordering::Release);\n-                    self.cache_additions.store(additions + 1, Ordering::Relaxed);\n+                let cached_nodes = self.consumer.cached_nodes.load(Ordering::Relaxed);\n+                if cached_nodes < self.consumer.cache_bound && !(*tail).cached {\n+                    self.consumer.cached_nodes.store(cached_nodes, Ordering::Relaxed);\n+                    (*tail).cached = true;\n+                }\n+\n+                if (*tail).cached {\n+                    self.consumer.tail_prev.store(tail, Ordering::Release);\n                 } else {\n-                    (*self.tail_prev.load(Ordering::Relaxed))\n-                          .next.store(next, Ordering::Relaxed);\n+                    (*self.consumer.tail_prev.load(Ordering::Relaxed))\n+                        .next.store(next, Ordering::Relaxed);\n                     // We have successfully erased all references to 'tail', so\n                     // now we can safely drop it.\n                     let _: Box<Node<T>> = Box::from_raw(tail);\n@@ -194,17 +237,25 @@ impl<T> Queue<T> {\n         // This is essentially the same as above with all the popping bits\n         // stripped out.\n         unsafe {\n-            let tail = *self.tail.get();\n+            let tail = *self.consumer.tail.get();\n             let next = (*tail).next.load(Ordering::Acquire);\n             if next.is_null() { None } else { (*next).value.as_mut() }\n         }\n     }\n+\n+    pub fn producer_addition(&self) -> &ProducerAddition {\n+        &self.producer.addition\n+    }\n+\n+    pub fn consumer_addition(&self) -> &ConsumerAddition {\n+        &self.consumer.addition\n+    }\n }\n \n-impl<T> Drop for Queue<T> {\n+impl<T, ProducerAddition, ConsumerAddition> Drop for Queue<T, ProducerAddition, ConsumerAddition> {\n     fn drop(&mut self) {\n         unsafe {\n-            let mut cur = *self.first.get();\n+            let mut cur = *self.producer.first.get();\n             while !cur.is_null() {\n                 let next = (*cur).next.load(Ordering::Relaxed);\n                 let _n: Box<Node<T>> = Box::from_raw(cur);"}, {"sha": "d1515eba68c3eea8ffbf88af03db9661614134e5", "filename": "src/libstd/sync/mpsc/stream.rs", "status": "modified", "additions": 61, "deletions": 44, "changes": 105, "blob_url": "https://github.com/rust-lang/rust/blob/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Fsync%2Fmpsc%2Fstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/68341a91eecb830c57779cd2423733a7395258ab/src%2Flibstd%2Fsync%2Fmpsc%2Fstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fmpsc%2Fstream.rs?ref=68341a91eecb830c57779cd2423733a7395258ab", "patch": "@@ -41,15 +41,22 @@ const MAX_STEALS: isize = 5;\n const MAX_STEALS: isize = 1 << 20;\n \n pub struct Packet<T> {\n-    queue: spsc::Queue<Message<T>>, // internal queue for all message\n+    // internal queue for all messages\n+    queue: spsc::Queue<Message<T>, ProducerAddition, ConsumerAddition>,\n+}\n \n+struct ProducerAddition {\n     cnt: AtomicIsize, // How many items are on this channel\n-    steals: UnsafeCell<isize>, // How many times has a port received without blocking?\n     to_wake: AtomicUsize, // SignalToken for the blocked thread to wake up\n \n     port_dropped: AtomicBool, // flag if the channel has been destroyed.\n }\n \n+struct ConsumerAddition {\n+    steals: UnsafeCell<isize>,  // How many times has a port received without blocking?\n+}\n+\n+\n pub enum Failure<T> {\n     Empty,\n     Disconnected,\n@@ -78,21 +85,26 @@ enum Message<T> {\n impl<T> Packet<T> {\n     pub fn new() -> Packet<T> {\n         Packet {\n-            queue: unsafe { spsc::Queue::new(128) },\n-\n-            cnt: AtomicIsize::new(0),\n-            steals: UnsafeCell::new(0),\n-            to_wake: AtomicUsize::new(0),\n-\n-            port_dropped: AtomicBool::new(false),\n+            queue: unsafe { spsc::Queue::with_additions(\n+                128,\n+                ProducerAddition {\n+                    cnt: AtomicIsize::new(0),\n+                    to_wake: AtomicUsize::new(0),\n+\n+                    port_dropped: AtomicBool::new(false),\n+                },\n+                ConsumerAddition {\n+                    steals: UnsafeCell::new(0),\n+                }\n+            )},\n         }\n     }\n \n     pub fn send(&self, t: T) -> Result<(), T> {\n         // If the other port has deterministically gone away, then definitely\n         // must return the data back up the stack. Otherwise, the data is\n         // considered as being sent.\n-        if self.port_dropped.load(Ordering::SeqCst) { return Err(t) }\n+        if self.queue.producer_addition().port_dropped.load(Ordering::SeqCst) { return Err(t) }\n \n         match self.do_send(Data(t)) {\n             UpSuccess | UpDisconnected => {},\n@@ -104,14 +116,16 @@ impl<T> Packet<T> {\n     pub fn upgrade(&self, up: Receiver<T>) -> UpgradeResult {\n         // If the port has gone away, then there's no need to proceed any\n         // further.\n-        if self.port_dropped.load(Ordering::SeqCst) { return UpDisconnected }\n+        if self.queue.producer_addition().port_dropped.load(Ordering::SeqCst) {\n+            return UpDisconnected\n+        }\n \n         self.do_send(GoUp(up))\n     }\n \n     fn do_send(&self, t: Message<T>) -> UpgradeResult {\n         self.queue.push(t);\n-        match self.cnt.fetch_add(1, Ordering::SeqCst) {\n+        match self.queue.producer_addition().cnt.fetch_add(1, Ordering::SeqCst) {\n             // As described in the mod's doc comment, -1 == wakeup\n             -1 => UpWoke(self.take_to_wake()),\n             // As as described before, SPSC queues must be >= -2\n@@ -125,7 +139,7 @@ impl<T> Packet<T> {\n             // will never remove this data. We can only have at most one item to\n             // drain (the port drains the rest).\n             DISCONNECTED => {\n-                self.cnt.store(DISCONNECTED, Ordering::SeqCst);\n+                self.queue.producer_addition().cnt.store(DISCONNECTED, Ordering::SeqCst);\n                 let first = self.queue.pop();\n                 let second = self.queue.pop();\n                 assert!(second.is_none());\n@@ -144,8 +158,8 @@ impl<T> Packet<T> {\n \n     // Consumes ownership of the 'to_wake' field.\n     fn take_to_wake(&self) -> SignalToken {\n-        let ptr = self.to_wake.load(Ordering::SeqCst);\n-        self.to_wake.store(0, Ordering::SeqCst);\n+        let ptr = self.queue.producer_addition().to_wake.load(Ordering::SeqCst);\n+        self.queue.producer_addition().to_wake.store(0, Ordering::SeqCst);\n         assert!(ptr != 0);\n         unsafe { SignalToken::cast_from_usize(ptr) }\n     }\n@@ -154,14 +168,16 @@ impl<T> Packet<T> {\n     // back if it shouldn't sleep. Note that this is the location where we take\n     // steals into account.\n     fn decrement(&self, token: SignalToken) -> Result<(), SignalToken> {\n-        assert_eq!(self.to_wake.load(Ordering::SeqCst), 0);\n+        assert_eq!(self.queue.producer_addition().to_wake.load(Ordering::SeqCst), 0);\n         let ptr = unsafe { token.cast_to_usize() };\n-        self.to_wake.store(ptr, Ordering::SeqCst);\n+        self.queue.producer_addition().to_wake.store(ptr, Ordering::SeqCst);\n \n-        let steals = unsafe { ptr::replace(self.steals.get(), 0) };\n+        let steals = unsafe { ptr::replace(self.queue.consumer_addition().steals.get(), 0) };\n \n-        match self.cnt.fetch_sub(1 + steals, Ordering::SeqCst) {\n-            DISCONNECTED => { self.cnt.store(DISCONNECTED, Ordering::SeqCst); }\n+        match self.queue.producer_addition().cnt.fetch_sub(1 + steals, Ordering::SeqCst) {\n+            DISCONNECTED => {\n+                self.queue.producer_addition().cnt.store(DISCONNECTED, Ordering::SeqCst);\n+            }\n             // If we factor in our steals and notice that the channel has no\n             // data, we successfully sleep\n             n => {\n@@ -170,7 +186,7 @@ impl<T> Packet<T> {\n             }\n         }\n \n-        self.to_wake.store(0, Ordering::SeqCst);\n+        self.queue.producer_addition().to_wake.store(0, Ordering::SeqCst);\n         Err(unsafe { SignalToken::cast_from_usize(ptr) })\n     }\n \n@@ -201,7 +217,7 @@ impl<T> Packet<T> {\n             // \"steal\" factored into the channel count above).\n             data @ Ok(..) |\n             data @ Err(Upgraded(..)) => unsafe {\n-                *self.steals.get() -= 1;\n+                *self.queue.consumer_addition().steals.get() -= 1;\n                 data\n             },\n \n@@ -223,28 +239,29 @@ impl<T> Packet<T> {\n             // down as much as possible (without going negative), and then\n             // adding back in whatever we couldn't factor into steals.\n             Some(data) => unsafe {\n-                if *self.steals.get() > MAX_STEALS {\n-                    match self.cnt.swap(0, Ordering::SeqCst) {\n+                if *self.queue.consumer_addition().steals.get() > MAX_STEALS {\n+                    match self.queue.producer_addition().cnt.swap(0, Ordering::SeqCst) {\n                         DISCONNECTED => {\n-                            self.cnt.store(DISCONNECTED, Ordering::SeqCst);\n+                            self.queue.producer_addition().cnt.store(\n+                                DISCONNECTED, Ordering::SeqCst);\n                         }\n                         n => {\n-                            let m = cmp::min(n, *self.steals.get());\n-                            *self.steals.get() -= m;\n+                            let m = cmp::min(n, *self.queue.consumer_addition().steals.get());\n+                            *self.queue.consumer_addition().steals.get() -= m;\n                             self.bump(n - m);\n                         }\n                     }\n-                    assert!(*self.steals.get() >= 0);\n+                    assert!(*self.queue.consumer_addition().steals.get() >= 0);\n                 }\n-                *self.steals.get() += 1;\n+                *self.queue.consumer_addition().steals.get() += 1;\n                 match data {\n                     Data(t) => Ok(t),\n                     GoUp(up) => Err(Upgraded(up)),\n                 }\n             },\n \n             None => {\n-                match self.cnt.load(Ordering::SeqCst) {\n+                match self.queue.producer_addition().cnt.load(Ordering::SeqCst) {\n                     n if n != DISCONNECTED => Err(Empty),\n \n                     // This is a little bit of a tricky case. We failed to pop\n@@ -273,7 +290,7 @@ impl<T> Packet<T> {\n     pub fn drop_chan(&self) {\n         // Dropping a channel is pretty simple, we just flag it as disconnected\n         // and then wakeup a blocker if there is one.\n-        match self.cnt.swap(DISCONNECTED, Ordering::SeqCst) {\n+        match self.queue.producer_addition().cnt.swap(DISCONNECTED, Ordering::SeqCst) {\n             -1 => { self.take_to_wake().signal(); }\n             DISCONNECTED => {}\n             n => { assert!(n >= 0); }\n@@ -300,7 +317,7 @@ impl<T> Packet<T> {\n         // sends are gated on this flag, so we're immediately guaranteed that\n         // there are a bounded number of active sends that we'll have to deal\n         // with.\n-        self.port_dropped.store(true, Ordering::SeqCst);\n+        self.queue.producer_addition().port_dropped.store(true, Ordering::SeqCst);\n \n         // Now that we're guaranteed to deal with a bounded number of senders,\n         // we need to drain the queue. This draining process happens atomically\n@@ -310,9 +327,9 @@ impl<T> Packet<T> {\n         // continue to fail while active senders send data while we're dropping\n         // data, but eventually we're guaranteed to break out of this loop\n         // (because there is a bounded number of senders).\n-        let mut steals = unsafe { *self.steals.get() };\n+        let mut steals = unsafe { *self.queue.consumer_addition().steals.get() };\n         while {\n-            let cnt = self.cnt.compare_and_swap(\n+            let cnt = self.queue.producer_addition().cnt.compare_and_swap(\n                             steals, DISCONNECTED, Ordering::SeqCst);\n             cnt != DISCONNECTED && cnt != steals\n         } {\n@@ -353,9 +370,9 @@ impl<T> Packet<T> {\n \n     // increment the count on the channel (used for selection)\n     fn bump(&self, amt: isize) -> isize {\n-        match self.cnt.fetch_add(amt, Ordering::SeqCst) {\n+        match self.queue.producer_addition().cnt.fetch_add(amt, Ordering::SeqCst) {\n             DISCONNECTED => {\n-                self.cnt.store(DISCONNECTED, Ordering::SeqCst);\n+                self.queue.producer_addition().cnt.store(DISCONNECTED, Ordering::SeqCst);\n                 DISCONNECTED\n             }\n             n => n\n@@ -404,8 +421,8 @@ impl<T> Packet<T> {\n         // this end. This is fine because we know it's a small bounded windows\n         // of time until the data is actually sent.\n         if was_upgrade {\n-            assert_eq!(unsafe { *self.steals.get() }, 0);\n-            assert_eq!(self.to_wake.load(Ordering::SeqCst), 0);\n+            assert_eq!(unsafe { *self.queue.consumer_addition().steals.get() }, 0);\n+            assert_eq!(self.queue.producer_addition().to_wake.load(Ordering::SeqCst), 0);\n             return Ok(true)\n         }\n \n@@ -418,7 +435,7 @@ impl<T> Packet<T> {\n         // If we were previously disconnected, then we know for sure that there\n         // is no thread in to_wake, so just keep going\n         let has_data = if prev == DISCONNECTED {\n-            assert_eq!(self.to_wake.load(Ordering::SeqCst), 0);\n+            assert_eq!(self.queue.producer_addition().to_wake.load(Ordering::SeqCst), 0);\n             true // there is data, that data is that we're disconnected\n         } else {\n             let cur = prev + steals + 1;\n@@ -441,13 +458,13 @@ impl<T> Packet<T> {\n             if prev < 0 {\n                 drop(self.take_to_wake());\n             } else {\n-                while self.to_wake.load(Ordering::SeqCst) != 0 {\n+                while self.queue.producer_addition().to_wake.load(Ordering::SeqCst) != 0 {\n                     thread::yield_now();\n                 }\n             }\n             unsafe {\n-                assert_eq!(*self.steals.get(), 0);\n-                *self.steals.get() = steals;\n+                assert_eq!(*self.queue.consumer_addition().steals.get(), 0);\n+                *self.queue.consumer_addition().steals.get() = steals;\n             }\n \n             // if we were previously positive, then there's surely data to\n@@ -481,7 +498,7 @@ impl<T> Drop for Packet<T> {\n         // disconnection, but also a proper fence before the read of\n         // `to_wake`, so this assert cannot be removed with also removing\n         // the `to_wake` assert.\n-        assert_eq!(self.cnt.load(Ordering::SeqCst), DISCONNECTED);\n-        assert_eq!(self.to_wake.load(Ordering::SeqCst), 0);\n+        assert_eq!(self.queue.producer_addition().cnt.load(Ordering::SeqCst), DISCONNECTED);\n+        assert_eq!(self.queue.producer_addition().to_wake.load(Ordering::SeqCst), 0);\n     }\n }"}]}