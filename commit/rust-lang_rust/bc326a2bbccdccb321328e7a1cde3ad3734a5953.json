{"sha": "bc326a2bbccdccb321328e7a1cde3ad3734a5953", "node_id": "C_kwDOAAsO6NoAKGJjMzI2YTJiYmNjZGNjYjMyMTMyOGU3YTFjZGUzYWQzNzM0YTU5NTM", "commit": {"author": {"name": "Jubilee Young", "email": "workingjubilee@gmail.com", "date": "2021-12-22T02:28:57Z"}, "committer": {"name": "Jubilee Young", "email": "workingjubilee@gmail.com", "date": "2021-12-22T23:37:05Z"}, "message": "Refactor ops.rs with a recursive macro\n\nThis approaches reducing macro nesting in a slightly different way.\nInstead of just flattening details, make one macro apply another.\nThis allows specifying all details up-front in the first macro\ninvocation, making it easier to audit and refactor in the future.", "tree": {"sha": "22936fbe0123f0a67a5303de8a91c3f191b45854", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/22936fbe0123f0a67a5303de8a91c3f191b45854"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/bc326a2bbccdccb321328e7a1cde3ad3734a5953", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/bc326a2bbccdccb321328e7a1cde3ad3734a5953", "html_url": "https://github.com/rust-lang/rust/commit/bc326a2bbccdccb321328e7a1cde3ad3734a5953", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/bc326a2bbccdccb321328e7a1cde3ad3734a5953/comments", "author": {"login": "workingjubilee", "id": 46493976, "node_id": "MDQ6VXNlcjQ2NDkzOTc2", "avatar_url": "https://avatars.githubusercontent.com/u/46493976?v=4", "gravatar_id": "", "url": "https://api.github.com/users/workingjubilee", "html_url": "https://github.com/workingjubilee", "followers_url": "https://api.github.com/users/workingjubilee/followers", "following_url": "https://api.github.com/users/workingjubilee/following{/other_user}", "gists_url": "https://api.github.com/users/workingjubilee/gists{/gist_id}", "starred_url": "https://api.github.com/users/workingjubilee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/workingjubilee/subscriptions", "organizations_url": "https://api.github.com/users/workingjubilee/orgs", "repos_url": "https://api.github.com/users/workingjubilee/repos", "events_url": "https://api.github.com/users/workingjubilee/events{/privacy}", "received_events_url": "https://api.github.com/users/workingjubilee/received_events", "type": "User", "site_admin": false}, "committer": {"login": "workingjubilee", "id": 46493976, "node_id": "MDQ6VXNlcjQ2NDkzOTc2", "avatar_url": "https://avatars.githubusercontent.com/u/46493976?v=4", "gravatar_id": "", "url": "https://api.github.com/users/workingjubilee", "html_url": "https://github.com/workingjubilee", "followers_url": "https://api.github.com/users/workingjubilee/followers", "following_url": "https://api.github.com/users/workingjubilee/following{/other_user}", "gists_url": "https://api.github.com/users/workingjubilee/gists{/gist_id}", "starred_url": "https://api.github.com/users/workingjubilee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/workingjubilee/subscriptions", "organizations_url": "https://api.github.com/users/workingjubilee/orgs", "repos_url": "https://api.github.com/users/workingjubilee/repos", "events_url": "https://api.github.com/users/workingjubilee/events{/privacy}", "received_events_url": "https://api.github.com/users/workingjubilee/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "5dcd397f47a17aec3b049af2d7541530b859e47b", "url": "https://api.github.com/repos/rust-lang/rust/commits/5dcd397f47a17aec3b049af2d7541530b859e47b", "html_url": "https://github.com/rust-lang/rust/commit/5dcd397f47a17aec3b049af2d7541530b859e47b"}], "stats": {"total": 508, "additions": 147, "deletions": 361}, "files": [{"sha": "6cfc8f80b53c7327b2de63baf3e1d81b79aa5e0f", "filename": "crates/core_simd/src/ops.rs", "status": "modified", "additions": 147, "deletions": 361, "changes": 508, "blob_url": "https://github.com/rust-lang/rust/blob/bc326a2bbccdccb321328e7a1cde3ad3734a5953/crates%2Fcore_simd%2Fsrc%2Fops.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bc326a2bbccdccb321328e7a1cde3ad3734a5953/crates%2Fcore_simd%2Fsrc%2Fops.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fcore_simd%2Fsrc%2Fops.rs?ref=bc326a2bbccdccb321328e7a1cde3ad3734a5953", "patch": "@@ -31,27 +31,10 @@ where\n     }\n }\n \n-macro_rules! unsafe_base_op {\n-    ($(impl<const LANES: usize> $op:ident for Simd<$scalar:ty, LANES> {\n-        fn $call:ident(self, rhs: Self) -> Self::Output {\n-            unsafe{ $simd_call:ident }\n-        }\n-    })*) => {\n-        $(impl<const LANES: usize> $op for Simd<$scalar, LANES>\n-            where\n-                $scalar: SimdElement,\n-                LaneCount<LANES>: SupportedLaneCount,\n-            {\n-                type Output = Self;\n-\n-                #[inline]\n-                #[must_use = \"operator returns a new vector without mutating the inputs\"]\n-                fn $call(self, rhs: Self) -> Self::Output {\n-                    unsafe { $crate::intrinsics::$simd_call(self, rhs) }\n-                }\n-            }\n-        )*\n-    }\n+macro_rules! unsafe_base {\n+    ($lhs:ident, $rhs:ident, {$simd_call:ident}, $($_:tt)*) => {\n+        unsafe { $crate::intrinsics::$simd_call($lhs, $rhs) }\n+    };\n }\n \n /// SAFETY: This macro should not be used for anything except Shl or Shr, and passed the appropriate shift intrinsic.\n@@ -64,388 +47,191 @@ macro_rules! unsafe_base_op {\n // FIXME: Consider implementing this in cg_llvm instead?\n // cg_clif defaults to this, and scalar MIR shifts also default to wrapping\n macro_rules! wrap_bitshift {\n-    ($(impl<const LANES: usize> $op:ident for Simd<$int:ty, LANES> {\n-        fn $call:ident(self, rhs: Self) -> Self::Output {\n-            unsafe { $simd_call:ident }\n+    ($lhs:ident, $rhs:ident, {$simd_call:ident}, $int:ident) => {\n+        unsafe {\n+            $crate::intrinsics::$simd_call($lhs, $rhs.bitand(Simd::splat(<$int>::BITS as $int - 1)))\n         }\n-    })*) => {\n-        $(impl<const LANES: usize> $op for Simd<$int, LANES>\n-        where\n-            $int: SimdElement,\n-            LaneCount<LANES>: SupportedLaneCount,\n-        {\n-            type Output = Self;\n-\n-            #[inline]\n-            #[must_use = \"operator returns a new vector without mutating the inputs\"]\n-            fn $call(self, rhs: Self) -> Self::Output {\n-                unsafe {\n-                    $crate::intrinsics::$simd_call(self, rhs.bitand(Simd::splat(<$int>::BITS as $int - 1)))\n-                }\n-            }\n-        })*\n     };\n }\n \n-macro_rules! bitops {\n-    ($(impl<const LANES: usize> BitOps for Simd<$int:ty, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n-     })*) => {\n-        $(\n-            unsafe_base_op!{\n-                impl<const LANES: usize> BitAnd for Simd<$int, LANES> {\n-                    fn bitand(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_and }\n-                    }\n-                }\n-\n-                impl<const LANES: usize> BitOr for Simd<$int, LANES> {\n-                    fn bitor(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_or }\n-                    }\n-                }\n-\n-                impl<const LANES: usize> BitXor for Simd<$int, LANES> {\n-                    fn bitxor(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_xor }\n-                    }\n-                }\n-            }\n-            wrap_bitshift! {\n-                impl<const LANES: usize> Shl for Simd<$int, LANES> {\n-                    fn shl(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_shl }\n-                    }\n-                }\n-\n-                impl<const LANES: usize> Shr for Simd<$int, LANES> {\n-                    fn shr(self, rhs: Self) -> Self::Output {\n-                        // This automatically monomorphizes to lshr or ashr, depending,\n-                        // so it's fine to use it for both UInts and SInts.\n-                        unsafe { simd_shr }\n-                    }\n-                }\n-            }\n-        )*\n+// Division by zero is poison, according to LLVM.\n+// So is dividing the MIN value of a signed integer by -1,\n+// since that would return MAX + 1.\n+// FIXME: Rust allows <SInt>::MIN / -1,\n+// so we should probably figure out how to make that safe.\n+macro_rules! int_divrem_guard {\n+    (   $lhs:ident,\n+        $rhs:ident,\n+        {   const PANIC_ZERO: &'static str = $zero:literal;\n+            const PANIC_OVERFLOW: &'static str = $overflow:literal;\n+            $simd_call:ident\n+        },\n+        $int:ident ) => {\n+        if $rhs.lanes_eq(Simd::splat(0)).any() {\n+            panic!($zero);\n+        } else if <$int>::MIN != 0\n+            && $lhs.lanes_eq(Simd::splat(<$int>::MIN)) & $rhs.lanes_eq(Simd::splat(-1 as _))\n+                != Mask::splat(false)\n+        {\n+            panic!($overflow);\n+        } else {\n+            unsafe { $crate::intrinsics::$simd_call($lhs, $rhs) }\n+        }\n     };\n }\n \n-// Integers can always accept bitand, bitor, and bitxor.\n-// The only question is how to handle shifts >= <Int>::BITS?\n-// Our current solution uses wrapping logic.\n-bitops! {\n-    impl<const LANES: usize> BitOps for Simd<i8, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+macro_rules! for_base_types {\n+    (   T = ($($scalar:ident),*);\n+        type Lhs = Simd<T, N>;\n+        type Rhs = Simd<T, N>;\n+        type Output = $out:ty;\n+\n+        impl $op:ident::$call:ident {\n+            $macro_impl:ident $inner:tt\n+        }) => {\n+            $(\n+                impl<const N: usize> $op<Self> for Simd<$scalar, N>\n+                where\n+                    $scalar: SimdElement,\n+                    LaneCount<N>: SupportedLaneCount,\n+                {\n+                    type Output = $out;\n+\n+                    #[inline]\n+                    #[must_use = \"operator returns a new vector without mutating the inputs\"]\n+                    fn $call(self, rhs: Self) -> Self::Output {\n+                        $macro_impl!(self, rhs, $inner, $scalar)\n+                    }\n+                })*\n     }\n+}\n \n-    impl<const LANES: usize> BitOps for Simd<i16, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+// A \"TokenTree muncher\": takes a set of scalar types `T = {};`\n+// type parameters for the ops it implements, `Op::fn` names,\n+// and a macro that expands into an expr, substituting in an intrinsic.\n+// It passes that to for_base_types, which expands an impl for the types,\n+// using the expanded expr in the function, and recurses with itself.\n+//\n+// tl;dr impls a set of ops::{Traits} for a set of types\n+macro_rules! for_base_ops {\n+    (\n+        T = $types:tt;\n+        type Lhs = Simd<T, N>;\n+        type Rhs = Simd<T, N>;\n+        type Output = $out:ident;\n+        impl $op:ident::$call:ident\n+            $inner:tt\n+        $($rest:tt)*\n+    ) => {\n+        for_base_types! {\n+            T = $types;\n+            type Lhs = Simd<T, N>;\n+            type Rhs = Simd<T, N>;\n+            type Output = $out;\n+            impl $op::$call\n+                $inner\n+        }\n+        for_base_ops! {\n+            T = $types;\n+            type Lhs = Simd<T, N>;\n+            type Rhs = Simd<T, N>;\n+            type Output = $out;\n+            $($rest)*\n+        }\n+    };\n+    ($($done:tt)*) => {\n+        // Done.\n     }\n+}\n \n-    impl<const LANES: usize> BitOps for Simd<i32, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n-    }\n+// Integers can always accept add, mul, sub, bitand, bitor, and bitxor.\n+// For all of these operations, simd_* intrinsics apply wrapping logic.\n+for_base_ops! {\n+    T = (i8, i16, i32, i64, isize, u8, u16, u32, u64, usize);\n+    type Lhs = Simd<T, N>;\n+    type Rhs = Simd<T, N>;\n+    type Output = Self;\n \n-    impl<const LANES: usize> BitOps for Simd<i64, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+    impl Add::add {\n+        unsafe_base { simd_add }\n     }\n \n-    impl<const LANES: usize> BitOps for Simd<isize, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+    impl Mul::mul {\n+        unsafe_base { simd_mul }\n     }\n \n-    impl<const LANES: usize> BitOps for Simd<u8, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+    impl Sub::sub {\n+        unsafe_base { simd_sub }\n     }\n \n-    impl<const LANES: usize> BitOps for Simd<u16, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+    impl BitAnd::bitand {\n+        unsafe_base { simd_and }\n     }\n \n-    impl<const LANES: usize> BitOps for Simd<u32, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+    impl BitOr::bitor {\n+        unsafe_base { simd_or }\n     }\n \n-    impl<const LANES: usize> BitOps for Simd<u64, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+    impl BitXor::bitxor {\n+        unsafe_base { simd_xor }\n     }\n \n-    impl<const LANES: usize> BitOps for Simd<usize, LANES> {\n-        fn bitand(self, rhs: Self) -> Self::Output;\n-        fn bitor(self, rhs: Self) -> Self::Output;\n-        fn bitxor(self, rhs: Self) -> Self::Output;\n-        fn shl(self, rhs: Self) -> Self::Output;\n-        fn shr(self, rhs: Self) -> Self::Output;\n+    impl Div::div {\n+        int_divrem_guard {\n+            const PANIC_ZERO: &'static str = \"attempt to divide by zero\";\n+            const PANIC_OVERFLOW: &'static str = \"attempt to divide with overflow\";\n+            simd_div\n+        }\n     }\n-}\n-\n-macro_rules! float_arith {\n-    ($(impl<const LANES: usize> FloatArith for Simd<$float:ty, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n-     })*) => {\n-        $(\n-            unsafe_base_op!{\n-                impl<const LANES: usize> Add for Simd<$float, LANES> {\n-                    fn add(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_add }\n-                    }\n-                }\n-\n-                impl<const LANES: usize> Mul for Simd<$float, LANES> {\n-                    fn mul(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_mul }\n-                    }\n-                }\n-\n-                impl<const LANES: usize> Sub for Simd<$float, LANES> {\n-                    fn sub(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_sub }\n-                    }\n-                }\n \n-                impl<const LANES: usize> Div for Simd<$float, LANES> {\n-                    fn div(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_div }\n-                    }\n-                }\n-\n-                impl<const LANES: usize> Rem for Simd<$float, LANES> {\n-                    fn rem(self, rhs: Self) -> Self::Output {\n-                        unsafe { simd_rem }\n-                    }\n-                }\n-            }\n-        )*\n-    };\n-}\n-\n-// We don't need any special precautions here:\n-// Floats always accept arithmetic ops, but may become NaN.\n-float_arith! {\n-    impl<const LANES: usize> FloatArith for Simd<f32, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n+    impl Rem::rem {\n+        int_divrem_guard {\n+            const PANIC_ZERO: &'static str = \"attempt to calculate the remainder with a divisor of zero\";\n+            const PANIC_OVERFLOW: &'static str = \"attempt to calculate the remainder with overflow\";\n+            simd_rem\n+        }\n     }\n \n-    impl<const LANES: usize> FloatArith for Simd<f64, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n+    // The only question is how to handle shifts >= <Int>::BITS?\n+    // Our current solution uses wrapping logic.\n+    impl Shl::shl {\n+        wrap_bitshift { simd_shl }\n     }\n-}\n-\n-// Division by zero is poison, according to LLVM.\n-// So is dividing the MIN value of a signed integer by -1,\n-// since that would return MAX + 1.\n-// FIXME: Rust allows <SInt>::MIN / -1,\n-// so we should probably figure out how to make that safe.\n-macro_rules! int_divrem_guard {\n-    ($(impl<const LANES: usize> $op:ident for Simd<$sint:ty, LANES> {\n-        const PANIC_ZERO: &'static str = $zero:literal;\n-        const PANIC_OVERFLOW: &'static str = $overflow:literal;\n-        fn $call:ident {\n-            unsafe { $simd_call:ident }\n-        }\n-    })*) => {\n-        $(impl<const LANES: usize> $op for Simd<$sint, LANES>\n-        where\n-            $sint: SimdElement,\n-            LaneCount<LANES>: SupportedLaneCount,\n-        {\n-            type Output = Self;\n-            #[inline]\n-            #[must_use = \"operator returns a new vector without mutating the inputs\"]\n-            fn $call(self, rhs: Self) -> Self::Output {\n-                if rhs.lanes_eq(Simd::splat(0)).any() {\n-                    panic!(\"attempt to calculate the remainder with a divisor of zero\");\n-                } else if <$sint>::MIN != 0 && self.lanes_eq(Simd::splat(<$sint>::MIN)) & rhs.lanes_eq(Simd::splat(-1 as _))\n-                    != Mask::splat(false)\n-                 {\n-                    panic!(\"attempt to calculate the remainder with overflow\");\n-                } else {\n-                    unsafe { $crate::intrinsics::$simd_call(self, rhs) }\n-                 }\n-             }\n-        })*\n-    };\n-}\n-\n-macro_rules! int_arith {\n-    ($(impl<const LANES: usize> IntArith for Simd<$sint:ty, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n-    })*) => {\n-        $(\n-        unsafe_base_op!{\n-            impl<const LANES: usize> Add for Simd<$sint, LANES> {\n-                fn add(self, rhs: Self) -> Self::Output {\n-                    unsafe { simd_add }\n-                }\n-            }\n-\n-            impl<const LANES: usize> Mul for Simd<$sint, LANES> {\n-                fn mul(self, rhs: Self) -> Self::Output {\n-                    unsafe { simd_mul }\n-                }\n-            }\n \n-            impl<const LANES: usize> Sub for Simd<$sint, LANES> {\n-                fn sub(self, rhs: Self) -> Self::Output {\n-                    unsafe { simd_sub }\n-                }\n-            }\n+    impl Shr::shr {\n+        wrap_bitshift {\n+            // This automatically monomorphizes to lshr or ashr, depending,\n+            // so it's fine to use it for both UInts and SInts.\n+            simd_shr\n         }\n-\n-        int_divrem_guard!{\n-            impl<const LANES: usize> Div for Simd<$sint, LANES> {\n-                const PANIC_ZERO: &'static str = \"attempt to divide by zero\";\n-                const PANIC_OVERFLOW: &'static str = \"attempt to divide with overflow\";\n-                fn div {\n-                    unsafe { simd_div }\n-                }\n-            }\n-\n-            impl<const LANES: usize> Rem for Simd<$sint, LANES> {\n-                const PANIC_ZERO: &'static str = \"attempt to calculate the remainder with a divisor of zero\";\n-                const PANIC_OVERFLOW: &'static str = \"attempt to calculate the remainder with overflow\";\n-                fn rem {\n-                    unsafe { simd_rem }\n-                }\n-            }\n-        })*\n     }\n }\n \n-int_arith! {\n-    impl<const LANES: usize> IntArith for Simd<i8, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n-    }\n-\n-    impl<const LANES: usize> IntArith for Simd<i16, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n-    }\n-\n-    impl<const LANES: usize> IntArith for Simd<i32, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n-    }\n-\n-    impl<const LANES: usize> IntArith for Simd<i64, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n-    }\n-\n-    impl<const LANES: usize> IntArith for Simd<isize, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n-    }\n+// We don't need any special precautions here:\n+// Floats always accept arithmetic ops, but may become NaN.\n+for_base_ops! {\n+    T = (f32, f64);\n+    type Lhs = Simd<T, N>;\n+    type Rhs = Simd<T, N>;\n+    type Output = Self;\n \n-    impl<const LANES: usize> IntArith for Simd<u8, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n+    impl Add::add {\n+        unsafe_base { simd_add }\n     }\n \n-    impl<const LANES: usize> IntArith for Simd<u16, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n+    impl Mul::mul {\n+        unsafe_base { simd_mul }\n     }\n \n-    impl<const LANES: usize> IntArith for Simd<u32, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n+    impl Sub::sub {\n+        unsafe_base { simd_sub }\n     }\n \n-    impl<const LANES: usize> IntArith for Simd<u64, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n+    impl Div::div {\n+        unsafe_base { simd_div }\n     }\n \n-    impl<const LANES: usize> IntArith for Simd<usize, LANES> {\n-        fn add(self, rhs: Self) -> Self::Output;\n-        fn mul(self, rhs: Self) -> Self::Output;\n-        fn sub(self, rhs: Self) -> Self::Output;\n-        fn div(self, rhs: Self) -> Self::Output;\n-        fn rem(self, rhs: Self) -> Self::Output;\n+    impl Rem::rem {\n+        unsafe_base { simd_rem }\n     }\n }"}]}