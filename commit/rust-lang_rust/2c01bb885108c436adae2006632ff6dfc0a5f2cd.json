{"sha": "2c01bb885108c436adae2006632ff6dfc0a5f2cd", "node_id": "MDY6Q29tbWl0NzI0NzEyOjJjMDFiYjg4NTEwOGM0MzZhZGFlMjAwNjYzMmZmNmRmYzBhNWYyY2Q=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2016-08-31T19:56:15Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2016-08-31T19:56:15Z"}, "message": "Auto merge of #35718 - michaelwoerister:incr-comp-dir-locking, r=alexcrichton\n\nImplement synchronization scheme for incr. comp. directory\n\nThis PR implements a copy-on-write-based synchronization scheme for the incremental compilation cache directory. For technical details, see the documentation at the beginning of `rustc_incremental/persist/fs.rs`.\n\nThe PR contains unit tests for some functions but for testing whether the scheme properly handles races, a more elaborate test setup would be needed. It would probably involve a small tool that allows to manipulate the incremental compilation directory in a controlled way and then letting a compiler instance run against directories in different states. I don't know if it's worth the trouble of adding another test category to `compiletest`, but I'd be happy to do so.\n\nFixes #32754\nFixes #34957", "tree": {"sha": "096e830596ea574c97955f2c2081974721399b96", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/096e830596ea574c97955f2c2081974721399b96"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/2c01bb885108c436adae2006632ff6dfc0a5f2cd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/2c01bb885108c436adae2006632ff6dfc0a5f2cd", "html_url": "https://github.com/rust-lang/rust/commit/2c01bb885108c436adae2006632ff6dfc0a5f2cd", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/2c01bb885108c436adae2006632ff6dfc0a5f2cd/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "7a187c39c79e21cd61d1688d449bdd52d7510281", "url": "https://api.github.com/repos/rust-lang/rust/commits/7a187c39c79e21cd61d1688d449bdd52d7510281", "html_url": "https://github.com/rust-lang/rust/commit/7a187c39c79e21cd61d1688d449bdd52d7510281"}, {"sha": "bcd2f905c46158f9137fa5b63aafebcb60083385", "url": "https://api.github.com/repos/rust-lang/rust/commits/bcd2f905c46158f9137fa5b63aafebcb60083385", "html_url": "https://github.com/rust-lang/rust/commit/bcd2f905c46158f9137fa5b63aafebcb60083385"}], "stats": {"total": 1637, "additions": 1459, "deletions": 178}, "files": [{"sha": "a915d07384f3cd712dc012af7e5684e1ddb61c92", "filename": "mk/crates.mk", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/mk%2Fcrates.mk", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/mk%2Fcrates.mk", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/mk%2Fcrates.mk?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -112,7 +112,7 @@ DEPS_rustc := syntax fmt_macros flate arena serialize getopts rbml \\\n \t      rustc_const_math syntax_pos rustc_errors\n DEPS_rustc_back := std syntax flate log libc\n DEPS_rustc_borrowck := rustc log graphviz syntax syntax_pos rustc_errors rustc_mir\n-DEPS_rustc_data_structures := std log serialize\n+DEPS_rustc_data_structures := std log serialize libc\n DEPS_rustc_driver := arena flate getopts graphviz libc rustc rustc_back rustc_borrowck \\\n                      rustc_typeck rustc_mir rustc_resolve log syntax serialize rustc_llvm \\\n                      rustc_trans rustc_privacy rustc_lint rustc_plugin \\\n@@ -137,9 +137,8 @@ DEPS_rustc_save_analysis := rustc log syntax syntax_pos serialize\n DEPS_rustc_typeck := rustc syntax syntax_pos rustc_platform_intrinsics rustc_const_math \\\n                      rustc_const_eval rustc_errors\n \n-DEPS_rustdoc := rustc rustc_driver native:hoedown serialize getopts \\\n-                test rustc_lint rustc_const_eval syntax_pos\n-\n+DEPS_rustdoc := rustc rustc_driver native:hoedown serialize getopts test \\\n+                rustc_lint rustc_const_eval syntax_pos rustc_data_structures\n \n TOOL_DEPS_compiletest := test getopts log serialize\n TOOL_DEPS_rustdoc := rustdoc"}, {"sha": "ae1f9d3028c2c08075934fc18654d212bb7decaa", "filename": "src/librustc/hir/svh.rs", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc%2Fhir%2Fsvh.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc%2Fhir%2Fsvh.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Fsvh.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -17,6 +17,7 @@\n \n use std::fmt;\n use std::hash::{Hash, Hasher};\n+use serialize::{Encodable, Decodable, Encoder, Decoder};\n \n #[derive(Copy, Clone, PartialEq, Eq, Debug)]\n pub struct Svh {\n@@ -51,3 +52,17 @@ impl fmt::Display for Svh {\n         f.pad(&self.to_string())\n     }\n }\n+\n+impl Encodable for Svh {\n+    fn encode<S: Encoder>(&self, s: &mut S) -> Result<(), S::Error> {\n+        s.emit_u64(self.as_u64().to_le())\n+    }\n+}\n+\n+impl Decodable for Svh {\n+    fn decode<D: Decoder>(d: &mut D) -> Result<Svh, D::Error> {\n+        d.read_u64()\n+         .map(u64::from_le)\n+         .map(Svh::new)\n+    }\n+}"}, {"sha": "338c65637995919216cd0bb5148e091fe4452cd2", "filename": "src/librustc/session/mod.rs", "status": "modified", "additions": 100, "deletions": 1, "changes": 101, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc%2Fsession%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc%2Fsession%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fsession%2Fmod.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -33,10 +33,11 @@ use syntax::feature_gate::AttributeType;\n use syntax_pos::{Span, MultiSpan};\n \n use rustc_back::target::Target;\n+use rustc_data_structures::flock;\n use llvm;\n \n use std::path::{Path, PathBuf};\n-use std::cell::{Cell, RefCell};\n+use std::cell::{self, Cell, RefCell};\n use std::collections::{HashMap, HashSet};\n use std::env;\n use std::ffi::CString;\n@@ -101,6 +102,8 @@ pub struct Session {\n     /// macro name and defintion span in the source crate.\n     pub imported_macro_spans: RefCell<HashMap<Span, (String, Span)>>,\n \n+    incr_comp_session: RefCell<IncrCompSession>,\n+\n     next_node_id: Cell<ast::NodeId>,\n }\n \n@@ -331,6 +334,76 @@ impl Session {\n             &self.opts.search_paths,\n             kind)\n     }\n+\n+    pub fn init_incr_comp_session(&self,\n+                                  session_dir: PathBuf,\n+                                  lock_file: flock::Lock) {\n+        let mut incr_comp_session = self.incr_comp_session.borrow_mut();\n+\n+        if let IncrCompSession::NotInitialized = *incr_comp_session { } else {\n+            bug!(\"Trying to initialize IncrCompSession `{:?}`\", *incr_comp_session)\n+        }\n+\n+        *incr_comp_session = IncrCompSession::Active {\n+            session_directory: session_dir,\n+            lock_file: lock_file,\n+        };\n+    }\n+\n+    pub fn finalize_incr_comp_session(&self, new_directory_path: PathBuf) {\n+        let mut incr_comp_session = self.incr_comp_session.borrow_mut();\n+\n+        if let IncrCompSession::Active { .. } = *incr_comp_session { } else {\n+            bug!(\"Trying to finalize IncrCompSession `{:?}`\", *incr_comp_session)\n+        }\n+\n+        // Note: This will also drop the lock file, thus unlocking the directory\n+        *incr_comp_session = IncrCompSession::Finalized {\n+            session_directory: new_directory_path,\n+        };\n+    }\n+\n+    pub fn mark_incr_comp_session_as_invalid(&self) {\n+        let mut incr_comp_session = self.incr_comp_session.borrow_mut();\n+\n+        let session_directory = match *incr_comp_session {\n+            IncrCompSession::Active { ref session_directory, .. } => {\n+                session_directory.clone()\n+            }\n+            _ => bug!(\"Trying to invalidate IncrCompSession `{:?}`\",\n+                      *incr_comp_session),\n+        };\n+\n+        // Note: This will also drop the lock file, thus unlocking the directory\n+        *incr_comp_session = IncrCompSession::InvalidBecauseOfErrors {\n+            session_directory: session_directory\n+        };\n+    }\n+\n+    pub fn incr_comp_session_dir(&self) -> cell::Ref<PathBuf> {\n+        let incr_comp_session = self.incr_comp_session.borrow();\n+        cell::Ref::map(incr_comp_session, |incr_comp_session| {\n+            match *incr_comp_session {\n+                IncrCompSession::NotInitialized => {\n+                    bug!(\"Trying to get session directory from IncrCompSession `{:?}`\",\n+                        *incr_comp_session)\n+                }\n+                IncrCompSession::Active { ref session_directory, .. } |\n+                IncrCompSession::Finalized { ref session_directory } |\n+                IncrCompSession::InvalidBecauseOfErrors { ref session_directory } => {\n+                    session_directory\n+                }\n+            }\n+        })\n+    }\n+\n+    pub fn incr_comp_session_dir_opt(&self) -> Option<cell::Ref<PathBuf>> {\n+        if self.opts.incremental.is_some() {\n+            Some(self.incr_comp_session_dir())\n+        } else {\n+            None\n+        }\n+    }\n }\n \n pub fn build_session(sopts: config::Options,\n@@ -446,13 +519,39 @@ pub fn build_session_(sopts: config::Options,\n         injected_panic_runtime: Cell::new(None),\n         available_macros: RefCell::new(HashSet::new()),\n         imported_macro_spans: RefCell::new(HashMap::new()),\n+        incr_comp_session: RefCell::new(IncrCompSession::NotInitialized),\n     };\n \n     init_llvm(&sess);\n \n     sess\n }\n \n+/// Holds data on the current incremental compilation session, if there is one.\n+#[derive(Debug)]\n+pub enum IncrCompSession {\n+    // This is the state the session will be in until the incr. comp. dir is\n+    // needed.\n+    NotInitialized,\n+    // This is the state during which the session directory is private and can\n+    // be modified.\n+    Active {\n+        session_directory: PathBuf,\n+        lock_file: flock::Lock,\n+    },\n+    // This is the state after the session directory has been finalized. In this\n+    // state, the contents of the directory must not be modified any more.\n+    Finalized {\n+        session_directory: PathBuf,\n+    },\n+    // This is an error state that is reached when some compilation error has\n+    // occurred. It indicates that the contents of the session directory must\n+    // not be used, since they might be invalid.\n+    InvalidBecauseOfErrors {\n+        session_directory: PathBuf,\n+    }\n+}\n+\n fn init_llvm(sess: &Session) {\n     unsafe {\n         // Before we touch LLVM, make sure that multithreading is enabled."}, {"sha": "d7800ccaa5dd393e598cdba8d106598c47ed3af6", "filename": "src/librustc/util/fs.rs", "status": "modified", "additions": 38, "deletions": 3, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc%2Futil%2Ffs.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc%2Futil%2Ffs.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Futil%2Ffs.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -56,14 +56,49 @@ pub fn fix_windows_verbatim_for_gcc(p: &Path) -> PathBuf {\n     }\n }\n \n+pub enum LinkOrCopy {\n+    Link,\n+    Copy\n+}\n+\n /// Copy `p` into `q`, preferring to use hard-linking if possible. If\n /// `q` already exists, it is removed first.\n-pub fn link_or_copy<P: AsRef<Path>, Q: AsRef<Path>>(p: P, q: Q) -> io::Result<()> {\n+/// The result indicates which of the two operations has been performed.\n+pub fn link_or_copy<P: AsRef<Path>, Q: AsRef<Path>>(p: P, q: Q) -> io::Result<LinkOrCopy> {\n     let p = p.as_ref();\n     let q = q.as_ref();\n     if q.exists() {\n         try!(fs::remove_file(&q));\n     }\n-    fs::hard_link(p, q)\n-        .or_else(|_| fs::copy(p, q).map(|_| ()))\n+\n+    match fs::hard_link(p, q) {\n+        Ok(()) => Ok(LinkOrCopy::Link),\n+        Err(_) => {\n+            match fs::copy(p, q) {\n+                Ok(_) => Ok(LinkOrCopy::Copy),\n+                Err(e) => Err(e)\n+            }\n+        }\n+    }\n+}\n+\n+// Like std::fs::create_dir_all, except handles concurrent calls among multiple\n+// threads or processes.\n+pub fn create_dir_racy(path: &Path) -> io::Result<()> {\n+    match fs::create_dir(path) {\n+        Ok(()) => return Ok(()),\n+        Err(ref e) if e.kind() == io::ErrorKind::AlreadyExists => return Ok(()),\n+        Err(ref e) if e.kind() == io::ErrorKind::NotFound => {}\n+        Err(e) => return Err(e),\n+    }\n+    match path.parent() {\n+        Some(p) => try!(create_dir_racy(p)),\n+        None => return Err(io::Error::new(io::ErrorKind::Other,\n+                                          \"failed to create whole tree\")),\n+    }\n+    match fs::create_dir(path) {\n+        Ok(()) => Ok(()),\n+        Err(ref e) if e.kind() == io::ErrorKind::AlreadyExists => Ok(()),\n+        Err(e) => Err(e),\n+    }\n }"}, {"sha": "4a184d3174dff51deb5918b57604988f703050d7", "filename": "src/librustc_data_structures/flock.rs", "status": "renamed", "additions": 121, "deletions": 20, "changes": 141, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_data_structures%2Fflock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_data_structures%2Fflock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_data_structures%2Fflock.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -15,6 +15,7 @@\n //! librustdoc, it is not production quality at all.\n \n #![allow(non_camel_case_types)]\n+use std::path::Path;\n \n pub use self::imp::Lock;\n \n@@ -41,6 +42,7 @@ mod imp {\n             pub l_sysid: libc::c_int,\n         }\n \n+        pub const F_RDLCK: libc::c_short = 0;\n         pub const F_WRLCK: libc::c_short = 1;\n         pub const F_UNLCK: libc::c_short = 2;\n         pub const F_SETLK: libc::c_int = 6;\n@@ -60,6 +62,7 @@ mod imp {\n             pub l_sysid: libc::c_int,\n         }\n \n+        pub const F_RDLCK: libc::c_short = 1;\n         pub const F_UNLCK: libc::c_short = 2;\n         pub const F_WRLCK: libc::c_short = 3;\n         pub const F_SETLK: libc::c_int = 12;\n@@ -84,6 +87,7 @@ mod imp {\n             pub l_sysid: libc::c_int,\n         }\n \n+        pub const F_RDLCK: libc::c_short = 1;\n         pub const F_UNLCK: libc::c_short = 2;\n         pub const F_WRLCK: libc::c_short = 3;\n         pub const F_SETLK: libc::c_int = 8;\n@@ -105,6 +109,7 @@ mod imp {\n             pub l_sysid: libc::c_int,\n         }\n \n+        pub const F_RDLCK: libc::c_short = 1;\n         pub const F_UNLCK: libc::c_short = 2;\n         pub const F_WRLCK: libc::c_short = 3;\n         pub const F_SETLK: libc::c_int = 8;\n@@ -124,43 +129,66 @@ mod imp {\n             pub l_pid: libc::pid_t,\n         }\n \n+        pub const F_RDLCK: libc::c_short = 1;\n         pub const F_WRLCK: libc::c_short = 2;\n         pub const F_UNLCK: libc::c_short = 3;\n         pub const F_SETLK: libc::c_int = 6;\n         pub const F_SETLKW: libc::c_int = 7;\n     }\n \n+    #[derive(Debug)]\n     pub struct Lock {\n         fd: libc::c_int,\n     }\n \n     impl Lock {\n-        pub fn new(p: &Path) -> Lock {\n+        pub fn new(p: &Path,\n+                   wait: bool,\n+                   create: bool,\n+                   exclusive: bool)\n+                   -> io::Result<Lock> {\n             let os: &OsStr = p.as_ref();\n             let buf = CString::new(os.as_bytes()).unwrap();\n+            let open_flags = if create {\n+                libc::O_RDWR | libc::O_CREAT\n+            } else {\n+                libc::O_RDWR\n+            };\n+\n             let fd = unsafe {\n-                libc::open(buf.as_ptr(), libc::O_RDWR | libc::O_CREAT,\n+                libc::open(buf.as_ptr(), open_flags,\n                            libc::S_IRWXU as libc::c_int)\n             };\n-            assert!(fd > 0, \"failed to open lockfile: {}\",\n-                    io::Error::last_os_error());\n+\n+            if fd < 0 {\n+                return Err(io::Error::last_os_error());\n+            }\n+\n+            let lock_type = if exclusive {\n+                os::F_WRLCK\n+            } else {\n+                os::F_RDLCK\n+            };\n+\n             let flock = os::flock {\n                 l_start: 0,\n                 l_len: 0,\n                 l_pid: 0,\n                 l_whence: libc::SEEK_SET as libc::c_short,\n-                l_type: os::F_WRLCK,\n+                l_type: lock_type,\n                 l_sysid: 0,\n             };\n+            let cmd = if wait { os::F_SETLKW } else { os::F_SETLK };\n             let ret = unsafe {\n-                libc::fcntl(fd, os::F_SETLKW, &flock)\n+                libc::fcntl(fd, cmd, &flock)\n             };\n             if ret == -1 {\n                 let err = io::Error::last_os_error();\n                 unsafe { libc::close(fd); }\n-                panic!(\"could not lock `{}`: {}\", p.display(), err);\n+                Err(err)\n+            } else {\n+                Ok(Lock { fd: fd })\n             }\n-            Lock { fd: fd }\n         }\n     }\n \n@@ -191,18 +219,27 @@ mod imp {\n     use std::os::windows::raw::HANDLE;\n     use std::path::Path;\n     use std::fs::{File, OpenOptions};\n+    use std::os::raw::{c_ulong, c_ulonglong, c_int};\n+\n+    type DWORD = c_ulong;\n+    type BOOL = c_int;\n+    type ULONG_PTR = c_ulonglong;\n \n-    type DWORD = u32;\n     type LPOVERLAPPED = *mut OVERLAPPED;\n-    type BOOL = i32;\n     const LOCKFILE_EXCLUSIVE_LOCK: DWORD = 0x00000002;\n+    const LOCKFILE_FAIL_IMMEDIATELY: DWORD = 0x00000001;\n+\n+    const FILE_SHARE_DELETE: DWORD = 0x4;\n+    const FILE_SHARE_READ: DWORD = 0x1;\n+    const FILE_SHARE_WRITE: DWORD = 0x2;\n \n     #[repr(C)]\n     struct OVERLAPPED {\n-        Internal: usize,\n-        InternalHigh: usize,\n-        Pointer: *mut u8,\n-        hEvent: *mut u8,\n+        Internal: ULONG_PTR,\n+        InternalHigh: ULONG_PTR,\n+        Offset: DWORD,\n+        OffsetHigh: DWORD,\n+        hEvent: HANDLE,\n     }\n \n     extern \"system\" {\n@@ -214,24 +251,88 @@ mod imp {\n                       lpOverlapped: LPOVERLAPPED) -> BOOL;\n     }\n \n+    #[derive(Debug)]\n     pub struct Lock {\n         _file: File,\n     }\n \n     impl Lock {\n-        pub fn new(p: &Path) -> Lock {\n-            let f = OpenOptions::new().read(true).write(true).create(true)\n-                                      .open(p).unwrap();\n+        pub fn new(p: &Path,\n+                   wait: bool,\n+                   create: bool,\n+                   exclusive: bool)\n+                   -> io::Result<Lock> {\n+            assert!(p.parent().unwrap().exists(),\n+                \"Parent directory of lock-file must exist: {}\",\n+                p.display());\n+\n+            let share_mode = FILE_SHARE_DELETE | FILE_SHARE_READ | FILE_SHARE_WRITE;\n+\n+            let mut open_options = OpenOptions::new();\n+            open_options.read(true)\n+                        .share_mode(share_mode);\n+\n+            if create {\n+                open_options.create(true)\n+                            .write(true);\n+            }\n+\n+            debug!(\"Attempting to open lock file `{}`\", p.display());\n+            let file = match open_options.open(p) {\n+                Ok(file) => {\n+                    debug!(\"Lock file opened successfully\");\n+                    file\n+                }\n+                Err(err) => {\n+                    debug!(\"Error opening lock file: {}\", err);\n+                    return Err(err)\n+                }\n+            };\n+\n             let ret = unsafe {\n                 let mut overlapped: OVERLAPPED = mem::zeroed();\n-                LockFileEx(f.as_raw_handle(), LOCKFILE_EXCLUSIVE_LOCK, 0, 100, 0,\n+\n+                let mut dwFlags = 0;\n+                if !wait {\n+                    dwFlags |= LOCKFILE_FAIL_IMMEDIATELY;\n+                }\n+\n+                if exclusive {\n+                    dwFlags |= LOCKFILE_EXCLUSIVE_LOCK;\n+                }\n+\n+                debug!(\"Attempting to acquire lock on lock file `{}`\",\n+                       p.display());\n+                LockFileEx(file.as_raw_handle(),\n+                           dwFlags,\n+                           0,\n+                           0xFFFF_FFFF,\n+                           0xFFFF_FFFF,\n                            &mut overlapped)\n             };\n             if ret == 0 {\n                 let err = io::Error::last_os_error();\n-                panic!(\"could not lock `{}`: {}\", p.display(), err);\n+                debug!(\"Failed acquiring file lock: {}\", err);\n+                Err(err)\n+            } else {\n+                debug!(\"Successfully acquired lock.\");\n+                Ok(Lock { _file: file })\n             }\n-            Lock { _file: f }\n         }\n     }\n+\n+    // Note that we don't need a Drop impl on the Windows: The file is unlocked\n+    // automatically when it's closed.\n+}\n+\n+impl imp::Lock {\n+    pub fn panicking_new(p: &Path,\n+                         wait: bool,\n+                         create: bool,\n+                         exclusive: bool)\n+                         -> Lock {\n+        Lock::new(p, wait, create, exclusive).unwrap_or_else(|err| {\n+            panic!(\"could not lock `{}`: {}\", p.display(), err);\n+        })\n+    }\n }", "previous_filename": "src/librustdoc/flock.rs"}, {"sha": "e7da18cef10f967eb83b78a74d187241c23b7d64", "filename": "src/librustc_data_structures/lib.rs", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_data_structures%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_data_structures%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_data_structures%2Flib.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -31,12 +31,15 @@\n #![feature(unboxed_closures)]\n #![feature(fn_traits)]\n \n+#![cfg_attr(unix, feature(libc))]\n #![cfg_attr(test, feature(test))]\n \n extern crate core;\n #[macro_use]\n extern crate log;\n extern crate serialize as rustc_serialize; // used by deriving\n+#[cfg(unix)]\n+extern crate libc;\n \n pub mod bitvec;\n pub mod graph;\n@@ -51,6 +54,7 @@ pub mod fnv;\n pub mod tuple_slice;\n pub mod veccell;\n pub mod control_flow_graph;\n+pub mod flock;\n \n // See comments in src/librustc/lib.rs\n #[doc(hidden)]"}, {"sha": "94092be4922b52fd35fb764ca9293e5d04acc811", "filename": "src/librustc_driver/driver.rs", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_driver%2Fdriver.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_driver%2Fdriver.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_driver%2Fdriver.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -88,7 +88,7 @@ pub fn compile_input(sess: &Session,\n     // We need nested scopes here, because the intermediate results can keep\n     // large chunks of memory alive and we want to free them as soon as\n     // possible to keep the peak memory usage low\n-    let (outputs, trans, crate_name) = {\n+    let (outputs, trans) = {\n         let krate = match phase_1_parse_input(sess, cfg, input) {\n             Ok(krate) => krate,\n             Err(mut parse_error) => {\n@@ -213,11 +213,11 @@ pub fn compile_input(sess: &Session,\n             // Discard interned strings as they are no longer required.\n             token::clear_ident_interner();\n \n-            Ok((outputs, trans, crate_name.clone()))\n+            Ok((outputs, trans))\n         })??\n     };\n \n-    let phase5_result = phase_5_run_llvm_passes(sess, &crate_name, &trans, &outputs);\n+    let phase5_result = phase_5_run_llvm_passes(sess, &trans, &outputs);\n \n     controller_entry_point!(after_llvm,\n                             sess,\n@@ -229,6 +229,10 @@ pub fn compile_input(sess: &Session,\n \n     phase_6_link_output(sess, &trans, &outputs);\n \n+    // Now that we won't touch anything in the incremental compilation directory\n+    // any more, we can finalize it (which involves renaming it)\n+    rustc_incremental::finalize_session_directory(sess, trans.link.crate_hash);\n+\n     controller_entry_point!(compilation_done,\n                             sess,\n                             CompileState::state_when_compilation_done(input, sess, outdir, output),\n@@ -1026,19 +1030,19 @@ pub fn phase_4_translate_to_llvm<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n \n     time(time_passes,\n          \"assert dep graph\",\n-         move || rustc_incremental::assert_dep_graph(tcx));\n+         || rustc_incremental::assert_dep_graph(tcx));\n \n     time(time_passes,\n          \"serialize dep graph\",\n-         move || rustc_incremental::save_dep_graph(tcx, &incremental_hashes_map));\n-\n+         || rustc_incremental::save_dep_graph(tcx,\n+                                              &incremental_hashes_map,\n+                                              translation.link.crate_hash));\n     translation\n }\n \n /// Run LLVM itself, producing a bitcode file, assembly file or object file\n /// as a side effect.\n pub fn phase_5_run_llvm_passes(sess: &Session,\n-                               crate_name: &str,\n                                trans: &trans::CrateTranslation,\n                                outputs: &OutputFilenames) -> CompileResult {\n     if sess.opts.cg.no_integrated_as {\n@@ -1061,7 +1065,7 @@ pub fn phase_5_run_llvm_passes(sess: &Session,\n \n     time(sess.time_passes(),\n          \"serialize work products\",\n-         move || rustc_incremental::save_work_products(sess, crate_name));\n+         move || rustc_incremental::save_work_products(sess));\n \n     if sess.err_count() > 0 {\n         Err(sess.err_count())"}, {"sha": "511ba8ec19cc7fe3e9ceea29a359757271df2317", "filename": "src/librustc_incremental/lib.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Flib.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -22,6 +22,7 @@\n #![feature(question_mark)]\n #![feature(rustc_private)]\n #![feature(staged_api)]\n+#![feature(rand)]\n \n extern crate graphviz;\n extern crate rbml;\n@@ -45,3 +46,4 @@ pub use persist::save_dep_graph;\n pub use persist::save_trans_partition;\n pub use persist::save_work_products;\n pub use persist::in_incr_comp_dir;\n+pub use persist::finalize_session_directory;"}, {"sha": "4ad4b115759c46a608999fbdfae0868d9d28c559", "filename": "src/librustc_incremental/persist/fs.rs", "status": "added", "additions": 1049, "deletions": 0, "changes": 1049, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Ffs.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Ffs.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Ffs.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -0,0 +1,1049 @@\n+// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+\n+//! This module manages how the incremental compilation cache is represented in\n+//! the file system.\n+//!\n+//! Incremental compilation caches are managed according to a copy-on-write\n+//! strategy: Once a complete, consistent cache version is finalized, it is\n+//! never modified. Instead, when a subsequent compilation session is started,\n+//! the compiler will allocate a new version of the cache that starts out as\n+//! a copy of the previous version. Then only this new copy is modified and it\n+//! will not be visible to other processes until it is finalized. This ensures\n+//! that multiple compiler processes can be executed concurrently for the same\n+//! crate without interfering with each other or blocking each other.\n+//!\n+//! More concretely this is implemented via the following protocol:\n+//!\n+//! 1. For a newly started compilation session, the compiler allocates a\n+//!    new `session` directory within the incremental compilation directory.\n+//!    This session directory will have a unique name that ends with the suffix\n+//!    \"-working\" and that contains a creation timestamp.\n+//! 2. Next, the compiler looks for the newest finalized session directory,\n+//!    that is, a session directory from a previous compilation session that\n+//!    has been marked as valid and consistent. A session directory is\n+//!    considered finalized if the \"-working\" suffix in the directory name has\n+//!    been replaced by the SVH of the crate.\n+//! 3. Once the compiler has found a valid, finalized session directory, it will\n+//!    hard-link/copy its contents into the new \"-working\" directory. If all\n+//!    goes well, it will have its own, private copy of the source directory and\n+//!    subsequently not have to worry about synchronizing with other compiler\n+//!    processes.\n+//! 4. Now the compiler can do its normal compilation process, which involves\n+//!    reading and updating its private session directory.\n+//! 5. When compilation finishes without errors, the private session directory\n+//!    will be in a state where it can be used as input for other compilation\n+//!    sessions. That is, it will contain a dependency graph and cache artifacts\n+//!    that are consistent with the state of the source code it was compiled\n+//!    from, with no need to change them ever again. At this point, the compiler\n+//!    finalizes and \"publishes\" its private session directory by renaming it\n+//!    from \"s-{timestamp}-{random}-working\" to \"s-{timestamp}-{SVH}\".\n+//! 6. At this point the \"old\" session directory that we copied our data from\n+//!    at the beginning of the session has become obsolete because we have just\n+//!    published a more current version. Thus the compiler will delete it.\n+//!\n+//! ## Garbage Collection\n+//!\n+//! Naively following the above protocol might lead to old session directories\n+//! piling up if a compiler instance crashes for some reason before its able to\n+//! remove its private session directory. In order to avoid wasting disk space,\n+//! the compiler also does some garbage collection each time it is started in\n+//! incremental compilation mode. Specifically, it will scan the incremental\n+//! compilation directory for private session directories that are not in use\n+//! any more and will delete those. It will also delete any finalized session\n+//! directories for a given crate except for the most recent one.\n+//!\n+//! ## Synchronization\n+//!\n+//! There is some synchronization needed in order for the compiler to be able to\n+//! determine whether a given private session directory is not in used any more.\n+//! This is done by creating a lock file for each session directory and\n+//! locking it while the directory is still being used. Since file locks have\n+//! operating system support, we can rely on the lock being released if the\n+//! compiler process dies for some unexpected reason. Thus, when garbage\n+//! collecting private session directories, the collecting process can determine\n+//! whether the directory is still in use by trying to acquire a lock on the\n+//! file. If locking the file fails, the original process must still be alive.\n+//! If locking the file succeeds, we know that the owning process is not alive\n+//! any more and we can safely delete the directory.\n+//! There is still a small time window between the original process creating the\n+//! lock file and actually locking it. In order to minimize the chance that\n+//! another process tries to acquire the lock in just that instance, only\n+//! session directories that are older than a few seconds are considered for\n+//! garbage collection.\n+//!\n+//! Another case that has to be considered is what happens if one process\n+//! deletes a finalized session directory that another process is currently\n+//! trying to copy from. This case is also handled via the lock file. Before\n+//! a process starts copying a finalized session directory, it will acquire a\n+//! shared lock on the directory's lock file. Any garbage collecting process,\n+//! on the other hand, will acquire an exclusive lock on the lock file.\n+//! Thus, if a directory is being collected, any reader process will fail\n+//! acquiring the shared lock and will leave the directory alone. Conversely,\n+//! if a collecting process can't acquire the exclusive lock because the\n+//! directory is currently being read from, it will leave collecting that\n+//! directory to another process at a later point in time.\n+//! The exact same scheme is also used when reading the metadata hashes file\n+//! from an extern crate. When a crate is compiled, the hash values of its\n+//! metadata are stored in a file in its session directory. When the\n+//! compilation session of another crate imports the first crate's metadata,\n+//! it also has to read in the accompanying metadata hashes. It thus will access\n+//! the finalized session directory of all crates it links to and while doing\n+//! so, it will also place a read lock on that the respective session directory\n+//! so that it won't be deleted while the metadata hashes are loaded.\n+//!\n+//! ## Preconditions\n+//!\n+//! This system relies on two features being available in the file system in\n+//! order to work really well: file locking and hard linking.\n+//! If hard linking is not available (like on FAT) the data in the cache\n+//! actually has to be copied at the beginning of each session.\n+//! If file locking does not work reliably (like on NFS), some of the\n+//! synchronization will go haywire.\n+//! In both cases we recommend to locate the incremental compilation directory\n+//! on a file system that supports these things.\n+//! It might be a good idea though to try and detect whether we are on an\n+//! unsupported file system and emit a warning in that case. This is not yet\n+//! implemented.\n+\n+use rustc::hir::svh::Svh;\n+use rustc::middle::cstore::LOCAL_CRATE;\n+use rustc::session::Session;\n+use rustc::ty::TyCtxt;\n+use rustc::util::fs as fs_util;\n+use rustc_data_structures::flock;\n+use rustc_data_structures::fnv::{FnvHashSet, FnvHashMap};\n+\n+use std::ffi::OsString;\n+use std::fs as std_fs;\n+use std::io;\n+use std::mem;\n+use std::path::{Path, PathBuf};\n+use std::time::{UNIX_EPOCH, SystemTime, Duration};\n+use std::__rand::{thread_rng, Rng};\n+use syntax::ast;\n+\n+const LOCK_FILE_EXT: &'static str = \".lock\";\n+const DEP_GRAPH_FILENAME: &'static str = \"dep-graph.bin\";\n+const WORK_PRODUCTS_FILENAME: &'static str = \"work-products.bin\";\n+const METADATA_HASHES_FILENAME: &'static str = \"metadata.bin\";\n+\n+pub fn dep_graph_path(sess: &Session) -> PathBuf {\n+    in_incr_comp_dir_sess(sess, DEP_GRAPH_FILENAME)\n+}\n+\n+pub fn work_products_path(sess: &Session) -> PathBuf {\n+    in_incr_comp_dir_sess(sess, WORK_PRODUCTS_FILENAME)\n+}\n+\n+pub fn metadata_hash_export_path(sess: &Session) -> PathBuf {\n+    in_incr_comp_dir_sess(sess, METADATA_HASHES_FILENAME)\n+}\n+\n+pub fn metadata_hash_import_path(import_session_dir: &Path) -> PathBuf {\n+    import_session_dir.join(METADATA_HASHES_FILENAME)\n+}\n+\n+pub fn lock_file_path(session_dir: &Path) -> PathBuf {\n+    let crate_dir = session_dir.parent().unwrap();\n+\n+    let directory_name = session_dir.file_name().unwrap().to_string_lossy();\n+    assert_no_characters_lost(&directory_name);\n+\n+    let dash_indices: Vec<_> = directory_name.match_indices(\"-\")\n+                                             .map(|(idx, _)| idx)\n+                                             .collect();\n+    if dash_indices.len() != 3 {\n+        bug!(\"Encountered incremental compilation session directory with \\\n+              malformed name: {}\",\n+             session_dir.display())\n+    }\n+\n+    crate_dir.join(&directory_name[0 .. dash_indices[2]])\n+             .with_extension(&LOCK_FILE_EXT[1..])\n+}\n+\n+pub fn in_incr_comp_dir_sess(sess: &Session, file_name: &str) -> PathBuf {\n+    in_incr_comp_dir(&sess.incr_comp_session_dir(), file_name)\n+}\n+\n+pub fn in_incr_comp_dir(incr_comp_session_dir: &Path, file_name: &str) -> PathBuf {\n+    incr_comp_session_dir.join(file_name)\n+}\n+\n+/// Allocates the private session directory. The boolean in the Ok() result\n+/// indicates whether we should try loading a dep graph from the successfully\n+/// initialized directory, or not.\n+/// The post-condition of this fn is that we have a valid incremental\n+/// compilation session directory, if the result is `Ok`. A valid session\n+/// directory is one that contains a locked lock file. It may or may not contain\n+/// a dep-graph and work products from a previous session.\n+/// If the call fails, the fn may leave behind an invalid session directory.\n+/// The garbage collection will take care of it.\n+pub fn prepare_session_directory(tcx: TyCtxt) -> Result<bool, ()> {\n+    debug!(\"prepare_session_directory\");\n+\n+    // {incr-comp-dir}/{crate-name-and-disambiguator}\n+    let crate_dir = crate_path_tcx(tcx, LOCAL_CRATE);\n+    debug!(\"crate-dir: {}\", crate_dir.display());\n+    try!(create_dir(tcx.sess, &crate_dir, \"crate\"));\n+\n+    let mut source_directories_already_tried = FnvHashSet();\n+\n+    loop {\n+        // Generate a session directory of the form:\n+        //\n+        // {incr-comp-dir}/{crate-name-and-disambiguator}/s-{timestamp}-{random}-working\n+        let session_dir = generate_session_dir_path(&crate_dir);\n+        debug!(\"session-dir: {}\", session_dir.display());\n+\n+        // Lock the new session directory. If this fails, return an\n+        // error without retrying\n+        let (directory_lock, lock_file_path) = try!(lock_directory(tcx.sess, &session_dir));\n+\n+        // Now that we have the lock, we can actually create the session\n+        // directory\n+        try!(create_dir(tcx.sess, &session_dir, \"session\"));\n+\n+        // Find a suitable source directory to copy from. Ignore those that we\n+        // have already tried before.\n+        let source_directory = find_source_directory(&crate_dir,\n+                                                     &source_directories_already_tried);\n+\n+        let source_directory = if let Some(dir) = source_directory {\n+            dir\n+        } else {\n+            // There's nowhere to copy from, we're done\n+            debug!(\"no source directory found. Continuing with empty session \\\n+                    directory.\");\n+\n+            tcx.sess.init_incr_comp_session(session_dir, directory_lock);\n+            return Ok(false)\n+        };\n+\n+        debug!(\"attempting to copy data from source: {}\",\n+               source_directory.display());\n+\n+        let print_file_copy_stats = tcx.sess.opts.debugging_opts.incremental_info;\n+\n+        // Try copying over all files from the source directory\n+        if copy_files(&session_dir, &source_directory, print_file_copy_stats).is_ok() {\n+            debug!(\"successfully copied data from: {}\",\n+                   source_directory.display());\n+\n+            tcx.sess.init_incr_comp_session(session_dir, directory_lock);\n+            return Ok(true)\n+        } else {\n+             debug!(\"copying failed - trying next directory\");\n+\n+            // Something went wrong while trying to copy/link files from the\n+            // source directory. Try again with a different one.\n+            source_directories_already_tried.insert(source_directory);\n+\n+            // Try to remove the session directory we just allocated. We don't\n+            // know if there's any garbage in it from the failed copy action.\n+            if let Err(err) = safe_remove_dir_all(&session_dir) {\n+                tcx.sess.warn(&format!(\"Failed to delete partly initialized \\\n+                                        session dir `{}`: {}\",\n+                                       session_dir.display(),\n+                                       err));\n+            }\n+\n+            delete_session_dir_lock_file(tcx.sess, &lock_file_path);\n+            mem::drop(directory_lock);\n+        }\n+    }\n+}\n+\n+\n+/// This function finalizes and thus 'publishes' the session directory by\n+/// renaming it to `s-{timestamp}-{svh}` and releasing the file lock.\n+/// If there have been compilation errors, however, this function will just\n+/// delete the presumably invalid session directory.\n+pub fn finalize_session_directory(sess: &Session, svh: Svh) {\n+    if sess.opts.incremental.is_none() {\n+        return;\n+    }\n+\n+    let incr_comp_session_dir: PathBuf = sess.incr_comp_session_dir().clone();\n+\n+    if sess.has_errors() {\n+        // If there have been any errors during compilation, we don't want to\n+        // publish this session directory. Rather, we'll just delete it.\n+\n+        debug!(\"finalize_session_directory() - invalidating session directory: {}\",\n+                incr_comp_session_dir.display());\n+\n+        if let Err(err) = safe_remove_dir_all(&*incr_comp_session_dir) {\n+            sess.warn(&format!(\"Error deleting incremental compilation \\\n+                                session directory `{}`: {}\",\n+                               incr_comp_session_dir.display(),\n+                               err));\n+        }\n+\n+        let lock_file_path = lock_file_path(&*incr_comp_session_dir);\n+        delete_session_dir_lock_file(sess, &lock_file_path);\n+        sess.mark_incr_comp_session_as_invalid();\n+    }\n+\n+    debug!(\"finalize_session_directory() - session directory: {}\",\n+            incr_comp_session_dir.display());\n+\n+    let old_sub_dir_name = incr_comp_session_dir.file_name()\n+                                                .unwrap()\n+                                                .to_string_lossy();\n+    assert_no_characters_lost(&old_sub_dir_name);\n+\n+    // Keep the 's-{timestamp}-{random-number}' prefix, but replace the\n+    // '-working' part with the SVH of the crate\n+    let dash_indices: Vec<_> = old_sub_dir_name.match_indices(\"-\")\n+                                               .map(|(idx, _)| idx)\n+                                               .collect();\n+    if dash_indices.len() != 3 {\n+        bug!(\"Encountered incremental compilation session directory with \\\n+              malformed name: {}\",\n+             incr_comp_session_dir.display())\n+    }\n+\n+    // State: \"s-{timestamp}-{random-number}-\"\n+    let mut new_sub_dir_name = String::from(&old_sub_dir_name[.. dash_indices[2] + 1]);\n+\n+    // Append the svh\n+    new_sub_dir_name.push_str(&encode_base_36(svh.as_u64()));\n+\n+    // Create the full path\n+    let new_path = incr_comp_session_dir.parent().unwrap().join(new_sub_dir_name);\n+    debug!(\"finalize_session_directory() - new path: {}\", new_path.display());\n+\n+    match std_fs::rename(&*incr_comp_session_dir, &new_path) {\n+        Ok(_) => {\n+            debug!(\"finalize_session_directory() - directory renamed successfully\");\n+\n+            // This unlocks the directory\n+            sess.finalize_incr_comp_session(new_path);\n+        }\n+        Err(e) => {\n+            // Warn about the error. However, no need to abort compilation now.\n+            sess.warn(&format!(\"Error finalizing incremental compilation \\\n+                               session directory `{}`: {}\",\n+                               incr_comp_session_dir.display(),\n+                               e));\n+\n+            debug!(\"finalize_session_directory() - error, marking as invalid\");\n+            // Drop the file lock, so we can garage collect\n+            sess.mark_incr_comp_session_as_invalid();\n+        }\n+    }\n+\n+    let _ = garbage_collect_session_directories(sess);\n+}\n+\n+fn copy_files(target_dir: &Path,\n+              source_dir: &Path,\n+              print_stats_on_success: bool)\n+              -> Result<(), ()> {\n+    // We acquire a shared lock on the lock file of the directory, so that\n+    // nobody deletes it out from under us while we are reading from it.\n+    let lock_file_path = lock_file_path(source_dir);\n+    let _lock = if let Ok(lock) = flock::Lock::new(&lock_file_path,\n+                                                   false,   // don't wait,\n+                                                   false,   // don't create\n+                                                   false) { // not exclusive\n+        lock\n+    } else {\n+        // Could not acquire the lock, don't try to copy from here\n+        return Err(())\n+    };\n+\n+    let source_dir_iterator = match source_dir.read_dir() {\n+        Ok(it) => it,\n+        Err(_) => return Err(())\n+    };\n+\n+    let mut files_linked = 0;\n+    let mut files_copied = 0;\n+\n+    for entry in source_dir_iterator {\n+        match entry {\n+            Ok(entry) => {\n+                let file_name = entry.file_name();\n+\n+                let target_file_path = target_dir.join(file_name);\n+                let source_path = entry.path();\n+\n+                debug!(\"copying into session dir: {}\", source_path.display());\n+                match fs_util::link_or_copy(source_path, target_file_path) {\n+                    Ok(fs_util::LinkOrCopy::Link) => {\n+                        files_linked += 1\n+                    }\n+                    Ok(fs_util::LinkOrCopy::Copy) => {\n+                        files_copied += 1\n+                    }\n+                    Err(_) => return Err(())\n+                }\n+            }\n+            Err(_) => {\n+                return Err(())\n+            }\n+        }\n+    }\n+\n+    if print_stats_on_success {\n+        println!(\"incr. comp. session directory: {} files hard-linked\", files_linked);\n+        println!(\"incr. comp. session directory: {} files copied\", files_copied);\n+    }\n+\n+    Ok(())\n+}\n+\n+/// Generate unique directory path of the form:\n+/// {crate_dir}/s-{timestamp}-{random-number}-working\n+fn generate_session_dir_path(crate_dir: &Path) -> PathBuf {\n+    let timestamp = timestamp_to_string(SystemTime::now());\n+    debug!(\"generate_session_dir_path: timestamp = {}\", timestamp);\n+    let random_number = thread_rng().next_u32();\n+    debug!(\"generate_session_dir_path: random_number = {}\", random_number);\n+\n+    let directory_name = format!(\"s-{}-{}-working\",\n+                                  timestamp,\n+                                  encode_base_36(random_number as u64));\n+    debug!(\"generate_session_dir_path: directory_name = {}\", directory_name);\n+    let directory_path = crate_dir.join(directory_name);\n+    debug!(\"generate_session_dir_path: directory_path = {}\", directory_path.display());\n+    directory_path\n+}\n+\n+fn create_dir(sess: &Session, path: &Path, dir_tag: &str) -> Result<(),()> {\n+    match fs_util::create_dir_racy(path) {\n+        Ok(()) => {\n+            debug!(\"{} directory created successfully\", dir_tag);\n+            Ok(())\n+        }\n+        Err(err) => {\n+            sess.err(&format!(\"Could not create incremental compilation {} \\\n+                               directory `{}`: {}\",\n+                              dir_tag,\n+                              path.display(),\n+                              err));\n+            Err(())\n+        }\n+    }\n+}\n+\n+/// Allocate a the lock-file and lock it.\n+fn lock_directory(sess: &Session,\n+                  session_dir: &Path)\n+                  -> Result<(flock::Lock, PathBuf), ()> {\n+    let lock_file_path = lock_file_path(session_dir);\n+    debug!(\"lock_directory() - lock_file: {}\", lock_file_path.display());\n+\n+    match flock::Lock::new(&lock_file_path,\n+                           false, // don't wait\n+                           true,  // create the lock file\n+                           true) { // the lock should be exclusive\n+        Ok(lock) => Ok((lock, lock_file_path)),\n+        Err(err) => {\n+            sess.err(&format!(\"incremental compilation: could not create \\\n+                               session directory lock file: {}\", err));\n+            Err(())\n+        }\n+    }\n+}\n+\n+fn delete_session_dir_lock_file(sess: &Session,\n+                                lock_file_path: &Path) {\n+    if let Err(err) = safe_remove_file(&lock_file_path) {\n+        sess.warn(&format!(\"Error deleting lock file for incremental \\\n+                            compilation session directory `{}`: {}\",\n+                           lock_file_path.display(),\n+                           err));\n+    }\n+}\n+\n+/// Find the most recent published session directory that is not in the\n+/// ignore-list.\n+fn find_source_directory(crate_dir: &Path,\n+                         source_directories_already_tried: &FnvHashSet<PathBuf>)\n+                         -> Option<PathBuf> {\n+    let iter = crate_dir.read_dir()\n+                        .unwrap() // FIXME\n+                        .filter_map(|e| e.ok().map(|e| e.path()));\n+\n+    find_source_directory_in_iter(iter, source_directories_already_tried)\n+}\n+\n+fn find_source_directory_in_iter<I>(iter: I,\n+                                    source_directories_already_tried: &FnvHashSet<PathBuf>)\n+                                    -> Option<PathBuf>\n+    where I: Iterator<Item=PathBuf>\n+{\n+    let mut best_candidate = (UNIX_EPOCH, None);\n+\n+    for session_dir in iter {\n+        debug!(\"find_source_directory_in_iter - inspecting `{}`\",\n+               session_dir.display());\n+\n+        let directory_name = session_dir.file_name().unwrap().to_string_lossy();\n+        assert_no_characters_lost(&directory_name);\n+\n+        if source_directories_already_tried.contains(&session_dir) ||\n+           !is_session_directory(&directory_name) ||\n+           !is_finalized(&directory_name) {\n+            debug!(\"find_source_directory_in_iter - ignoring.\");\n+            continue\n+        }\n+\n+        let timestamp = extract_timestamp_from_session_dir(&directory_name)\n+            .unwrap_or_else(|_| {\n+                bug!(\"unexpected incr-comp session dir: {}\", session_dir.display())\n+            });\n+\n+        if timestamp > best_candidate.0 {\n+            best_candidate = (timestamp, Some(session_dir.clone()));\n+        }\n+    }\n+\n+    best_candidate.1\n+}\n+\n+fn is_finalized(directory_name: &str) -> bool {\n+    !directory_name.ends_with(\"-working\")\n+}\n+\n+fn is_session_directory(directory_name: &str) -> bool {\n+    directory_name.starts_with(\"s-\") &&\n+    !directory_name.ends_with(LOCK_FILE_EXT)\n+}\n+\n+fn is_session_directory_lock_file(file_name: &str) -> bool {\n+    file_name.starts_with(\"s-\") && file_name.ends_with(LOCK_FILE_EXT)\n+}\n+\n+fn extract_timestamp_from_session_dir(directory_name: &str)\n+                                      -> Result<SystemTime, ()> {\n+    if !is_session_directory(directory_name) {\n+        return Err(())\n+    }\n+\n+    let dash_indices: Vec<_> = directory_name.match_indices(\"-\")\n+                                             .map(|(idx, _)| idx)\n+                                             .collect();\n+    if dash_indices.len() != 3 {\n+        return Err(())\n+    }\n+\n+    string_to_timestamp(&directory_name[dash_indices[0]+1 .. dash_indices[1]])\n+}\n+\n+const BASE_36: &'static [u8] = b\"0123456789abcdefghijklmnopqrstuvwxyz\";\n+\n+fn encode_base_36(mut n: u64) -> String {\n+    let mut s = Vec::with_capacity(13);\n+    loop {\n+        s.push(BASE_36[(n % 36) as usize]);\n+        n /= 36;\n+\n+        if n == 0 {\n+            break;\n+        }\n+    }\n+    s.reverse();\n+    String::from_utf8(s).unwrap()\n+}\n+\n+fn timestamp_to_string(timestamp: SystemTime) -> String {\n+    let duration = timestamp.duration_since(UNIX_EPOCH).unwrap();\n+    let micros = duration.as_secs() * 1_000_000 +\n+                (duration.subsec_nanos() as u64) / 1000;\n+    encode_base_36(micros)\n+}\n+\n+fn string_to_timestamp(s: &str) -> Result<SystemTime, ()> {\n+    let micros_since_unix_epoch = u64::from_str_radix(s, 36);\n+\n+    if micros_since_unix_epoch.is_err() {\n+        return Err(())\n+    }\n+\n+    let micros_since_unix_epoch = micros_since_unix_epoch.unwrap();\n+\n+    let duration = Duration::new(micros_since_unix_epoch / 1_000_000,\n+                                 1000 * (micros_since_unix_epoch % 1_000_000) as u32);\n+    Ok(UNIX_EPOCH + duration)\n+}\n+\n+fn crate_path_tcx(tcx: TyCtxt, cnum: ast::CrateNum) -> PathBuf {\n+    crate_path(tcx.sess, &tcx.crate_name(cnum), &tcx.crate_disambiguator(cnum))\n+}\n+\n+/// Finds the session directory containing the correct metadata hashes file for\n+/// the given crate. In order to do that it has to compute the crate directory\n+/// of the given crate, and in there, look for the session directory with the\n+/// correct SVH in it.\n+/// Note that we have to match on the exact SVH here, not just the\n+/// crate's (name, disambiguator) pair. The metadata hashes are only valid for\n+/// the exact version of the binary we are reading from now (i.e. the hashes\n+/// are part of the dependency graph of a specific compilation session).\n+pub fn find_metadata_hashes_for(tcx: TyCtxt, cnum: ast::CrateNum) -> Option<PathBuf> {\n+    let crate_directory = crate_path_tcx(tcx, cnum);\n+\n+    if !crate_directory.exists() {\n+        return None\n+    }\n+\n+    let dir_entries = match crate_directory.read_dir() {\n+        Ok(dir_entries) => dir_entries,\n+        Err(e) => {\n+            tcx.sess\n+               .err(&format!(\"incremental compilation: Could not read crate directory `{}`: {}\",\n+                             crate_directory.display(), e));\n+            return None\n+        }\n+    };\n+\n+    let target_svh = tcx.sess.cstore.crate_hash(cnum);\n+    let target_svh = encode_base_36(target_svh.as_u64());\n+\n+    let sub_dir = find_metadata_hashes_iter(&target_svh, dir_entries.filter_map(|e| {\n+        e.ok().map(|e| e.file_name().to_string_lossy().into_owned())\n+    }));\n+\n+    sub_dir.map(|sub_dir_name| crate_directory.join(&sub_dir_name))\n+}\n+\n+fn find_metadata_hashes_iter<'a, I>(target_svh: &str, iter: I) -> Option<OsString>\n+    where I: Iterator<Item=String>\n+{\n+    for sub_dir_name in iter {\n+        if !is_session_directory(&sub_dir_name) || !is_finalized(&sub_dir_name) {\n+            // This is not a usable session directory\n+            continue\n+        }\n+\n+        let is_match = if let Some(last_dash_pos) = sub_dir_name.rfind(\"-\") {\n+            let candidate_svh = &sub_dir_name[last_dash_pos + 1 .. ];\n+            target_svh == candidate_svh\n+        } else {\n+            // some kind of invalid directory name\n+            continue\n+        };\n+\n+        if is_match {\n+            return Some(OsString::from(sub_dir_name))\n+        }\n+    }\n+\n+    None\n+}\n+\n+fn crate_path(sess: &Session,\n+              crate_name: &str,\n+              crate_disambiguator: &str)\n+              -> PathBuf {\n+    use std::hash::{SipHasher, Hasher, Hash};\n+\n+    let incr_dir = sess.opts.incremental.as_ref().unwrap().clone();\n+\n+    // The full crate disambiguator is really long. A hash of it should be\n+    // sufficient.\n+    let mut hasher = SipHasher::new();\n+    crate_disambiguator.hash(&mut hasher);\n+\n+    let crate_name = format!(\"{}-{}\", crate_name, encode_base_36(hasher.finish()));\n+    incr_dir.join(crate_name)\n+}\n+\n+fn assert_no_characters_lost(s: &str) {\n+    if s.contains('\\u{FFFD}') {\n+        bug!(\"Could not losslessly convert '{}'.\", s)\n+    }\n+}\n+\n+fn is_old_enough_to_be_collected(timestamp: SystemTime) -> bool {\n+    timestamp < SystemTime::now() - Duration::from_secs(10)\n+}\n+\n+pub fn garbage_collect_session_directories(sess: &Session) -> io::Result<()> {\n+    debug!(\"garbage_collect_session_directories() - begin\");\n+\n+    let session_directory = sess.incr_comp_session_dir();\n+    debug!(\"garbage_collect_session_directories() - session directory: {}\",\n+        session_directory.display());\n+\n+    let crate_directory = session_directory.parent().unwrap();\n+    debug!(\"garbage_collect_session_directories() - crate directory: {}\",\n+        crate_directory.display());\n+\n+    // First do a pass over the crate directory, collecting lock files and\n+    // session directories\n+    let mut session_directories = FnvHashSet();\n+    let mut lock_files = FnvHashSet();\n+\n+    for dir_entry in try!(crate_directory.read_dir()) {\n+        let dir_entry = match dir_entry {\n+            Ok(dir_entry) => dir_entry,\n+            _ => {\n+                // Ignore any errors\n+                continue\n+            }\n+        };\n+\n+        let entry_name = dir_entry.file_name();\n+        let entry_name = entry_name.to_string_lossy();\n+\n+        if is_session_directory_lock_file(&entry_name) {\n+            assert_no_characters_lost(&entry_name);\n+            lock_files.insert(entry_name.into_owned());\n+        } else if is_session_directory(&entry_name) {\n+            assert_no_characters_lost(&entry_name);\n+            session_directories.insert(entry_name.into_owned());\n+        } else {\n+            // This is something we don't know, leave it alone\n+        }\n+    }\n+\n+    // Now map from lock files to session directories\n+    let lock_file_to_session_dir: FnvHashMap<String, Option<String>> =\n+        lock_files.into_iter()\n+                  .map(|lock_file_name| {\n+                        assert!(lock_file_name.ends_with(LOCK_FILE_EXT));\n+                        let dir_prefix_end = lock_file_name.len() - LOCK_FILE_EXT.len();\n+                        let session_dir = {\n+                            let dir_prefix = &lock_file_name[0 .. dir_prefix_end];\n+                            session_directories.iter()\n+                                               .find(|dir_name| dir_name.starts_with(dir_prefix))\n+                        };\n+                        (lock_file_name, session_dir.map(String::clone))\n+                    })\n+                  .collect();\n+\n+    // Delete all lock files, that don't have an associated directory. They must\n+    // be some kind of leftover\n+    for (lock_file_name, directory_name) in &lock_file_to_session_dir {\n+        if directory_name.is_none() {\n+            let timestamp = match extract_timestamp_from_session_dir(lock_file_name) {\n+                Ok(timestamp) => timestamp,\n+                Err(()) => {\n+                    debug!(\"Found lock-file with malformed timestamp: {}\",\n+                        crate_directory.join(&lock_file_name).display());\n+                    // Ignore it\n+                    continue\n+                }\n+            };\n+\n+            let lock_file_path = crate_directory.join(&**lock_file_name);\n+\n+            if is_old_enough_to_be_collected(timestamp) {\n+                debug!(\"garbage_collect_session_directories() - deleting \\\n+                        garbage lock file: {}\", lock_file_path.display());\n+                delete_session_dir_lock_file(sess, &lock_file_path);\n+            } else {\n+                debug!(\"garbage_collect_session_directories() - lock file with \\\n+                        no session dir not old enough to be collected: {}\",\n+                       lock_file_path.display());\n+            }\n+        }\n+    }\n+\n+    // Filter out `None` directories\n+    let lock_file_to_session_dir: FnvHashMap<String, String> =\n+        lock_file_to_session_dir.into_iter()\n+                                .filter_map(|(lock_file_name, directory_name)| {\n+                                    directory_name.map(|n| (lock_file_name, n))\n+                                })\n+                                .collect();\n+\n+    let mut deletion_candidates = vec![];\n+    let mut definitely_delete = vec![];\n+\n+    for (lock_file_name, directory_name) in &lock_file_to_session_dir {\n+        debug!(\"garbage_collect_session_directories() - inspecting: {}\",\n+                directory_name);\n+\n+        let timestamp = match extract_timestamp_from_session_dir(directory_name) {\n+            Ok(timestamp) => timestamp,\n+            Err(()) => {\n+                debug!(\"Found session-dir with malformed timestamp: {}\",\n+                        crate_directory.join(directory_name).display());\n+                // Ignore it\n+                continue\n+            }\n+        };\n+\n+        if is_finalized(directory_name) {\n+            let lock_file_path = crate_directory.join(lock_file_name);\n+            match flock::Lock::new(&lock_file_path,\n+                                   false,  // don't wait\n+                                   false,  // don't create the lock-file\n+                                   true) { // get an exclusive lock\n+                Ok(lock) => {\n+                    debug!(\"garbage_collect_session_directories() - \\\n+                            successfully acquired lock\");\n+                    debug!(\"garbage_collect_session_directories() - adding \\\n+                            deletion candidate: {}\", directory_name);\n+\n+                    // Note that we are holding on to the lock\n+                    deletion_candidates.push((timestamp,\n+                                              crate_directory.join(directory_name),\n+                                              Some(lock)));\n+                }\n+                Err(_) => {\n+                    debug!(\"garbage_collect_session_directories() - \\\n+                            not collecting, still in use\");\n+                }\n+            }\n+        } else if is_old_enough_to_be_collected(timestamp) {\n+            // When cleaning out \"-working\" session directories, i.e.\n+            // session directories that might still be in use by another\n+            // compiler instance, we only look a directories that are\n+            // at least ten seconds old. This is supposed to reduce the\n+            // chance of deleting a directory in the time window where\n+            // the process has allocated the directory but has not yet\n+            // acquired the file-lock on it.\n+\n+            // Try to acquire the directory lock. If we can't, it\n+            // means that the owning process is still alive and we\n+            // leave this directory alone.\n+            let lock_file_path = crate_directory.join(lock_file_name);\n+            match flock::Lock::new(&lock_file_path,\n+                                   false,  // don't wait\n+                                   false,  // don't create the lock-file\n+                                   true) { // get an exclusive lock\n+                Ok(lock) => {\n+                    debug!(\"garbage_collect_session_directories() - \\\n+                            successfully acquired lock\");\n+\n+                    // Note that we are holding on to the lock\n+                    definitely_delete.push((crate_directory.join(directory_name),\n+                                            Some(lock)));\n+                }\n+                Err(_) => {\n+                    debug!(\"garbage_collect_session_directories() - \\\n+                            not collecting, still in use\");\n+                }\n+            }\n+        } else {\n+            debug!(\"garbage_collect_session_directories() - not finalized, not \\\n+                    old enough\");\n+        }\n+    }\n+\n+    // Delete all but the most recent of the candidates\n+    for (path, lock) in all_except_most_recent(deletion_candidates) {\n+        debug!(\"garbage_collect_session_directories() - deleting `{}`\",\n+                path.display());\n+\n+        if let Err(err) = safe_remove_dir_all(&path) {\n+            sess.warn(&format!(\"Failed to garbage collect finalized incremental \\\n+                                compilation session directory `{}`: {}\",\n+                               path.display(),\n+                               err));\n+        } else {\n+            delete_session_dir_lock_file(sess, &lock_file_path(&path));\n+        }\n+\n+\n+        // Let's make it explicit that the file lock is released at this point,\n+        // or rather, that we held on to it until here\n+        mem::drop(lock);\n+    }\n+\n+    for (path, lock) in definitely_delete {\n+        debug!(\"garbage_collect_session_directories() - deleting `{}`\",\n+                path.display());\n+\n+        if let Err(err) = safe_remove_dir_all(&path) {\n+            sess.warn(&format!(\"Failed to garbage collect incremental \\\n+                                compilation session directory `{}`: {}\",\n+                               path.display(),\n+                               err));\n+        } else {\n+            delete_session_dir_lock_file(sess, &lock_file_path(&path));\n+        }\n+\n+        // Let's make it explicit that the file lock is released at this point,\n+        // or rather, that we held on to it until here\n+        mem::drop(lock);\n+    }\n+\n+    Ok(())\n+}\n+\n+fn all_except_most_recent(deletion_candidates: Vec<(SystemTime, PathBuf, Option<flock::Lock>)>)\n+                          -> FnvHashMap<PathBuf, Option<flock::Lock>> {\n+    let most_recent = deletion_candidates.iter()\n+                                         .map(|&(timestamp, _, _)| timestamp)\n+                                         .max();\n+\n+    if let Some(most_recent) = most_recent {\n+        deletion_candidates.into_iter()\n+                           .filter(|&(timestamp, _, _)| timestamp != most_recent)\n+                           .map(|(_, path, lock)| (path, lock))\n+                           .collect()\n+    } else {\n+        FnvHashMap()\n+    }\n+}\n+\n+/// Since paths of artifacts within session directories can get quite long, we\n+/// need to support deleting files with very long paths. The regular\n+/// WinApi functions only support paths up to 260 characters, however. In order\n+/// to circumvent this limitation, we canonicalize the path of the directory\n+/// before passing it to std::fs::remove_dir_all(). This will convert the path\n+/// into the '\\\\?\\' format, which supports much longer paths.\n+fn safe_remove_dir_all(p: &Path) -> io::Result<()> {\n+    if p.exists() {\n+        let canonicalized = try!(p.canonicalize());\n+        std_fs::remove_dir_all(canonicalized)\n+    } else {\n+        Ok(())\n+    }\n+}\n+\n+fn safe_remove_file(p: &Path) -> io::Result<()> {\n+    if p.exists() {\n+        let canonicalized = try!(p.canonicalize());\n+        std_fs::remove_file(canonicalized)\n+    } else {\n+        Ok(())\n+    }\n+}\n+\n+#[test]\n+fn test_all_except_most_recent() {\n+    assert_eq!(all_except_most_recent(\n+        vec![\n+            (UNIX_EPOCH + Duration::new(4, 0), PathBuf::from(\"4\"), None),\n+            (UNIX_EPOCH + Duration::new(1, 0), PathBuf::from(\"1\"), None),\n+            (UNIX_EPOCH + Duration::new(5, 0), PathBuf::from(\"5\"), None),\n+            (UNIX_EPOCH + Duration::new(3, 0), PathBuf::from(\"3\"), None),\n+            (UNIX_EPOCH + Duration::new(2, 0), PathBuf::from(\"2\"), None),\n+        ]).keys().cloned().collect::<FnvHashSet<PathBuf>>(),\n+        vec![\n+            PathBuf::from(\"1\"),\n+            PathBuf::from(\"2\"),\n+            PathBuf::from(\"3\"),\n+            PathBuf::from(\"4\"),\n+        ].into_iter().collect::<FnvHashSet<PathBuf>>()\n+    );\n+\n+    assert_eq!(all_except_most_recent(\n+        vec![\n+        ]).keys().cloned().collect::<FnvHashSet<PathBuf>>(),\n+        FnvHashSet()\n+    );\n+}\n+\n+#[test]\n+fn test_timestamp_serialization() {\n+    for i in 0 .. 1_000u64 {\n+        let time = UNIX_EPOCH + Duration::new(i * 1_434_578, (i as u32) * 239_000);\n+        let s = timestamp_to_string(time);\n+        assert_eq!(Ok(time), string_to_timestamp(&s));\n+    }\n+}\n+\n+#[test]\n+fn test_find_source_directory_in_iter() {\n+    let already_visited = FnvHashSet();\n+\n+    // Find newest\n+    assert_eq!(find_source_directory_in_iter(\n+        vec![PathBuf::from(\"crate-dir/s-3234-0000-svh\"),\n+             PathBuf::from(\"crate-dir/s-2234-0000-svh\"),\n+             PathBuf::from(\"crate-dir/s-1234-0000-svh\")].into_iter(), &already_visited),\n+        Some(PathBuf::from(\"crate-dir/s-3234-0000-svh\")));\n+\n+    // Filter out \"-working\"\n+    assert_eq!(find_source_directory_in_iter(\n+        vec![PathBuf::from(\"crate-dir/s-3234-0000-working\"),\n+             PathBuf::from(\"crate-dir/s-2234-0000-svh\"),\n+             PathBuf::from(\"crate-dir/s-1234-0000-svh\")].into_iter(), &already_visited),\n+        Some(PathBuf::from(\"crate-dir/s-2234-0000-svh\")));\n+\n+    // Handle empty\n+    assert_eq!(find_source_directory_in_iter(vec![].into_iter(), &already_visited),\n+               None);\n+\n+    // Handle only working\n+    assert_eq!(find_source_directory_in_iter(\n+        vec![PathBuf::from(\"crate-dir/s-3234-0000-working\"),\n+             PathBuf::from(\"crate-dir/s-2234-0000-working\"),\n+             PathBuf::from(\"crate-dir/s-1234-0000-working\")].into_iter(), &already_visited),\n+        None);\n+}\n+\n+#[test]\n+fn test_find_metadata_hashes_iter()\n+{\n+    assert_eq!(find_metadata_hashes_iter(\"testsvh2\",\n+        vec![\n+            String::from(\"s-timestamp1-testsvh1\"),\n+            String::from(\"s-timestamp2-testsvh2\"),\n+            String::from(\"s-timestamp3-testsvh3\"),\n+        ].into_iter()),\n+        Some(OsString::from(\"s-timestamp2-testsvh2\"))\n+    );\n+\n+    assert_eq!(find_metadata_hashes_iter(\"testsvh2\",\n+        vec![\n+            String::from(\"s-timestamp1-testsvh1\"),\n+            String::from(\"s-timestamp2-testsvh2\"),\n+            String::from(\"invalid-name\"),\n+        ].into_iter()),\n+        Some(OsString::from(\"s-timestamp2-testsvh2\"))\n+    );\n+\n+    assert_eq!(find_metadata_hashes_iter(\"testsvh2\",\n+        vec![\n+            String::from(\"s-timestamp1-testsvh1\"),\n+            String::from(\"s-timestamp2-testsvh2-working\"),\n+            String::from(\"s-timestamp3-testsvh3\"),\n+        ].into_iter()),\n+        None\n+    );\n+\n+    assert_eq!(find_metadata_hashes_iter(\"testsvh1\",\n+        vec![\n+            String::from(\"s-timestamp1-random1-working\"),\n+            String::from(\"s-timestamp2-random2-working\"),\n+            String::from(\"s-timestamp3-random3-working\"),\n+        ].into_iter()),\n+        None\n+    );\n+\n+    assert_eq!(find_metadata_hashes_iter(\"testsvh2\",\n+        vec![\n+            String::from(\"timestamp1-testsvh2\"),\n+            String::from(\"timestamp2-testsvh2\"),\n+            String::from(\"timestamp3-testsvh2\"),\n+        ].into_iter()),\n+        None\n+    );\n+}\n+\n+#[test]\n+fn test_encode_base_36() {\n+    fn test(n: u64) {\n+        assert_eq!(Ok(n), u64::from_str_radix(&encode_base_36(n)[..], 36));\n+    }\n+\n+    test(0);\n+    test(1);\n+    test(35);\n+    test(36);\n+    test(37);\n+    test(u64::max_value());\n+\n+    for i in 0 .. 1_000 {\n+        test(i * 983);\n+    }\n+}"}, {"sha": "95bee669d3256151cafdc3ba174f21ec4391e591", "filename": "src/librustc_incremental/persist/hash.rs", "status": "modified", "additions": 45, "deletions": 9, "changes": 54, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fhash.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fhash.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fhash.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -15,14 +15,15 @@ use rustc::hir::def_id::DefId;\n use rustc::hir::svh::Svh;\n use rustc::ty::TyCtxt;\n use rustc_data_structures::fnv::FnvHashMap;\n+use rustc_data_structures::flock;\n use rustc_serialize::Decodable;\n use std::io::{ErrorKind, Read};\n use std::fs::File;\n use syntax::ast;\n \n use IncrementalHashesMap;\n use super::data::*;\n-use super::util::*;\n+use super::fs::*;\n \n pub struct HashContext<'a, 'tcx: 'a> {\n     pub tcx: TyCtxt<'a, 'tcx, 'tcx>,\n@@ -128,19 +129,43 @@ impl<'a, 'tcx> HashContext<'a, 'tcx> {\n         debug!(\"load_data: svh={}\", svh);\n         assert!(old.is_none(), \"loaded data for crate {:?} twice\", cnum);\n \n-        if let Some(path) = metadata_hash_path(self.tcx, cnum) {\n-            debug!(\"load_data: path={:?}\", path);\n+        if let Some(session_dir) = find_metadata_hashes_for(self.tcx, cnum) {\n+            debug!(\"load_data: session_dir={:?}\", session_dir);\n+\n+            // Lock the directory we'll be reading  the hashes from.\n+            let lock_file_path = lock_file_path(&session_dir);\n+            let _lock = match flock::Lock::new(&lock_file_path,\n+                                               false,   // don't wait\n+                                               false,   // don't create the lock-file\n+                                               false) { // shared lock\n+                Ok(lock) => lock,\n+                Err(err) => {\n+                    debug!(\"Could not acquire lock on `{}` while trying to \\\n+                            load metadata hashes: {}\",\n+                            lock_file_path.display(),\n+                            err);\n+\n+                    // Could not acquire the lock. The directory is probably in\n+                    // in the process of being deleted. It's OK to just exit\n+                    // here. It's the same scenario as if the file had not\n+                    // existed in the first place.\n+                    return\n+                }\n+            };\n+\n+            let hashes_file_path = metadata_hash_import_path(&session_dir);\n+\n             let mut data = vec![];\n             match\n-                File::open(&path)\n-                .and_then(|mut file| file.read_to_end(&mut data))\n+                File::open(&hashes_file_path)\n+                     .and_then(|mut file| file.read_to_end(&mut data))\n             {\n                 Ok(_) => {\n-                    match self.load_from_data(cnum, &data) {\n+                    match self.load_from_data(cnum, &data, svh) {\n                         Ok(()) => { }\n                         Err(err) => {\n                             bug!(\"decoding error in dep-graph from `{}`: {}\",\n-                                 path.display(), err);\n+                                 &hashes_file_path.display(), err);\n                         }\n                     }\n                 }\n@@ -152,7 +177,7 @@ impl<'a, 'tcx> HashContext<'a, 'tcx> {\n                         _ => {\n                             self.tcx.sess.err(\n                                 &format!(\"could not load dep information from `{}`: {}\",\n-                                         path.display(), err));\n+                                         hashes_file_path.display(), err));\n                             return;\n                         }\n                     }\n@@ -161,11 +186,22 @@ impl<'a, 'tcx> HashContext<'a, 'tcx> {\n         }\n     }\n \n-    fn load_from_data(&mut self, cnum: ast::CrateNum, data: &[u8]) -> Result<(), Error> {\n+    fn load_from_data(&mut self,\n+                      cnum: ast::CrateNum,\n+                      data: &[u8],\n+                      expected_svh: Svh) -> Result<(), Error> {\n         debug!(\"load_from_data(cnum={})\", cnum);\n \n         // Load up the hashes for the def-ids from this crate.\n         let mut decoder = Decoder::new(data, 0);\n+        let svh_in_hashes_file = try!(Svh::decode(&mut decoder));\n+\n+        if svh_in_hashes_file != expected_svh {\n+            // We should not be able to get here. If we do, then\n+            // `fs::find_metadata_hashes_for()` has messed up.\n+            bug!(\"mismatch between SVH in crate and SVH in incr. comp. hashes\")\n+        }\n+\n         let serialized_hashes = try!(SerializedMetadataHashes::decode(&mut decoder));\n         for serialized_hash in serialized_hashes.hashes {\n             // the hashes are stored with just a def-index, which is"}, {"sha": "48f95430f26b6f47c650bb003b07924a0dee6c56", "filename": "src/librustc_incremental/persist/load.rs", "status": "modified", "additions": 23, "deletions": 5, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fload.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fload.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fload.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -27,7 +27,7 @@ use super::data::*;\n use super::directory::*;\n use super::dirty_clean;\n use super::hash::*;\n-use super::util::*;\n+use super::fs::*;\n \n pub type DirtyNodes = FnvHashSet<DepNode<DefPathIndex>>;\n \n@@ -45,19 +45,37 @@ pub fn load_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n         return;\n     }\n \n+    match prepare_session_directory(tcx) {\n+        Ok(true) => {\n+            // We successfully allocated a session directory and there is\n+            // something in it to load, so continue\n+        }\n+        Ok(false) => {\n+            // We successfully allocated a session directory, but there is no\n+            // dep-graph data in it to load (because this is the first\n+            // compilation session with this incr. comp. dir.)\n+            return\n+        }\n+        Err(()) => {\n+            // Something went wrong while trying to allocate the session\n+            // directory. Don't try to use it any further.\n+            return\n+        }\n+    }\n+\n     let _ignore = tcx.dep_graph.in_ignore();\n     load_dep_graph_if_exists(tcx, incremental_hashes_map);\n }\n \n fn load_dep_graph_if_exists<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n                                       incremental_hashes_map: &IncrementalHashesMap) {\n-    let dep_graph_path = dep_graph_path(tcx).unwrap();\n+    let dep_graph_path = dep_graph_path(tcx.sess);\n     let dep_graph_data = match load_data(tcx.sess, &dep_graph_path) {\n         Some(p) => p,\n         None => return // no file\n     };\n \n-    let work_products_path = tcx_work_products_path(tcx).unwrap();\n+    let work_products_path = work_products_path(tcx.sess);\n     let work_products_data = match load_data(tcx.sess, &work_products_path) {\n         Some(p) => p,\n         None => return // no file\n@@ -258,7 +276,7 @@ fn reconcile_work_products<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n                    .saved_files\n                    .iter()\n                    .all(|&(_, ref file_name)| {\n-                       let path = in_incr_comp_dir(tcx.sess, &file_name).unwrap();\n+                       let path = in_incr_comp_dir_sess(tcx.sess, &file_name);\n                        path.exists()\n                    });\n             if all_files_exist {\n@@ -276,7 +294,7 @@ fn delete_dirty_work_product(tcx: TyCtxt,\n                              swp: SerializedWorkProduct) {\n     debug!(\"delete_dirty_work_product({:?})\", swp);\n     for &(_, ref file_name) in &swp.work_product.saved_files {\n-        let path = in_incr_comp_dir(tcx.sess, file_name).unwrap();\n+        let path = in_incr_comp_dir_sess(tcx.sess, file_name);\n         match fs::remove_file(&path) {\n             Ok(()) => { }\n             Err(err) => {"}, {"sha": "ba0f71971bb45e9afce432e341843b41d164816c", "filename": "src/librustc_incremental/persist/mod.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fmod.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -15,15 +15,16 @@\n mod data;\n mod directory;\n mod dirty_clean;\n+mod fs;\n mod hash;\n mod load;\n mod preds;\n mod save;\n-mod util;\n mod work_product;\n \n+pub use self::fs::finalize_session_directory;\n+pub use self::fs::in_incr_comp_dir;\n pub use self::load::load_dep_graph;\n pub use self::save::save_dep_graph;\n pub use self::save::save_work_products;\n pub use self::work_product::save_trans_partition;\n-pub use self::util::in_incr_comp_dir;"}, {"sha": "d31252be5e857295bce2ea16ffb321148e009c66", "filename": "src/librustc_incremental/persist/save.rs", "status": "modified", "additions": 19, "deletions": 16, "changes": 35, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -11,7 +11,7 @@\n use rbml::opaque::Encoder;\n use rustc::dep_graph::DepNode;\n use rustc::hir::def_id::DefId;\n-use rustc::middle::cstore::LOCAL_CRATE;\n+use rustc::hir::svh::Svh;\n use rustc::session::Session;\n use rustc::ty::TyCtxt;\n use rustc_data_structures::fnv::FnvHashMap;\n@@ -26,10 +26,11 @@ use super::data::*;\n use super::directory::*;\n use super::hash::*;\n use super::preds::*;\n-use super::util::*;\n+use super::fs::*;\n \n pub fn save_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n-                                incremental_hashes_map: &IncrementalHashesMap) {\n+                                incremental_hashes_map: &IncrementalHashesMap,\n+                                svh: Svh) {\n     debug!(\"save_dep_graph()\");\n     let _ignore = tcx.dep_graph.in_ignore();\n     let sess = tcx.sess;\n@@ -41,31 +42,31 @@ pub fn save_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     let query = tcx.dep_graph.query();\n     let preds = Predecessors::new(&query, &mut hcx);\n     save_in(sess,\n-            dep_graph_path(tcx),\n+            dep_graph_path(sess),\n             |e| encode_dep_graph(&preds, &mut builder, e));\n     save_in(sess,\n-            metadata_hash_path(tcx, LOCAL_CRATE),\n-            |e| encode_metadata_hashes(tcx, &preds, &mut builder, e));\n+            metadata_hash_export_path(sess),\n+            |e| encode_metadata_hashes(tcx, svh, &preds, &mut builder, e));\n }\n \n-pub fn save_work_products(sess: &Session, local_crate_name: &str) {\n+pub fn save_work_products(sess: &Session) {\n+    if sess.opts.incremental.is_none() {\n+        return;\n+    }\n+\n     debug!(\"save_work_products()\");\n     let _ignore = sess.dep_graph.in_ignore();\n-    let path = sess_work_products_path(sess, local_crate_name);\n+    let path = work_products_path(sess);\n     save_in(sess, path, |e| encode_work_products(sess, e));\n }\n \n-fn save_in<F>(sess: &Session, opt_path_buf: Option<PathBuf>, encode: F)\n+fn save_in<F>(sess: &Session, path_buf: PathBuf, encode: F)\n     where F: FnOnce(&mut Encoder) -> io::Result<()>\n {\n-    let path_buf = match opt_path_buf {\n-        Some(p) => p,\n-        None => return,\n-    };\n-\n-    // FIXME(#32754) lock file?\n-\n     // delete the old dep-graph, if any\n+    // Note: It's important that we actually delete the old file and not just\n+    // truncate and overwrite it, since it might be a shared hard-link, the\n+    // underlying data of which we don't want to modify\n     if path_buf.exists() {\n         match fs::remove_file(&path_buf) {\n             Ok(()) => {}\n@@ -155,6 +156,7 @@ pub fn encode_dep_graph(preds: &Predecessors,\n }\n \n pub fn encode_metadata_hashes(tcx: TyCtxt,\n+                              svh: Svh,\n                               preds: &Predecessors,\n                               builder: &mut DefIdDirectoryBuilder,\n                               encoder: &mut Encoder)\n@@ -220,6 +222,7 @@ pub fn encode_metadata_hashes(tcx: TyCtxt,\n     }\n \n     // Encode everything.\n+    try!(svh.encode(encoder));\n     try!(serialized_hashes.encode(encoder));\n \n     Ok(())"}, {"sha": "f1e81fdb266b9b201271281c02851e8248b16083", "filename": "src/librustc_incremental/persist/util.rs", "status": "removed", "additions": 0, "deletions": 95, "changes": 95, "blob_url": "https://github.com/rust-lang/rust/blob/7a187c39c79e21cd61d1688d449bdd52d7510281/src%2Flibrustc_incremental%2Fpersist%2Futil.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7a187c39c79e21cd61d1688d449bdd52d7510281/src%2Flibrustc_incremental%2Fpersist%2Futil.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Futil.rs?ref=7a187c39c79e21cd61d1688d449bdd52d7510281", "patch": "@@ -1,95 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-use rustc::middle::cstore::LOCAL_CRATE;\n-use rustc::session::Session;\n-use rustc::ty::TyCtxt;\n-\n-use std::fs;\n-use std::io;\n-use std::path::{Path, PathBuf};\n-use syntax::ast;\n-\n-pub fn dep_graph_path(tcx: TyCtxt) -> Option<PathBuf> {\n-    tcx_path(tcx, LOCAL_CRATE, \"local\")\n-}\n-\n-pub fn metadata_hash_path(tcx: TyCtxt, cnum: ast::CrateNum) -> Option<PathBuf> {\n-    tcx_path(tcx, cnum, \"metadata\")\n-}\n-\n-pub fn tcx_work_products_path(tcx: TyCtxt) -> Option<PathBuf> {\n-    let crate_name = tcx.crate_name(LOCAL_CRATE);\n-    sess_work_products_path(tcx.sess, &crate_name)\n-}\n-\n-pub fn sess_work_products_path(sess: &Session,\n-                               local_crate_name: &str)\n-                               -> Option<PathBuf> {\n-    let crate_disambiguator = sess.local_crate_disambiguator();\n-    path(sess, local_crate_name, &crate_disambiguator, \"work-products\")\n-}\n-\n-pub fn in_incr_comp_dir(sess: &Session, file_name: &str) -> Option<PathBuf> {\n-    sess.opts.incremental.as_ref().map(|incr_dir| incr_dir.join(file_name))\n-}\n-\n-fn tcx_path(tcx: TyCtxt,\n-            cnum: ast::CrateNum,\n-            middle: &str)\n-            -> Option<PathBuf> {\n-    path(tcx.sess, &tcx.crate_name(cnum), &tcx.crate_disambiguator(cnum), middle)\n-}\n-\n-fn path(sess: &Session,\n-        crate_name: &str,\n-        crate_disambiguator: &str,\n-        middle: &str)\n-        -> Option<PathBuf> {\n-    // For now, just save/load dep-graph from\n-    // directory/dep_graph.rbml\n-    sess.opts.incremental.as_ref().and_then(|incr_dir| {\n-        match create_dir_racy(&incr_dir) {\n-            Ok(()) => {}\n-            Err(err) => {\n-                sess.err(\n-                    &format!(\"could not create the directory `{}`: {}\",\n-                             incr_dir.display(), err));\n-                return None;\n-            }\n-        }\n-\n-        let file_name = format!(\"{}-{}.{}.bin\", crate_name, crate_disambiguator, middle);\n-\n-        Some(incr_dir.join(file_name))\n-    })\n-}\n-\n-// Like std::fs::create_dir_all, except handles concurrent calls among multiple\n-// threads or processes.\n-fn create_dir_racy(path: &Path) -> io::Result<()> {\n-    match fs::create_dir(path) {\n-        Ok(()) => return Ok(()),\n-        Err(ref e) if e.kind() == io::ErrorKind::AlreadyExists => return Ok(()),\n-        Err(ref e) if e.kind() == io::ErrorKind::NotFound => {}\n-        Err(e) => return Err(e),\n-    }\n-    match path.parent() {\n-        Some(p) => try!(create_dir_racy(p)),\n-        None => return Err(io::Error::new(io::ErrorKind::Other,\n-                                          \"failed to create whole tree\")),\n-    }\n-    match fs::create_dir(path) {\n-        Ok(()) => Ok(()),\n-        Err(ref e) if e.kind() == io::ErrorKind::AlreadyExists => Ok(()),\n-        Err(e) => Err(e),\n-    }\n-}\n-"}, {"sha": "a9ebd27ce9928fc22eec587d5420de8cf38745b1", "filename": "src/librustc_incremental/persist/work_product.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fwork_product.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_incremental%2Fpersist%2Fwork_product.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fwork_product.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -10,7 +10,7 @@\n \n //! This module contains files for saving intermediate work-products.\n \n-use persist::util::*;\n+use persist::fs::*;\n use rustc::dep_graph::{WorkProduct, WorkProductId};\n use rustc::session::Session;\n use rustc::session::config::OutputType;\n@@ -35,7 +35,7 @@ pub fn save_trans_partition(sess: &Session,\n         files.iter()\n              .map(|&(kind, ref path)| {\n                  let file_name = format!(\"cgu-{}.{}\", cgu_name, kind.extension());\n-                 let path_in_incr_dir = in_incr_comp_dir(sess, &file_name).unwrap();\n+                 let path_in_incr_dir = in_incr_comp_dir_sess(sess, &file_name);\n                  match link_or_copy(path, &path_in_incr_dir) {\n                      Ok(_) => Some((kind, file_name)),\n                      Err(err) => {"}, {"sha": "081b4431bd7b80dfe6b9df5e7de7627705df30ea", "filename": "src/librustc_trans/back/write.rs", "status": "modified", "additions": 13, "deletions": 9, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_trans%2Fback%2Fwrite.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustc_trans%2Fback%2Fwrite.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fback%2Fwrite.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -10,7 +10,7 @@\n \n use back::lto;\n use back::link::{get_linker, remove};\n-use rustc_incremental::save_trans_partition;\n+use rustc_incremental::{save_trans_partition, in_incr_comp_dir};\n use session::config::{OutputFilenames, OutputTypes, Passes, SomePasses, AllPasses};\n use session::Session;\n use session::config::{self, OutputType};\n@@ -328,8 +328,9 @@ struct CodegenContext<'a> {\n     remark: Passes,\n     // Worker thread number\n     worker: usize,\n-    // Directory where incremental data is stored (if any)\n-    incremental: Option<PathBuf>,\n+    // The incremental compilation session directory, or None if we are not\n+    // compiling incrementally\n+    incr_comp_session_dir: Option<PathBuf>\n }\n \n impl<'a> CodegenContext<'a> {\n@@ -340,7 +341,7 @@ impl<'a> CodegenContext<'a> {\n             plugin_passes: sess.plugin_llvm_passes.borrow().clone(),\n             remark: sess.opts.cg.remark.clone(),\n             worker: 0,\n-            incremental: sess.opts.incremental.clone(),\n+            incr_comp_session_dir: sess.incr_comp_session_dir_opt().map(|r| r.clone())\n         }\n     }\n }\n@@ -962,17 +963,20 @@ fn execute_work_item(cgcx: &CodegenContext,\n                                      work_item.output_names);\n             }\n             ModuleSource::Preexisting(wp) => {\n-                let incremental = cgcx.incremental.as_ref().unwrap();\n+                let incr_comp_session_dir = cgcx.incr_comp_session_dir\n+                                                .as_ref()\n+                                                .unwrap();\n                 let name = &work_item.mtrans.name;\n                 for (kind, saved_file) in wp.saved_files {\n                     let obj_out = work_item.output_names.temp_path(kind, Some(name));\n-                    let source_file = incremental.join(&saved_file);\n+                    let source_file = in_incr_comp_dir(&incr_comp_session_dir,\n+                                                       &saved_file);\n                     debug!(\"copying pre-existing module `{}` from {:?} to {}\",\n                            work_item.mtrans.name,\n                            source_file,\n                            obj_out.display());\n                     match link_or_copy(&source_file, &obj_out) {\n-                        Ok(()) => { }\n+                        Ok(_) => { }\n                         Err(err) => {\n                             cgcx.handler.err(&format!(\"unable to copy {} to {}: {}\",\n                                                       source_file.display(),\n@@ -1018,7 +1022,7 @@ fn run_work_multithreaded(sess: &Session,\n         let mut tx = Some(tx);\n         futures.push(rx);\n \n-        let incremental = sess.opts.incremental.clone();\n+        let incr_comp_session_dir = sess.incr_comp_session_dir_opt().map(|r| r.clone());\n \n         thread::Builder::new().name(format!(\"codegen-{}\", i)).spawn(move || {\n             let diag_handler = Handler::with_emitter(true, false, box diag_emitter);\n@@ -1031,7 +1035,7 @@ fn run_work_multithreaded(sess: &Session,\n                 plugin_passes: plugin_passes,\n                 remark: remark,\n                 worker: i,\n-                incremental: incremental,\n+                incr_comp_session_dir: incr_comp_session_dir\n             };\n \n             loop {"}, {"sha": "d66d2001f2304d626ef79277f3116d44c1702873", "filename": "src/librustdoc/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustdoc%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustdoc%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2FCargo.toml?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -16,6 +16,7 @@ rustc_back = { path = \"../librustc_back\" }\n rustc_const_eval = { path = \"../librustc_const_eval\" }\n rustc_const_math = { path = \"../librustc_const_math\" }\n rustc_driver = { path = \"../librustc_driver\" }\n+rustc_data_structures = { path = \"../librustc_data_structures\" }\n rustc_errors = { path = \"../librustc_errors\" }\n rustc_lint = { path = \"../librustc_lint\" }\n rustc_metadata = { path = \"../librustc_metadata\" }"}, {"sha": "6d523ff381556799d5715fdbcdaa18eb45fdf98b", "filename": "src/librustdoc/html/render.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustdoc%2Fhtml%2Frender.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustdoc%2Fhtml%2Frender.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Frender.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -62,6 +62,7 @@ use rustc::middle::stability;\n use rustc::session::config::get_unstable_features_setting;\n use rustc::hir;\n use rustc::util::nodemap::{FnvHashMap, FnvHashSet};\n+use rustc_data_structures::flock;\n \n use clean::{self, Attributes, GetDefId};\n use doctree;\n@@ -651,7 +652,7 @@ fn write_shared(cx: &Context,\n     // docs placed in the output directory, so this needs to be a synchronized\n     // operation with respect to all other rustdocs running around.\n     try_err!(mkdir(&cx.dst), &cx.dst);\n-    let _lock = ::flock::Lock::new(&cx.dst.join(\".lock\"));\n+    let _lock = flock::Lock::panicking_new(&cx.dst.join(\".lock\"), true, true, true);\n \n     // Add all the static files. These may already exist, but we just\n     // overwrite them anyway to make sure that they're fresh and up-to-date."}, {"sha": "0e685f063bd7b52dfc40f5f067bebaede45f7c65", "filename": "src/librustdoc/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustdoc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Flibrustdoc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Flib.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -35,6 +35,7 @@ extern crate libc;\n extern crate rustc;\n extern crate rustc_const_eval;\n extern crate rustc_const_math;\n+extern crate rustc_data_structures;\n extern crate rustc_trans;\n extern crate rustc_driver;\n extern crate rustc_resolve;\n@@ -86,7 +87,6 @@ pub mod plugins;\n pub mod visit_ast;\n pub mod visit_lib;\n pub mod test;\n-mod flock;\n \n use clean::Attributes;\n "}, {"sha": "fde2f83e220f92b4a5784904f0f37e9fb3b6a4f0", "filename": "src/rustc/Cargo.lock", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Frustc%2FCargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Frustc%2FCargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frustc%2FCargo.lock?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -363,6 +363,7 @@ dependencies = [\n  \"rustc_back 0.0.0\",\n  \"rustc_const_eval 0.0.0\",\n  \"rustc_const_math 0.0.0\",\n+ \"rustc_data_structures 0.0.0\",\n  \"rustc_driver 0.0.0\",\n  \"rustc_errors 0.0.0\",\n  \"rustc_lint 0.0.0\","}, {"sha": "228d6ada01dcc4d1dc3b2d98ac83e7ba44edf54a", "filename": "src/tools/compiletest/src/runtest.rs", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Ftools%2Fcompiletest%2Fsrc%2Fruntest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2c01bb885108c436adae2006632ff6dfc0a5f2cd/src%2Ftools%2Fcompiletest%2Fsrc%2Fruntest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fcompiletest%2Fsrc%2Fruntest.rs?ref=2c01bb885108c436adae2006632ff6dfc0a5f2cd", "patch": "@@ -1976,7 +1976,10 @@ actual:\\n\\\n         // runs.\n         let incremental_dir = self.incremental_dir();\n         if incremental_dir.exists() {\n-            fs::remove_dir_all(&incremental_dir).unwrap();\n+            // Canonicalizing the path will convert it to the //?/ format\n+            // on Windows, which enables paths longer than 260 character\n+            let canonicalized = incremental_dir.canonicalize().unwrap();\n+            fs::remove_dir_all(canonicalized).unwrap();\n         }\n         fs::create_dir_all(&incremental_dir).unwrap();\n \n@@ -2041,7 +2044,7 @@ actual:\\n\\\n \n     /// Directory where incremental work products are stored.\n     fn incremental_dir(&self) -> PathBuf {\n-        self.output_base_name().with_extension(\"incremental\")\n+        self.output_base_name().with_extension(\"inc\")\n     }\n \n     fn run_rmake_test(&self) {"}]}