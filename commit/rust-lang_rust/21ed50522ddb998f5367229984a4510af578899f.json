{"sha": "21ed50522ddb998f5367229984a4510af578899f", "node_id": "MDY6Q29tbWl0NzI0NzEyOjIxZWQ1MDUyMmRkYjk5OGY1MzY3MjI5OTg0YTQ1MTBhZjU3ODg5OWY=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-02-14T01:37:50Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-02-14T01:37:50Z"}, "message": "Auto merge of #68693 - Zoxc:query-no-arc, r=michaelwoerister\n\nConstruct query job latches on-demand\n\nr? @michaelwoerister", "tree": {"sha": "1b6195db57d69b4b442e4542dd45db6cea77b305", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/1b6195db57d69b4b442e4542dd45db6cea77b305"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/21ed50522ddb998f5367229984a4510af578899f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/21ed50522ddb998f5367229984a4510af578899f", "html_url": "https://github.com/rust-lang/rust/commit/21ed50522ddb998f5367229984a4510af578899f", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/21ed50522ddb998f5367229984a4510af578899f/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "10104085e4f9f52f405fa1cc5e5e18c4c4cc72d1", "url": "https://api.github.com/repos/rust-lang/rust/commits/10104085e4f9f52f405fa1cc5e5e18c4c4cc72d1", "html_url": "https://github.com/rust-lang/rust/commit/10104085e4f9f52f405fa1cc5e5e18c4c4cc72d1"}, {"sha": "5206827933177ab83e91c38042597b9061c85b96", "url": "https://api.github.com/repos/rust-lang/rust/commits/5206827933177ab83e91c38042597b9061c85b96", "html_url": "https://github.com/rust-lang/rust/commit/5206827933177ab83e91c38042597b9061c85b96"}], "stats": {"total": 484, "additions": 323, "deletions": 161}, "files": [{"sha": "5a415fa954f0d52bda6ea5ee5b49ff961a5250f9", "filename": "src/librustc/ty/context.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc%2Fty%2Fcontext.rs", "raw_url": "https://github.com/rust-lang/rust/raw/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc%2Fty%2Fcontext.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fty%2Fcontext.rs?ref=21ed50522ddb998f5367229984a4510af578899f", "patch": "@@ -1612,7 +1612,7 @@ pub mod tls {\n \n     use crate::dep_graph::TaskDeps;\n     use crate::ty::query;\n-    use rustc_data_structures::sync::{self, Lock, Lrc};\n+    use rustc_data_structures::sync::{self, Lock};\n     use rustc_data_structures::thin_vec::ThinVec;\n     use rustc_data_structures::OnDrop;\n     use rustc_errors::Diagnostic;\n@@ -1637,7 +1637,7 @@ pub mod tls {\n \n         /// The current query job, if any. This is updated by `JobOwner::start` in\n         /// `ty::query::plumbing` when executing a query.\n-        pub query: Option<Lrc<query::QueryJob<'tcx>>>,\n+        pub query: Option<query::QueryJobId>,\n \n         /// Where to store diagnostics for the current query job, if any.\n         /// This is updated by `JobOwner::start` in `ty::query::plumbing` when executing a query."}, {"sha": "8aae57e72cd527b4239a662b87ff6318f0d61618", "filename": "src/librustc/ty/query/job.rs", "status": "modified", "additions": 203, "deletions": 102, "changes": 305, "blob_url": "https://github.com/rust-lang/rust/blob/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc%2Fty%2Fquery%2Fjob.rs", "raw_url": "https://github.com/rust-lang/rust/raw/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc%2Fty%2Fquery%2Fjob.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fty%2Fquery%2Fjob.rs?ref=21ed50522ddb998f5367229984a4510af578899f", "patch": "@@ -1,20 +1,23 @@\n+use crate::dep_graph::DepKind;\n use crate::ty::context::TyCtxt;\n use crate::ty::query::plumbing::CycleError;\n use crate::ty::query::Query;\n use crate::ty::tls;\n \n-use rustc_data_structures::sync::Lrc;\n+use rustc_data_structures::fx::FxHashMap;\n use rustc_span::Span;\n \n-#[cfg(not(parallel_compiler))]\n-use std::ptr;\n+use std::convert::TryFrom;\n+use std::marker::PhantomData;\n+use std::num::NonZeroU32;\n \n #[cfg(parallel_compiler)]\n use {\n     parking_lot::{Condvar, Mutex},\n     rustc_data_structures::fx::FxHashSet,\n     rustc_data_structures::stable_hasher::{HashStable, StableHasher},\n     rustc_data_structures::sync::Lock,\n+    rustc_data_structures::sync::Lrc,\n     rustc_data_structures::{jobserver, OnDrop},\n     rustc_rayon_core as rayon_core,\n     rustc_span::DUMMY_SP,\n@@ -30,61 +33,130 @@ pub struct QueryInfo<'tcx> {\n     pub query: Query<'tcx>,\n }\n \n-/// Representss an object representing an active query job.\n-pub struct QueryJob<'tcx> {\n+type QueryMap<'tcx> = FxHashMap<QueryJobId, QueryJobInfo<'tcx>>;\n+\n+/// A value uniquely identifiying an active query job within a shard in the query cache.\n+#[derive(Copy, Clone, Eq, PartialEq, Hash)]\n+pub struct QueryShardJobId(pub NonZeroU32);\n+\n+/// A value uniquely identifiying an active query job.\n+#[derive(Copy, Clone, Eq, PartialEq, Hash)]\n+pub struct QueryJobId {\n+    /// Which job within a shard is this\n+    pub job: QueryShardJobId,\n+\n+    /// In which shard is this job\n+    pub shard: u16,\n+\n+    /// What kind of query this job is\n+    pub kind: DepKind,\n+}\n+\n+impl QueryJobId {\n+    pub fn new(job: QueryShardJobId, shard: usize, kind: DepKind) -> Self {\n+        QueryJobId { job, shard: u16::try_from(shard).unwrap(), kind }\n+    }\n+\n+    fn query<'tcx>(self, map: &QueryMap<'tcx>) -> Query<'tcx> {\n+        map.get(&self).unwrap().info.query.clone()\n+    }\n+\n+    #[cfg(parallel_compiler)]\n+    fn span(self, map: &QueryMap<'_>) -> Span {\n+        map.get(&self).unwrap().job.span\n+    }\n+\n+    #[cfg(parallel_compiler)]\n+    fn parent(self, map: &QueryMap<'_>) -> Option<QueryJobId> {\n+        map.get(&self).unwrap().job.parent\n+    }\n+\n+    #[cfg(parallel_compiler)]\n+    fn latch<'a, 'tcx>(self, map: &'a QueryMap<'tcx>) -> Option<&'a QueryLatch<'tcx>> {\n+        map.get(&self).unwrap().job.latch.as_ref()\n+    }\n+}\n+\n+pub struct QueryJobInfo<'tcx> {\n     pub info: QueryInfo<'tcx>,\n+    pub job: QueryJob<'tcx>,\n+}\n+\n+/// Represents an active query job.\n+#[derive(Clone)]\n+pub struct QueryJob<'tcx> {\n+    pub id: QueryShardJobId,\n+\n+    /// The span corresponding to the reason for which this query was required.\n+    pub span: Span,\n \n     /// The parent query job which created this job and is implicitly waiting on it.\n-    pub parent: Option<Lrc<QueryJob<'tcx>>>,\n+    pub parent: Option<QueryJobId>,\n \n     /// The latch that is used to wait on this job.\n     #[cfg(parallel_compiler)]\n-    latch: QueryLatch<'tcx>,\n+    latch: Option<QueryLatch<'tcx>>,\n+\n+    dummy: PhantomData<QueryLatch<'tcx>>,\n }\n \n impl<'tcx> QueryJob<'tcx> {\n     /// Creates a new query job.\n-    pub fn new(info: QueryInfo<'tcx>, parent: Option<Lrc<QueryJob<'tcx>>>) -> Self {\n+    pub fn new(id: QueryShardJobId, span: Span, parent: Option<QueryJobId>) -> Self {\n         QueryJob {\n-            info,\n+            id,\n+            span,\n             parent,\n             #[cfg(parallel_compiler)]\n-            latch: QueryLatch::new(),\n+            latch: None,\n+            dummy: PhantomData,\n         }\n     }\n \n-    /// Awaits for the query job to complete.\n     #[cfg(parallel_compiler)]\n-    pub(super) fn r#await(&self, tcx: TyCtxt<'tcx>, span: Span) -> Result<(), CycleError<'tcx>> {\n-        tls::with_related_context(tcx, move |icx| {\n-            let waiter = Lrc::new(QueryWaiter {\n-                query: icx.query.clone(),\n-                span,\n-                cycle: Lock::new(None),\n-                condvar: Condvar::new(),\n-            });\n-            self.latch.r#await(&waiter);\n-            // FIXME: Get rid of this lock. We have ownership of the QueryWaiter\n-            // although another thread may still have a Lrc reference so we cannot\n-            // use Lrc::get_mut\n-            let mut cycle = waiter.cycle.lock();\n-            match cycle.take() {\n-                None => Ok(()),\n-                Some(cycle) => Err(cycle),\n-            }\n-        })\n+    pub(super) fn latch(&mut self, _id: QueryJobId) -> QueryLatch<'tcx> {\n+        if self.latch.is_none() {\n+            self.latch = Some(QueryLatch::new());\n+        }\n+        self.latch.as_ref().unwrap().clone()\n     }\n \n     #[cfg(not(parallel_compiler))]\n+    pub(super) fn latch(&mut self, id: QueryJobId) -> QueryLatch<'tcx> {\n+        QueryLatch { id, dummy: PhantomData }\n+    }\n+\n+    /// Signals to waiters that the query is complete.\n+    ///\n+    /// This does nothing for single threaded rustc,\n+    /// as there are no concurrent jobs which could be waiting on us\n+    pub fn signal_complete(self) {\n+        #[cfg(parallel_compiler)]\n+        self.latch.map(|latch| latch.set());\n+    }\n+}\n+\n+#[cfg(not(parallel_compiler))]\n+#[derive(Clone)]\n+pub(super) struct QueryLatch<'tcx> {\n+    id: QueryJobId,\n+    dummy: PhantomData<&'tcx ()>,\n+}\n+\n+#[cfg(not(parallel_compiler))]\n+impl<'tcx> QueryLatch<'tcx> {\n     pub(super) fn find_cycle_in_stack(&self, tcx: TyCtxt<'tcx>, span: Span) -> CycleError<'tcx> {\n+        let query_map = tcx.queries.try_collect_active_jobs().unwrap();\n+\n         // Get the current executing query (waiter) and find the waitee amongst its parents\n-        let mut current_job = tls::with_related_context(tcx, |icx| icx.query.clone());\n+        let mut current_job = tls::with_related_context(tcx, |icx| icx.query);\n         let mut cycle = Vec::new();\n \n         while let Some(job) = current_job {\n-            cycle.push(job.info.clone());\n+            let info = query_map.get(&job).unwrap();\n+            cycle.push(info.info.clone());\n \n-            if ptr::eq(&*job, self) {\n+            if job == self.id {\n                 cycle.reverse();\n \n                 // This is the end of the cycle\n@@ -93,35 +165,24 @@ impl<'tcx> QueryJob<'tcx> {\n                 // Replace it with the span which caused the cycle to form\n                 cycle[0].span = span;\n                 // Find out why the cycle itself was used\n-                let usage =\n-                    job.parent.as_ref().map(|parent| (job.info.span, parent.info.query.clone()));\n+                let usage = info\n+                    .job\n+                    .parent\n+                    .as_ref()\n+                    .map(|parent| (info.info.span, parent.query(&query_map)));\n                 return CycleError { usage, cycle };\n             }\n \n-            current_job = job.parent.clone();\n+            current_job = info.job.parent.clone();\n         }\n \n         panic!(\"did not find a cycle\")\n     }\n-\n-    /// Signals to waiters that the query is complete.\n-    ///\n-    /// This does nothing for single threaded rustc,\n-    /// as there are no concurrent jobs which could be waiting on us\n-    pub fn signal_complete(&self) {\n-        #[cfg(parallel_compiler)]\n-        self.latch.set();\n-    }\n-\n-    #[cfg(parallel_compiler)]\n-    fn as_ptr(&self) -> *const QueryJob<'tcx> {\n-        self as *const _\n-    }\n }\n \n #[cfg(parallel_compiler)]\n struct QueryWaiter<'tcx> {\n-    query: Option<Lrc<QueryJob<'tcx>>>,\n+    query: Option<QueryJobId>,\n     condvar: Condvar,\n     span: Span,\n     cycle: Lock<Option<CycleError<'tcx>>>,\n@@ -142,18 +203,43 @@ struct QueryLatchInfo<'tcx> {\n }\n \n #[cfg(parallel_compiler)]\n-struct QueryLatch<'tcx> {\n-    info: Mutex<QueryLatchInfo<'tcx>>,\n+#[derive(Clone)]\n+pub(super) struct QueryLatch<'tcx> {\n+    info: Lrc<Mutex<QueryLatchInfo<'tcx>>>,\n }\n \n #[cfg(parallel_compiler)]\n impl<'tcx> QueryLatch<'tcx> {\n     fn new() -> Self {\n-        QueryLatch { info: Mutex::new(QueryLatchInfo { complete: false, waiters: Vec::new() }) }\n+        QueryLatch {\n+            info: Lrc::new(Mutex::new(QueryLatchInfo { complete: false, waiters: Vec::new() })),\n+        }\n+    }\n+\n+    /// Awaits for the query job to complete.\n+    #[cfg(parallel_compiler)]\n+    pub(super) fn wait_on(&self, tcx: TyCtxt<'tcx>, span: Span) -> Result<(), CycleError<'tcx>> {\n+        tls::with_related_context(tcx, move |icx| {\n+            let waiter = Lrc::new(QueryWaiter {\n+                query: icx.query,\n+                span,\n+                cycle: Lock::new(None),\n+                condvar: Condvar::new(),\n+            });\n+            self.wait_on_inner(&waiter);\n+            // FIXME: Get rid of this lock. We have ownership of the QueryWaiter\n+            // although another thread may still have a Lrc reference so we cannot\n+            // use Lrc::get_mut\n+            let mut cycle = waiter.cycle.lock();\n+            match cycle.take() {\n+                None => Ok(()),\n+                Some(cycle) => Err(cycle),\n+            }\n+        })\n     }\n \n     /// Awaits the caller on this latch by blocking the current thread.\n-    fn r#await(&self, waiter: &Lrc<QueryWaiter<'tcx>>) {\n+    fn wait_on_inner(&self, waiter: &Lrc<QueryWaiter<'tcx>>) {\n         let mut info = self.info.lock();\n         if !info.complete {\n             // We push the waiter on to the `waiters` list. It can be accessed inside\n@@ -197,7 +283,7 @@ impl<'tcx> QueryLatch<'tcx> {\n \n /// A resumable waiter of a query. The usize is the index into waiters in the query's latch\n #[cfg(parallel_compiler)]\n-type Waiter<'tcx> = (Lrc<QueryJob<'tcx>>, usize);\n+type Waiter = (QueryJobId, usize);\n \n /// Visits all the non-resumable and resumable waiters of a query.\n /// Only waiters in a query are visited.\n@@ -209,26 +295,33 @@ type Waiter<'tcx> = (Lrc<QueryJob<'tcx>>, usize);\n /// required information to resume the waiter.\n /// If all `visit` calls returns None, this function also returns None.\n #[cfg(parallel_compiler)]\n-fn visit_waiters<'tcx, F>(query: Lrc<QueryJob<'tcx>>, mut visit: F) -> Option<Option<Waiter<'tcx>>>\n+fn visit_waiters<'tcx, F>(\n+    query_map: &QueryMap<'tcx>,\n+    query: QueryJobId,\n+    mut visit: F,\n+) -> Option<Option<Waiter>>\n where\n-    F: FnMut(Span, Lrc<QueryJob<'tcx>>) -> Option<Option<Waiter<'tcx>>>,\n+    F: FnMut(Span, QueryJobId) -> Option<Option<Waiter>>,\n {\n     // Visit the parent query which is a non-resumable waiter since it's on the same stack\n-    if let Some(ref parent) = query.parent {\n-        if let Some(cycle) = visit(query.info.span, parent.clone()) {\n+    if let Some(parent) = query.parent(query_map) {\n+        if let Some(cycle) = visit(query.span(query_map), parent) {\n             return Some(cycle);\n         }\n     }\n \n     // Visit the explicit waiters which use condvars and are resumable\n-    for (i, waiter) in query.latch.info.lock().waiters.iter().enumerate() {\n-        if let Some(ref waiter_query) = waiter.query {\n-            if visit(waiter.span, waiter_query.clone()).is_some() {\n-                // Return a value which indicates that this waiter can be resumed\n-                return Some(Some((query.clone(), i)));\n+    if let Some(latch) = query.latch(query_map) {\n+        for (i, waiter) in latch.info.lock().waiters.iter().enumerate() {\n+            if let Some(waiter_query) = waiter.query {\n+                if visit(waiter.span, waiter_query).is_some() {\n+                    // Return a value which indicates that this waiter can be resumed\n+                    return Some(Some((query, i)));\n+                }\n             }\n         }\n     }\n+\n     None\n }\n \n@@ -238,13 +331,14 @@ where\n /// the cycle.\n #[cfg(parallel_compiler)]\n fn cycle_check<'tcx>(\n-    query: Lrc<QueryJob<'tcx>>,\n+    query_map: &QueryMap<'tcx>,\n+    query: QueryJobId,\n     span: Span,\n-    stack: &mut Vec<(Span, Lrc<QueryJob<'tcx>>)>,\n-    visited: &mut FxHashSet<*const QueryJob<'tcx>>,\n-) -> Option<Option<Waiter<'tcx>>> {\n-    if !visited.insert(query.as_ptr()) {\n-        return if let Some(p) = stack.iter().position(|q| q.1.as_ptr() == query.as_ptr()) {\n+    stack: &mut Vec<(Span, QueryJobId)>,\n+    visited: &mut FxHashSet<QueryJobId>,\n+) -> Option<Option<Waiter>> {\n+    if !visited.insert(query) {\n+        return if let Some(p) = stack.iter().position(|q| q.1 == query) {\n             // We detected a query cycle, fix up the initial span and return Some\n \n             // Remove previous stack entries\n@@ -258,10 +352,12 @@ fn cycle_check<'tcx>(\n     }\n \n     // Query marked as visited is added it to the stack\n-    stack.push((span, query.clone()));\n+    stack.push((span, query));\n \n     // Visit all the waiters\n-    let r = visit_waiters(query, |span, successor| cycle_check(successor, span, stack, visited));\n+    let r = visit_waiters(query_map, query, |span, successor| {\n+        cycle_check(query_map, successor, span, stack, visited)\n+    });\n \n     // Remove the entry in our stack if we didn't find a cycle\n     if r.is_none() {\n@@ -276,26 +372,30 @@ fn cycle_check<'tcx>(\n /// This is achieved with a depth first search.\n #[cfg(parallel_compiler)]\n fn connected_to_root<'tcx>(\n-    query: Lrc<QueryJob<'tcx>>,\n-    visited: &mut FxHashSet<*const QueryJob<'tcx>>,\n+    query_map: &QueryMap<'tcx>,\n+    query: QueryJobId,\n+    visited: &mut FxHashSet<QueryJobId>,\n ) -> bool {\n     // We already visited this or we're deliberately ignoring it\n-    if !visited.insert(query.as_ptr()) {\n+    if !visited.insert(query) {\n         return false;\n     }\n \n     // This query is connected to the root (it has no query parent), return true\n-    if query.parent.is_none() {\n+    if query.parent(query_map).is_none() {\n         return true;\n     }\n \n-    visit_waiters(query, |_, successor| connected_to_root(successor, visited).then_some(None))\n-        .is_some()\n+    visit_waiters(query_map, query, |_, successor| {\n+        connected_to_root(query_map, successor, visited).then_some(None)\n+    })\n+    .is_some()\n }\n \n // Deterministically pick an query from a list\n #[cfg(parallel_compiler)]\n-fn pick_query<'a, 'tcx, T, F: Fn(&T) -> (Span, Lrc<QueryJob<'tcx>>)>(\n+fn pick_query<'a, 'tcx, T, F: Fn(&T) -> (Span, QueryJobId)>(\n+    query_map: &QueryMap<'tcx>,\n     tcx: TyCtxt<'tcx>,\n     queries: &'a [T],\n     f: F,\n@@ -308,7 +408,7 @@ fn pick_query<'a, 'tcx, T, F: Fn(&T) -> (Span, Lrc<QueryJob<'tcx>>)>(\n         .min_by_key(|v| {\n             let (span, query) = f(v);\n             let mut stable_hasher = StableHasher::new();\n-            query.info.query.hash_stable(&mut hcx, &mut stable_hasher);\n+            query.query(query_map).hash_stable(&mut hcx, &mut stable_hasher);\n             // Prefer entry points which have valid spans for nicer error messages\n             // We add an integer to the tuple ensuring that entry points\n             // with valid spans are picked first\n@@ -325,14 +425,17 @@ fn pick_query<'a, 'tcx, T, F: Fn(&T) -> (Span, Lrc<QueryJob<'tcx>>)>(\n /// the function returns false.\n #[cfg(parallel_compiler)]\n fn remove_cycle<'tcx>(\n-    jobs: &mut Vec<Lrc<QueryJob<'tcx>>>,\n+    query_map: &QueryMap<'tcx>,\n+    jobs: &mut Vec<QueryJobId>,\n     wakelist: &mut Vec<Lrc<QueryWaiter<'tcx>>>,\n     tcx: TyCtxt<'tcx>,\n ) -> bool {\n     let mut visited = FxHashSet::default();\n     let mut stack = Vec::new();\n     // Look for a cycle starting with the last query in `jobs`\n-    if let Some(waiter) = cycle_check(jobs.pop().unwrap(), DUMMY_SP, &mut stack, &mut visited) {\n+    if let Some(waiter) =\n+        cycle_check(query_map, jobs.pop().unwrap(), DUMMY_SP, &mut stack, &mut visited)\n+    {\n         // The stack is a vector of pairs of spans and queries; reverse it so that\n         // the earlier entries require later entries\n         let (mut spans, queries): (Vec<_>, Vec<_>) = stack.into_iter().rev().unzip();\n@@ -345,27 +448,25 @@ fn remove_cycle<'tcx>(\n \n         // Remove the queries in our cycle from the list of jobs to look at\n         for r in &stack {\n-            if let Some(pos) = jobs.iter().position(|j| j.as_ptr() == r.1.as_ptr()) {\n-                jobs.remove(pos);\n-            }\n+            jobs.remove_item(&r.1);\n         }\n \n         // Find the queries in the cycle which are\n         // connected to queries outside the cycle\n         let entry_points = stack\n             .iter()\n-            .filter_map(|(span, query)| {\n-                if query.parent.is_none() {\n+            .filter_map(|&(span, query)| {\n+                if query.parent(query_map).is_none() {\n                     // This query is connected to the root (it has no query parent)\n-                    Some((*span, query.clone(), None))\n+                    Some((span, query, None))\n                 } else {\n                     let mut waiters = Vec::new();\n                     // Find all the direct waiters who lead to the root\n-                    visit_waiters(query.clone(), |span, waiter| {\n+                    visit_waiters(query_map, query, |span, waiter| {\n                         // Mark all the other queries in the cycle as already visited\n-                        let mut visited = FxHashSet::from_iter(stack.iter().map(|q| q.1.as_ptr()));\n+                        let mut visited = FxHashSet::from_iter(stack.iter().map(|q| q.1));\n \n-                        if connected_to_root(waiter.clone(), &mut visited) {\n+                        if connected_to_root(query_map, waiter, &mut visited) {\n                             waiters.push((span, waiter));\n                         }\n \n@@ -375,31 +476,30 @@ fn remove_cycle<'tcx>(\n                         None\n                     } else {\n                         // Deterministically pick one of the waiters to show to the user\n-                        let waiter = pick_query(tcx, &waiters, |s| s.clone()).clone();\n-                        Some((*span, query.clone(), Some(waiter)))\n+                        let waiter = *pick_query(query_map, tcx, &waiters, |s| *s);\n+                        Some((span, query, Some(waiter)))\n                     }\n                 }\n             })\n-            .collect::<Vec<(Span, Lrc<QueryJob<'tcx>>, Option<(Span, Lrc<QueryJob<'tcx>>)>)>>();\n+            .collect::<Vec<(Span, QueryJobId, Option<(Span, QueryJobId)>)>>();\n \n         // Deterministically pick an entry point\n-        let (_, entry_point, usage) = pick_query(tcx, &entry_points, |e| (e.0, e.1.clone()));\n+        let (_, entry_point, usage) = pick_query(query_map, tcx, &entry_points, |e| (e.0, e.1));\n \n         // Shift the stack so that our entry point is first\n-        let entry_point_pos =\n-            stack.iter().position(|(_, query)| query.as_ptr() == entry_point.as_ptr());\n+        let entry_point_pos = stack.iter().position(|(_, query)| query == entry_point);\n         if let Some(pos) = entry_point_pos {\n             stack.rotate_left(pos);\n         }\n \n-        let usage = usage.as_ref().map(|(span, query)| (*span, query.info.query.clone()));\n+        let usage = usage.as_ref().map(|(span, query)| (*span, query.query(query_map)));\n \n         // Create the cycle error\n         let error = CycleError {\n             usage,\n             cycle: stack\n                 .iter()\n-                .map(|&(s, ref q)| QueryInfo { span: s, query: q.info.query.clone() })\n+                .map(|&(s, ref q)| QueryInfo { span: s, query: q.query(query_map) })\n                 .collect(),\n         };\n \n@@ -408,7 +508,7 @@ fn remove_cycle<'tcx>(\n         let (waitee_query, waiter_idx) = waiter.unwrap();\n \n         // Extract the waiter we want to resume\n-        let waiter = waitee_query.latch.extract_waiter(waiter_idx);\n+        let waiter = waitee_query.latch(query_map).unwrap().extract_waiter(waiter_idx);\n \n         // Set the cycle error so it will be picked up when resumed\n         *waiter.cycle.lock() = Some(error);\n@@ -460,12 +560,13 @@ fn deadlock(tcx: TyCtxt<'_>, registry: &rayon_core::Registry) {\n     });\n \n     let mut wakelist = Vec::new();\n-    let mut jobs: Vec<_> = tcx.queries.collect_active_jobs();\n+    let query_map = tcx.queries.try_collect_active_jobs().unwrap();\n+    let mut jobs: Vec<QueryJobId> = query_map.keys().cloned().collect();\n \n     let mut found_cycle = false;\n \n     while jobs.len() > 0 {\n-        if remove_cycle(&mut jobs, &mut wakelist, tcx) {\n+        if remove_cycle(&query_map, &mut jobs, &mut wakelist, tcx) {\n             found_cycle = true;\n         }\n     }"}, {"sha": "ddaaab412a477ec8fd6e16941f73bba36ab08561", "filename": "src/librustc/ty/query/mod.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc%2Fty%2Fquery%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc%2Fty%2Fquery%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fty%2Fquery%2Fmod.rs?ref=21ed50522ddb998f5367229984a4510af578899f", "patch": "@@ -54,6 +54,7 @@ use rustc_span::symbol::Symbol;\n use rustc_span::{Span, DUMMY_SP};\n use std::any::type_name;\n use std::borrow::Cow;\n+use std::convert::TryFrom;\n use std::ops::Deref;\n use std::sync::Arc;\n use syntax::ast;\n@@ -66,7 +67,8 @@ pub use self::plumbing::{force_from_dep_node, CycleError};\n mod job;\n #[cfg(parallel_compiler)]\n pub use self::job::handle_deadlock;\n-pub use self::job::{QueryInfo, QueryJob};\n+use self::job::QueryJobInfo;\n+pub use self::job::{QueryInfo, QueryJob, QueryJobId};\n \n mod keys;\n use self::keys::Key;"}, {"sha": "8b787915de605e56f4cdb7512ea0a2e1f18d47a3", "filename": "src/librustc/ty/query/plumbing.rs", "status": "modified", "additions": 104, "deletions": 54, "changes": 158, "blob_url": "https://github.com/rust-lang/rust/blob/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc%2Fty%2Fquery%2Fplumbing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc%2Fty%2Fquery%2Fplumbing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fty%2Fquery%2Fplumbing.rs?ref=21ed50522ddb998f5367229984a4510af578899f", "patch": "@@ -4,7 +4,7 @@\n \n use crate::dep_graph::{DepKind, DepNode, DepNodeIndex, SerializedDepNodeIndex};\n use crate::ty::query::config::{QueryConfig, QueryDescription};\n-use crate::ty::query::job::{QueryInfo, QueryJob};\n+use crate::ty::query::job::{QueryInfo, QueryJob, QueryJobId, QueryShardJobId};\n use crate::ty::query::Query;\n use crate::ty::tls;\n use crate::ty::{self, TyCtxt};\n@@ -15,19 +15,24 @@ use rustc_data_structures::fx::{FxHashMap, FxHasher};\n #[cfg(parallel_compiler)]\n use rustc_data_structures::profiling::TimingGuard;\n use rustc_data_structures::sharded::Sharded;\n-use rustc_data_structures::sync::{Lock, Lrc};\n+use rustc_data_structures::sync::Lock;\n use rustc_data_structures::thin_vec::ThinVec;\n use rustc_errors::{struct_span_err, Diagnostic, DiagnosticBuilder, FatalError, Handler, Level};\n use rustc_span::source_map::DUMMY_SP;\n use rustc_span::Span;\n use std::collections::hash_map::Entry;\n use std::hash::{Hash, Hasher};\n use std::mem;\n+use std::num::NonZeroU32;\n use std::ptr;\n \n pub struct QueryCache<'tcx, D: QueryConfig<'tcx> + ?Sized> {\n     pub(super) results: FxHashMap<D::Key, QueryValue<D::Value>>,\n     pub(super) active: FxHashMap<D::Key, QueryResult<'tcx>>,\n+\n+    /// Used to generate unique ids for active jobs.\n+    pub(super) jobs: u32,\n+\n     #[cfg(debug_assertions)]\n     pub(super) cache_hits: usize,\n }\n@@ -46,9 +51,9 @@ impl<T> QueryValue<T> {\n /// Indicates the state of a query for a given key in a query map.\n pub(super) enum QueryResult<'tcx> {\n     /// An already executing query. The query job can be used to await for its completion.\n-    Started(Lrc<QueryJob<'tcx>>),\n+    Started(QueryJob<'tcx>),\n \n-    /// The query panicked. Queries trying to wait on this will raise a fatal error or\n+    /// The query panicked. Queries trying to wait on this will raise a fatal error which will\n     /// silently panic.\n     Poisoned,\n }\n@@ -58,6 +63,7 @@ impl<'tcx, M: QueryConfig<'tcx>> Default for QueryCache<'tcx, M> {\n         QueryCache {\n             results: FxHashMap::default(),\n             active: FxHashMap::default(),\n+            jobs: 0,\n             #[cfg(debug_assertions)]\n             cache_hits: 0,\n         }\n@@ -69,7 +75,7 @@ impl<'tcx, M: QueryConfig<'tcx>> Default for QueryCache<'tcx, M> {\n pub(super) struct JobOwner<'a, 'tcx, Q: QueryDescription<'tcx>> {\n     cache: &'a Sharded<QueryCache<'tcx, Q>>,\n     key: Q::Key,\n-    job: Lrc<QueryJob<'tcx>>,\n+    id: QueryJobId,\n }\n \n impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n@@ -104,7 +110,10 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n             key.hash(&mut state);\n             let key_hash = state.finish();\n \n-            let mut lock = cache.get_shard_by_hash(key_hash).lock();\n+            let shard = cache.get_shard_index_by_hash(key_hash);\n+            let mut lock_guard = cache.get_shard_by_index(shard).lock();\n+            let lock = &mut *lock_guard;\n+\n             if let Some((_, value)) =\n                 lock.results.raw_entry().from_key_hashed_nocheck(key_hash, key)\n             {\n@@ -127,10 +136,10 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n                 return TryGetJob::JobCompleted(result);\n             }\n \n-            let job = match lock.active.entry((*key).clone()) {\n-                Entry::Occupied(entry) => {\n-                    match *entry.get() {\n-                        QueryResult::Started(ref job) => {\n+            let latch = match lock.active.entry((*key).clone()) {\n+                Entry::Occupied(mut entry) => {\n+                    match entry.get_mut() {\n+                        QueryResult::Started(job) => {\n                             // For parallel queries, we'll block and wait until the query running\n                             // in another thread has completed. Record how long we wait in the\n                             // self-profiler.\n@@ -139,39 +148,47 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n                                 query_blocked_prof_timer = Some(tcx.prof.query_blocked());\n                             }\n \n-                            job.clone()\n+                            // Create the id of the job we're waiting for\n+                            let id = QueryJobId::new(job.id, shard, Q::dep_kind());\n+\n+                            job.latch(id)\n                         }\n                         QueryResult::Poisoned => FatalError.raise(),\n                     }\n                 }\n                 Entry::Vacant(entry) => {\n                     // No job entry for this query. Return a new one to be started later.\n-                    return tls::with_related_context(tcx, |icx| {\n-                        // Create the `parent` variable before `info`. This allows LLVM\n-                        // to elide the move of `info`\n-                        let parent = icx.query.clone();\n-                        let info = QueryInfo { span, query: Q::query(key.clone()) };\n-                        let job = Lrc::new(QueryJob::new(info, parent));\n-                        let owner = JobOwner { cache, job: job.clone(), key: (*key).clone() };\n-                        entry.insert(QueryResult::Started(job));\n-                        TryGetJob::NotYetStarted(owner)\n-                    });\n+\n+                    // Generate an id unique within this shard.\n+                    let id = lock.jobs.checked_add(1).unwrap();\n+                    lock.jobs = id;\n+                    let id = QueryShardJobId(NonZeroU32::new(id).unwrap());\n+\n+                    let global_id = QueryJobId::new(id, shard, Q::dep_kind());\n+\n+                    let job =\n+                        tls::with_related_context(tcx, |icx| QueryJob::new(id, span, icx.query));\n+\n+                    entry.insert(QueryResult::Started(job));\n+\n+                    let owner = JobOwner { cache, id: global_id, key: (*key).clone() };\n+                    return TryGetJob::NotYetStarted(owner);\n                 }\n             };\n-            mem::drop(lock);\n+            mem::drop(lock_guard);\n \n             // If we are single-threaded we know that we have cycle error,\n             // so we just return the error.\n             #[cfg(not(parallel_compiler))]\n             return TryGetJob::Cycle(cold_path(|| {\n-                Q::handle_cycle_error(tcx, job.find_cycle_in_stack(tcx, span))\n+                Q::handle_cycle_error(tcx, latch.find_cycle_in_stack(tcx, span))\n             }));\n \n             // With parallel queries we might just have to wait on some other\n             // thread.\n             #[cfg(parallel_compiler)]\n             {\n-                let result = job.r#await(tcx, span);\n+                let result = latch.wait_on(tcx, span);\n \n                 if let Err(cycle) = result {\n                     return TryGetJob::Cycle(Q::handle_cycle_error(tcx, cycle));\n@@ -186,18 +203,21 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n     pub(super) fn complete(self, result: &Q::Value, dep_node_index: DepNodeIndex) {\n         // We can move out of `self` here because we `mem::forget` it below\n         let key = unsafe { ptr::read(&self.key) };\n-        let job = unsafe { ptr::read(&self.job) };\n         let cache = self.cache;\n \n         // Forget ourself so our destructor won't poison the query\n         mem::forget(self);\n \n         let value = QueryValue::new(result.clone(), dep_node_index);\n-        {\n+        let job = {\n             let mut lock = cache.get_shard_by_value(&key).lock();\n-            lock.active.remove(&key);\n+            let job = match lock.active.remove(&key).unwrap() {\n+                QueryResult::Started(job) => job,\n+                QueryResult::Poisoned => panic!(),\n+            };\n             lock.results.insert(key, value);\n-        }\n+            job\n+        };\n \n         job.signal_complete();\n     }\n@@ -219,10 +239,18 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> Drop for JobOwner<'a, 'tcx, Q> {\n     fn drop(&mut self) {\n         // Poison the query so jobs waiting on it panic.\n         let shard = self.cache.get_shard_by_value(&self.key);\n-        shard.lock().active.insert(self.key.clone(), QueryResult::Poisoned);\n+        let job = {\n+            let mut shard = shard.lock();\n+            let job = match shard.active.remove(&self.key).unwrap() {\n+                QueryResult::Started(job) => job,\n+                QueryResult::Poisoned => panic!(),\n+            };\n+            shard.active.insert(self.key.clone(), QueryResult::Poisoned);\n+            job\n+        };\n         // Also signal the completion of the job, so waiters\n         // will continue execution.\n-        self.job.signal_complete();\n+        job.signal_complete();\n     }\n }\n \n@@ -254,7 +282,7 @@ impl<'tcx> TyCtxt<'tcx> {\n     #[inline(always)]\n     pub(super) fn start_query<F, R>(\n         self,\n-        job: Lrc<QueryJob<'tcx>>,\n+        token: QueryJobId,\n         diagnostics: Option<&Lock<ThinVec<Diagnostic>>>,\n         compute: F,\n     ) -> R\n@@ -268,7 +296,7 @@ impl<'tcx> TyCtxt<'tcx> {\n             // Update the `ImplicitCtxt` to point to our new query job.\n             let new_icx = tls::ImplicitCtxt {\n                 tcx: self,\n-                query: Some(job),\n+                query: Some(token),\n                 diagnostics,\n                 layout_depth: current_icx.layout_depth,\n                 task_deps: current_icx.task_deps,\n@@ -335,23 +363,31 @@ impl<'tcx> TyCtxt<'tcx> {\n         // state if it was responsible for triggering the panic.\n         tls::with_context_opt(|icx| {\n             if let Some(icx) = icx {\n-                let mut current_query = icx.query.clone();\n+                let query_map = icx.tcx.queries.try_collect_active_jobs();\n+\n+                let mut current_query = icx.query;\n                 let mut i = 0;\n \n                 while let Some(query) = current_query {\n+                    let query_info =\n+                        if let Some(info) = query_map.as_ref().and_then(|map| map.get(&query)) {\n+                            info\n+                        } else {\n+                            break;\n+                        };\n                     let mut diag = Diagnostic::new(\n                         Level::FailureNote,\n                         &format!(\n                             \"#{} [{}] {}\",\n                             i,\n-                            query.info.query.name(),\n-                            query.info.query.describe(icx.tcx)\n+                            query_info.info.query.name(),\n+                            query_info.info.query.describe(icx.tcx)\n                         ),\n                     );\n-                    diag.span = icx.tcx.sess.source_map().def_span(query.info.span).into();\n+                    diag.span = icx.tcx.sess.source_map().def_span(query_info.info.span).into();\n                     handler.force_print_diagnostic(diag);\n \n-                    current_query = query.parent.clone();\n+                    current_query = query_info.job.parent;\n                     i += 1;\n                 }\n             }\n@@ -384,7 +420,7 @@ impl<'tcx> TyCtxt<'tcx> {\n             let prof_timer = self.prof.query_provider();\n \n             let ((result, dep_node_index), diagnostics) = with_diagnostics(|diagnostics| {\n-                self.start_query(job.job.clone(), diagnostics, |tcx| {\n+                self.start_query(job.id, diagnostics, |tcx| {\n                     tcx.dep_graph.with_anon_task(Q::dep_kind(), || Q::compute(tcx, key))\n                 })\n             });\n@@ -410,7 +446,7 @@ impl<'tcx> TyCtxt<'tcx> {\n             // The diagnostics for this query will be\n             // promoted to the current session during\n             // `try_mark_green()`, so we can ignore them here.\n-            let loaded = self.start_query(job.job.clone(), None, |tcx| {\n+            let loaded = self.start_query(job.id, None, |tcx| {\n                 let marked = tcx.dep_graph.try_mark_green_and_read(tcx, &dep_node);\n                 marked.map(|(prev_dep_node_index, dep_node_index)| {\n                     (\n@@ -544,7 +580,7 @@ impl<'tcx> TyCtxt<'tcx> {\n         let prof_timer = self.prof.query_provider();\n \n         let ((result, dep_node_index), diagnostics) = with_diagnostics(|diagnostics| {\n-            self.start_query(job.job.clone(), diagnostics, |tcx| {\n+            self.start_query(job.id, diagnostics, |tcx| {\n                 if Q::EVAL_ALWAYS {\n                     tcx.dep_graph.with_eval_always_task(\n                         dep_node,\n@@ -716,24 +752,38 @@ macro_rules! define_queries_inner {\n                 }\n             }\n \n-            #[cfg(parallel_compiler)]\n-            pub fn collect_active_jobs(&self) -> Vec<Lrc<QueryJob<$tcx>>> {\n-                let mut jobs = Vec::new();\n+            pub fn try_collect_active_jobs(\n+                &self\n+            ) -> Option<FxHashMap<QueryJobId, QueryJobInfo<'tcx>>> {\n+                let mut jobs = FxHashMap::default();\n \n-                // We use try_lock_shards here since we are only called from the\n-                // deadlock handler, and this shouldn't be locked.\n                 $(\n-                    let shards = self.$name.try_lock_shards().unwrap();\n-                    jobs.extend(shards.iter().flat_map(|shard| shard.active.values().filter_map(|v|\n-                        if let QueryResult::Started(ref job) = *v {\n-                            Some(job.clone())\n-                        } else {\n-                            None\n-                        }\n-                    )));\n+                    // We use try_lock_shards here since we are called from the\n+                    // deadlock handler, and this shouldn't be locked.\n+                    let shards = self.$name.try_lock_shards()?;\n+                    let shards = shards.iter().enumerate();\n+                    jobs.extend(shards.flat_map(|(shard_id, shard)| {\n+                        shard.active.iter().filter_map(move |(k, v)| {\n+                            if let QueryResult::Started(ref job) = *v {\n+                                let id = QueryJobId {\n+                                    job: job.id,\n+                                    shard:  u16::try_from(shard_id).unwrap(),\n+                                    kind:\n+                                        <queries::$name<'tcx> as QueryAccessors<'tcx>>::dep_kind(),\n+                                };\n+                                let info = QueryInfo {\n+                                    span: job.span,\n+                                    query: queries::$name::query(k.clone())\n+                                };\n+                                Some((id, QueryJobInfo { info,  job: job.clone() }))\n+                            } else {\n+                                None\n+                            }\n+                        })\n+                    }));\n                 )*\n \n-                jobs\n+                Some(jobs)\n             }\n \n             pub fn print_stats(&self) {"}, {"sha": "15d1e2dd0b644c08086e60a9c8b82586fda0589d", "filename": "src/librustc_data_structures/sharded.rs", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc_data_structures%2Fsharded.rs", "raw_url": "https://github.com/rust-lang/rust/raw/21ed50522ddb998f5367229984a4510af578899f/src%2Flibrustc_data_structures%2Fsharded.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_data_structures%2Fsharded.rs?ref=21ed50522ddb998f5367229984a4510af578899f", "patch": "@@ -69,12 +69,21 @@ impl<T> Sharded<T> {\n     /// `hash` can be computed with any hasher, so long as that hasher is used\n     /// consistently for each `Sharded` instance.\n     #[inline]\n-    pub fn get_shard_by_hash(&self, hash: u64) -> &Lock<T> {\n+    pub fn get_shard_index_by_hash(&self, hash: u64) -> usize {\n         let hash_len = mem::size_of::<usize>();\n         // Ignore the top 7 bits as hashbrown uses these and get the next SHARD_BITS highest bits.\n         // hashbrown also uses the lowest bits, so we can't use those\n         let bits = (hash >> (hash_len * 8 - 7 - SHARD_BITS)) as usize;\n-        let i = bits % SHARDS;\n+        bits % SHARDS\n+    }\n+\n+    #[inline]\n+    pub fn get_shard_by_hash(&self, hash: u64) -> &Lock<T> {\n+        &self.shards[self.get_shard_index_by_hash(hash)].0\n+    }\n+\n+    #[inline]\n+    pub fn get_shard_by_index(&self, i: usize) -> &Lock<T> {\n         &self.shards[i].0\n     }\n "}]}