{"sha": "076e64475a54bacf7b0f0040e48800bca9b728dd", "node_id": "MDY6Q29tbWl0NzI0NzEyOjA3NmU2NDQ3NWE1NGJhY2Y3YjBmMDA0MGU0ODgwMGJjYTliNzI4ZGQ=", "commit": {"author": {"name": "Felix S. Klock II", "email": "pnkfelix@pnkfx.org", "date": "2015-11-12T19:55:28Z"}, "committer": {"name": "Felix S. Klock II", "email": "pnkfelix@pnkfx.org", "date": "2016-01-07T19:53:33Z"}, "message": "macro_rules: proper FIRST/FOLLOW computations for checking macro_rules validity.\n\nSee RFC amendment 1384 and tracking issue 30450:\n  https://github.com/rust-lang/rfcs/pull/1384\n  https://github.com/rust-lang/rust/issues/30450\n\nMoved old check_matcher code into check_matcher_old\n\ncombined the two checks to enable a warning cycle (where we will\ncontinue to error if the two checks agree to reject, accept if the new\ncheck says accept, and warn if the old check accepts but the new check\nrejects).", "tree": {"sha": "21ec03c2950249145a7862b3e3d15c57f67cb1b5", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/21ec03c2950249145a7862b3e3d15c57f67cb1b5"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/076e64475a54bacf7b0f0040e48800bca9b728dd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/076e64475a54bacf7b0f0040e48800bca9b728dd", "html_url": "https://github.com/rust-lang/rust/commit/076e64475a54bacf7b0f0040e48800bca9b728dd", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/076e64475a54bacf7b0f0040e48800bca9b728dd/comments", "author": {"login": "pnkfelix", "id": 173127, "node_id": "MDQ6VXNlcjE3MzEyNw==", "avatar_url": "https://avatars.githubusercontent.com/u/173127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pnkfelix", "html_url": "https://github.com/pnkfelix", "followers_url": "https://api.github.com/users/pnkfelix/followers", "following_url": "https://api.github.com/users/pnkfelix/following{/other_user}", "gists_url": "https://api.github.com/users/pnkfelix/gists{/gist_id}", "starred_url": "https://api.github.com/users/pnkfelix/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pnkfelix/subscriptions", "organizations_url": "https://api.github.com/users/pnkfelix/orgs", "repos_url": "https://api.github.com/users/pnkfelix/repos", "events_url": "https://api.github.com/users/pnkfelix/events{/privacy}", "received_events_url": "https://api.github.com/users/pnkfelix/received_events", "type": "User", "site_admin": false}, "committer": {"login": "pnkfelix", "id": 173127, "node_id": "MDQ6VXNlcjE3MzEyNw==", "avatar_url": "https://avatars.githubusercontent.com/u/173127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pnkfelix", "html_url": "https://github.com/pnkfelix", "followers_url": "https://api.github.com/users/pnkfelix/followers", "following_url": "https://api.github.com/users/pnkfelix/following{/other_user}", "gists_url": "https://api.github.com/users/pnkfelix/gists{/gist_id}", "starred_url": "https://api.github.com/users/pnkfelix/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pnkfelix/subscriptions", "organizations_url": "https://api.github.com/users/pnkfelix/orgs", "repos_url": "https://api.github.com/users/pnkfelix/repos", "events_url": "https://api.github.com/users/pnkfelix/events{/privacy}", "received_events_url": "https://api.github.com/users/pnkfelix/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "25d1f4bc21a2ab77e12ebcd8c5fb479b563d3bf7", "url": "https://api.github.com/repos/rust-lang/rust/commits/25d1f4bc21a2ab77e12ebcd8c5fb479b563d3bf7", "html_url": "https://github.com/rust-lang/rust/commit/25d1f4bc21a2ab77e12ebcd8c5fb479b563d3bf7"}], "stats": {"total": 516, "additions": 500, "deletions": 16}, "files": [{"sha": "add10612cd8cffb87eada4fbd21e8786e7aa48aa", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 500, "deletions": 16, "changes": 516, "blob_url": "https://github.com/rust-lang/rust/blob/076e64475a54bacf7b0f0040e48800bca9b728dd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/076e64475a54bacf7b0f0040e48800bca9b728dd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=076e64475a54bacf7b0f0040e48800bca9b728dd", "patch": "@@ -25,8 +25,9 @@ use ptr::P;\n use util::small_vector::SmallVector;\n \n use std::cell::RefCell;\n+use std::collections::{HashMap};\n+use std::collections::hash_map::{Entry};\n use std::rc::Rc;\n-use std::iter::once;\n \n struct ParserAnyMacro<'a> {\n     parser: RefCell<Parser<'a>>,\n@@ -320,15 +321,18 @@ pub fn compile<'cx>(cx: &'cx mut ExtCtxt,\n     NormalTT(exp, Some(def.span), def.allow_internal_unstable)\n }\n \n+// why is this here? because of https://github.com/rust-lang/rust/issues/27774\n+fn ref_slice<A>(s: &A) -> &[A] { use std::slice::from_raw_parts; unsafe { from_raw_parts(s, 1) } }\n+\n fn check_lhs_nt_follows(cx: &mut ExtCtxt, lhs: &TokenTree, sp: Span) {\n     // lhs is going to be like TokenTree::Delimited(...), where the\n     // entire lhs is those tts. Or, it can be a \"bare sequence\", not wrapped in parens.\n     match lhs {\n         &TokenTree::Delimited(_, ref tts) => {\n-            check_matcher(cx, tts.tts.iter(), &Eof);\n+            check_matcher(cx, &tts.tts);\n         },\n         tt @ &TokenTree::Sequence(..) => {\n-            check_matcher(cx, Some(tt).into_iter(), &Eof);\n+            check_matcher(cx, ref_slice(tt));\n         },\n         _ => cx.span_err(sp, \"invalid macro matcher; matchers must be contained \\\n                               in balanced delimiters or a repetition indicator\")\n@@ -345,10 +349,59 @@ fn check_rhs(cx: &mut ExtCtxt, rhs: &TokenTree) -> bool {\n     false\n }\n \n-// returns the last token that was checked, for TokenTree::Sequence. this gets used later on.\n-fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n+// Issue 30450: when we are through a warning cycle, we can just error\n+// on all failure conditions and remove this struct and enum.\n+\n+#[derive(Debug)]\n+struct OnFail {\n+    saw_failure: bool,\n+    action: OnFailAction,\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+enum OnFailAction { Warn, Error, DoNothing }\n+\n+impl OnFail {\n+    fn warn() -> OnFail { OnFail { saw_failure: false, action: OnFailAction::Warn } }\n+    fn error() -> OnFail { OnFail { saw_failure: false, action: OnFailAction::Error } }\n+    fn do_nothing() -> OnFail { OnFail { saw_failure: false, action: OnFailAction::DoNothing } }\n+    fn react(&mut self, cx: &mut ExtCtxt, sp: Span, msg: &str) {\n+        match self.action {\n+            OnFailAction::DoNothing => {}\n+            OnFailAction::Error => cx.span_err(sp, msg),\n+            OnFailAction::Warn => {\n+                cx.struct_span_warn(sp, msg)\n+                    .span_note(sp, \"The above warning will be a hard error in the next release.\")\n+                    .emit();\n+            }\n+        };\n+        self.saw_failure = true;\n+    }\n+}\n+\n+fn check_matcher(cx: &mut ExtCtxt, matcher: &[TokenTree]) {\n+    // Issue 30450: when we are through a warning cycle, we can just\n+    // error on all failure conditions (and remove check_matcher_old).\n+\n+    // First run the old-pass, but *only* to find out if it would have failed.\n+    let mut on_fail = OnFail::do_nothing();\n+    check_matcher_old(cx, matcher.iter(), &Eof, &mut on_fail);\n+    // Then run the new pass, but merely warn if the old pass accepts and new pass rejects.\n+    // (Note this silently accepts code if new pass accepts.)\n+    let mut on_fail = if on_fail.saw_failure {\n+        OnFail::error()\n+    } else {\n+        OnFail::warn()\n+    };\n+    check_matcher_new(cx, matcher, &mut on_fail);\n+}\n+\n+// returns the last token that was checked, for TokenTree::Sequence.\n+// return value is used by recursive calls.\n+fn check_matcher_old<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token, on_fail: &mut OnFail)\n -> Option<(Span, Token)> where I: Iterator<Item=&'a TokenTree> {\n     use print::pprust::token_to_string;\n+    use std::iter::once;\n \n     let mut last = None;\n \n@@ -375,7 +428,7 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                             // look at the token that follows the\n                             // sequence, which may itself be a sequence,\n                             // and so on).\n-                            cx.span_err(sp,\n+                            on_fail.react(cx, sp,\n                                         &format!(\"`${0}:{1}` is followed by a \\\n                                                   sequence repetition, which is not \\\n                                                   allowed for `{1}` fragments\",\n@@ -398,13 +451,13 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                     // If T' is in the set FOLLOW(NT), continue. Else, reject.\n                     match (&next_token, is_in_follow(cx, &next_token, &frag_spec.name.as_str())) {\n                         (_, Err(msg)) => {\n-                            cx.span_err(sp, &msg);\n+                            on_fail.react(cx, sp, &msg);\n                             continue\n                         }\n                         (&Eof, _) => return Some((sp, tok.clone())),\n                         (_, Ok(true)) => continue,\n                         (next, Ok(false)) => {\n-                            cx.span_err(sp, &format!(\"`${0}:{1}` is followed by `{2}`, which \\\n+                            on_fail.react(cx, sp, &format!(\"`${0}:{1}` is followed by `{2}`, which \\\n                                                       is not allowed for `{1}` fragments\",\n                                                      name, frag_spec,\n                                                      token_to_string(next)));\n@@ -420,7 +473,7 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                     // run the algorithm on the contents with F set to U. If it\n                     // accepts, continue, else, reject.\n                     Some(ref u) => {\n-                        let last = check_matcher(cx, seq.tts.iter(), u);\n+                        let last = check_matcher_old(cx, seq.tts.iter(), u, on_fail);\n                         match last {\n                             // Since the delimiter isn't required after the last\n                             // repetition, make sure that the *next* token is\n@@ -434,14 +487,14 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                                     Some(&&TokenTree::Delimited(_, ref delim)) =>\n                                         delim.close_token(),\n                                     Some(_) => {\n-                                        cx.span_err(sp, \"sequence repetition followed by \\\n+                                        on_fail.react(cx, sp, \"sequence repetition followed by \\\n                                                 another sequence repetition, which is not allowed\");\n                                         Eof\n                                     },\n                                     None => Eof\n                                 };\n-                                check_matcher(cx, once(&TokenTree::Token(span, tok.clone())),\n-                                              &fol)\n+                                check_matcher_old(cx, once(&TokenTree::Token(span, tok.clone())),\n+                                                  &fol, on_fail)\n                             },\n                             None => last,\n                         }\n@@ -454,13 +507,13 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                             Some(&&TokenTree::Token(_, ref tok)) => tok.clone(),\n                             Some(&&TokenTree::Delimited(_, ref delim)) => delim.close_token(),\n                             Some(_) => {\n-                                cx.span_err(sp, \"sequence repetition followed by another \\\n+                                on_fail.react(cx, sp, \"sequence repetition followed by another \\\n                                              sequence repetition, which is not allowed\");\n                                 Eof\n                             },\n                             None => Eof\n                         };\n-                        check_matcher(cx, seq.tts.iter(), &fol)\n+                        check_matcher_old(cx, seq.tts.iter(), &fol, on_fail)\n                     }\n                 }\n             },\n@@ -471,13 +524,425 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n             TokenTree::Delimited(_, ref tts) => {\n                 // if we don't pass in that close delimiter, we'll incorrectly consider the matcher\n                 // `{ $foo:ty }` as having a follow that isn't `RBrace`\n-                check_matcher(cx, tts.tts.iter(), &tts.close_token())\n+                check_matcher_old(cx, tts.tts.iter(), &tts.close_token(), on_fail)\n             }\n         }\n     }\n     last\n }\n \n+fn check_matcher_new(cx: &mut ExtCtxt, matcher: &[TokenTree], on_fail: &mut OnFail) {\n+    let first_sets = FirstSets::new(matcher);\n+    let empty_suffix = TokenSet::empty();\n+    check_matcher_core(cx, &first_sets, matcher, &empty_suffix, on_fail);\n+}\n+\n+// The FirstSets for a matcher is a mapping from subsequences in the\n+// matcher to the FIRST set for that subsequence.\n+//\n+// This mapping is partially precomputed via a backwards scan over the\n+// token trees of the matcher, which provides a mapping from each\n+// repetition sequence to its FIRST set.\n+//\n+// (Hypothetically sequences should be uniquely identifiable via their\n+// spans, though perhaps that is false e.g. for macro-generated macros\n+// that do not try to inject artificial span information. My plan is\n+// to try to catch such cases ahead of time and not include them in\n+// the precomputed mapping.)\n+struct FirstSets {\n+    // this maps each TokenTree::Sequence `$(tt ...) SEP OP` that is uniquely identified by its\n+    // span in the original matcher to the First set for the inner sequence `tt ...`.\n+    //\n+    // If two sequences have the same span in a matcher, then map that\n+    // span to None (invalidating the mapping here and forcing the code to\n+    // use a slow path).\n+    first: HashMap<Span, Option<TokenSet>>,\n+}\n+\n+impl FirstSets {\n+    fn new(tts: &[TokenTree]) -> FirstSets {\n+        let mut sets = FirstSets { first: HashMap::new() };\n+        build_recur(&mut sets, tts);\n+        return sets;\n+\n+        // walks backward over `tts`, returning the FIRST for `tts`\n+        // and updating `sets` at the same time for all sequence\n+        // substructure we find within `tts`.\n+        fn build_recur(sets: &mut FirstSets, tts: &[TokenTree]) -> TokenSet {\n+            let mut first = TokenSet::empty();\n+            for tt in tts.iter().rev() {\n+                match *tt {\n+                    TokenTree::Token(sp, ref tok) => {\n+                        first.replace_with((sp, tok.clone()));\n+                    }\n+                    TokenTree::Delimited(_, ref delimited) => {\n+                        build_recur(sets, &delimited.tts[..]);\n+                        first.replace_with((delimited.open_span,\n+                                            Token::OpenDelim(delimited.delim)));\n+                    }\n+                    TokenTree::Sequence(sp, ref seq_rep) => {\n+                        let subfirst = build_recur(sets, &seq_rep.tts[..]);\n+\n+                        match sets.first.entry(sp) {\n+                            Entry::Vacant(vac) => {\n+                                vac.insert(Some(subfirst.clone()));\n+                            }\n+                            Entry::Occupied(mut occ) => {\n+                                // if there is already an entry, then a span must have collided.\n+                                // This should not happen with typical macro_rules macros,\n+                                // but syntax extensions need not maintain distinct spans,\n+                                // so distinct syntax trees can be assigned the same span.\n+                                // In such a case, the map cannot be trusted; so mark this\n+                                // entry as unusable.\n+                                occ.insert(None);\n+                            }\n+                        }\n+\n+                        // If the sequence contents can be empty, then the first\n+                        // token could be the separator token itself.\n+\n+                        if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n+                                                        subfirst.maybe_empty) {\n+                            first.add_one_maybe((sp, sep.clone()));\n+                        }\n+\n+                        // Reverse scan: Sequence comes before `first`.\n+                        if subfirst.maybe_empty || seq_rep.op == ast::KleeneOp::ZeroOrMore {\n+                            // If sequence is potentially empty, then\n+                            // union them (preserving first emptiness).\n+                            first.add_all(&TokenSet { maybe_empty: true, ..subfirst });\n+                        } else {\n+                            // Otherwise, sequence guaranteed\n+                            // non-empty; replace first.\n+                            first = subfirst;\n+                        }\n+                    }\n+                }\n+            }\n+\n+            return first;\n+        }\n+    }\n+\n+    // walks forward over `tts` until all potential FIRST tokens are\n+    // identified.\n+    fn first(&self, tts: &[TokenTree]) -> TokenSet {\n+        let mut first = TokenSet::empty();\n+        for tt in tts.iter() {\n+            assert!(first.maybe_empty);\n+            match *tt {\n+                TokenTree::Token(sp, ref tok) => {\n+                    first.add_one((sp, tok.clone()));\n+                    return first;\n+                }\n+                TokenTree::Delimited(_, ref delimited) => {\n+                    first.add_one((delimited.open_span,\n+                                   Token::OpenDelim(delimited.delim)));\n+                    return first;\n+                }\n+                TokenTree::Sequence(sp, ref seq_rep) => {\n+                    match self.first.get(&sp) {\n+                        Some(&Some(ref subfirst)) => {\n+\n+                            // If the sequence contents can be empty, then the first\n+                            // token could be the separator token itself.\n+\n+                            if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n+                                                            subfirst.maybe_empty) {\n+                                first.add_one_maybe((sp, sep.clone()));\n+                            }\n+\n+                            assert!(first.maybe_empty);\n+                            first.add_all(subfirst);\n+                            if subfirst.maybe_empty || seq_rep.op == ast::KleeneOp::ZeroOrMore {\n+                                // continue scanning for more first\n+                                // tokens, but also make sure we\n+                                // restore empty-tracking state\n+                                first.maybe_empty = true;\n+                                continue;\n+                            } else {\n+                                return first;\n+                            }\n+                        }\n+\n+                        Some(&None) => {\n+                            panic!(\"assume all sequences have (unique) spans for now\");\n+                        }\n+\n+                        None => {\n+                            panic!(\"We missed a sequence during FirstSets construction\");\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        // we only exit the loop if `tts` was empty or if every\n+        // element of `tts` matches the empty sequence.\n+        assert!(first.maybe_empty);\n+        return first;\n+    }\n+}\n+\n+// A set of Tokens, which may include MatchNt tokens (for\n+// macro-by-example syntactic variables). It also carries the\n+// `maybe_empty` flag; that is true if and only if the matcher can\n+// match an empty token sequence.\n+//\n+// The First set is computed on submatchers like `$($a:expr b),* $(c)* d`,\n+// which has corresponding FIRST = {$a:expr, c, d}.\n+// Likewise, `$($a:expr b),* $(c)+ d` has FIRST = {$a:expr, c}.\n+//\n+// (Notably, we must allow for *-op to occur zero times.)\n+#[derive(Clone, Debug)]\n+struct TokenSet {\n+    tokens: Vec<(Span, Token)>,\n+    maybe_empty: bool,\n+}\n+\n+impl TokenSet {\n+    // Returns a set for the empty sequence.\n+    fn empty() -> Self { TokenSet { tokens: Vec::new(), maybe_empty: true } }\n+\n+    // Returns the set `{ tok }` for the single-token (and thus\n+    // non-empty) sequence [tok].\n+    fn singleton(tok: (Span, Token)) -> Self {\n+        TokenSet { tokens: vec![tok], maybe_empty: false }\n+    }\n+\n+    // Changes self to be the set `{ tok }`.\n+    // Since `tok` is always present, marks self as non-empty.\n+    fn replace_with(&mut self, tok: (Span, Token)) {\n+        self.tokens.clear();\n+        self.tokens.push(tok);\n+        self.maybe_empty = false;\n+    }\n+\n+    // Changes self to be the empty set `{}`; meant for use when\n+    // the particular token does not matter, but we want to\n+    // record that it occurs.\n+    fn replace_with_irrelevant(&mut self) {\n+        self.tokens.clear();\n+        self.maybe_empty = false;\n+    }\n+\n+    // Adds `tok` to the set for `self`, marking sequence as non-empy.\n+    fn add_one(&mut self, tok: (Span, Token)) {\n+        if !self.tokens.contains(&tok) {\n+            self.tokens.push(tok);\n+        }\n+        self.maybe_empty = false;\n+    }\n+\n+    // Adds `tok` to the set for `self`. (Leaves `maybe_empty` flag alone.)\n+    fn add_one_maybe(&mut self, tok: (Span, Token)) {\n+        if !self.tokens.contains(&tok) {\n+            self.tokens.push(tok);\n+        }\n+    }\n+\n+    // Adds all elements of `other` to this.\n+    //\n+    // (Since this is a set, we filter out duplicates.)\n+    //\n+    // If `other` is potentially empty, then preserves the previous\n+    // setting of the empty flag of `self`. If `other` is guaranteed\n+    // non-empty, then `self` is marked non-empty.\n+    fn add_all(&mut self, other: &Self) {\n+        for tok in &other.tokens {\n+            if !self.tokens.contains(tok) {\n+                self.tokens.push(tok.clone());\n+            }\n+        }\n+        if !other.maybe_empty {\n+            self.maybe_empty = false;\n+        }\n+    }\n+}\n+\n+// Checks that `matcher` is internally consistent and that it\n+// can legally by followed by a token N, for all N in `follow`.\n+// (If `follow` is empty, then it imposes no constraint on\n+// the `matcher`.)\n+//\n+// Returns the set of NT tokens that could possibly come last in\n+// `matcher`. (If `matcher` matches the empty sequence, then\n+// `maybe_empty` will be set to true.)\n+//\n+// Requires that `first_sets` is pre-computed for `matcher`;\n+// see `FirstSets::new`.\n+fn check_matcher_core(cx: &mut ExtCtxt,\n+                      first_sets: &FirstSets,\n+                      matcher: &[TokenTree],\n+                      follow: &TokenSet,\n+                      on_fail: &mut OnFail) -> TokenSet {\n+    use print::pprust::token_to_string;\n+\n+    let mut last = TokenSet::empty();\n+\n+    // 2. For each token and suffix  [T, SUFFIX] in M:\n+    // ensure that T can be followed by SUFFIX, and if SUFFIX may be empty,\n+    // then ensure T can also be followed by any element of FOLLOW.\n+    'each_token: for i in 0..matcher.len() {\n+        let token = &matcher[i];\n+        let suffix = &matcher[i+1..];\n+\n+        let build_suffix_first = || {\n+            let mut s = first_sets.first(suffix);\n+            if s.maybe_empty { s.add_all(follow); }\n+            return s;\n+        };\n+\n+        // (we build `suffix_first` on demand below; you can tell\n+        // which cases are supposed to fall through by looking for the\n+        // initialization of this variable.)\n+        let suffix_first;\n+\n+        // First, update `last` so that it corresponds to the set\n+        // of NT tokens that might end the sequence `... token`.\n+        match *token {\n+            TokenTree::Token(sp, ref tok) => {\n+                let can_be_followed_by_any;\n+                if let Err(bad_frag) = has_legal_fragment_specifier(tok) {\n+                    on_fail.react(cx, sp, &format!(\"invalid fragment specifier `{}`\", bad_frag));\n+                    // (This eliminates false positives and duplicates\n+                    // from error messages.)\n+                    can_be_followed_by_any = true;\n+                } else {\n+                    can_be_followed_by_any = token_can_be_followed_by_any(tok);\n+                }\n+\n+                if can_be_followed_by_any {\n+                    // don't need to track tokens that work with any,\n+                    last.replace_with_irrelevant();\n+                    // ... and don't need to check tokens that can be\n+                    // followed by anything against SUFFIX.\n+                    continue 'each_token;\n+                } else {\n+                    last.replace_with((sp, tok.clone()));\n+                    suffix_first = build_suffix_first();\n+                }\n+            }\n+            TokenTree::Delimited(_, ref d) => {\n+                let my_suffix = TokenSet::singleton((d.close_span, Token::CloseDelim(d.delim)));\n+                check_matcher_core(cx, first_sets, &d.tts, &my_suffix, on_fail);\n+                // don't track non NT tokens\n+                last.replace_with_irrelevant();\n+\n+                // also, we don't need to check delimited sequences\n+                // against SUFFIX\n+                continue 'each_token;\n+            }\n+            TokenTree::Sequence(sp, ref seq_rep) => {\n+                suffix_first = build_suffix_first();\n+                // The trick here: when we check the interior, we want\n+                // to include the separator (if any) as a potential\n+                // (but not guaranteed) element of FOLLOW. So in that\n+                // case, we make a temp copy of suffix and stuff\n+                // delimiter in there.\n+                //\n+                // FIXME: Should I first scan suffix_first to see if\n+                // delimiter is already in it before I go through the\n+                // work of cloning it? But then again, this way I may\n+                // get a \"tighter\" span?\n+                let mut new;\n+                let my_suffix = if let Some(ref u) = seq_rep.separator {\n+                    new = suffix_first.clone();\n+                    new.add_one_maybe((sp, u.clone()));\n+                    &new\n+                } else {\n+                    &suffix_first\n+                };\n+\n+                // At this point, `suffix_first` is built, and\n+                // `my_suffix` is some TokenSet that we can use\n+                // for checking the interior of `seq_rep`.\n+                let next = check_matcher_core(cx, first_sets, &seq_rep.tts, my_suffix, on_fail);\n+                if next.maybe_empty {\n+                    last.add_all(&next);\n+                } else {\n+                    last = next;\n+                }\n+\n+                // the recursive call to check_matcher_core already ran the 'each_last\n+                // check below, so we can just keep going forward here.\n+                continue 'each_token;\n+            }\n+        }\n+\n+        // (`suffix_first` guaranteed initialized once reaching here.)\n+\n+        // Now `last` holds the complete set of NT tokens that could\n+        // end the sequence before SUFFIX. Check that every one works with `suffix`.\n+        'each_last: for &(_sp, ref t) in &last.tokens {\n+            if let MatchNt(ref name, ref frag_spec, _, _) = *t {\n+                for &(sp, ref next_token) in &suffix_first.tokens {\n+                    match is_in_follow(cx, next_token, &frag_spec.name.as_str()) {\n+                        Err(msg) => {\n+                            on_fail.react(cx, sp, &msg);\n+                            // don't bother reporting every source of\n+                            // conflict for a particular element of `last`.\n+                            continue 'each_last;\n+                        }\n+                        Ok(true) => {}\n+                        Ok(false) => {\n+                            let may_be = if last.tokens.len() == 1 &&\n+                                suffix_first.tokens.len() == 1\n+                            {\n+                                \"is\"\n+                            } else {\n+                                \"may be\"\n+                            };\n+\n+                            on_fail.react(\n+                                cx, sp,\n+                                &format!(\"`${name}:{frag}` {may_be} followed by `{next}`, which \\\n+                                          is not allowed for `{frag}` fragments\",\n+                                         name=name,\n+                                         frag=frag_spec,\n+                                         next=token_to_string(next_token),\n+                                         may_be=may_be));\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+    last\n+}\n+\n+\n+fn token_can_be_followed_by_any(tok: &Token) -> bool {\n+    if let &MatchNt(_, ref frag_spec, _, _) = tok {\n+        frag_can_be_followed_by_any(&frag_spec.name.as_str())\n+    } else {\n+        // (Non NT's can always be followed by anthing in matchers.)\n+        true\n+    }\n+}\n+\n+/// True if a fragment of type `frag` can be followed by any sort of\n+/// token.  We use this (among other things) as a useful approximation\n+/// for when `frag` can be followed by a repetition like `$(...)*` or\n+/// `$(...)+`. In general, these can be a bit tricky to reason about,\n+/// so we adopt a conservative position that says that any fragment\n+/// specifier which consumes at most one token tree can be followed by\n+/// a fragment specifier (indeed, these fragments can be followed by\n+/// ANYTHING without fear of future compatibility hazards).\n+fn frag_can_be_followed_by_any(frag: &str) -> bool {\n+    match frag {\n+        \"item\" |  // always terminated by `}` or `;`\n+        \"block\" | // exactly one token tree\n+        \"ident\" | // exactly one token tree\n+        \"meta\" |  // exactly one token tree\n+        \"tt\" =>    // exactly one token tree\n+            true,\n+\n+        _ =>\n+            false,\n+    }\n+}\n+\n /// True if a fragment of type `frag` can be followed by any sort of\n /// token.  We use this (among other things) as a useful approximation\n /// for when `frag` can be followed by a repetition like `$(...)*` or\n@@ -501,7 +966,7 @@ fn can_be_followed_by_any(frag: &str) -> bool {\n }\n \n /// True if `frag` can legally be followed by the token `tok`. For\n-/// fragments that can consume an unbounded numbe of tokens, `tok`\n+/// fragments that can consume an unbounded number of tokens, `tok`\n /// must be within a well-defined follow set. This is intended to\n /// guarantee future compatibility: for example, without this rule, if\n /// we expanded `expr` to include a new binary operator, we might\n@@ -557,3 +1022,22 @@ fn is_in_follow(_: &ExtCtxt, tok: &Token, frag: &str) -> Result<bool, String> {\n         }\n     }\n }\n+\n+fn has_legal_fragment_specifier(tok: &Token) -> Result<(), String> {\n+    debug!(\"has_legal_fragment_specifier({:?})\", tok);\n+    if let &MatchNt(_, ref frag_spec, _, _) = tok {\n+        let s = &frag_spec.name.as_str();\n+        if !is_legal_fragment_specifier(s) {\n+            return Err(s.to_string());\n+        }\n+    }\n+    Ok(())\n+}\n+\n+fn is_legal_fragment_specifier(frag: &str) -> bool {\n+    match frag {\n+        \"item\" | \"block\" | \"stmt\" | \"expr\" | \"pat\" |\n+        \"path\" | \"ty\" | \"ident\" | \"meta\" | \"tt\" => true,\n+        _ => false,\n+    }\n+}"}]}