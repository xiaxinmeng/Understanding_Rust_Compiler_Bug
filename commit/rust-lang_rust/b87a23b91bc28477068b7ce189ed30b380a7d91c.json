{"sha": "b87a23b91bc28477068b7ce189ed30b380a7d91c", "node_id": "C_kwDOAAsO6NoAKGI4N2EyM2I5MWJjMjg0NzcwNjhiN2NlMTg5ZWQzMGIzODBhN2Q5MWM", "commit": {"author": {"name": "Ryo Yoshida", "email": "low.ryoshida@gmail.com", "date": "2022-11-02T14:26:29Z"}, "committer": {"name": "Ryo Yoshida", "email": "low.ryoshida@gmail.com", "date": "2022-11-05T10:41:08Z"}, "message": "Rename convertor -> converter", "tree": {"sha": "a4249bd53482738e4d3021de81071b2c8578012d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/a4249bd53482738e4d3021de81071b2c8578012d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b87a23b91bc28477068b7ce189ed30b380a7d91c", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCgAdFiEEkSbsQIURluxz4rzf4laYqTBYYXEFAmNmPfUACgkQ4laYqTBY\nYXFcGQ//ZjVOl4XFRVo/qVXgDx66CJyQekGwA3bMFndLJmknjcIejLhCu1l/pLIe\nid8T7x12HVwLmK/T1884wRQkGsCib9px8fOMu8YkcDXW8xFJ1ix2g/vB+U0+o7KQ\nXX67oOc454aAvBrV6PQvbCJY3Ishc2GklvuK2YHuOS5lX3J08tZOBrXgCYWLsy8s\n3xDH/Weg9fuFdpq0pkgydQiDcRUnbNhM8TngQ8PO7ppxK02cpS8tfEhYsSHMP8jN\nJxYQ4AbjUoJtg2ndwmjLkAF1Hw8sBBD+YRH1kWNcTUdehFq9OxIoUpkYW4SpZAHt\nDcLhyQml+mcZB54Nq8zzMjOVwNKgXR+j9GAQD9lwRYQRa+nX3+zw2RhHJhg8xeYO\nwp6Gz1a+LHOHSJRX4lUc0RskPnJrAbI/y5CD5DtvEqKf0+JG60ZxZ5rhFH5fJYww\nZzQ0R/AsHciYqNbQcCz+L/cVF4vUZv2LHklq9mCVJyoY/WJzksDwvF6sYryVWJOS\nfKgrtJ+A8Gukjm7J2OnnTIG8RnxOQTrMtB7kO/5jQ1l7Ntaac0ZYELBPu8EaM9ER\ntgRs11d33/4pbEGgPYRij1tILe063RL47nNjxUiHcVyuK2Gg3WlJwgLDEcMFy1Is\ndYTSzBGlhf/oPVmGFxe6Sxk5hN19/btRaL8gUHD0+3mRt11YRVk=\n=pKdS\n-----END PGP SIGNATURE-----", "payload": "tree a4249bd53482738e4d3021de81071b2c8578012d\nparent 66900a7e05313242d997a61e896d4f7ff1c8ead0\nauthor Ryo Yoshida <low.ryoshida@gmail.com> 1667399189 +0900\ncommitter Ryo Yoshida <low.ryoshida@gmail.com> 1667644868 +0900\n\nRename convertor -> converter\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b87a23b91bc28477068b7ce189ed30b380a7d91c", "html_url": "https://github.com/rust-lang/rust/commit/b87a23b91bc28477068b7ce189ed30b380a7d91c", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b87a23b91bc28477068b7ce189ed30b380a7d91c/comments", "author": {"login": "lowr", "id": 24381114, "node_id": "MDQ6VXNlcjI0MzgxMTE0", "avatar_url": "https://avatars.githubusercontent.com/u/24381114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lowr", "html_url": "https://github.com/lowr", "followers_url": "https://api.github.com/users/lowr/followers", "following_url": "https://api.github.com/users/lowr/following{/other_user}", "gists_url": "https://api.github.com/users/lowr/gists{/gist_id}", "starred_url": "https://api.github.com/users/lowr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lowr/subscriptions", "organizations_url": "https://api.github.com/users/lowr/orgs", "repos_url": "https://api.github.com/users/lowr/repos", "events_url": "https://api.github.com/users/lowr/events{/privacy}", "received_events_url": "https://api.github.com/users/lowr/received_events", "type": "User", "site_admin": false}, "committer": {"login": "lowr", "id": 24381114, "node_id": "MDQ6VXNlcjI0MzgxMTE0", "avatar_url": "https://avatars.githubusercontent.com/u/24381114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lowr", "html_url": "https://github.com/lowr", "followers_url": "https://api.github.com/users/lowr/followers", "following_url": "https://api.github.com/users/lowr/following{/other_user}", "gists_url": "https://api.github.com/users/lowr/gists{/gist_id}", "starred_url": "https://api.github.com/users/lowr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lowr/subscriptions", "organizations_url": "https://api.github.com/users/lowr/orgs", "repos_url": "https://api.github.com/users/lowr/repos", "events_url": "https://api.github.com/users/lowr/events{/privacy}", "received_events_url": "https://api.github.com/users/lowr/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "66900a7e05313242d997a61e896d4f7ff1c8ead0", "url": "https://api.github.com/repos/rust-lang/rust/commits/66900a7e05313242d997a61e896d4f7ff1c8ead0", "html_url": "https://github.com/rust-lang/rust/commit/66900a7e05313242d997a61e896d4f7ff1c8ead0"}], "stats": {"total": 44, "additions": 22, "deletions": 22}, "files": [{"sha": "1df3a6f56f07ae97b9d8f6ddedc606d0ac36783e", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 22, "deletions": 22, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/b87a23b91bc28477068b7ce189ed30b380a7d91c/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b87a23b91bc28477068b7ce189ed30b380a7d91c/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=b87a23b91bc28477068b7ce189ed30b380a7d91c", "patch": "@@ -35,7 +35,7 @@ pub fn syntax_node_to_token_tree_with_modifications(\n     append: FxHashMap<SyntaxElement, Vec<SyntheticToken>>,\n ) -> (tt::Subtree, TokenMap, u32) {\n     let global_offset = node.text_range().start();\n-    let mut c = Convertor::new(node, global_offset, existing_token_map, next_id, replace, append);\n+    let mut c = Converter::new(node, global_offset, existing_token_map, next_id, replace, append);\n     let subtree = convert_tokens(&mut c);\n     c.id_alloc.map.shrink_to_fit();\n     always!(c.replace.is_empty(), \"replace: {:?}\", c.replace);\n@@ -100,7 +100,7 @@ pub fn parse_to_token_tree(text: &str) -> Option<(tt::Subtree, TokenMap)> {\n         return None;\n     }\n \n-    let mut conv = RawConvertor {\n+    let mut conv = RawConverter {\n         lexed,\n         pos: 0,\n         id_alloc: TokenIdAlloc {\n@@ -148,7 +148,7 @@ pub fn parse_exprs_with_sep(tt: &tt::Subtree, sep: char) -> Vec<tt::Subtree> {\n     res\n }\n \n-fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n+fn convert_tokens<C: TokenConverter>(conv: &mut C) -> tt::Subtree {\n     struct StackEntry {\n         subtree: tt::Subtree,\n         idx: usize,\n@@ -425,8 +425,8 @@ impl TokenIdAlloc {\n     }\n }\n \n-/// A raw token (straight from lexer) convertor\n-struct RawConvertor<'a> {\n+/// A raw token (straight from lexer) converter\n+struct RawConverter<'a> {\n     lexed: parser::LexedStr<'a>,\n     pos: usize,\n     id_alloc: TokenIdAlloc,\n@@ -442,7 +442,7 @@ trait SrcToken<Ctx>: std::fmt::Debug {\n     fn synthetic_id(&self, ctx: &Ctx) -> Option<SyntheticTokenId>;\n }\n \n-trait TokenConvertor: Sized {\n+trait TokenConverter: Sized {\n     type Token: SrcToken<Self>;\n \n     fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>>;\n@@ -454,25 +454,25 @@ trait TokenConvertor: Sized {\n     fn id_alloc(&mut self) -> &mut TokenIdAlloc;\n }\n \n-impl<'a> SrcToken<RawConvertor<'a>> for usize {\n-    fn kind(&self, ctx: &RawConvertor<'a>) -> SyntaxKind {\n+impl<'a> SrcToken<RawConverter<'a>> for usize {\n+    fn kind(&self, ctx: &RawConverter<'a>) -> SyntaxKind {\n         ctx.lexed.kind(*self)\n     }\n \n-    fn to_char(&self, ctx: &RawConvertor<'a>) -> Option<char> {\n+    fn to_char(&self, ctx: &RawConverter<'a>) -> Option<char> {\n         ctx.lexed.text(*self).chars().next()\n     }\n \n-    fn to_text(&self, ctx: &RawConvertor<'_>) -> SmolStr {\n+    fn to_text(&self, ctx: &RawConverter<'_>) -> SmolStr {\n         ctx.lexed.text(*self).into()\n     }\n \n-    fn synthetic_id(&self, _ctx: &RawConvertor<'a>) -> Option<SyntheticTokenId> {\n+    fn synthetic_id(&self, _ctx: &RawConverter<'a>) -> Option<SyntheticTokenId> {\n         None\n     }\n }\n \n-impl<'a> TokenConvertor for RawConvertor<'a> {\n+impl<'a> TokenConverter for RawConverter<'a> {\n     type Token = usize;\n \n     fn convert_doc_comment(&self, &token: &usize) -> Option<Vec<tt::TokenTree>> {\n@@ -504,7 +504,7 @@ impl<'a> TokenConvertor for RawConvertor<'a> {\n     }\n }\n \n-struct Convertor {\n+struct Converter {\n     id_alloc: TokenIdAlloc,\n     current: Option<SyntaxToken>,\n     current_synthetic: Vec<SyntheticToken>,\n@@ -515,19 +515,19 @@ struct Convertor {\n     punct_offset: Option<(SyntaxToken, TextSize)>,\n }\n \n-impl Convertor {\n+impl Converter {\n     fn new(\n         node: &SyntaxNode,\n         global_offset: TextSize,\n         existing_token_map: TokenMap,\n         next_id: u32,\n         mut replace: FxHashMap<SyntaxElement, Vec<SyntheticToken>>,\n         mut append: FxHashMap<SyntaxElement, Vec<SyntheticToken>>,\n-    ) -> Convertor {\n+    ) -> Converter {\n         let range = node.text_range();\n         let mut preorder = node.preorder_with_tokens();\n         let (first, synthetic) = Self::next_token(&mut preorder, &mut replace, &mut append);\n-        Convertor {\n+        Converter {\n             id_alloc: { TokenIdAlloc { map: existing_token_map, global_offset, next_id } },\n             current: first,\n             current_synthetic: synthetic,\n@@ -590,39 +590,39 @@ impl SynToken {\n     }\n }\n \n-impl SrcToken<Convertor> for SynToken {\n-    fn kind(&self, _ctx: &Convertor) -> SyntaxKind {\n+impl SrcToken<Converter> for SynToken {\n+    fn kind(&self, _ctx: &Converter) -> SyntaxKind {\n         match self {\n             SynToken::Ordinary(token) => token.kind(),\n             SynToken::Punch(token, _) => token.kind(),\n             SynToken::Synthetic(token) => token.kind,\n         }\n     }\n-    fn to_char(&self, _ctx: &Convertor) -> Option<char> {\n+    fn to_char(&self, _ctx: &Converter) -> Option<char> {\n         match self {\n             SynToken::Ordinary(_) => None,\n             SynToken::Punch(it, i) => it.text().chars().nth((*i).into()),\n             SynToken::Synthetic(token) if token.text.len() == 1 => token.text.chars().next(),\n             SynToken::Synthetic(_) => None,\n         }\n     }\n-    fn to_text(&self, _ctx: &Convertor) -> SmolStr {\n+    fn to_text(&self, _ctx: &Converter) -> SmolStr {\n         match self {\n             SynToken::Ordinary(token) => token.text().into(),\n             SynToken::Punch(token, _) => token.text().into(),\n             SynToken::Synthetic(token) => token.text.clone(),\n         }\n     }\n \n-    fn synthetic_id(&self, _ctx: &Convertor) -> Option<SyntheticTokenId> {\n+    fn synthetic_id(&self, _ctx: &Converter) -> Option<SyntheticTokenId> {\n         match self {\n             SynToken::Synthetic(token) => Some(token.id),\n             _ => None,\n         }\n     }\n }\n \n-impl TokenConvertor for Convertor {\n+impl TokenConverter for Converter {\n     type Token = SynToken;\n     fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n         convert_doc_comment(token.token()?)"}]}